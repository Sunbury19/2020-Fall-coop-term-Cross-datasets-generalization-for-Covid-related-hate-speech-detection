{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack Comments Classification\n",
    "2020.10.03\n",
    "\n",
    "Modify on the data label\n",
    "\n",
    "Student: Xuanyu Su                                                                 \n",
    "Supervisor: Isar Nejadgholi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3OBmC3UmYaRP"
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aVCytM2-YWWR"
   },
   "outputs": [],
   "source": [
    "source_folder = 'Data'\n",
    "destination_folder = 'Model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "comments = pd.read_csv('attack_annotated_comments.tsv',sep='\\t',encoding = \"ISO-8859-1\")\n",
    "annotation = pd.read_csv('attack_annotations.tsv',sep='\\t',encoding = \"ISO-8859-1\")\n",
    "worker = pd.read_csv('attack_worker_demographics.tsv',sep='\\t',encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>quoting_attack</th>\n",
       "      <th>recipient_attack</th>\n",
       "      <th>third_party_attack</th>\n",
       "      <th>other_attack</th>\n",
       "      <th>attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37675</td>\n",
       "      <td>1362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37675</td>\n",
       "      <td>2408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37675</td>\n",
       "      <td>1493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37675</td>\n",
       "      <td>1439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37675</td>\n",
       "      <td>170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rev_id  worker_id  quoting_attack  recipient_attack  third_party_attack  \\\n",
       "0   37675       1362             0.0               0.0                 0.0   \n",
       "1   37675       2408             0.0               0.0                 0.0   \n",
       "2   37675       1493             0.0               0.0                 0.0   \n",
       "3   37675       1439             0.0               0.0                 0.0   \n",
       "4   37675        170             0.0               0.0                 0.0   \n",
       "\n",
       "   other_attack  attack  \n",
       "0           0.0     0.0  \n",
       "1           0.0     0.0  \n",
       "2           0.0     0.0  \n",
       "3           0.0     0.0  \n",
       "4           0.0     0.0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37675</td>\n",
       "      <td>`-NEWLINE_TOKENThis is not ``creative``.  Thos...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44816</td>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKEN:: the term ``stand...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49851</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENTrue or false, the s...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89320</td>\n",
       "      <td>Next, maybe you could work on being less cond...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93890</td>\n",
       "      <td>This page will need disambiguation.</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rev_id                                            comment  year  logged_in  \\\n",
       "0   37675  `-NEWLINE_TOKENThis is not ``creative``.  Thos...  2002      False   \n",
       "1   44816  `NEWLINE_TOKENNEWLINE_TOKEN:: the term ``stand...  2002      False   \n",
       "2   49851  NEWLINE_TOKENNEWLINE_TOKENTrue or false, the s...  2002      False   \n",
       "3   89320   Next, maybe you could work on being less cond...  2002       True   \n",
       "4   93890               This page will need disambiguation.   2002       True   \n",
       "\n",
       "        ns  sample  split  \n",
       "0  article  random  train  \n",
       "1  article  random  train  \n",
       "2  article  random  train  \n",
       "3  article  random    dev  \n",
       "4  article  random  train  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worker_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>english_first_language</th>\n",
       "      <th>age_group</th>\n",
       "      <th>education</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>833</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>45-60</td>\n",
       "      <td>bachelors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1072</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>30-45</td>\n",
       "      <td>bachelors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>872</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>18-30</td>\n",
       "      <td>hs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2116</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>30-45</td>\n",
       "      <td>professional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>453</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>30-45</td>\n",
       "      <td>hs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   worker_id  gender  english_first_language age_group     education\n",
       "0        833  female                       0     45-60     bachelors\n",
       "1       1072    male                       0     30-45     bachelors\n",
       "2        872    male                       0     18-30            hs\n",
       "3       2116    male                       0     30-45  professional\n",
       "4        453    male                       0     30-45            hs"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worker.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# temp = 0\n",
    "# for i in tqdm(range(len(annotation))):\n",
    "#     temp2 = 0\n",
    "#     temp2 = annotation['quoting_attack'][i] + annotation['recipient_attack'][i] + annotation['other_attack'][i] + annotation['third_party_attack'][i] + annotation['attack'][i]\n",
    "#     if temp2 > 0:\n",
    "#         temp+=1\n",
    "# print('ratio of attack',temp/len(annotation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(comments, annotation, how='left', on=['rev_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>quoting_attack</th>\n",
       "      <th>recipient_attack</th>\n",
       "      <th>third_party_attack</th>\n",
       "      <th>other_attack</th>\n",
       "      <th>attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37675</td>\n",
       "      <td>`-NEWLINE_TOKENThis is not ``creative``.  Thos...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>1362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37675</td>\n",
       "      <td>`-NEWLINE_TOKENThis is not ``creative``.  Thos...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>2408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37675</td>\n",
       "      <td>`-NEWLINE_TOKENThis is not ``creative``.  Thos...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>1493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37675</td>\n",
       "      <td>`-NEWLINE_TOKENThis is not ``creative``.  Thos...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>1439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37675</td>\n",
       "      <td>`-NEWLINE_TOKENThis is not ``creative``.  Thos...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rev_id                                            comment  year  logged_in  \\\n",
       "0   37675  `-NEWLINE_TOKENThis is not ``creative``.  Thos...  2002      False   \n",
       "1   37675  `-NEWLINE_TOKENThis is not ``creative``.  Thos...  2002      False   \n",
       "2   37675  `-NEWLINE_TOKENThis is not ``creative``.  Thos...  2002      False   \n",
       "3   37675  `-NEWLINE_TOKENThis is not ``creative``.  Thos...  2002      False   \n",
       "4   37675  `-NEWLINE_TOKENThis is not ``creative``.  Thos...  2002      False   \n",
       "\n",
       "        ns  sample  split  worker_id  quoting_attack  recipient_attack  \\\n",
       "0  article  random  train       1362             0.0               0.0   \n",
       "1  article  random  train       2408             0.0               0.0   \n",
       "2  article  random  train       1493             0.0               0.0   \n",
       "3  article  random  train       1439             0.0               0.0   \n",
       "4  article  random  train        170             0.0               0.0   \n",
       "\n",
       "   third_party_attack  other_attack  attack  \n",
       "0                 0.0           0.0     0.0  \n",
       "1                 0.0           0.0     0.0  \n",
       "2                 0.0           0.0     0.0  \n",
       "3                 0.0           0.0     0.0  \n",
       "4                 0.0           0.0     0.0  "
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label = result.groupby('rev_id')['attack'].mean() > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115864"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115864"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "len(np.unique(result['rev_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.drop_duplicates(subset=['rev_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = []\n",
    "for i in new_label:\n",
    "    if i is True:\n",
    "        label_list.append(1.0)\n",
    "    else:\n",
    "        label_list.append(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['attack'] = label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>quoting_attack</th>\n",
       "      <th>recipient_attack</th>\n",
       "      <th>third_party_attack</th>\n",
       "      <th>other_attack</th>\n",
       "      <th>attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37675</td>\n",
       "      <td>`-NEWLINE_TOKENThis is not ``creative``.  Thos...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>1362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>44816</td>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKEN:: the term ``stand...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>1158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>49851</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENTrue or false, the s...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>89320</td>\n",
       "      <td>Next, maybe you could work on being less cond...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>dev</td>\n",
       "      <td>3307</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>93890</td>\n",
       "      <td>This page will need disambiguation.</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>1369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rev_id                                            comment  year  \\\n",
       "0    37675  `-NEWLINE_TOKENThis is not ``creative``.  Thos...  2002   \n",
       "10   44816  `NEWLINE_TOKENNEWLINE_TOKEN:: the term ``stand...  2002   \n",
       "19   49851  NEWLINE_TOKENNEWLINE_TOKENTrue or false, the s...  2002   \n",
       "29   89320   Next, maybe you could work on being less cond...  2002   \n",
       "38   93890               This page will need disambiguation.   2002   \n",
       "\n",
       "    logged_in       ns  sample  split  worker_id  quoting_attack  \\\n",
       "0       False  article  random  train       1362             0.0   \n",
       "10      False  article  random  train       1158             0.0   \n",
       "19      False  article  random  train        447             0.0   \n",
       "29       True  article  random    dev       3307             0.0   \n",
       "38       True  article  random  train       1369             0.0   \n",
       "\n",
       "    recipient_attack  third_party_attack  other_attack  attack  \n",
       "0                0.0                 0.0           0.0     0.0  \n",
       "10               0.0                 0.0           0.0     0.0  \n",
       "19               0.0                 0.0           0.0     0.0  \n",
       "29               0.0                 0.0           0.0     0.0  \n",
       "38               0.0                 0.0           0.0     0.0  "
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`-NEWLINE_TOKENThis is not ``creative``.  Those are the dictionary definitions of the terms ``insurance`` and ``ensurance`` as properly applied to ``destruction``.  If you don't understand that, fine, legitimate criticism, I'll write up ``three man cell`` and ``bounty hunter`` and then it will be easy to understand why ``ensured`` and ``insured`` are different - and why both differ from ``assured``.NEWLINE_TOKENNEWLINE_TOKENThe sentence you quote is absolutely neutral.  You just aren't familiar with the underlying theory of strike-back (e.g. submarines as employed in nuclear warfare) guiding the insurance, nor likely the three man cell structure that kept the IRA from being broken by the British.  If that's my fault, fine, I can fix that to explain.  But ther'es nothing ``personal`` or ``creative`` about it.NEWLINE_TOKENNEWLINE_TOKENI'm tired of arguing with you.  Re: the other article, ``multi-party`` turns up plenty, and there is more use of ``mutually`` than ``mutual``.  If I were to apply your standard I'd be moving ``Mutual Assured Destruction`` to ``talk`` for not appealing to a Reagan voter's biases about its effectiveness, and for dropping the ``ly``.NEWLINE_TOKENNEWLINE_TOKENThere is a double standard in your edits.  If it comes from some US history book, like ``peace movement`` or 'M.A.D.' as defined in 1950, you like it, even if the definition is totally useless in 2002 and only of historical interest.  NEWLINE_TOKENNEWLINE_TOKENIf it makes any even-obvious connection or implication from the language chosen in multiple profession-specific terms, you consider it somehow non-neutral...  Gandhi thinks ``eye for an eye`` describes riots, death penalty, and war all at once, but you don't.  What do you know that Gandhi doesn't?NEWLINE_TOKENNEWLINE_TOKENGuess what:  reality is not neutral.  Current use of terms is slightly more controversial.  Neutrality requires negotiation, and some willingness to learn.NEWLINE_TOKENNEWLINE_TOKENThis is your problem not mine.  You may dislike the writing, fine, that can be fixed.  But disregarding fundamental axioms of philosphy with names that recur in multiple phrases, or failing to make critical distinctions like 'insurance' versus 'assurance' versus 'ensurance' (which are made in one quote by an Air Force general in an in-context quote), is just a disservice to the reader.NEWLINE_TOKENNEWLINE_TOKENIf someone comes here to research a topic like MAD, they want some context, beyond history.NEWLINE_TOKENNEWLINE_TOKENIf this is a history book, fine, it's a history book.  But that wasn't what it was claimed to be...NEWLINE_TOKEN`\n",
      "   \n",
      "`NEWLINE_TOKENNEWLINE_TOKEN:: the term ``standard model`` is itself less NPOV than I think we'd prefer...NEWLINE_TOKENNEWLINE_TOKEN:: if it's ``new-age speak`` then a lot of old-age people speak it - Karl Popper, the Pope, etc.  here's Karl Popper's view of this.NEWLINE_TOKENNEWLINE_TOKEN:: The clearest title for this article would be ``particle physics cosmology`` - but as I say that would require broader treatment of issues like the Anthropic Principle, cognitive bias beyond the particle physics zoo, etc.NEWLINE_TOKENNEWLINE_TOKEN:: as to accelerators, it's clear that while they are in use, someone is still looking for particles.  So this is not yet a settled ``cosmology`` so certain that we abandon the search... nor is it an arbitrary foundation ontology as you suggest, not subject to question.`\n",
      "   \n",
      "NEWLINE_TOKENNEWLINE_TOKENTrue or false, the situation as of March 2002 was such: NEWLINE_TOKENA Saudi proposal of Land for Peace AND recognition by ALL arab countries was made. The day the proposal was to be made formal by the Arab League was the day the Israeli's under the command of Ariel Sharon began the invasion of the Palestinian self-rule areas. user:Arab.\n",
      "   \n",
      " Next, maybe you could work on being less condescending with your suggestions about reading the naming conventions and FDL, both of which I read quite a while ago, thanks. I really liked the bit where you were explaining why you had no interest in fixing things I complained about because you felt insulted, yet you were being extremely insulting at the time. With any luck, you can learn to be less of a jerk. GregLindahlNEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKENNEWLINE_TOKEN \n",
      "   \n"
     ]
    }
   ],
   "source": [
    "for i in result['comment'][:4]:\n",
    "    print(i)\n",
    "    print('   ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "result['comment'] = result['comment'].apply(lambda x:re.sub(r'[^A-Za-z0-9 ]+', ' ', x).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['comment'] = result['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "result['comment'] = result['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 27/27 [00:01<00:00, 19.71it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "symbol_list = ['!','@','#','$','%','^','&','*','(',')','-','+','?','>','<','=','/',\"`\",'\"','.',':',';','  ','   ','    ','      ','      ']\n",
    "for i in tqdm(symbol_list):\n",
    "    result['comment'] = result['comment'].apply(lambda x: x.replace(i, ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>quoting_attack</th>\n",
       "      <th>recipient_attack</th>\n",
       "      <th>third_party_attack</th>\n",
       "      <th>other_attack</th>\n",
       "      <th>attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1365169</th>\n",
       "      <td>699848324</td>\n",
       "      <td>newline tokennewline tokennewline tokenthese ...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1365178</th>\n",
       "      <td>699851288</td>\n",
       "      <td>newline tokennewline tokenthe institute for hi...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>test</td>\n",
       "      <td>337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1365188</th>\n",
       "      <td>699857133</td>\n",
       "      <td>newline token the way you re trying to describ...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1365197</th>\n",
       "      <td>699891012</td>\n",
       "      <td>newline tokennewline token warning newline tok...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>2593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1365207</th>\n",
       "      <td>699897151</td>\n",
       "      <td>alternate option newline tokenis there perhaps...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rev_id                                            comment  year  \\\n",
       "1365169  699848324   newline tokennewline tokennewline tokenthese ...  2016   \n",
       "1365178  699851288  newline tokennewline tokenthe institute for hi...  2016   \n",
       "1365188  699857133  newline token the way you re trying to describ...  2016   \n",
       "1365197  699891012  newline tokennewline token warning newline tok...  2016   \n",
       "1365207  699897151  alternate option newline tokenis there perhaps...  2016   \n",
       "\n",
       "         logged_in       ns   sample  split  worker_id  quoting_attack  \\\n",
       "1365169       True  article  blocked  train       1842             0.0   \n",
       "1365178       True  article  blocked   test        337             0.0   \n",
       "1365188       True  article  blocked  train        331             0.0   \n",
       "1365197       True     user  blocked    dev       2593             0.0   \n",
       "1365207       True  article  blocked  train        151             0.0   \n",
       "\n",
       "         recipient_attack  third_party_attack  other_attack  attack  \n",
       "1365169               0.0                 0.0           0.0     0.0  \n",
       "1365178               0.0                 0.0           0.0     0.0  \n",
       "1365188               0.0                 0.0           0.0     0.0  \n",
       "1365197               0.0                 0.0           0.0     0.0  \n",
       "1365207               0.0                 0.0           0.0     0.0  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "749      iraq is not good  newline tokennewline token ...\n",
       "2680    newline tokennewline token newline tokenfuck o...\n",
       "4574           i have a dick its bigger than yours hahaha\n",
       "6299    newline tokennewline token renault newline tok...\n",
       "6309    newline tokennewline token renault newline tok...\n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[result['attack'] == 1]['comment'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = result.drop(columns = ['rev_id','year','logged_in','ns','sample','worker_id','quoting_attack','recipient_attack','third_party_attack','other_attack'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a141934dc0>"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in result2['comment']]\n",
    "pd.Series(seq_len).hist(bins = 50,range=[0,500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_comment = result2[result2['split']=='train']\n",
    "dec_comment = result2[result2['split']=='dev']\n",
    "test_comment = result2[result2['split']=='test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train set is 69526\n",
      "length of dev set is 23160\n",
      "length of test set is 23178\n"
     ]
    }
   ],
   "source": [
    "print('length of train set is',len(Train_comment))\n",
    "print('length of dev set is',len(dec_comment))\n",
    "print('length of test set is',len(test_comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_comment = Train_comment.drop(columns = ['split'])\n",
    "dec_comment = dec_comment.drop(columns = ['split'])\n",
    "test_comment = test_comment.drop(columns = ['split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>newline tokennewline token if i may butt in i ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>newline tokennewline tokennewline tokenon my y...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>newline tokennewline tokennewline tokennewline...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>see i was right newline tokennewline token</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>newline tokennewline tokennewline tokennewlin...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               comment  attack\n",
       "125  newline tokennewline token if i may butt in i ...     0.0\n",
       "134  newline tokennewline tokennewline tokenon my y...     0.0\n",
       "219  newline tokennewline tokennewline tokennewline...     0.0\n",
       "265         see i was right newline tokennewline token     0.0\n",
       "377   newline tokennewline tokennewline tokennewlin...     0.0"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_comment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>newline tokenthis is not creative  those are ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>newline tokennewline token the term standard ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>newline tokennewline tokentrue or false the si...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>this page will need disambiguation</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>newline token newline tokennewline tokenimport...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              comment  attack\n",
       "0    newline tokenthis is not creative  those are ...     0.0\n",
       "10   newline tokennewline token the term standard ...     0.0\n",
       "19  newline tokennewline tokentrue or false the si...     0.0\n",
       "38                this page will need disambiguation      0.0\n",
       "47  newline token newline tokennewline tokenimport...     0.0"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_comment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    0.883799\n",
       "1.0    0.116201\n",
       "Name: attack, dtype: float64"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check class distribution\n",
    "Train_comment['attack'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_comment.to_csv('Data/Train_comment.csv',index=False,header=True)\n",
    "dec_comment.to_csv('Data/Val_comment.csv',index=False,header=True)\n",
    "test_comment.to_csv('Data/Test_comment.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 69526 entries, 0 to 1365207\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   comment  69526 non-null  object \n",
      " 1   attack   69526 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "Train_comment.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n",
      "3.1.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EyZ65LZ8YgtG"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dl7rqQ4uZPiv"
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "# Preliminaries\n",
    "\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n",
    "\n",
    "# Models\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Training\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RDBAx2Q-ZMl7",
    "outputId": "e8a2eac3-3e34-48bf-8df6-6fe870fbbd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nqOJAYCiYlZs"
   },
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fF1DCVrCh6_d"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "fdGccqrAZYjw",
    "outputId": "40c9d8a8-0c04-4759-e05e-bbd78f145d2b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (726 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (930 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (797 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1104 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (663 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (923 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (952 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (587 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (582 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1608 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (614 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (836 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (957 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (875 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (560 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (830 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1004 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (955 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1494 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (968 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (924 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (624 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (617 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (901 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (926 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (701 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (838 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (926 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (853 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (891 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1938 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (934 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (609 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (911 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (712 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1878 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (730 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (541 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (635 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (613 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (927 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (561 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (885 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1232 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2289 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2196 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1347 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1140 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (772 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (530 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1234 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (573 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (759 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1041 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2112 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2154 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (516 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1628 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (891 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (932 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (577 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (544 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (945 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (960 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (878 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2059 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (835 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (586 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (897 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (945 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (942 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (944 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (593 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1307 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (915 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (682 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1304 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1199 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1212 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1011 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1313 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (995 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1022 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1933 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1084 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (944 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (938 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1178 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (970 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (777 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (783 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (922 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (529 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (573 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (889 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1031 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (516 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (686 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (988 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (632 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5672 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (727 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1020 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1133 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1923 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1296 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1019 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (533 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (968 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (579 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (643 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (588 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (834 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1946 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (623 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (896 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1066 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (889 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (742 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (699 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (721 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1231 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1755 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1053 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (715 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3334 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (954 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (600 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (645 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (539 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (691 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1762 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (586 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (627 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (960 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1179 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (598 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1443 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1465 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1875 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1445 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1706 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (839 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2128 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1627 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (596 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (886 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1141 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (529 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1527 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (580 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (804 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (760 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2956 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (690 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (539 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3267 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2131 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (623 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (809 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (710 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (953 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (521 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (793 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (610 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (682 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (599 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (958 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2700 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (591 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1087 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2823 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (929 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (570 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (772 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (778 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2849 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1267 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (598 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (512 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1005 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1546 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (628 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1005 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (554 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (563 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2143 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (723 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1049 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1095 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (646 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (879 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2083 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (975 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (512 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1664 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (937 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1328 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (678 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (754 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1842 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1030 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1056 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (895 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (932 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (715 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (871 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (676 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (615 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (824 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (971 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2858 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (561 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1004 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (568 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (709 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1029 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (568 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (551 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1736 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (633 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (813 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (818 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (926 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (816 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (698 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (550 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (861 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (828 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1674 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1668 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (571 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (809 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (572 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2309 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1385 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2104 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (769 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (897 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2206 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2204 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2213 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (592 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (764 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (511 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (866 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (887 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2729 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (858 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1230 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (531 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (551 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2705 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (801 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1034 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2261 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1086 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1085 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (803 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1420 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1630 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1167 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (995 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (524 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1043 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (805 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (933 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1277 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (813 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (723 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1210 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1137 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (555 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (654 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (841 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (616 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (855 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1186 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (910 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (660 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (680 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (522 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1281 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1031 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (911 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (648 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (951 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1613 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (609 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1010 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (745 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1164 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (883 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (662 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (596 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (750 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (784 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (758 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1606 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (900 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (608 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1199 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (601 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (605 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1271 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1222 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1068 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (660 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (673 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (828 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (694 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (744 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (744 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (958 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (967 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (712 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (809 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1091 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (809 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1016 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2098 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (544 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (949 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1942 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (642 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (765 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (948 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (673 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1144 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1647 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (674 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (958 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1536 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1164 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (747 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1091 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (928 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (698 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (718 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (917 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (562 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (850 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (717 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1250 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (815 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2113 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (658 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (835 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (826 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (530 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (672 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1384 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1609 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1056 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1062 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1847 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (604 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3206 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (676 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (640 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (616 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (743 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1026 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (511 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1550 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (557 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (675 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (727 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (890 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (675 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (967 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1338 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2496 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3914 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4412 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (836 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (931 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (993 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (760 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (906 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1109 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (547 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (685 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (637 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (916 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1619 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1073 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (624 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (721 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (769 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (961 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (602 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (671 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (643 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1329 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (775 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1534 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (905 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (675 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (668 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1873 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (926 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1961 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (714 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (713 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1587 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (786 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1091 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1059 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (619 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (576 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (744 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1048 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (616 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (816 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (570 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (864 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2089 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1938 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (904 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (674 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1517 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (891 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1158 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1893 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (846 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (760 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1001 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (613 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (619 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (940 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (963 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1332 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2361 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1589 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (997 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (949 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (880 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (996 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (604 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1122 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2574 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1638 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (550 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (648 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1083 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (821 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (587 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1887 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (852 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (621 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (964 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1014 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (555 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (921 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (577 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1691 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (529 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (585 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (605 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1203 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (966 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (707 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (981 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (745 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (913 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (775 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (755 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (879 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (888 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (756 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1033 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (982 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (929 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (958 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (973 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (803 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (924 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (905 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (800 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (957 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (540 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (768 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (986 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (862 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (573 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (630 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (971 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (715 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1007 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (753 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (851 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (874 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (681 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1112 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1087 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1143 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (713 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (620 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (594 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (560 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1419 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1500 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (697 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (762 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (762 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (741 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1250 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (586 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1889 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (731 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2081 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2215 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (803 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (956 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1034 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2076 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1398 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (761 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (869 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2667 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1430 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (675 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (607 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (567 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (984 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1544 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (526 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (674 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1571 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (758 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4962 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (570 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1742 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (577 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (555 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (871 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1633 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2505 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (872 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (855 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1065 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3750 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1607 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2090 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2103 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (869 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (714 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (518 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (552 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (981 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (906 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (756 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (592 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1001 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1711 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1547 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (984 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (993 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (746 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (651 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (561 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1074 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (758 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (659 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1574 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (864 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (840 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (685 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (593 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (552 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (697 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (861 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1994 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (869 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1885 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1589 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (648 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (580 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (526 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (548 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1874 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1586 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (834 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1013 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (656 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3372 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3155 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1995 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2123 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1989 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1335 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (610 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (904 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (708 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1378 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1680 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (945 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1791 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1870 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (553 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1107 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (562 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5229 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (903 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (567 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (597 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (896 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (810 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (583 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (910 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (582 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (687 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1276 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1441 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (550 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (871 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1588 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (935 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (634 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1225 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (767 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1201 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (646 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (610 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (905 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2072 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2863 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (609 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2340 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2503 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2508 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1665 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1996 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (724 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1954 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (787 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (752 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3750 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1056 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2055 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1579 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (662 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1829 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1024 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (774 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (614 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1163 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1165 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (587 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (606 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (683 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1908 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1693 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (768 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (637 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (674 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1985 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1666 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (870 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (989 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1828 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (557 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (669 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1631 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (577 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (752 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (629 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (742 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (886 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (851 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (885 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (631 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (703 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (893 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (586 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1796 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (670 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (775 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3999 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (602 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2572 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1042 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (884 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (849 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (761 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (784 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (938 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (928 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1325 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1097 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1623 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (800 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1434 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1725 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1434 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1103 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (931 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (623 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1132 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (625 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (693 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2128 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (545 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (545 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (810 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (839 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (746 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (559 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1528 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1124 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1149 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (903 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (561 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1996 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1065 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (637 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3460 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1366 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (874 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (834 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1077 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1070 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (714 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (867 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1422 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1044 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1956 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (766 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (817 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (977 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3169 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (635 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (552 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1345 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (640 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (640 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (569 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (604 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1368 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (825 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (953 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (794 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (524 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1425 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1082 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1164 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1191 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1054 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (893 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1746 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2605 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (677 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (713 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (579 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (553 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1280 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (968 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (700 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (582 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (932 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (828 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (827 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (672 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2000 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3125 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (550 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (582 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (839 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3205 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2598 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (727 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1043 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (620 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (658 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1008 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (746 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (965 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (844 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (642 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (737 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (639 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (826 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2238 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1441 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (531 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1006 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (666 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (596 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (746 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1822 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1014 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1440 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (667 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (900 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (980 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (631 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (876 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (778 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (830 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1749 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (824 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2094 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (543 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (590 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (554 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (670 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2307 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (993 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (734 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (940 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (943 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2850 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (682 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (965 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (948 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1835 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1731 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (645 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1086 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (934 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (619 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (563 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (884 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1394 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (593 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1087 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1086 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (795 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (802 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2170 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (701 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (819 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (681 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (576 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (967 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (563 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (672 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (596 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (938 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (620 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (604 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (560 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (821 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (667 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1005 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (855 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (778 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2223 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (708 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1887 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (826 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (667 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (688 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (847 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (551 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (849 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (721 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (598 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1356 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2732 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (936 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (992 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (781 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (749 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (810 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (701 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (792 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1816 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1698 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (593 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1141 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1322 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (859 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (612 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (632 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (685 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (576 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (901 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3334 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2778 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2505 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1388 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2308 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1153 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1950 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (641 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (992 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1666 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1223 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1295 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (610 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (591 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (642 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (854 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (957 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (951 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (634 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (674 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (711 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (586 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (548 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1980 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1968 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1964 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1964 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1141 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (570 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (712 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (655 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1071 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1763 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (766 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (979 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1239 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (599 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1029 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (590 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (700 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (874 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (839 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2277 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (705 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (726 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (777 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (586 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (729 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1975 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (603 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (602 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (690 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (706 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1457 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (715 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1402 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1344 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (770 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (652 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1832 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1550 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1000 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1177 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1105 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1140 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (974 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (738 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (669 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (775 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (691 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4286 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (602 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (553 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (826 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (687 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (619 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (765 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1102 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (823 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (633 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1003 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (688 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (512 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (531 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (541 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (896 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2504 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4489 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (687 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (559 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (784 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (925 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (719 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (721 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (533 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (899 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (996 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (526 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (994 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (912 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (567 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (722 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (687 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (963 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (548 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (562 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3333 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3182 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (828 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1557 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1336 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (580 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (560 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1369 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (914 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (939 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (638 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1235 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (894 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (522 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (689 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1635 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (590 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1351 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (652 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (556 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (682 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (560 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (830 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (942 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1284 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (556 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (722 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (577 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (727 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1456 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (640 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1666 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (840 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (839 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (990 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1469 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1853 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (919 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (648 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (808 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (665 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (949 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (703 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (746 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1526 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1276 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1021 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (557 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (621 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1161 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1620 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (543 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (942 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (934 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1970 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (821 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (983 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (736 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (866 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (988 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1372 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (727 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (704 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1011 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2034 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (578 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (907 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2351 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (933 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (607 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (569 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (967 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (930 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (521 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (893 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (541 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (686 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (586 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3595 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3595 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (943 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (602 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (835 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1575 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (550 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (522 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (586 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (627 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (530 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (553 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (933 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1813 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (999 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (631 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (530 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (761 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1533 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (762 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1088 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (512 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2015 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (650 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (969 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (547 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2060 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (886 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (739 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1035 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1022 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (829 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (636 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (697 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (779 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (634 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (572 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (723 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (786 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (804 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1467 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (627 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (717 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (708 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (668 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (837 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (896 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (880 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (697 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1708 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (680 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (600 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (522 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (797 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2601 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2353 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1999 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (601 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (965 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (652 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (615 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (870 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (559 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (647 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (624 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1612 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (747 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (530 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (607 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (877 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (522 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (822 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2001 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (579 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (717 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (787 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1995 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (563 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1055 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (719 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1001 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (515 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (570 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1294 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (632 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (633 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (699 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (834 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (540 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (860 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (697 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1443 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (545 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (579 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1876 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1883 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (785 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1365 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (555 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (683 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (773 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (530 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (701 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (600 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (557 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (557 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (854 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (660 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1503 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (675 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1425 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (691 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (674 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2071 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (795 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (563 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (871 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (883 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (862 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (813 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (817 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1278 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (981 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (710 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (806 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (628 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1372 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (588 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2094 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1478 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (547 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (662 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (599 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1088 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (592 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (691 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1119 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (883 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1572 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (643 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1476 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (641 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (854 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1606 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1289 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (660 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1403 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1245 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (869 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (840 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (556 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (602 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (895 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (719 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (988 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (619 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (617 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (581 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1184 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (688 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1086 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (786 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (709 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (917 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (874 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (672 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (580 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (777 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (592 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1082 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (526 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (567 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (582 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (612 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (585 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (966 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1240 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (698 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1000 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (648 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (610 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (792 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (590 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (833 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (628 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (888 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (993 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1778 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1202 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2129 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (738 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (524 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (568 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (712 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (672 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (748 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (672 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (562 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (729 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (623 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (619 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (887 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (709 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (744 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1032 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (918 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2190 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1624 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1278 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1290 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (714 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (797 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1032 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (706 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (990 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (963 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1141 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1910 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1956 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (839 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1032 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (837 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (568 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (579 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1278 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (590 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (901 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (884 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (726 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (634 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1061 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (643 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (670 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (941 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (556 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (591 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (600 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (595 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (598 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (981 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2547 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (897 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (925 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (590 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (521 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1442 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2358 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1788 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1038 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (579 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (668 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (755 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (546 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (712 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (646 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (952 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1625 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1730 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3498 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (699 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (760 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (542 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (801 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (801 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (745 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (982 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (616 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (728 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (548 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2000 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3380 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (552 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (582 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (705 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (595 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1230 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1006 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1384 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (551 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (550 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (561 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (610 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (871 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (689 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (587 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (561 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1154 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (590 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1041 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (807 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1312 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (850 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (721 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (668 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (807 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1124 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1004 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (562 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (545 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (598 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (869 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1205 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (969 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (541 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (925 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (772 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (582 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (554 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1028 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (662 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (726 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (836 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (860 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (579 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (897 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1384 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (878 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (877 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (840 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (744 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (662 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (711 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1307 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (970 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (992 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (541 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (767 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1874 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1525 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1022 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (573 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (572 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (637 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (753 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (725 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (860 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1264 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1246 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (740 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1021 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (533 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (983 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (992 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (624 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1012 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (743 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (577 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1056 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (690 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1029 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (516 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (976 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (911 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1057 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1862 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (876 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2204 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1607 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (937 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (932 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1448 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (839 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1926 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (552 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (661 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (958 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2228 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2018 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2340 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (695 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (980 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (606 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1018 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (974 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (786 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (945 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1284 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1283 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1084 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (936 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (792 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (943 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (736 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (950 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (635 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (974 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (681 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (631 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (840 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (946 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1403 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1479 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1664 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (518 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (567 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1043 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (512 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (994 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (587 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (594 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (674 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1659 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (745 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1136 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1133 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (718 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2005 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2007 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (962 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1279 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2059 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1354 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (882 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (596 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1097 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1149 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (976 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (870 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (931 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1387 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (615 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (910 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (548 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (670 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1159 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (965 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2398 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (511 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2390 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1063 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1488 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (839 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (995 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (555 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (808 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (751 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (833 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (904 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (676 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (863 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (691 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (612 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1738 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (910 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1507 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (921 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (668 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1250 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1428 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1251 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (980 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (613 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (637 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (891 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (934 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (990 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (778 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (867 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (606 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (752 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1282 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (747 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (563 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (724 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (637 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (985 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (673 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (595 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (864 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (529 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (675 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (560 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (565 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1640 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (636 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (550 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (578 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1557 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (854 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (619 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (544 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (656 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (758 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1135 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (675 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (753 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (668 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1100 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1768 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (699 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (616 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (526 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (747 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (598 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1242 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (747 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (675 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (515 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (606 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (893 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (731 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (563 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (742 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (799 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1521 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (995 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (648 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (722 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1154 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (540 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (817 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (763 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1641 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (739 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (787 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1675 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (516 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (986 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1218 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2091 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (548 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (813 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (547 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (734 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1201 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (524 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (950 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (569 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1296 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (838 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1598 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (630 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1498 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (766 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (943 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (928 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (545 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (884 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (921 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (516 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (844 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1144 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1877 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1422 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1110 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (550 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (850 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1481 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (745 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1260 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (772 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (619 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1791 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (681 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1755 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1759 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (533 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (666 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (608 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (542 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (617 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (943 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2000 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (813 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (613 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (521 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (905 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (684 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (670 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (903 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (802 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (608 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1405 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1593 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (752 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (668 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (798 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2131 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1570 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (993 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (512 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (588 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (689 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (798 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (863 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (729 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (673 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1084 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3870 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (773 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2174 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1012 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2267 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (649 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (901 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (876 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1097 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2858 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (634 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (951 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1280 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (864 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (935 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (623 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (567 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (678 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (810 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (727 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (913 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1205 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (692 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (516 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (793 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (709 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (829 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (921 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (805 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (580 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (670 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (690 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (661 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (803 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2124 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2259 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1440 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1985 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1668 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (778 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1307 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (840 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (633 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (617 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (960 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (805 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1055 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1317 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1062 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1272 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3997 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (678 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (621 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1601 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (775 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (609 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (662 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (576 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (512 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (953 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (580 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (757 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (608 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1008 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (855 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (561 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (719 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1054 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1119 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1715 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (643 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (714 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (700 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (626 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (642 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (539 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (624 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (550 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (602 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (897 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3593 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2800 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2500 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (797 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (933 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (852 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1552 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (617 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (561 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1123 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (648 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (798 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (624 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (878 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1376 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (706 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1151 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (871 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (521 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (844 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (710 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (650 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (533 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (582 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (642 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (666 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (711 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (511 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (627 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (868 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (745 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (596 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (607 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (972 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (530 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2071 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (655 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (803 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (622 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (634 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (892 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1458 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (821 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (803 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (609 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2175 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1504 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1433 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (544 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (594 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (630 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (732 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (601 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1660 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1207 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (759 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (798 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (900 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1486 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1114 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (697 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1587 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (707 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1310 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (850 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (959 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (999 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (999 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1250 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1486 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1332 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (978 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (814 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1638 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1030 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (563 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (793 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (626 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1665 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (608 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (522 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (802 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (772 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (652 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1123 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (692 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1369 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (734 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (715 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (820 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (743 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (817 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (587 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (937 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1131 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1032 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1306 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (649 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (921 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (591 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (686 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (789 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (617 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (521 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1149 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1602 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1499 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (803 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1012 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1643 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1091 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (801 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (778 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (656 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1034 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (800 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (875 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (515 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (701 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1148 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (629 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (898 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (661 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1713 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Model parameter\n",
    "MAX_SEQ_LEN = 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
    "\n",
    "# Fields\n",
    "\n",
    "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
    "text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n",
    "                   fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\n",
    "fields = [('comment', text_field),('attack', label_field)]\n",
    "\n",
    "# TabularDataset\n",
    "\n",
    "train, valid, test = TabularDataset.splits(path=source_folder, train='Train_comment.csv', validation='Val_comment.csv',\n",
    "                                           test='test6.csv', format='CSV', fields=fields, skip_header=True)\n",
    "\n",
    "\n",
    "train_iter = BucketIterator(train, batch_size=16, sort_key=lambda x: len(x.comment),\n",
    "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
    "valid_iter = BucketIterator(valid, batch_size=16, sort_key=lambda x: len(x.comment),\n",
    "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
    "test_iter = Iterator(test, batch_size=16, device=device, train=False, shuffle=False, sort=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IX-lWIMaYnsA"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2RkcXCHSph1_"
   },
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "\n",
    "        options_name = \"bert-base-uncased\"\n",
    "        self.encoder = BertForSequenceClassification.from_pretrained(options_name)\n",
    "\n",
    "    def forward(self, text, label):\n",
    "        loss, text_fea = self.encoder(text, labels=label)[:2]\n",
    "        \n",
    "\n",
    "        return loss, text_fea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z81slSELYqO1"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JRpTJUGhklDv"
   },
   "outputs": [],
   "source": [
    "# Save and Load Functions\n",
    "\n",
    "def save_checkpoint(save_path, model, valid_loss):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'valid_loss': valid_loss}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "def load_checkpoint(load_path, model):\n",
    "    \n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    return state_dict['valid_loss']\n",
    "\n",
    "\n",
    "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'train_loss_list': train_loss_list,\n",
    "                  'valid_loss_list': valid_loss_list,\n",
    "                  'global_steps_list': global_steps_list}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_metrics(load_path):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "81w1lahhkozO"
   },
   "outputs": [],
   "source": [
    "# Training Function\n",
    "\n",
    "def train(model,\n",
    "          optimizer,\n",
    "          criterion = nn.BCELoss(),\n",
    "          train_loader = train_iter,\n",
    "          valid_loader = valid_iter,\n",
    "          num_epochs = 5,\n",
    "          eval_every = len(train_iter) // 2,\n",
    "          file_path = destination_folder,\n",
    "          best_valid_loss = float(\"Inf\")):\n",
    "    \n",
    "    # initialize running values\n",
    "    running_loss = 0.0\n",
    "    valid_running_loss = 0.0\n",
    "    global_step = 0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    global_steps_list = []\n",
    "\n",
    "    # training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        batch_no = 1\n",
    "        for (comment, attack), _ in train_loader:\n",
    "            attack = attack.type(torch.LongTensor)           \n",
    "            attack = attack.to(device)\n",
    "            comment = comment.type(torch.LongTensor)  \n",
    "            comment = comment.to(device)\n",
    "            output = model(comment, attack)\n",
    "            loss, _ = output\n",
    "            print('batch_no [{}/{}]:'.format(batch_no, int(len(Train_comment)/16)),'training_loss:',loss)\n",
    "            batch_no+=1\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update running values\n",
    "            running_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            # evaluation step\n",
    "            if global_step % eval_every == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():                    \n",
    "\n",
    "                    # validation loop\n",
    "                    for (comment, attack), _ in valid_loader:\n",
    "                        attack = attack.type(torch.LongTensor)           \n",
    "                        attack = attack.to(device)\n",
    "                        comment = comment.type(torch.LongTensor)  \n",
    "                        comment = comment.to(device)\n",
    "                        output = model(comment, attack)\n",
    "                        loss, _ = output\n",
    "                        \n",
    "                        valid_running_loss += loss.item()\n",
    "\n",
    "                # evaluation\n",
    "                average_train_loss = running_loss / eval_every\n",
    "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
    "                train_loss_list.append(average_train_loss)\n",
    "                valid_loss_list.append(average_valid_loss)\n",
    "                global_steps_list.append(global_step)\n",
    "\n",
    "                # resetting running values\n",
    "                running_loss = 0.0                \n",
    "                valid_running_loss = 0.0\n",
    "                model.train()\n",
    "\n",
    "                # print progress\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
    "                              average_train_loss, average_valid_loss))\n",
    "                \n",
    "                # checkpoint\n",
    "                if best_valid_loss > average_valid_loss:\n",
    "                    best_valid_loss = average_valid_loss\n",
    "                    save_checkpoint(file_path + '/' + 'model.pt', model, best_valid_loss)\n",
    "                    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    \n",
    "    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    print('Finished Training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name encoder.bert.embeddings.word_embeddings.weight\n",
      "name encoder.bert.embeddings.position_embeddings.weight\n",
      "name encoder.bert.embeddings.token_type_embeddings.weight\n",
      "name encoder.bert.embeddings.LayerNorm.weight\n",
      "name encoder.bert.embeddings.LayerNorm.bias\n",
      "name encoder.bert.encoder.layer.0.attention.self.query.weight\n",
      "name encoder.bert.encoder.layer.0.attention.self.query.bias\n",
      "name encoder.bert.encoder.layer.0.attention.self.key.weight\n",
      "name encoder.bert.encoder.layer.0.attention.self.key.bias\n",
      "name encoder.bert.encoder.layer.0.attention.self.value.weight\n",
      "name encoder.bert.encoder.layer.0.attention.self.value.bias\n",
      "name encoder.bert.encoder.layer.0.attention.output.dense.weight\n",
      "name encoder.bert.encoder.layer.0.attention.output.dense.bias\n",
      "name encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "name encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "name encoder.bert.encoder.layer.0.intermediate.dense.weight\n",
      "name encoder.bert.encoder.layer.0.intermediate.dense.bias\n",
      "name encoder.bert.encoder.layer.0.output.dense.weight\n",
      "name encoder.bert.encoder.layer.0.output.dense.bias\n",
      "name encoder.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "name encoder.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "name encoder.bert.encoder.layer.1.attention.self.query.weight\n",
      "name encoder.bert.encoder.layer.1.attention.self.query.bias\n",
      "name encoder.bert.encoder.layer.1.attention.self.key.weight\n",
      "name encoder.bert.encoder.layer.1.attention.self.key.bias\n",
      "name encoder.bert.encoder.layer.1.attention.self.value.weight\n",
      "name encoder.bert.encoder.layer.1.attention.self.value.bias\n",
      "name encoder.bert.encoder.layer.1.attention.output.dense.weight\n",
      "name encoder.bert.encoder.layer.1.attention.output.dense.bias\n",
      "name encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "name encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "name encoder.bert.encoder.layer.1.intermediate.dense.weight\n",
      "name encoder.bert.encoder.layer.1.intermediate.dense.bias\n",
      "name encoder.bert.encoder.layer.1.output.dense.weight\n",
      "name encoder.bert.encoder.layer.1.output.dense.bias\n",
      "name encoder.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "name encoder.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "name encoder.bert.encoder.layer.2.attention.self.query.weight\n",
      "name encoder.bert.encoder.layer.2.attention.self.query.bias\n",
      "name encoder.bert.encoder.layer.2.attention.self.key.weight\n",
      "name encoder.bert.encoder.layer.2.attention.self.key.bias\n",
      "name encoder.bert.encoder.layer.2.attention.self.value.weight\n",
      "name encoder.bert.encoder.layer.2.attention.self.value.bias\n",
      "name encoder.bert.encoder.layer.2.attention.output.dense.weight\n",
      "name encoder.bert.encoder.layer.2.attention.output.dense.bias\n",
      "name encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "name encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "name encoder.bert.encoder.layer.2.intermediate.dense.weight\n",
      "name encoder.bert.encoder.layer.2.intermediate.dense.bias\n",
      "name encoder.bert.encoder.layer.2.output.dense.weight\n",
      "name encoder.bert.encoder.layer.2.output.dense.bias\n",
      "name encoder.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "name encoder.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "name encoder.bert.encoder.layer.3.attention.self.query.weight\n",
      "name encoder.bert.encoder.layer.3.attention.self.query.bias\n",
      "name encoder.bert.encoder.layer.3.attention.self.key.weight\n",
      "name encoder.bert.encoder.layer.3.attention.self.key.bias\n",
      "name encoder.bert.encoder.layer.3.attention.self.value.weight\n",
      "name encoder.bert.encoder.layer.3.attention.self.value.bias\n",
      "name encoder.bert.encoder.layer.3.attention.output.dense.weight\n",
      "name encoder.bert.encoder.layer.3.attention.output.dense.bias\n",
      "name encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "name encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "name encoder.bert.encoder.layer.3.intermediate.dense.weight\n",
      "name encoder.bert.encoder.layer.3.intermediate.dense.bias\n",
      "name encoder.bert.encoder.layer.3.output.dense.weight\n",
      "name encoder.bert.encoder.layer.3.output.dense.bias\n",
      "name encoder.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "name encoder.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "name encoder.bert.encoder.layer.4.attention.self.query.weight\n",
      "name encoder.bert.encoder.layer.4.attention.self.query.bias\n",
      "name encoder.bert.encoder.layer.4.attention.self.key.weight\n",
      "name encoder.bert.encoder.layer.4.attention.self.key.bias\n",
      "name encoder.bert.encoder.layer.4.attention.self.value.weight\n",
      "name encoder.bert.encoder.layer.4.attention.self.value.bias\n",
      "name encoder.bert.encoder.layer.4.attention.output.dense.weight\n",
      "name encoder.bert.encoder.layer.4.attention.output.dense.bias\n",
      "name encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "name encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "name encoder.bert.encoder.layer.4.intermediate.dense.weight\n",
      "name encoder.bert.encoder.layer.4.intermediate.dense.bias\n",
      "name encoder.bert.encoder.layer.4.output.dense.weight\n",
      "name encoder.bert.encoder.layer.4.output.dense.bias\n",
      "name encoder.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "name encoder.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "name encoder.bert.encoder.layer.5.attention.self.query.weight\n",
      "name encoder.bert.encoder.layer.5.attention.self.query.bias\n",
      "name encoder.bert.encoder.layer.5.attention.self.key.weight\n",
      "name encoder.bert.encoder.layer.5.attention.self.key.bias\n",
      "name encoder.bert.encoder.layer.5.attention.self.value.weight\n",
      "name encoder.bert.encoder.layer.5.attention.self.value.bias\n",
      "name encoder.bert.encoder.layer.5.attention.output.dense.weight\n",
      "name encoder.bert.encoder.layer.5.attention.output.dense.bias\n",
      "name encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "name encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "name encoder.bert.encoder.layer.5.intermediate.dense.weight\n",
      "name encoder.bert.encoder.layer.5.intermediate.dense.bias\n",
      "name encoder.bert.encoder.layer.5.output.dense.weight\n",
      "name encoder.bert.encoder.layer.5.output.dense.bias\n",
      "name encoder.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "name encoder.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "name encoder.bert.encoder.layer.6.attention.self.query.weight\n",
      "name encoder.bert.encoder.layer.6.attention.self.query.bias\n",
      "name encoder.bert.encoder.layer.6.attention.self.key.weight\n",
      "name encoder.bert.encoder.layer.6.attention.self.key.bias\n",
      "name encoder.bert.encoder.layer.6.attention.self.value.weight\n",
      "name encoder.bert.encoder.layer.6.attention.self.value.bias\n",
      "name encoder.bert.encoder.layer.6.attention.output.dense.weight\n",
      "name encoder.bert.encoder.layer.6.attention.output.dense.bias\n",
      "name encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "name encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "name encoder.bert.encoder.layer.6.intermediate.dense.weight\n",
      "name encoder.bert.encoder.layer.6.intermediate.dense.bias\n",
      "name encoder.bert.encoder.layer.6.output.dense.weight\n",
      "name encoder.bert.encoder.layer.6.output.dense.bias\n",
      "name encoder.bert.encoder.layer.6.output.LayerNorm.weight\n",
      "name encoder.bert.encoder.layer.6.output.LayerNorm.bias\n",
      "name encoder.bert.encoder.layer.7.attention.self.query.weight\n",
      "name encoder.bert.encoder.layer.7.attention.self.query.bias\n",
      "name encoder.bert.encoder.layer.7.attention.self.key.weight\n",
      "name encoder.bert.encoder.layer.7.attention.self.key.bias\n",
      "name encoder.bert.encoder.layer.7.attention.self.value.weight\n",
      "name encoder.bert.encoder.layer.7.attention.self.value.bias\n",
      "name encoder.bert.encoder.layer.7.attention.output.dense.weight\n",
      "name encoder.bert.encoder.layer.7.attention.output.dense.bias\n",
      "name encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "name encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "name encoder.bert.encoder.layer.7.intermediate.dense.weight\n",
      "name encoder.bert.encoder.layer.7.intermediate.dense.bias\n",
      "name encoder.bert.encoder.layer.7.output.dense.weight\n",
      "name encoder.bert.encoder.layer.7.output.dense.bias\n",
      "name encoder.bert.encoder.layer.7.output.LayerNorm.weight\n",
      "name encoder.bert.encoder.layer.7.output.LayerNorm.bias\n",
      "name encoder.bert.encoder.layer.8.attention.self.query.weight\n",
      "name encoder.bert.encoder.layer.8.attention.self.query.bias\n",
      "name encoder.bert.encoder.layer.8.attention.self.key.weight\n",
      "name encoder.bert.encoder.layer.8.attention.self.key.bias\n",
      "name encoder.bert.encoder.layer.8.attention.self.value.weight\n",
      "name encoder.bert.encoder.layer.8.attention.self.value.bias\n",
      "name encoder.bert.encoder.layer.8.attention.output.dense.weight\n",
      "name encoder.bert.encoder.layer.8.attention.output.dense.bias\n",
      "name encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "name encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "name encoder.bert.encoder.layer.8.intermediate.dense.weight\n",
      "name encoder.bert.encoder.layer.8.intermediate.dense.bias\n",
      "name encoder.bert.encoder.layer.8.output.dense.weight\n",
      "name encoder.bert.encoder.layer.8.output.dense.bias\n",
      "name encoder.bert.encoder.layer.8.output.LayerNorm.weight\n",
      "name encoder.bert.encoder.layer.8.output.LayerNorm.bias\n",
      "name encoder.bert.encoder.layer.9.attention.self.query.weight\n",
      "name encoder.bert.encoder.layer.9.attention.self.query.bias\n",
      "name encoder.bert.encoder.layer.9.attention.self.key.weight\n",
      "name encoder.bert.encoder.layer.9.attention.self.key.bias\n",
      "name encoder.bert.encoder.layer.9.attention.self.value.weight\n",
      "name encoder.bert.encoder.layer.9.attention.self.value.bias\n",
      "name encoder.bert.encoder.layer.9.attention.output.dense.weight\n",
      "name encoder.bert.encoder.layer.9.attention.output.dense.bias\n",
      "name encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "name encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "name encoder.bert.encoder.layer.9.intermediate.dense.weight\n",
      "name encoder.bert.encoder.layer.9.intermediate.dense.bias\n",
      "name encoder.bert.encoder.layer.9.output.dense.weight\n",
      "name encoder.bert.encoder.layer.9.output.dense.bias\n",
      "name encoder.bert.encoder.layer.9.output.LayerNorm.weight\n",
      "name encoder.bert.encoder.layer.9.output.LayerNorm.bias\n",
      "name encoder.bert.encoder.layer.10.attention.self.query.weight\n",
      "name encoder.bert.encoder.layer.10.attention.self.query.bias\n",
      "name encoder.bert.encoder.layer.10.attention.self.key.weight\n",
      "name encoder.bert.encoder.layer.10.attention.self.key.bias\n",
      "name encoder.bert.encoder.layer.10.attention.self.value.weight\n",
      "name encoder.bert.encoder.layer.10.attention.self.value.bias\n",
      "name encoder.bert.encoder.layer.10.attention.output.dense.weight\n",
      "name encoder.bert.encoder.layer.10.attention.output.dense.bias\n",
      "name encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "name encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "name encoder.bert.encoder.layer.10.intermediate.dense.weight\n",
      "name encoder.bert.encoder.layer.10.intermediate.dense.bias\n",
      "name encoder.bert.encoder.layer.10.output.dense.weight\n",
      "name encoder.bert.encoder.layer.10.output.dense.bias\n",
      "name encoder.bert.encoder.layer.10.output.LayerNorm.weight\n",
      "name encoder.bert.encoder.layer.10.output.LayerNorm.bias\n",
      "name encoder.bert.encoder.layer.11.attention.self.query.weight\n",
      "name encoder.bert.encoder.layer.11.attention.self.query.bias\n",
      "name encoder.bert.encoder.layer.11.attention.self.key.weight\n",
      "name encoder.bert.encoder.layer.11.attention.self.key.bias\n",
      "name encoder.bert.encoder.layer.11.attention.self.value.weight\n",
      "name encoder.bert.encoder.layer.11.attention.self.value.bias\n",
      "name encoder.bert.encoder.layer.11.attention.output.dense.weight\n",
      "name encoder.bert.encoder.layer.11.attention.output.dense.bias\n",
      "name encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "name encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "name encoder.bert.encoder.layer.11.intermediate.dense.weight\n",
      "name encoder.bert.encoder.layer.11.intermediate.dense.bias\n",
      "name encoder.bert.encoder.layer.11.output.dense.weight\n",
      "name encoder.bert.encoder.layer.11.output.dense.bias\n",
      "name encoder.bert.encoder.layer.11.output.LayerNorm.weight\n",
      "name encoder.bert.encoder.layer.11.output.LayerNorm.bias\n",
      "name encoder.bert.pooler.dense.weight\n",
      "name encoder.bert.pooler.dense.bias\n",
      "name encoder.classifier.weight\n",
      "name encoder.classifier.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print('name',name)\n",
    "#     print('parameter',param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523,
     "referenced_widgets": [
      "78f713ef2a9b4d46a6f359dd151c8904",
      "db05a76085614d789d15b245fa83e73b",
      "0ff4905c17974a6092918a8d77b9d100",
      "07a9cf8752b743679a39fd412702301d",
      "8f673376fffb43a59f88fda6509e002a",
      "7cddb6e6e8c948d981adb8697a03ad9a",
      "23c685a4d97447ca95727f733d2c2c9d",
      "03029ccf12164dafbe4fd9bb6e3722cc",
      "7128ff90beac44faa7f71b36b1e56099",
      "8672b461409d421facbada337e8f273d",
      "1ffc393f193a4c3f8390dc8307df77fb",
      "af544453c65043b2a7f6fdb21d06a940",
      "e4a0002383884d32a62cb26f0db5276d",
      "7a01574d4d574cb9ac60e3d688125fae",
      "4f5be3cee49247c8973561ced8e5181d",
      "4a83eca6a3e84bb980085bd7ced5ced5"
     ]
    },
    "colab_type": "code",
    "id": "nHdi_cyEvC9K",
    "outputId": "da4003b8-bc21-4df9-94a8-63209dd1ec7d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1/4345]: training_loss: tensor(0.4143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2/4345]: training_loss: tensor(0.2754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3/4345]: training_loss: tensor(0.2154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4/4345]: training_loss: tensor(0.2082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [5/4345]: training_loss: tensor(0.2918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [6/4345]: training_loss: tensor(0.2598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [7/4345]: training_loss: tensor(0.3955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [8/4345]: training_loss: tensor(0.2714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [9/4345]: training_loss: tensor(0.3525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [10/4345]: training_loss: tensor(0.1460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [11/4345]: training_loss: tensor(0.1345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [12/4345]: training_loss: tensor(0.2780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [13/4345]: training_loss: tensor(0.4948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [14/4345]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [15/4345]: training_loss: tensor(0.3907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [16/4345]: training_loss: tensor(0.3920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [17/4345]: training_loss: tensor(0.2310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [18/4345]: training_loss: tensor(0.3996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [19/4345]: training_loss: tensor(0.2587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [20/4345]: training_loss: tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [21/4345]: training_loss: tensor(0.2140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [22/4345]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [23/4345]: training_loss: tensor(0.2553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [24/4345]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [25/4345]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [26/4345]: training_loss: tensor(0.2294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [27/4345]: training_loss: tensor(0.3736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [28/4345]: training_loss: tensor(0.5794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [29/4345]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [30/4345]: training_loss: tensor(0.4014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [31/4345]: training_loss: tensor(0.2522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [32/4345]: training_loss: tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [33/4345]: training_loss: tensor(0.4230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [34/4345]: training_loss: tensor(0.2429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [35/4345]: training_loss: tensor(0.5771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [36/4345]: training_loss: tensor(0.5585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [37/4345]: training_loss: tensor(0.5099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [38/4345]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [39/4345]: training_loss: tensor(0.9669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [40/4345]: training_loss: tensor(0.1171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [41/4345]: training_loss: tensor(0.3778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [42/4345]: training_loss: tensor(0.3629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [43/4345]: training_loss: tensor(0.3658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [44/4345]: training_loss: tensor(0.9324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [45/4345]: training_loss: tensor(0.2900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [46/4345]: training_loss: tensor(0.3075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [47/4345]: training_loss: tensor(0.2671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [48/4345]: training_loss: tensor(0.1660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [49/4345]: training_loss: tensor(0.3718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [50/4345]: training_loss: tensor(0.3872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [51/4345]: training_loss: tensor(0.1391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [52/4345]: training_loss: tensor(0.4661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [53/4345]: training_loss: tensor(0.4945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [54/4345]: training_loss: tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [55/4345]: training_loss: tensor(0.3543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [56/4345]: training_loss: tensor(0.4895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [57/4345]: training_loss: tensor(0.5017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [58/4345]: training_loss: tensor(0.5532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [59/4345]: training_loss: tensor(0.2316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [60/4345]: training_loss: tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [61/4345]: training_loss: tensor(0.2618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [62/4345]: training_loss: tensor(0.4033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [63/4345]: training_loss: tensor(0.2475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [64/4345]: training_loss: tensor(0.3980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [65/4345]: training_loss: tensor(0.1204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [66/4345]: training_loss: tensor(0.3754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [67/4345]: training_loss: tensor(0.3393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [68/4345]: training_loss: tensor(0.8067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [69/4345]: training_loss: tensor(0.2291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [70/4345]: training_loss: tensor(0.2491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [71/4345]: training_loss: tensor(0.3530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [72/4345]: training_loss: tensor(0.2389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [73/4345]: training_loss: tensor(0.5311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [74/4345]: training_loss: tensor(0.5059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [75/4345]: training_loss: tensor(0.6306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [76/4345]: training_loss: tensor(0.3906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [77/4345]: training_loss: tensor(0.3987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [78/4345]: training_loss: tensor(0.4013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [79/4345]: training_loss: tensor(0.2622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [80/4345]: training_loss: tensor(0.1299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [81/4345]: training_loss: tensor(0.4088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [82/4345]: training_loss: tensor(0.4923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [83/4345]: training_loss: tensor(0.2653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [84/4345]: training_loss: tensor(0.2464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [85/4345]: training_loss: tensor(0.6359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [86/4345]: training_loss: tensor(0.5883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [87/4345]: training_loss: tensor(0.1381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [88/4345]: training_loss: tensor(0.2515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [89/4345]: training_loss: tensor(0.3771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [90/4345]: training_loss: tensor(0.2510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [91/4345]: training_loss: tensor(0.4743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [92/4345]: training_loss: tensor(0.2269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [93/4345]: training_loss: tensor(0.2461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [94/4345]: training_loss: tensor(0.2632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [95/4345]: training_loss: tensor(0.6574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [96/4345]: training_loss: tensor(0.6173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [97/4345]: training_loss: tensor(0.2440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [98/4345]: training_loss: tensor(0.8802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [99/4345]: training_loss: tensor(0.2633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [100/4345]: training_loss: tensor(0.3844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [101/4345]: training_loss: tensor(0.2588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [102/4345]: training_loss: tensor(0.6267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [103/4345]: training_loss: tensor(0.4717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [104/4345]: training_loss: tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [105/4345]: training_loss: tensor(0.4686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [106/4345]: training_loss: tensor(0.8089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [107/4345]: training_loss: tensor(0.5897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [108/4345]: training_loss: tensor(0.4781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [109/4345]: training_loss: tensor(0.2655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [110/4345]: training_loss: tensor(0.5343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [111/4345]: training_loss: tensor(0.6857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [112/4345]: training_loss: tensor(0.3768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [113/4345]: training_loss: tensor(0.4139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [114/4345]: training_loss: tensor(0.5587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [115/4345]: training_loss: tensor(0.3038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [116/4345]: training_loss: tensor(0.4508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [117/4345]: training_loss: tensor(0.3171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [118/4345]: training_loss: tensor(0.5094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [119/4345]: training_loss: tensor(0.2716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [120/4345]: training_loss: tensor(0.4463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [121/4345]: training_loss: tensor(0.2146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [122/4345]: training_loss: tensor(0.5568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [123/4345]: training_loss: tensor(0.5696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [124/4345]: training_loss: tensor(0.2106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [125/4345]: training_loss: tensor(0.2575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [126/4345]: training_loss: tensor(0.2825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [127/4345]: training_loss: tensor(0.2521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [128/4345]: training_loss: tensor(0.2325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [129/4345]: training_loss: tensor(0.2977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [130/4345]: training_loss: tensor(0.4264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [131/4345]: training_loss: tensor(0.3284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [132/4345]: training_loss: tensor(0.3684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [133/4345]: training_loss: tensor(0.4113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [134/4345]: training_loss: tensor(0.1849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [135/4345]: training_loss: tensor(0.2797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [136/4345]: training_loss: tensor(0.1319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [137/4345]: training_loss: tensor(0.2647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [138/4345]: training_loss: tensor(0.3461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [139/4345]: training_loss: tensor(0.5583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [140/4345]: training_loss: tensor(0.3204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [141/4345]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [142/4345]: training_loss: tensor(0.1479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [143/4345]: training_loss: tensor(0.1454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [144/4345]: training_loss: tensor(0.1784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [145/4345]: training_loss: tensor(0.2827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [146/4345]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [147/4345]: training_loss: tensor(0.4598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [148/4345]: training_loss: tensor(0.1650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [149/4345]: training_loss: tensor(0.4432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [150/4345]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [151/4345]: training_loss: tensor(0.3637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [152/4345]: training_loss: tensor(0.3234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [153/4345]: training_loss: tensor(0.4286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [154/4345]: training_loss: tensor(0.2534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [155/4345]: training_loss: tensor(0.4512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [156/4345]: training_loss: tensor(0.3453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [157/4345]: training_loss: tensor(0.2559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [158/4345]: training_loss: tensor(0.1909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [159/4345]: training_loss: tensor(0.1468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [160/4345]: training_loss: tensor(0.1967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [161/4345]: training_loss: tensor(0.2173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [162/4345]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [163/4345]: training_loss: tensor(0.2289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [164/4345]: training_loss: tensor(0.2976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [165/4345]: training_loss: tensor(0.5697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [166/4345]: training_loss: tensor(0.3709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [167/4345]: training_loss: tensor(0.1535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [168/4345]: training_loss: tensor(0.2607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [169/4345]: training_loss: tensor(0.1811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [170/4345]: training_loss: tensor(0.5395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [171/4345]: training_loss: tensor(0.4220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [172/4345]: training_loss: tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [173/4345]: training_loss: tensor(0.3518, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [174/4345]: training_loss: tensor(0.1403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [175/4345]: training_loss: tensor(0.1484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [176/4345]: training_loss: tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [177/4345]: training_loss: tensor(0.4288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [178/4345]: training_loss: tensor(0.4055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [179/4345]: training_loss: tensor(0.1361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [180/4345]: training_loss: tensor(0.6593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [181/4345]: training_loss: tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [182/4345]: training_loss: tensor(0.2891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [183/4345]: training_loss: tensor(0.3346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [184/4345]: training_loss: tensor(0.2792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [185/4345]: training_loss: tensor(0.1551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [186/4345]: training_loss: tensor(0.2763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [187/4345]: training_loss: tensor(0.2233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [188/4345]: training_loss: tensor(0.3130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [189/4345]: training_loss: tensor(0.2784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [190/4345]: training_loss: tensor(0.2326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [191/4345]: training_loss: tensor(0.4910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [192/4345]: training_loss: tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [193/4345]: training_loss: tensor(0.1355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [194/4345]: training_loss: tensor(0.2151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [195/4345]: training_loss: tensor(0.1848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [196/4345]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [197/4345]: training_loss: tensor(0.4038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [198/4345]: training_loss: tensor(0.1750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [199/4345]: training_loss: tensor(0.1820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [200/4345]: training_loss: tensor(0.1360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [201/4345]: training_loss: tensor(0.1949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [202/4345]: training_loss: tensor(0.2499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [203/4345]: training_loss: tensor(0.1295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [204/4345]: training_loss: tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [205/4345]: training_loss: tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [206/4345]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [207/4345]: training_loss: tensor(0.2206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [208/4345]: training_loss: tensor(0.1672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [209/4345]: training_loss: tensor(0.1154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [210/4345]: training_loss: tensor(0.2315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [211/4345]: training_loss: tensor(0.1486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [212/4345]: training_loss: tensor(0.3529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [213/4345]: training_loss: tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [214/4345]: training_loss: tensor(0.5408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [215/4345]: training_loss: tensor(0.2949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [216/4345]: training_loss: tensor(0.3769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [217/4345]: training_loss: tensor(0.2210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [218/4345]: training_loss: tensor(0.3897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [219/4345]: training_loss: tensor(0.3093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [220/4345]: training_loss: tensor(0.1353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [221/4345]: training_loss: tensor(0.1201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [222/4345]: training_loss: tensor(0.3391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [223/4345]: training_loss: tensor(0.2037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [224/4345]: training_loss: tensor(0.4662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [225/4345]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [226/4345]: training_loss: tensor(0.1386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [227/4345]: training_loss: tensor(0.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [228/4345]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [229/4345]: training_loss: tensor(0.3269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [230/4345]: training_loss: tensor(0.2981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [231/4345]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [232/4345]: training_loss: tensor(0.7068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [233/4345]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [234/4345]: training_loss: tensor(0.2382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [235/4345]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [236/4345]: training_loss: tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [237/4345]: training_loss: tensor(0.1371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [238/4345]: training_loss: tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [239/4345]: training_loss: tensor(0.1928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [240/4345]: training_loss: tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [241/4345]: training_loss: tensor(0.2540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [242/4345]: training_loss: tensor(0.3410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [243/4345]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [244/4345]: training_loss: tensor(0.2515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [245/4345]: training_loss: tensor(0.1678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [246/4345]: training_loss: tensor(0.2490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [247/4345]: training_loss: tensor(0.3191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [248/4345]: training_loss: tensor(0.2046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [249/4345]: training_loss: tensor(0.4731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [250/4345]: training_loss: tensor(0.2316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [251/4345]: training_loss: tensor(0.1724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [252/4345]: training_loss: tensor(0.6412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [253/4345]: training_loss: tensor(0.2496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [254/4345]: training_loss: tensor(0.1717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [255/4345]: training_loss: tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [256/4345]: training_loss: tensor(0.2362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [257/4345]: training_loss: tensor(0.2412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [258/4345]: training_loss: tensor(0.1755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [259/4345]: training_loss: tensor(0.1156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [260/4345]: training_loss: tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [261/4345]: training_loss: tensor(0.1142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [262/4345]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [263/4345]: training_loss: tensor(0.2539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [264/4345]: training_loss: tensor(0.2910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [265/4345]: training_loss: tensor(0.0459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [266/4345]: training_loss: tensor(0.1105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [267/4345]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [268/4345]: training_loss: tensor(0.2384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [269/4345]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [270/4345]: training_loss: tensor(0.5290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [271/4345]: training_loss: tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [272/4345]: training_loss: tensor(0.2454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [273/4345]: training_loss: tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [274/4345]: training_loss: tensor(0.1390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [275/4345]: training_loss: tensor(0.3476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [276/4345]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [277/4345]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [278/4345]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [279/4345]: training_loss: tensor(0.1650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [280/4345]: training_loss: tensor(0.2885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [281/4345]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [282/4345]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [283/4345]: training_loss: tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [284/4345]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [285/4345]: training_loss: tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [286/4345]: training_loss: tensor(0.2855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [287/4345]: training_loss: tensor(0.1672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [288/4345]: training_loss: tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [289/4345]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [290/4345]: training_loss: tensor(0.0592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [291/4345]: training_loss: tensor(0.4860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [292/4345]: training_loss: tensor(0.3006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [293/4345]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [294/4345]: training_loss: tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [295/4345]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [296/4345]: training_loss: tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [297/4345]: training_loss: tensor(0.4891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [298/4345]: training_loss: tensor(0.2488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [299/4345]: training_loss: tensor(0.1128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [300/4345]: training_loss: tensor(0.2267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [301/4345]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [302/4345]: training_loss: tensor(0.2408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [303/4345]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [304/4345]: training_loss: tensor(0.2639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [305/4345]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [306/4345]: training_loss: tensor(0.4272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [307/4345]: training_loss: tensor(0.2063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [308/4345]: training_loss: tensor(0.4835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [309/4345]: training_loss: tensor(0.3880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [310/4345]: training_loss: tensor(0.2814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [311/4345]: training_loss: tensor(0.2460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [312/4345]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [313/4345]: training_loss: tensor(0.2924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [314/4345]: training_loss: tensor(0.2462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [315/4345]: training_loss: tensor(0.3107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [316/4345]: training_loss: tensor(0.2576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [317/4345]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [318/4345]: training_loss: tensor(0.2105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [319/4345]: training_loss: tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [320/4345]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [321/4345]: training_loss: tensor(0.1949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [322/4345]: training_loss: tensor(0.1871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [323/4345]: training_loss: tensor(0.1071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [324/4345]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [325/4345]: training_loss: tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [326/4345]: training_loss: tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [327/4345]: training_loss: tensor(0.7351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [328/4345]: training_loss: tensor(0.5262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [329/4345]: training_loss: tensor(0.3525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [330/4345]: training_loss: tensor(0.1592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [331/4345]: training_loss: tensor(0.2031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [332/4345]: training_loss: tensor(0.4218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [333/4345]: training_loss: tensor(0.3385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [334/4345]: training_loss: tensor(0.2792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [335/4345]: training_loss: tensor(0.1373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [336/4345]: training_loss: tensor(1.0269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [337/4345]: training_loss: tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [338/4345]: training_loss: tensor(0.1988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [339/4345]: training_loss: tensor(0.2067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [340/4345]: training_loss: tensor(0.4301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [341/4345]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [342/4345]: training_loss: tensor(0.3403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [343/4345]: training_loss: tensor(0.3015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [344/4345]: training_loss: tensor(0.4952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [345/4345]: training_loss: tensor(0.2532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [346/4345]: training_loss: tensor(0.1649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [347/4345]: training_loss: tensor(0.4450, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [348/4345]: training_loss: tensor(0.3673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [349/4345]: training_loss: tensor(0.2049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [350/4345]: training_loss: tensor(0.3582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [351/4345]: training_loss: tensor(0.2099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [352/4345]: training_loss: tensor(0.1348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [353/4345]: training_loss: tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [354/4345]: training_loss: tensor(0.5453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [355/4345]: training_loss: tensor(0.3284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [356/4345]: training_loss: tensor(0.1752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [357/4345]: training_loss: tensor(0.3980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [358/4345]: training_loss: tensor(0.3736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [359/4345]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [360/4345]: training_loss: tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [361/4345]: training_loss: tensor(0.0596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [362/4345]: training_loss: tensor(0.1637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [363/4345]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [364/4345]: training_loss: tensor(0.2514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [365/4345]: training_loss: tensor(0.2034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [366/4345]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [367/4345]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [368/4345]: training_loss: tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [369/4345]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [370/4345]: training_loss: tensor(0.0615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [371/4345]: training_loss: tensor(0.2044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [372/4345]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [373/4345]: training_loss: tensor(0.0650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [374/4345]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [375/4345]: training_loss: tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [376/4345]: training_loss: tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [377/4345]: training_loss: tensor(0.2769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [378/4345]: training_loss: tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [379/4345]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [380/4345]: training_loss: tensor(0.2334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [381/4345]: training_loss: tensor(0.5104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [382/4345]: training_loss: tensor(0.0575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [383/4345]: training_loss: tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [384/4345]: training_loss: tensor(0.2217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [385/4345]: training_loss: tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [386/4345]: training_loss: tensor(0.1596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [387/4345]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [388/4345]: training_loss: tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [389/4345]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [390/4345]: training_loss: tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [391/4345]: training_loss: tensor(0.5346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [392/4345]: training_loss: tensor(0.3281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [393/4345]: training_loss: tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [394/4345]: training_loss: tensor(0.1410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [395/4345]: training_loss: tensor(0.3149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [396/4345]: training_loss: tensor(0.1972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [397/4345]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [398/4345]: training_loss: tensor(0.0462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [399/4345]: training_loss: tensor(0.2025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [400/4345]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [401/4345]: training_loss: tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [402/4345]: training_loss: tensor(0.1711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [403/4345]: training_loss: tensor(0.1786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [404/4345]: training_loss: tensor(0.2225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [405/4345]: training_loss: tensor(0.2019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [406/4345]: training_loss: tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [407/4345]: training_loss: tensor(0.3144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [408/4345]: training_loss: tensor(0.2200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [409/4345]: training_loss: tensor(0.0640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [410/4345]: training_loss: tensor(0.1889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [411/4345]: training_loss: tensor(0.2503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [412/4345]: training_loss: tensor(0.3268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [413/4345]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [414/4345]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [415/4345]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [416/4345]: training_loss: tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [417/4345]: training_loss: tensor(0.1149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [418/4345]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [419/4345]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [420/4345]: training_loss: tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [421/4345]: training_loss: tensor(0.2440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [422/4345]: training_loss: tensor(0.0476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [423/4345]: training_loss: tensor(0.4321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [424/4345]: training_loss: tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [425/4345]: training_loss: tensor(0.1804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [426/4345]: training_loss: tensor(0.2601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [427/4345]: training_loss: tensor(0.2975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [428/4345]: training_loss: tensor(0.1754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [429/4345]: training_loss: tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [430/4345]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [431/4345]: training_loss: tensor(0.1365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [432/4345]: training_loss: tensor(0.1973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [433/4345]: training_loss: tensor(0.4071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [434/4345]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [435/4345]: training_loss: tensor(0.3579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [436/4345]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [437/4345]: training_loss: tensor(0.4963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [438/4345]: training_loss: tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [439/4345]: training_loss: tensor(0.4124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [440/4345]: training_loss: tensor(0.1491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [441/4345]: training_loss: tensor(0.2185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [442/4345]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [443/4345]: training_loss: tensor(0.1171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [444/4345]: training_loss: tensor(0.0475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [445/4345]: training_loss: tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [446/4345]: training_loss: tensor(0.0648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [447/4345]: training_loss: tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [448/4345]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [449/4345]: training_loss: tensor(0.1526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [450/4345]: training_loss: tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [451/4345]: training_loss: tensor(0.2844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [452/4345]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [453/4345]: training_loss: tensor(0.1320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [454/4345]: training_loss: tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [455/4345]: training_loss: tensor(0.5684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [456/4345]: training_loss: tensor(0.1492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [457/4345]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [458/4345]: training_loss: tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [459/4345]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [460/4345]: training_loss: tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [461/4345]: training_loss: tensor(0.2701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [462/4345]: training_loss: tensor(0.1259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [463/4345]: training_loss: tensor(0.1257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [464/4345]: training_loss: tensor(0.4987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [465/4345]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [466/4345]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [467/4345]: training_loss: tensor(0.2093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [468/4345]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [469/4345]: training_loss: tensor(0.1387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [470/4345]: training_loss: tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [471/4345]: training_loss: tensor(0.4949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [472/4345]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [473/4345]: training_loss: tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [474/4345]: training_loss: tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [475/4345]: training_loss: tensor(0.5987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [476/4345]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [477/4345]: training_loss: tensor(0.3178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [478/4345]: training_loss: tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [479/4345]: training_loss: tensor(0.1532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [480/4345]: training_loss: tensor(0.3882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [481/4345]: training_loss: tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [482/4345]: training_loss: tensor(0.2308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [483/4345]: training_loss: tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [484/4345]: training_loss: tensor(0.1606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [485/4345]: training_loss: tensor(0.2042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [486/4345]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [487/4345]: training_loss: tensor(0.1658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [488/4345]: training_loss: tensor(0.1737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [489/4345]: training_loss: tensor(0.2724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [490/4345]: training_loss: tensor(0.1443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [491/4345]: training_loss: tensor(0.2704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [492/4345]: training_loss: tensor(0.1186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [493/4345]: training_loss: tensor(0.2493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [494/4345]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [495/4345]: training_loss: tensor(0.3431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [496/4345]: training_loss: tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [497/4345]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [498/4345]: training_loss: tensor(0.2789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [499/4345]: training_loss: tensor(0.2724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [500/4345]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [501/4345]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [502/4345]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [503/4345]: training_loss: tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [504/4345]: training_loss: tensor(0.1398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [505/4345]: training_loss: tensor(0.6416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [506/4345]: training_loss: tensor(0.3325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [507/4345]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [508/4345]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [509/4345]: training_loss: tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [510/4345]: training_loss: tensor(0.1688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [511/4345]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [512/4345]: training_loss: tensor(0.3072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [513/4345]: training_loss: tensor(0.2486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [514/4345]: training_loss: tensor(0.0572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [515/4345]: training_loss: tensor(0.2878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [516/4345]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [517/4345]: training_loss: tensor(0.2131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [518/4345]: training_loss: tensor(0.1693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [519/4345]: training_loss: tensor(0.1486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [520/4345]: training_loss: tensor(0.2609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [521/4345]: training_loss: tensor(0.1564, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [522/4345]: training_loss: tensor(0.2787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [523/4345]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [524/4345]: training_loss: tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [525/4345]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [526/4345]: training_loss: tensor(0.1429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [527/4345]: training_loss: tensor(0.4620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [528/4345]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [529/4345]: training_loss: tensor(0.2558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [530/4345]: training_loss: tensor(0.2294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [531/4345]: training_loss: tensor(0.1965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [532/4345]: training_loss: tensor(0.1642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [533/4345]: training_loss: tensor(0.2606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [534/4345]: training_loss: tensor(0.1251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [535/4345]: training_loss: tensor(0.0587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [536/4345]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [537/4345]: training_loss: tensor(0.1172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [538/4345]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [539/4345]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [540/4345]: training_loss: tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [541/4345]: training_loss: tensor(0.1064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [542/4345]: training_loss: tensor(0.3929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [543/4345]: training_loss: tensor(0.3015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [544/4345]: training_loss: tensor(0.2873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [545/4345]: training_loss: tensor(0.2376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [546/4345]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [547/4345]: training_loss: tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [548/4345]: training_loss: tensor(0.0547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [549/4345]: training_loss: tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [550/4345]: training_loss: tensor(0.0606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [551/4345]: training_loss: tensor(0.1806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [552/4345]: training_loss: tensor(0.2383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [553/4345]: training_loss: tensor(0.2606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [554/4345]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [555/4345]: training_loss: tensor(0.2467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [556/4345]: training_loss: tensor(0.1417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [557/4345]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [558/4345]: training_loss: tensor(0.1167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [559/4345]: training_loss: tensor(0.3789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [560/4345]: training_loss: tensor(0.1590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [561/4345]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [562/4345]: training_loss: tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [563/4345]: training_loss: tensor(0.2028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [564/4345]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [565/4345]: training_loss: tensor(0.0555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [566/4345]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [567/4345]: training_loss: tensor(0.1591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [568/4345]: training_loss: tensor(0.0587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [569/4345]: training_loss: tensor(0.2627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [570/4345]: training_loss: tensor(0.1530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [571/4345]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [572/4345]: training_loss: tensor(0.1314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [573/4345]: training_loss: tensor(0.3306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [574/4345]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [575/4345]: training_loss: tensor(0.2129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [576/4345]: training_loss: tensor(0.1646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [577/4345]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [578/4345]: training_loss: tensor(0.2136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [579/4345]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [580/4345]: training_loss: tensor(0.1966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [581/4345]: training_loss: tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [582/4345]: training_loss: tensor(0.2250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [583/4345]: training_loss: tensor(0.1960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [584/4345]: training_loss: tensor(0.1865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [585/4345]: training_loss: tensor(0.1601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [586/4345]: training_loss: tensor(0.1530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [587/4345]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [588/4345]: training_loss: tensor(0.1182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [589/4345]: training_loss: tensor(0.1706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [590/4345]: training_loss: tensor(0.1991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [591/4345]: training_loss: tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [592/4345]: training_loss: tensor(0.3835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [593/4345]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [594/4345]: training_loss: tensor(0.4263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [595/4345]: training_loss: tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [596/4345]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [597/4345]: training_loss: tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [598/4345]: training_loss: tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [599/4345]: training_loss: tensor(0.1121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [600/4345]: training_loss: tensor(0.0361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [601/4345]: training_loss: tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [602/4345]: training_loss: tensor(0.4456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [603/4345]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [604/4345]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [605/4345]: training_loss: tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [606/4345]: training_loss: tensor(0.2704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [607/4345]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [608/4345]: training_loss: tensor(0.1154, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [609/4345]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [610/4345]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [611/4345]: training_loss: tensor(0.2430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [612/4345]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [613/4345]: training_loss: tensor(0.1818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [614/4345]: training_loss: tensor(0.1889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [615/4345]: training_loss: tensor(0.1302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [616/4345]: training_loss: tensor(0.1576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [617/4345]: training_loss: tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [618/4345]: training_loss: tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [619/4345]: training_loss: tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [620/4345]: training_loss: tensor(0.1153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [621/4345]: training_loss: tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [622/4345]: training_loss: tensor(0.4287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [623/4345]: training_loss: tensor(0.4784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [624/4345]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [625/4345]: training_loss: tensor(0.1740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [626/4345]: training_loss: tensor(0.2402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [627/4345]: training_loss: tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [628/4345]: training_loss: tensor(0.1238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [629/4345]: training_loss: tensor(0.2891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [630/4345]: training_loss: tensor(0.1165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [631/4345]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [632/4345]: training_loss: tensor(0.3615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [633/4345]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [634/4345]: training_loss: tensor(0.3058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [635/4345]: training_loss: tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [636/4345]: training_loss: tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [637/4345]: training_loss: tensor(0.2760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [638/4345]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [639/4345]: training_loss: tensor(0.4692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [640/4345]: training_loss: tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [641/4345]: training_loss: tensor(0.2824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [642/4345]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [643/4345]: training_loss: tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [644/4345]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [645/4345]: training_loss: tensor(0.1624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [646/4345]: training_loss: tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [647/4345]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [648/4345]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [649/4345]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [650/4345]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [651/4345]: training_loss: tensor(0.4641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [652/4345]: training_loss: tensor(0.5199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [653/4345]: training_loss: tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [654/4345]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [655/4345]: training_loss: tensor(0.2222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [656/4345]: training_loss: tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [657/4345]: training_loss: tensor(0.1472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [658/4345]: training_loss: tensor(0.1889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [659/4345]: training_loss: tensor(0.1132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [660/4345]: training_loss: tensor(0.1708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [661/4345]: training_loss: tensor(0.0621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [662/4345]: training_loss: tensor(0.2508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [663/4345]: training_loss: tensor(0.1320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [664/4345]: training_loss: tensor(0.1201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [665/4345]: training_loss: tensor(0.2499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [666/4345]: training_loss: tensor(0.2731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [667/4345]: training_loss: tensor(0.0645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [668/4345]: training_loss: tensor(0.2399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [669/4345]: training_loss: tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [670/4345]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [671/4345]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [672/4345]: training_loss: tensor(0.0364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [673/4345]: training_loss: tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [674/4345]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [675/4345]: training_loss: tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [676/4345]: training_loss: tensor(0.3827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [677/4345]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [678/4345]: training_loss: tensor(0.3440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [679/4345]: training_loss: tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [680/4345]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [681/4345]: training_loss: tensor(0.0629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [682/4345]: training_loss: tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [683/4345]: training_loss: tensor(0.4734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [684/4345]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [685/4345]: training_loss: tensor(0.3099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [686/4345]: training_loss: tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [687/4345]: training_loss: tensor(0.2406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [688/4345]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [689/4345]: training_loss: tensor(0.2972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [690/4345]: training_loss: tensor(0.1526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [691/4345]: training_loss: tensor(0.2124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [692/4345]: training_loss: tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [693/4345]: training_loss: tensor(0.3577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [694/4345]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [695/4345]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [696/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [697/4345]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [698/4345]: training_loss: tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [699/4345]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [700/4345]: training_loss: tensor(0.2645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [701/4345]: training_loss: tensor(0.4318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [702/4345]: training_loss: tensor(0.1909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [703/4345]: training_loss: tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [704/4345]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [705/4345]: training_loss: tensor(0.3532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [706/4345]: training_loss: tensor(0.1918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [707/4345]: training_loss: tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [708/4345]: training_loss: tensor(0.1588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [709/4345]: training_loss: tensor(0.2015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [710/4345]: training_loss: tensor(0.3317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [711/4345]: training_loss: tensor(0.2765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [712/4345]: training_loss: tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [713/4345]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [714/4345]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [715/4345]: training_loss: tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [716/4345]: training_loss: tensor(0.3564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [717/4345]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [718/4345]: training_loss: tensor(0.1368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [719/4345]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [720/4345]: training_loss: tensor(0.2136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [721/4345]: training_loss: tensor(0.3341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [722/4345]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [723/4345]: training_loss: tensor(0.1532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [724/4345]: training_loss: tensor(0.2889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [725/4345]: training_loss: tensor(0.1937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [726/4345]: training_loss: tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [727/4345]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [728/4345]: training_loss: tensor(0.0436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [729/4345]: training_loss: tensor(0.1364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [730/4345]: training_loss: tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [731/4345]: training_loss: tensor(0.1821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [732/4345]: training_loss: tensor(0.1608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [733/4345]: training_loss: tensor(0.1753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [734/4345]: training_loss: tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [735/4345]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [736/4345]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [737/4345]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [738/4345]: training_loss: tensor(0.1521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [739/4345]: training_loss: tensor(0.0615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [740/4345]: training_loss: tensor(0.0486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [741/4345]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [742/4345]: training_loss: tensor(0.2291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [743/4345]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [744/4345]: training_loss: tensor(0.4255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [745/4345]: training_loss: tensor(0.1941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [746/4345]: training_loss: tensor(0.2757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [747/4345]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [748/4345]: training_loss: tensor(0.1650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [749/4345]: training_loss: tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [750/4345]: training_loss: tensor(0.1814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [751/4345]: training_loss: tensor(0.3466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [752/4345]: training_loss: tensor(0.1337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [753/4345]: training_loss: tensor(0.1906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [754/4345]: training_loss: tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [755/4345]: training_loss: tensor(0.4061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [756/4345]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [757/4345]: training_loss: tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [758/4345]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [759/4345]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [760/4345]: training_loss: tensor(0.1891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [761/4345]: training_loss: tensor(0.2934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [762/4345]: training_loss: tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [763/4345]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [764/4345]: training_loss: tensor(0.2385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [765/4345]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [766/4345]: training_loss: tensor(0.1880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [767/4345]: training_loss: tensor(0.1642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [768/4345]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [769/4345]: training_loss: tensor(0.3563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [770/4345]: training_loss: tensor(0.1713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [771/4345]: training_loss: tensor(0.4057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [772/4345]: training_loss: tensor(0.1177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [773/4345]: training_loss: tensor(0.7062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [774/4345]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [775/4345]: training_loss: tensor(0.2502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [776/4345]: training_loss: tensor(0.1293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [777/4345]: training_loss: tensor(0.1373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [778/4345]: training_loss: tensor(0.2679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [779/4345]: training_loss: tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [780/4345]: training_loss: tensor(0.3007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [781/4345]: training_loss: tensor(0.1584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [782/4345]: training_loss: tensor(0.2946, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [783/4345]: training_loss: tensor(0.1476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [784/4345]: training_loss: tensor(0.2438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [785/4345]: training_loss: tensor(0.1335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [786/4345]: training_loss: tensor(0.2601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [787/4345]: training_loss: tensor(0.1629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [788/4345]: training_loss: tensor(0.1741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [789/4345]: training_loss: tensor(0.1544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [790/4345]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [791/4345]: training_loss: tensor(0.1521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [792/4345]: training_loss: tensor(0.1872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [793/4345]: training_loss: tensor(0.3036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [794/4345]: training_loss: tensor(0.1636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [795/4345]: training_loss: tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [796/4345]: training_loss: tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [797/4345]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [798/4345]: training_loss: tensor(0.3681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [799/4345]: training_loss: tensor(0.3426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [800/4345]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [801/4345]: training_loss: tensor(0.3552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [802/4345]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [803/4345]: training_loss: tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [804/4345]: training_loss: tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [805/4345]: training_loss: tensor(0.1790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [806/4345]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [807/4345]: training_loss: tensor(0.9218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [808/4345]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [809/4345]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [810/4345]: training_loss: tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [811/4345]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [812/4345]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [813/4345]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [814/4345]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [815/4345]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [816/4345]: training_loss: tensor(0.2812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [817/4345]: training_loss: tensor(0.3158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [818/4345]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [819/4345]: training_loss: tensor(0.3738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [820/4345]: training_loss: tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [821/4345]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [822/4345]: training_loss: tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [823/4345]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [824/4345]: training_loss: tensor(0.1467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [825/4345]: training_loss: tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [826/4345]: training_loss: tensor(0.2679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [827/4345]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [828/4345]: training_loss: tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [829/4345]: training_loss: tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [830/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [831/4345]: training_loss: tensor(0.2007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [832/4345]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [833/4345]: training_loss: tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [834/4345]: training_loss: tensor(0.2185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [835/4345]: training_loss: tensor(0.2022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [836/4345]: training_loss: tensor(0.1217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [837/4345]: training_loss: tensor(0.1219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [838/4345]: training_loss: tensor(0.1593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [839/4345]: training_loss: tensor(0.2683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [840/4345]: training_loss: tensor(0.1955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [841/4345]: training_loss: tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [842/4345]: training_loss: tensor(0.1576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [843/4345]: training_loss: tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [844/4345]: training_loss: tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [845/4345]: training_loss: tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [846/4345]: training_loss: tensor(0.3177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [847/4345]: training_loss: tensor(0.2778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [848/4345]: training_loss: tensor(0.1466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [849/4345]: training_loss: tensor(0.1530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [850/4345]: training_loss: tensor(0.1538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [851/4345]: training_loss: tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [852/4345]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [853/4345]: training_loss: tensor(0.1463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [854/4345]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [855/4345]: training_loss: tensor(0.1071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [856/4345]: training_loss: tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [857/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [858/4345]: training_loss: tensor(0.5307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [859/4345]: training_loss: tensor(0.2368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [860/4345]: training_loss: tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [861/4345]: training_loss: tensor(0.1128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [862/4345]: training_loss: tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [863/4345]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [864/4345]: training_loss: tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [865/4345]: training_loss: tensor(0.1751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [866/4345]: training_loss: tensor(0.4430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [867/4345]: training_loss: tensor(0.4095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [868/4345]: training_loss: tensor(0.1180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [869/4345]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [870/4345]: training_loss: tensor(0.0676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [871/4345]: training_loss: tensor(0.2415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [872/4345]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [873/4345]: training_loss: tensor(0.1858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [874/4345]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [875/4345]: training_loss: tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [876/4345]: training_loss: tensor(0.1303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [877/4345]: training_loss: tensor(0.1252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [878/4345]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [879/4345]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [880/4345]: training_loss: tensor(0.1267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [881/4345]: training_loss: tensor(0.2487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [882/4345]: training_loss: tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [883/4345]: training_loss: tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [884/4345]: training_loss: tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [885/4345]: training_loss: tensor(0.1325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [886/4345]: training_loss: tensor(0.2185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [887/4345]: training_loss: tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [888/4345]: training_loss: tensor(0.2269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [889/4345]: training_loss: tensor(0.2894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [890/4345]: training_loss: tensor(0.2556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [891/4345]: training_loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [892/4345]: training_loss: tensor(0.1700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [893/4345]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [894/4345]: training_loss: tensor(0.0594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [895/4345]: training_loss: tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [896/4345]: training_loss: tensor(0.2511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [897/4345]: training_loss: tensor(0.1380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [898/4345]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [899/4345]: training_loss: tensor(0.2616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [900/4345]: training_loss: tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [901/4345]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [902/4345]: training_loss: tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [903/4345]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [904/4345]: training_loss: tensor(0.4518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [905/4345]: training_loss: tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [906/4345]: training_loss: tensor(0.4238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [907/4345]: training_loss: tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [908/4345]: training_loss: tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [909/4345]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [910/4345]: training_loss: tensor(0.1431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [911/4345]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [912/4345]: training_loss: tensor(0.2148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [913/4345]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [914/4345]: training_loss: tensor(0.1357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [915/4345]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [916/4345]: training_loss: tensor(0.5787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [917/4345]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [918/4345]: training_loss: tensor(0.1372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [919/4345]: training_loss: tensor(0.1924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [920/4345]: training_loss: tensor(0.1043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [921/4345]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [922/4345]: training_loss: tensor(0.1425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [923/4345]: training_loss: tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [924/4345]: training_loss: tensor(0.1583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [925/4345]: training_loss: tensor(0.1603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [926/4345]: training_loss: tensor(0.0410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [927/4345]: training_loss: tensor(0.1214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [928/4345]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [929/4345]: training_loss: tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [930/4345]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [931/4345]: training_loss: tensor(0.2870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [932/4345]: training_loss: tensor(0.2200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [933/4345]: training_loss: tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [934/4345]: training_loss: tensor(0.2119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [935/4345]: training_loss: tensor(0.2088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [936/4345]: training_loss: tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [937/4345]: training_loss: tensor(0.1434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [938/4345]: training_loss: tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [939/4345]: training_loss: tensor(0.4195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [940/4345]: training_loss: tensor(0.2221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [941/4345]: training_loss: tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [942/4345]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [943/4345]: training_loss: tensor(0.1519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [944/4345]: training_loss: tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [945/4345]: training_loss: tensor(0.3905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [946/4345]: training_loss: tensor(0.2004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [947/4345]: training_loss: tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [948/4345]: training_loss: tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [949/4345]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [950/4345]: training_loss: tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [951/4345]: training_loss: tensor(0.3661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [952/4345]: training_loss: tensor(0.3622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [953/4345]: training_loss: tensor(0.1251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [954/4345]: training_loss: tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [955/4345]: training_loss: tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [956/4345]: training_loss: tensor(0.3146, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [957/4345]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [958/4345]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [959/4345]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [960/4345]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [961/4345]: training_loss: tensor(0.2280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [962/4345]: training_loss: tensor(0.4257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [963/4345]: training_loss: tensor(0.2817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [964/4345]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [965/4345]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [966/4345]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [967/4345]: training_loss: tensor(0.1205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [968/4345]: training_loss: tensor(0.1131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [969/4345]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [970/4345]: training_loss: tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [971/4345]: training_loss: tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [972/4345]: training_loss: tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [973/4345]: training_loss: tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [974/4345]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [975/4345]: training_loss: tensor(0.2305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [976/4345]: training_loss: tensor(0.1287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [977/4345]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [978/4345]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [979/4345]: training_loss: tensor(0.3198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [980/4345]: training_loss: tensor(0.2756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [981/4345]: training_loss: tensor(0.2043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [982/4345]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [983/4345]: training_loss: tensor(0.0494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [984/4345]: training_loss: tensor(0.1678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [985/4345]: training_loss: tensor(0.1142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [986/4345]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [987/4345]: training_loss: tensor(0.1867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [988/4345]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [989/4345]: training_loss: tensor(0.1703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [990/4345]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [991/4345]: training_loss: tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [992/4345]: training_loss: tensor(0.3440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [993/4345]: training_loss: tensor(0.2209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [994/4345]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [995/4345]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [996/4345]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [997/4345]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [998/4345]: training_loss: tensor(0.1395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [999/4345]: training_loss: tensor(0.1675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1000/4345]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1001/4345]: training_loss: tensor(0.2669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1002/4345]: training_loss: tensor(0.1630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1003/4345]: training_loss: tensor(0.1146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1004/4345]: training_loss: tensor(0.0541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1005/4345]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/4345]: training_loss: tensor(0.1984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/4345]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/4345]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/4345]: training_loss: tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/4345]: training_loss: tensor(0.3022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/4345]: training_loss: tensor(0.2296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/4345]: training_loss: tensor(0.2784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/4345]: training_loss: tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/4345]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/4345]: training_loss: tensor(0.1926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/4345]: training_loss: tensor(0.1623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/4345]: training_loss: tensor(0.1184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/4345]: training_loss: tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1019/4345]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/4345]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/4345]: training_loss: tensor(0.1447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/4345]: training_loss: tensor(0.2553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1023/4345]: training_loss: tensor(0.2967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/4345]: training_loss: tensor(0.1356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/4345]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/4345]: training_loss: tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/4345]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/4345]: training_loss: tensor(0.1059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/4345]: training_loss: tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/4345]: training_loss: tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/4345]: training_loss: tensor(0.0570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/4345]: training_loss: tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/4345]: training_loss: tensor(0.1577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/4345]: training_loss: tensor(0.3614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/4345]: training_loss: tensor(0.0534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/4345]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/4345]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/4345]: training_loss: tensor(0.1184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1040/4345]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/4345]: training_loss: tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/4345]: training_loss: tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1043/4345]: training_loss: tensor(0.2458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/4345]: training_loss: tensor(0.0676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/4345]: training_loss: tensor(0.4701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/4345]: training_loss: tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/4345]: training_loss: tensor(0.2493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/4345]: training_loss: tensor(0.1110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/4345]: training_loss: tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/4345]: training_loss: tensor(0.1876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/4345]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/4345]: training_loss: tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/4345]: training_loss: tensor(0.1145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/4345]: training_loss: tensor(0.4068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/4345]: training_loss: tensor(0.0507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/4345]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/4345]: training_loss: tensor(0.1162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/4345]: training_loss: tensor(0.1488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/4345]: training_loss: tensor(0.1395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/4345]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/4345]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/4345]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/4345]: training_loss: tensor(0.1756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/4345]: training_loss: tensor(0.1590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/4345]: training_loss: tensor(0.0404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/4345]: training_loss: tensor(0.2725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/4345]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/4345]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/4345]: training_loss: tensor(0.2029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/4345]: training_loss: tensor(0.1461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/4345]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/4345]: training_loss: tensor(0.1930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/4345]: training_loss: tensor(0.1326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/4345]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/4345]: training_loss: tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/4345]: training_loss: tensor(0.3327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/4345]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/4345]: training_loss: tensor(0.1899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/4345]: training_loss: tensor(0.2458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/4345]: training_loss: tensor(0.2280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/4345]: training_loss: tensor(0.3287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/4345]: training_loss: tensor(0.0502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/4345]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/4345]: training_loss: tensor(0.1453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/4345]: training_loss: tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/4345]: training_loss: tensor(0.2302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/4345]: training_loss: tensor(0.4387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/4345]: training_loss: tensor(0.3601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/4345]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/4345]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/4345]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/4345]: training_loss: tensor(0.1487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/4345]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/4345]: training_loss: tensor(0.7152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/4345]: training_loss: tensor(0.3368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/4345]: training_loss: tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/4345]: training_loss: tensor(0.2848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1100/4345]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/4345]: training_loss: tensor(0.3396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/4345]: training_loss: tensor(0.1570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/4345]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/4345]: training_loss: tensor(0.1996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/4345]: training_loss: tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/4345]: training_loss: tensor(0.1459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/4345]: training_loss: tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/4345]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/4345]: training_loss: tensor(0.1202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/4345]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/4345]: training_loss: tensor(0.1760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/4345]: training_loss: tensor(0.1510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/4345]: training_loss: tensor(0.2880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/4345]: training_loss: tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/4345]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/4345]: training_loss: tensor(0.1349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/4345]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/4345]: training_loss: tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/4345]: training_loss: tensor(0.2614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/4345]: training_loss: tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/4345]: training_loss: tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/4345]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/4345]: training_loss: tensor(0.2631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/4345]: training_loss: tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/4345]: training_loss: tensor(0.3341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/4345]: training_loss: tensor(0.1520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/4345]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1129/4345]: training_loss: tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/4345]: training_loss: tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/4345]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/4345]: training_loss: tensor(0.2592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/4345]: training_loss: tensor(0.5037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/4345]: training_loss: tensor(0.2345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/4345]: training_loss: tensor(0.1929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/4345]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/4345]: training_loss: tensor(0.1532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/4345]: training_loss: tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/4345]: training_loss: tensor(0.1425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/4345]: training_loss: tensor(0.1955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/4345]: training_loss: tensor(0.2206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/4345]: training_loss: tensor(0.3358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/4345]: training_loss: tensor(0.2509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/4345]: training_loss: tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/4345]: training_loss: tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/4345]: training_loss: tensor(0.1975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/4345]: training_loss: tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/4345]: training_loss: tensor(0.1320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/4345]: training_loss: tensor(0.0486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/4345]: training_loss: tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/4345]: training_loss: tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/4345]: training_loss: tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/4345]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/4345]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/4345]: training_loss: tensor(0.3002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/4345]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/4345]: training_loss: tensor(0.3794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/4345]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/4345]: training_loss: tensor(0.1733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/4345]: training_loss: tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/4345]: training_loss: tensor(0.4327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/4345]: training_loss: tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/4345]: training_loss: tensor(0.2849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/4345]: training_loss: tensor(0.1743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/4345]: training_loss: tensor(0.5387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/4345]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/4345]: training_loss: tensor(0.1352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/4345]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/4345]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/4345]: training_loss: tensor(0.1747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/4345]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/4345]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/4345]: training_loss: tensor(0.0404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/4345]: training_loss: tensor(0.1259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/4345]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/4345]: training_loss: tensor(0.1513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/4345]: training_loss: tensor(0.2314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/4345]: training_loss: tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/4345]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/4345]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/4345]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/4345]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/4345]: training_loss: tensor(0.1569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/4345]: training_loss: tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/4345]: training_loss: tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/4345]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/4345]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/4345]: training_loss: tensor(0.2668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/4345]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/4345]: training_loss: tensor(0.3136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/4345]: training_loss: tensor(0.1729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/4345]: training_loss: tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/4345]: training_loss: tensor(0.1641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/4345]: training_loss: tensor(0.0576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/4345]: training_loss: tensor(0.1909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/4345]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/4345]: training_loss: tensor(0.2236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/4345]: training_loss: tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/4345]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/4345]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/4345]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/4345]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/4345]: training_loss: tensor(0.1479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/4345]: training_loss: tensor(0.2503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/4345]: training_loss: tensor(0.1562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/4345]: training_loss: tensor(0.4435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/4345]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/4345]: training_loss: tensor(0.4568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/4345]: training_loss: tensor(0.2439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/4345]: training_loss: tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/4345]: training_loss: tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/4345]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1213/4345]: training_loss: tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/4345]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1215/4345]: training_loss: tensor(0.3178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/4345]: training_loss: tensor(0.1701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/4345]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/4345]: training_loss: tensor(0.5238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/4345]: training_loss: tensor(0.2304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/4345]: training_loss: tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/4345]: training_loss: tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/4345]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/4345]: training_loss: tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/4345]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/4345]: training_loss: tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/4345]: training_loss: tensor(0.1225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/4345]: training_loss: tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/4345]: training_loss: tensor(0.0446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/4345]: training_loss: tensor(0.1765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/4345]: training_loss: tensor(0.1424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/4345]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/4345]: training_loss: tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/4345]: training_loss: tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/4345]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/4345]: training_loss: tensor(0.2019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/4345]: training_loss: tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/4345]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/4345]: training_loss: tensor(0.1656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/4345]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/4345]: training_loss: tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/4345]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/4345]: training_loss: tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/4345]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/4345]: training_loss: tensor(0.2885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/4345]: training_loss: tensor(0.1471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/4345]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/4345]: training_loss: tensor(0.1581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/4345]: training_loss: tensor(0.2086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/4345]: training_loss: tensor(0.3902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/4345]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/4345]: training_loss: tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/4345]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/4345]: training_loss: tensor(0.1659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/4345]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/4345]: training_loss: tensor(0.1736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/4345]: training_loss: tensor(0.1350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/4345]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/4345]: training_loss: tensor(0.1282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/4345]: training_loss: tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/4345]: training_loss: tensor(0.4385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/4345]: training_loss: tensor(0.2381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/4345]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/4345]: training_loss: tensor(0.0586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/4345]: training_loss: tensor(0.4263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/4345]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/4345]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/4345]: training_loss: tensor(0.1745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/4345]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/4345]: training_loss: tensor(0.1935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/4345]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/4345]: training_loss: tensor(0.5128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/4345]: training_loss: tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/4345]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/4345]: training_loss: tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/4345]: training_loss: tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/4345]: training_loss: tensor(0.1773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/4345]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/4345]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/4345]: training_loss: tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/4345]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/4345]: training_loss: tensor(0.3696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/4345]: training_loss: tensor(0.1607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/4345]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/4345]: training_loss: tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/4345]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/4345]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/4345]: training_loss: tensor(0.1967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/4345]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/4345]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/4345]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/4345]: training_loss: tensor(0.3619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/4345]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/4345]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/4345]: training_loss: tensor(0.2871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/4345]: training_loss: tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/4345]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/4345]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1299/4345]: training_loss: tensor(0.4063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/4345]: training_loss: tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1301/4345]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/4345]: training_loss: tensor(0.2565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/4345]: training_loss: tensor(0.3020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/4345]: training_loss: tensor(0.2485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/4345]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/4345]: training_loss: tensor(0.4320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/4345]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/4345]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/4345]: training_loss: tensor(0.2030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/4345]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/4345]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/4345]: training_loss: tensor(0.2798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/4345]: training_loss: tensor(0.1216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/4345]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/4345]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/4345]: training_loss: tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/4345]: training_loss: tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/4345]: training_loss: tensor(0.1296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/4345]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/4345]: training_loss: tensor(0.1945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/4345]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/4345]: training_loss: tensor(0.4120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/4345]: training_loss: tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/4345]: training_loss: tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/4345]: training_loss: tensor(0.3692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/4345]: training_loss: tensor(0.2645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/4345]: training_loss: tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/4345]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/4345]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/4345]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/4345]: training_loss: tensor(0.4037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/4345]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/4345]: training_loss: tensor(0.1493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/4345]: training_loss: tensor(0.1204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/4345]: training_loss: tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/4345]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/4345]: training_loss: tensor(0.1623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/4345]: training_loss: tensor(0.2011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/4345]: training_loss: tensor(0.2249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/4345]: training_loss: tensor(0.2539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/4345]: training_loss: tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/4345]: training_loss: tensor(0.1820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/4345]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/4345]: training_loss: tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/4345]: training_loss: tensor(0.2543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/4345]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/4345]: training_loss: tensor(0.1650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/4345]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/4345]: training_loss: tensor(0.1441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/4345]: training_loss: tensor(0.2667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/4345]: training_loss: tensor(0.1362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/4345]: training_loss: tensor(0.4867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/4345]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/4345]: training_loss: tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/4345]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/4345]: training_loss: tensor(0.2698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/4345]: training_loss: tensor(0.1629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/4345]: training_loss: tensor(0.3342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/4345]: training_loss: tensor(0.1165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/4345]: training_loss: tensor(0.1449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1361/4345]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/4345]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1363/4345]: training_loss: tensor(0.1101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/4345]: training_loss: tensor(0.2957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/4345]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/4345]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/4345]: training_loss: tensor(0.3118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/4345]: training_loss: tensor(0.1782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/4345]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/4345]: training_loss: tensor(0.1282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/4345]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/4345]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/4345]: training_loss: tensor(0.3891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/4345]: training_loss: tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/4345]: training_loss: tensor(0.1716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/4345]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/4345]: training_loss: tensor(0.3085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/4345]: training_loss: tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/4345]: training_loss: tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/4345]: training_loss: tensor(0.2079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/4345]: training_loss: tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/4345]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/4345]: training_loss: tensor(0.1425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/4345]: training_loss: tensor(0.1948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1385/4345]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1386/4345]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1387/4345]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1388/4345]: training_loss: tensor(0.3389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1389/4345]: training_loss: tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1390/4345]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1391/4345]: training_loss: tensor(0.1616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1392/4345]: training_loss: tensor(0.3779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1393/4345]: training_loss: tensor(0.1619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1394/4345]: training_loss: tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1395/4345]: training_loss: tensor(0.3509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1396/4345]: training_loss: tensor(0.0545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1397/4345]: training_loss: tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1398/4345]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1399/4345]: training_loss: tensor(0.2257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1400/4345]: training_loss: tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1401/4345]: training_loss: tensor(0.1951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1402/4345]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1403/4345]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1404/4345]: training_loss: tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1405/4345]: training_loss: tensor(0.3111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1406/4345]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1407/4345]: training_loss: tensor(0.1939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1408/4345]: training_loss: tensor(0.0410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1409/4345]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1410/4345]: training_loss: tensor(0.1072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1411/4345]: training_loss: tensor(0.1439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1412/4345]: training_loss: tensor(0.1366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1413/4345]: training_loss: tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1414/4345]: training_loss: tensor(0.1220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1415/4345]: training_loss: tensor(0.1635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1416/4345]: training_loss: tensor(0.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1417/4345]: training_loss: tensor(0.4259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1418/4345]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1419/4345]: training_loss: tensor(0.0493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1420/4345]: training_loss: tensor(0.2805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1421/4345]: training_loss: tensor(0.2081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1422/4345]: training_loss: tensor(0.2521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1423/4345]: training_loss: tensor(0.1792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1424/4345]: training_loss: tensor(0.1568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1425/4345]: training_loss: tensor(0.1798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1426/4345]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1427/4345]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1428/4345]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1429/4345]: training_loss: tensor(0.1527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1430/4345]: training_loss: tensor(0.1408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1431/4345]: training_loss: tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1432/4345]: training_loss: tensor(0.3261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1433/4345]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1434/4345]: training_loss: tensor(0.1753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1435/4345]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1436/4345]: training_loss: tensor(0.1706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1437/4345]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1438/4345]: training_loss: tensor(0.1592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1439/4345]: training_loss: tensor(0.2464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1440/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1441/4345]: training_loss: tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1442/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1443/4345]: training_loss: tensor(0.2932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1444/4345]: training_loss: tensor(0.2148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1445/4345]: training_loss: tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1446/4345]: training_loss: tensor(0.2849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1447/4345]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1448/4345]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1449/4345]: training_loss: tensor(0.1614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1450/4345]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1451/4345]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1452/4345]: training_loss: tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1453/4345]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1454/4345]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1455/4345]: training_loss: tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1456/4345]: training_loss: tensor(0.2340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1457/4345]: training_loss: tensor(0.2407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1458/4345]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1459/4345]: training_loss: tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1460/4345]: training_loss: tensor(0.0410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1461/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1462/4345]: training_loss: tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1463/4345]: training_loss: tensor(0.2708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1464/4345]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1465/4345]: training_loss: tensor(0.3649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1466/4345]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1467/4345]: training_loss: tensor(0.3452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1468/4345]: training_loss: tensor(0.1524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1469/4345]: training_loss: tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1470/4345]: training_loss: tensor(0.2611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1471/4345]: training_loss: tensor(0.2068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1472/4345]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1473/4345]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1474/4345]: training_loss: tensor(0.1737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1475/4345]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1476/4345]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1477/4345]: training_loss: tensor(0.2062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1478/4345]: training_loss: tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1479/4345]: training_loss: tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1480/4345]: training_loss: tensor(0.1429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1481/4345]: training_loss: tensor(0.1255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1482/4345]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1483/4345]: training_loss: tensor(0.1178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1484/4345]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1485/4345]: training_loss: tensor(0.3598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1486/4345]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1487/4345]: training_loss: tensor(0.2129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1488/4345]: training_loss: tensor(0.2792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1489/4345]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1490/4345]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1491/4345]: training_loss: tensor(0.1653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1492/4345]: training_loss: tensor(0.1713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1493/4345]: training_loss: tensor(0.1616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1494/4345]: training_loss: tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1495/4345]: training_loss: tensor(0.1616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1496/4345]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1497/4345]: training_loss: tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1498/4345]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1499/4345]: training_loss: tensor(0.4192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1500/4345]: training_loss: tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1501/4345]: training_loss: tensor(0.1213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1502/4345]: training_loss: tensor(0.4749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1503/4345]: training_loss: tensor(0.0572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1504/4345]: training_loss: tensor(0.2591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1505/4345]: training_loss: tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1506/4345]: training_loss: tensor(0.1503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1507/4345]: training_loss: tensor(0.1277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1508/4345]: training_loss: tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1509/4345]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1510/4345]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1511/4345]: training_loss: tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1512/4345]: training_loss: tensor(0.2846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1513/4345]: training_loss: tensor(0.6718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1514/4345]: training_loss: tensor(0.0459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1515/4345]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1516/4345]: training_loss: tensor(0.1238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1517/4345]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1518/4345]: training_loss: tensor(0.2096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1519/4345]: training_loss: tensor(0.0545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1520/4345]: training_loss: tensor(0.0475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1521/4345]: training_loss: tensor(0.1648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1522/4345]: training_loss: tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1523/4345]: training_loss: tensor(0.2445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1524/4345]: training_loss: tensor(0.1334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1525/4345]: training_loss: tensor(0.1491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1526/4345]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1527/4345]: training_loss: tensor(0.1522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1528/4345]: training_loss: tensor(0.0586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1529/4345]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1530/4345]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1531/4345]: training_loss: tensor(0.1150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1532/4345]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1533/4345]: training_loss: tensor(0.1201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1534/4345]: training_loss: tensor(0.3314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1535/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1536/4345]: training_loss: tensor(0.1875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1537/4345]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1538/4345]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1539/4345]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1540/4345]: training_loss: tensor(0.1211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1541/4345]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1542/4345]: training_loss: tensor(0.4553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1543/4345]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1544/4345]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1545/4345]: training_loss: tensor(0.0616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1546/4345]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1547/4345]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1548/4345]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1549/4345]: training_loss: tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1550/4345]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1551/4345]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1552/4345]: training_loss: tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1553/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1554/4345]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1555/4345]: training_loss: tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1556/4345]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1557/4345]: training_loss: tensor(0.4168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1558/4345]: training_loss: tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1559/4345]: training_loss: tensor(0.2721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1560/4345]: training_loss: tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1561/4345]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1562/4345]: training_loss: tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1563/4345]: training_loss: tensor(0.1853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1564/4345]: training_loss: tensor(0.1635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1565/4345]: training_loss: tensor(0.2122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1566/4345]: training_loss: tensor(0.2306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1567/4345]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1568/4345]: training_loss: tensor(0.1272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1569/4345]: training_loss: tensor(0.3544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1570/4345]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1571/4345]: training_loss: tensor(0.1867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1572/4345]: training_loss: tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1573/4345]: training_loss: tensor(0.1362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1574/4345]: training_loss: tensor(0.1868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1575/4345]: training_loss: tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1576/4345]: training_loss: tensor(0.2185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1577/4345]: training_loss: tensor(0.1450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1578/4345]: training_loss: tensor(0.1901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1579/4345]: training_loss: tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1580/4345]: training_loss: tensor(0.1771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1581/4345]: training_loss: tensor(0.1460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1582/4345]: training_loss: tensor(0.0616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1583/4345]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1584/4345]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1585/4345]: training_loss: tensor(0.1137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1586/4345]: training_loss: tensor(0.2091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1587/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1588/4345]: training_loss: tensor(0.4945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1589/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1590/4345]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1591/4345]: training_loss: tensor(0.0439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1592/4345]: training_loss: tensor(0.3340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1593/4345]: training_loss: tensor(0.2934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1594/4345]: training_loss: tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1595/4345]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1596/4345]: training_loss: tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1597/4345]: training_loss: tensor(0.1460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1598/4345]: training_loss: tensor(0.1798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1599/4345]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1600/4345]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1601/4345]: training_loss: tensor(0.4375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1602/4345]: training_loss: tensor(0.1837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1603/4345]: training_loss: tensor(0.2766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1604/4345]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1605/4345]: training_loss: tensor(0.0615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1606/4345]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1607/4345]: training_loss: tensor(0.1939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1608/4345]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1609/4345]: training_loss: tensor(0.1432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1610/4345]: training_loss: tensor(0.1795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1611/4345]: training_loss: tensor(0.1874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1612/4345]: training_loss: tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1613/4345]: training_loss: tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1614/4345]: training_loss: tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1615/4345]: training_loss: tensor(0.2155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1616/4345]: training_loss: tensor(0.1388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1617/4345]: training_loss: tensor(0.3151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1618/4345]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1619/4345]: training_loss: tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1620/4345]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1621/4345]: training_loss: tensor(0.0635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1622/4345]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1623/4345]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1624/4345]: training_loss: tensor(0.3201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1625/4345]: training_loss: tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1626/4345]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1627/4345]: training_loss: tensor(0.3428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1628/4345]: training_loss: tensor(0.1516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1629/4345]: training_loss: tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1630/4345]: training_loss: tensor(0.2125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1631/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1632/4345]: training_loss: tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1633/4345]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1634/4345]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1635/4345]: training_loss: tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1636/4345]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1637/4345]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1638/4345]: training_loss: tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1639/4345]: training_loss: tensor(0.2275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1640/4345]: training_loss: tensor(0.0290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1641/4345]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1642/4345]: training_loss: tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1643/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1644/4345]: training_loss: tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1645/4345]: training_loss: tensor(0.3300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1646/4345]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1647/4345]: training_loss: tensor(0.1850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1648/4345]: training_loss: tensor(0.3750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1649/4345]: training_loss: tensor(0.2685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1650/4345]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1651/4345]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1652/4345]: training_loss: tensor(0.1144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1653/4345]: training_loss: tensor(0.3736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1654/4345]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1655/4345]: training_loss: tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1656/4345]: training_loss: tensor(0.1274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1657/4345]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1658/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1659/4345]: training_loss: tensor(0.1715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1660/4345]: training_loss: tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1661/4345]: training_loss: tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1662/4345]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1663/4345]: training_loss: tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1664/4345]: training_loss: tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1665/4345]: training_loss: tensor(0.2189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1666/4345]: training_loss: tensor(0.1384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1667/4345]: training_loss: tensor(0.1451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1668/4345]: training_loss: tensor(0.1969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1669/4345]: training_loss: tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1670/4345]: training_loss: tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1671/4345]: training_loss: tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1672/4345]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1673/4345]: training_loss: tensor(0.1348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1674/4345]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1675/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1676/4345]: training_loss: tensor(0.6127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1677/4345]: training_loss: tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1678/4345]: training_loss: tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1679/4345]: training_loss: tensor(0.0480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1680/4345]: training_loss: tensor(0.3225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1681/4345]: training_loss: tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1682/4345]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1683/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1684/4345]: training_loss: tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1685/4345]: training_loss: tensor(0.2553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1686/4345]: training_loss: tensor(0.4688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1687/4345]: training_loss: tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1688/4345]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1689/4345]: training_loss: tensor(0.1359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1690/4345]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1691/4345]: training_loss: tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1692/4345]: training_loss: tensor(0.1732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1693/4345]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1694/4345]: training_loss: tensor(0.4408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1695/4345]: training_loss: tensor(0.1467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1696/4345]: training_loss: tensor(0.1592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1697/4345]: training_loss: tensor(0.1790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1698/4345]: training_loss: tensor(0.1044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1699/4345]: training_loss: tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1700/4345]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1701/4345]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1702/4345]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1703/4345]: training_loss: tensor(0.0364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1704/4345]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1705/4345]: training_loss: tensor(0.1233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1706/4345]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1707/4345]: training_loss: tensor(0.2951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1708/4345]: training_loss: tensor(0.3541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1709/4345]: training_loss: tensor(0.3359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1710/4345]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1711/4345]: training_loss: tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1712/4345]: training_loss: tensor(0.6026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1713/4345]: training_loss: tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1714/4345]: training_loss: tensor(0.1078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1715/4345]: training_loss: tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1716/4345]: training_loss: tensor(0.1282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1717/4345]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1718/4345]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1719/4345]: training_loss: tensor(0.2849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1720/4345]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1721/4345]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1722/4345]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1723/4345]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1724/4345]: training_loss: tensor(0.1982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1725/4345]: training_loss: tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1726/4345]: training_loss: tensor(0.1599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1727/4345]: training_loss: tensor(0.1318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1728/4345]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1729/4345]: training_loss: tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1730/4345]: training_loss: tensor(0.1556, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1731/4345]: training_loss: tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1732/4345]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1733/4345]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1734/4345]: training_loss: tensor(0.1090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1735/4345]: training_loss: tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1736/4345]: training_loss: tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1737/4345]: training_loss: tensor(0.1230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1738/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1739/4345]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1740/4345]: training_loss: tensor(0.1165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1741/4345]: training_loss: tensor(0.2857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1742/4345]: training_loss: tensor(0.4642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1743/4345]: training_loss: tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1744/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1745/4345]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1746/4345]: training_loss: tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1747/4345]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1748/4345]: training_loss: tensor(0.1703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1749/4345]: training_loss: tensor(0.2302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1750/4345]: training_loss: tensor(0.2113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1751/4345]: training_loss: tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1752/4345]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1753/4345]: training_loss: tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1754/4345]: training_loss: tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1755/4345]: training_loss: tensor(0.1465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1756/4345]: training_loss: tensor(0.1198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1757/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1758/4345]: training_loss: tensor(0.2805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1759/4345]: training_loss: tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1760/4345]: training_loss: tensor(0.2574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1761/4345]: training_loss: tensor(0.2623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1762/4345]: training_loss: tensor(0.2007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1763/4345]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1764/4345]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1765/4345]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1766/4345]: training_loss: tensor(0.1290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1767/4345]: training_loss: tensor(0.1820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1768/4345]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1769/4345]: training_loss: tensor(0.0576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1770/4345]: training_loss: tensor(0.1377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1771/4345]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1772/4345]: training_loss: tensor(0.2686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1773/4345]: training_loss: tensor(0.2346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1774/4345]: training_loss: tensor(0.4225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1775/4345]: training_loss: tensor(0.3817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1776/4345]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1777/4345]: training_loss: tensor(0.1575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1778/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1779/4345]: training_loss: tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1780/4345]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1781/4345]: training_loss: tensor(0.1164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1782/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1783/4345]: training_loss: tensor(0.1809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1784/4345]: training_loss: tensor(0.2023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1785/4345]: training_loss: tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1786/4345]: training_loss: tensor(0.3714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1787/4345]: training_loss: tensor(0.2302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1788/4345]: training_loss: tensor(0.1978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1789/4345]: training_loss: tensor(0.1165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1790/4345]: training_loss: tensor(0.1435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1791/4345]: training_loss: tensor(0.1325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1792/4345]: training_loss: tensor(0.1889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1793/4345]: training_loss: tensor(0.0534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1794/4345]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1795/4345]: training_loss: tensor(0.3761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1796/4345]: training_loss: tensor(0.2613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1797/4345]: training_loss: tensor(0.1356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1798/4345]: training_loss: tensor(0.1532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1799/4345]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1800/4345]: training_loss: tensor(0.1155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1801/4345]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1802/4345]: training_loss: tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1803/4345]: training_loss: tensor(0.2580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1804/4345]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1805/4345]: training_loss: tensor(0.0476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1806/4345]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1807/4345]: training_loss: tensor(0.1535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1808/4345]: training_loss: tensor(0.3330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1809/4345]: training_loss: tensor(0.1605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1810/4345]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1811/4345]: training_loss: tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1812/4345]: training_loss: tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1813/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1814/4345]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1815/4345]: training_loss: tensor(0.1944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1816/4345]: training_loss: tensor(0.1236, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1817/4345]: training_loss: tensor(0.1146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1818/4345]: training_loss: tensor(0.1298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1819/4345]: training_loss: tensor(0.3436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1820/4345]: training_loss: tensor(0.1466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1821/4345]: training_loss: tensor(0.3919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1822/4345]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1823/4345]: training_loss: tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1824/4345]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1825/4345]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1826/4345]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1827/4345]: training_loss: tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1828/4345]: training_loss: tensor(0.1454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1829/4345]: training_loss: tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1830/4345]: training_loss: tensor(0.1391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1831/4345]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1832/4345]: training_loss: tensor(0.0645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1833/4345]: training_loss: tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1834/4345]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1835/4345]: training_loss: tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1836/4345]: training_loss: tensor(0.1442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1837/4345]: training_loss: tensor(0.1417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1838/4345]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1839/4345]: training_loss: tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1840/4345]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1841/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1842/4345]: training_loss: tensor(0.1747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1843/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1844/4345]: training_loss: tensor(0.1540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1845/4345]: training_loss: tensor(0.1145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1846/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1847/4345]: training_loss: tensor(0.1581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1848/4345]: training_loss: tensor(0.5916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1849/4345]: training_loss: tensor(0.1398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1850/4345]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1851/4345]: training_loss: tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1852/4345]: training_loss: tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1853/4345]: training_loss: tensor(0.1495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1854/4345]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1855/4345]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1856/4345]: training_loss: tensor(0.3254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1857/4345]: training_loss: tensor(0.2502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1858/4345]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1859/4345]: training_loss: tensor(0.2338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1860/4345]: training_loss: tensor(0.2154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1861/4345]: training_loss: tensor(0.2860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1862/4345]: training_loss: tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1863/4345]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1864/4345]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1865/4345]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1866/4345]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1867/4345]: training_loss: tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1868/4345]: training_loss: tensor(0.1504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1869/4345]: training_loss: tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1870/4345]: training_loss: tensor(0.1860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1871/4345]: training_loss: tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1872/4345]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1873/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1874/4345]: training_loss: tensor(0.4317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1875/4345]: training_loss: tensor(0.1322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1876/4345]: training_loss: tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1877/4345]: training_loss: tensor(0.3180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1878/4345]: training_loss: tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1879/4345]: training_loss: tensor(0.3475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1880/4345]: training_loss: tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1881/4345]: training_loss: tensor(0.3299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1882/4345]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1883/4345]: training_loss: tensor(0.1515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1884/4345]: training_loss: tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1885/4345]: training_loss: tensor(0.2044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1886/4345]: training_loss: tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1887/4345]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1888/4345]: training_loss: tensor(0.1142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1889/4345]: training_loss: tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1890/4345]: training_loss: tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1891/4345]: training_loss: tensor(0.3061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1892/4345]: training_loss: tensor(0.1912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1893/4345]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1894/4345]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1895/4345]: training_loss: tensor(0.1535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1896/4345]: training_loss: tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1897/4345]: training_loss: tensor(0.1639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1898/4345]: training_loss: tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1899/4345]: training_loss: tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1900/4345]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1901/4345]: training_loss: tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1902/4345]: training_loss: tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1903/4345]: training_loss: tensor(0.2190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1904/4345]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1905/4345]: training_loss: tensor(0.1132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1906/4345]: training_loss: tensor(0.3193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1907/4345]: training_loss: tensor(0.2010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1908/4345]: training_loss: tensor(0.1133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1909/4345]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1910/4345]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1911/4345]: training_loss: tensor(0.1795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1912/4345]: training_loss: tensor(0.1684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1913/4345]: training_loss: tensor(0.2863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1914/4345]: training_loss: tensor(0.1382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1915/4345]: training_loss: tensor(0.2485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1916/4345]: training_loss: tensor(0.2326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1917/4345]: training_loss: tensor(0.2179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1918/4345]: training_loss: tensor(0.0365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1919/4345]: training_loss: tensor(0.3029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1920/4345]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1921/4345]: training_loss: tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1922/4345]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1923/4345]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1924/4345]: training_loss: tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1925/4345]: training_loss: tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1926/4345]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1927/4345]: training_loss: tensor(0.2397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1928/4345]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1929/4345]: training_loss: tensor(0.2184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1930/4345]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1931/4345]: training_loss: tensor(0.4829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1932/4345]: training_loss: tensor(0.2955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1933/4345]: training_loss: tensor(0.1711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1934/4345]: training_loss: tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1935/4345]: training_loss: tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1936/4345]: training_loss: tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1937/4345]: training_loss: tensor(0.3068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1938/4345]: training_loss: tensor(0.3855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1939/4345]: training_loss: tensor(0.0480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1940/4345]: training_loss: tensor(0.1218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1941/4345]: training_loss: tensor(0.2095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1942/4345]: training_loss: tensor(0.1434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1943/4345]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1944/4345]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1945/4345]: training_loss: tensor(0.2926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1946/4345]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1947/4345]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1948/4345]: training_loss: tensor(0.1444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1949/4345]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1950/4345]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1951/4345]: training_loss: tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1952/4345]: training_loss: tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1953/4345]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1954/4345]: training_loss: tensor(0.1274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1955/4345]: training_loss: tensor(0.2445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1956/4345]: training_loss: tensor(0.1167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1957/4345]: training_loss: tensor(0.2166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1958/4345]: training_loss: tensor(0.1219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1959/4345]: training_loss: tensor(0.1076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1960/4345]: training_loss: tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1961/4345]: training_loss: tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1962/4345]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1963/4345]: training_loss: tensor(0.5704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1964/4345]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1965/4345]: training_loss: tensor(0.2867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1966/4345]: training_loss: tensor(0.1265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1967/4345]: training_loss: tensor(0.2548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1968/4345]: training_loss: tensor(0.1369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1969/4345]: training_loss: tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1970/4345]: training_loss: tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1971/4345]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1972/4345]: training_loss: tensor(0.4164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1973/4345]: training_loss: tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1974/4345]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1975/4345]: training_loss: tensor(0.1206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1976/4345]: training_loss: tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1977/4345]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1978/4345]: training_loss: tensor(0.1863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1979/4345]: training_loss: tensor(0.2659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1980/4345]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1981/4345]: training_loss: tensor(0.1105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1982/4345]: training_loss: tensor(0.4307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1983/4345]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1984/4345]: training_loss: tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1985/4345]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1986/4345]: training_loss: tensor(0.1190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1987/4345]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1988/4345]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1989/4345]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1990/4345]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1991/4345]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1992/4345]: training_loss: tensor(0.2245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1993/4345]: training_loss: tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1994/4345]: training_loss: tensor(0.2827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1995/4345]: training_loss: tensor(0.1131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1996/4345]: training_loss: tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1997/4345]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1998/4345]: training_loss: tensor(0.4713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1999/4345]: training_loss: tensor(0.3675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2000/4345]: training_loss: tensor(0.1376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2001/4345]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2002/4345]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2003/4345]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2004/4345]: training_loss: tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2005/4345]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2006/4345]: training_loss: tensor(0.2073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2007/4345]: training_loss: tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2008/4345]: training_loss: tensor(0.2792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2009/4345]: training_loss: tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2010/4345]: training_loss: tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2011/4345]: training_loss: tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2012/4345]: training_loss: tensor(0.3012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2013/4345]: training_loss: tensor(0.3820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2014/4345]: training_loss: tensor(0.1195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2015/4345]: training_loss: tensor(0.1974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2016/4345]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2017/4345]: training_loss: tensor(0.0469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2018/4345]: training_loss: tensor(0.5445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2019/4345]: training_loss: tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2020/4345]: training_loss: tensor(0.1665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2021/4345]: training_loss: tensor(0.3227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2022/4345]: training_loss: tensor(0.2075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2023/4345]: training_loss: tensor(0.2302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2024/4345]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2025/4345]: training_loss: tensor(0.2791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2026/4345]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2027/4345]: training_loss: tensor(0.2139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2028/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2029/4345]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2030/4345]: training_loss: tensor(0.1918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2031/4345]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2032/4345]: training_loss: tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2033/4345]: training_loss: tensor(0.1533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2034/4345]: training_loss: tensor(0.1459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2035/4345]: training_loss: tensor(0.2743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2036/4345]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2037/4345]: training_loss: tensor(0.1621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2038/4345]: training_loss: tensor(0.1634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2039/4345]: training_loss: tensor(0.1845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2040/4345]: training_loss: tensor(0.0462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2041/4345]: training_loss: tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2042/4345]: training_loss: tensor(0.1077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2043/4345]: training_loss: tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2044/4345]: training_loss: tensor(0.1679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2045/4345]: training_loss: tensor(0.2868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2046/4345]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2047/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2048/4345]: training_loss: tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2049/4345]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2050/4345]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2051/4345]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2052/4345]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2053/4345]: training_loss: tensor(0.1355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2054/4345]: training_loss: tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2055/4345]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2056/4345]: training_loss: tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2057/4345]: training_loss: tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2058/4345]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2059/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2060/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2061/4345]: training_loss: tensor(0.2377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2062/4345]: training_loss: tensor(0.1241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2063/4345]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2064/4345]: training_loss: tensor(0.2572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2065/4345]: training_loss: tensor(0.0547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2066/4345]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2067/4345]: training_loss: tensor(0.2599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2068/4345]: training_loss: tensor(0.2056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2069/4345]: training_loss: tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2070/4345]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2071/4345]: training_loss: tensor(0.2841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2072/4345]: training_loss: tensor(0.2416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2073/4345]: training_loss: tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2074/4345]: training_loss: tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2075/4345]: training_loss: tensor(0.1118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2076/4345]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2077/4345]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2078/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2079/4345]: training_loss: tensor(0.1500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2080/4345]: training_loss: tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2081/4345]: training_loss: tensor(0.1297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2082/4345]: training_loss: tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2083/4345]: training_loss: tensor(0.3116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2084/4345]: training_loss: tensor(0.3105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2085/4345]: training_loss: tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2086/4345]: training_loss: tensor(0.1871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2087/4345]: training_loss: tensor(0.5196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2088/4345]: training_loss: tensor(0.1192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2089/4345]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2090/4345]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2091/4345]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2092/4345]: training_loss: tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2093/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2094/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2095/4345]: training_loss: tensor(0.1355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2096/4345]: training_loss: tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2097/4345]: training_loss: tensor(0.2917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2098/4345]: training_loss: tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2099/4345]: training_loss: tensor(0.2441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2100/4345]: training_loss: tensor(0.3261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2101/4345]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2102/4345]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2103/4345]: training_loss: tensor(0.1811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2104/4345]: training_loss: tensor(0.2153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2105/4345]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2106/4345]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2107/4345]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2108/4345]: training_loss: tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2109/4345]: training_loss: tensor(0.1430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2110/4345]: training_loss: tensor(0.2233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2111/4345]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2112/4345]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2113/4345]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2114/4345]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2115/4345]: training_loss: tensor(0.1547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2116/4345]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2117/4345]: training_loss: tensor(0.2162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2118/4345]: training_loss: tensor(0.2227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2119/4345]: training_loss: tensor(0.1836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2120/4345]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2121/4345]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2122/4345]: training_loss: tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2123/4345]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2124/4345]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2125/4345]: training_loss: tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2126/4345]: training_loss: tensor(0.1480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2127/4345]: training_loss: tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2128/4345]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2129/4345]: training_loss: tensor(0.1442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2130/4345]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2131/4345]: training_loss: tensor(0.1856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2132/4345]: training_loss: tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2133/4345]: training_loss: tensor(0.1168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2134/4345]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2135/4345]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2136/4345]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2137/4345]: training_loss: tensor(0.1627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2138/4345]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2139/4345]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2140/4345]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2141/4345]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2142/4345]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2143/4345]: training_loss: tensor(0.1173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2144/4345]: training_loss: tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2145/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2146/4345]: training_loss: tensor(0.2418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2147/4345]: training_loss: tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2148/4345]: training_loss: tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2149/4345]: training_loss: tensor(0.1575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2150/4345]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2151/4345]: training_loss: tensor(0.1340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2152/4345]: training_loss: tensor(0.1969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2153/4345]: training_loss: tensor(0.1953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2154/4345]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2155/4345]: training_loss: tensor(0.1994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2156/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2157/4345]: training_loss: tensor(0.1586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2158/4345]: training_loss: tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2159/4345]: training_loss: tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2160/4345]: training_loss: tensor(0.1259, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2161/4345]: training_loss: tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2162/4345]: training_loss: tensor(0.1295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2163/4345]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2164/4345]: training_loss: tensor(0.4004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2165/4345]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2166/4345]: training_loss: tensor(0.1238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2167/4345]: training_loss: tensor(0.2510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2168/4345]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2169/4345]: training_loss: tensor(0.1452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2170/4345]: training_loss: tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2171/4345]: training_loss: tensor(0.2121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2172/4345]: training_loss: tensor(0.1482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2173/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [1/5], Step [2173/21730], Train Loss: 0.1621, Valid Loss: 0.1366\n",
      "Model saved to ==> Model/model.pt\n",
      "Model saved to ==> Model/metrics.pt\n",
      "batch_no [2174/4345]: training_loss: tensor(0.1701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2175/4345]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2176/4345]: training_loss: tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2177/4345]: training_loss: tensor(0.1147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2178/4345]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2179/4345]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2180/4345]: training_loss: tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2181/4345]: training_loss: tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2182/4345]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2183/4345]: training_loss: tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2184/4345]: training_loss: tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2185/4345]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2186/4345]: training_loss: tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2187/4345]: training_loss: tensor(0.1211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2188/4345]: training_loss: tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2189/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2190/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2191/4345]: training_loss: tensor(0.4322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2192/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2193/4345]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2194/4345]: training_loss: tensor(0.1078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2195/4345]: training_loss: tensor(0.2876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2196/4345]: training_loss: tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2197/4345]: training_loss: tensor(0.8913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2198/4345]: training_loss: tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2199/4345]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2200/4345]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2201/4345]: training_loss: tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2202/4345]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2203/4345]: training_loss: tensor(0.1965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2204/4345]: training_loss: tensor(0.3453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2205/4345]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2206/4345]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2207/4345]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2208/4345]: training_loss: tensor(0.3720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2209/4345]: training_loss: tensor(0.1659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2210/4345]: training_loss: tensor(0.2945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2211/4345]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2212/4345]: training_loss: tensor(0.1994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2213/4345]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2214/4345]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2215/4345]: training_loss: tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2216/4345]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2217/4345]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2218/4345]: training_loss: tensor(0.2666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2219/4345]: training_loss: tensor(0.1242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2220/4345]: training_loss: tensor(0.1924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2221/4345]: training_loss: tensor(0.2640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2222/4345]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2223/4345]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2224/4345]: training_loss: tensor(0.1076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2225/4345]: training_loss: tensor(0.4458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2226/4345]: training_loss: tensor(0.1751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2227/4345]: training_loss: tensor(0.2349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2228/4345]: training_loss: tensor(0.0365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2229/4345]: training_loss: tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2230/4345]: training_loss: tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2231/4345]: training_loss: tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2232/4345]: training_loss: tensor(0.1434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2233/4345]: training_loss: tensor(0.2325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2234/4345]: training_loss: tensor(0.1815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2235/4345]: training_loss: tensor(0.2399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2236/4345]: training_loss: tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2237/4345]: training_loss: tensor(0.2476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2238/4345]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2239/4345]: training_loss: tensor(0.1127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2240/4345]: training_loss: tensor(0.2046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2241/4345]: training_loss: tensor(0.0384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2242/4345]: training_loss: tensor(0.1772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2243/4345]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2244/4345]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2245/4345]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2246/4345]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2247/4345]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2248/4345]: training_loss: tensor(0.1291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2249/4345]: training_loss: tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2250/4345]: training_loss: tensor(0.0635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2251/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2252/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2253/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2254/4345]: training_loss: tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2255/4345]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2256/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2257/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2258/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2259/4345]: training_loss: tensor(0.5211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2260/4345]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2261/4345]: training_loss: tensor(0.2847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2262/4345]: training_loss: tensor(0.1720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2263/4345]: training_loss: tensor(0.1424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2264/4345]: training_loss: tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2265/4345]: training_loss: tensor(0.1429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2266/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2267/4345]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2268/4345]: training_loss: tensor(0.2289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2269/4345]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2270/4345]: training_loss: tensor(0.1732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2271/4345]: training_loss: tensor(0.2310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2272/4345]: training_loss: tensor(0.2490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2273/4345]: training_loss: tensor(0.1780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2274/4345]: training_loss: tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2275/4345]: training_loss: tensor(0.1591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2276/4345]: training_loss: tensor(0.1945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2277/4345]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2278/4345]: training_loss: tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2279/4345]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2280/4345]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2281/4345]: training_loss: tensor(0.1211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2282/4345]: training_loss: tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2283/4345]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2284/4345]: training_loss: tensor(0.2938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2285/4345]: training_loss: tensor(0.1659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2286/4345]: training_loss: tensor(0.3681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2287/4345]: training_loss: tensor(0.1576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2288/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2289/4345]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2290/4345]: training_loss: tensor(0.1653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2291/4345]: training_loss: tensor(0.1751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2292/4345]: training_loss: tensor(0.1660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2293/4345]: training_loss: tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2294/4345]: training_loss: tensor(0.2090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2295/4345]: training_loss: tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2296/4345]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2297/4345]: training_loss: tensor(0.1869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2298/4345]: training_loss: tensor(0.2995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2299/4345]: training_loss: tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2300/4345]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2301/4345]: training_loss: tensor(0.2082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2302/4345]: training_loss: tensor(0.1911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2303/4345]: training_loss: tensor(0.1414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2304/4345]: training_loss: tensor(0.1195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2305/4345]: training_loss: tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2306/4345]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2307/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2308/4345]: training_loss: tensor(0.1182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2309/4345]: training_loss: tensor(0.1078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2310/4345]: training_loss: tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2311/4345]: training_loss: tensor(0.2366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2312/4345]: training_loss: tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2313/4345]: training_loss: tensor(0.2259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2314/4345]: training_loss: tensor(0.3077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2315/4345]: training_loss: tensor(0.2094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2316/4345]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2317/4345]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2318/4345]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2319/4345]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2320/4345]: training_loss: tensor(0.2201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2321/4345]: training_loss: tensor(0.1121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2322/4345]: training_loss: tensor(0.2296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2323/4345]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2324/4345]: training_loss: tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2325/4345]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2326/4345]: training_loss: tensor(0.1130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2327/4345]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2328/4345]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2329/4345]: training_loss: tensor(0.2117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2330/4345]: training_loss: tensor(0.1756, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2331/4345]: training_loss: tensor(0.3384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2332/4345]: training_loss: tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2333/4345]: training_loss: tensor(0.2860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2334/4345]: training_loss: tensor(0.4172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2335/4345]: training_loss: tensor(0.4578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2336/4345]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2337/4345]: training_loss: tensor(0.2090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2338/4345]: training_loss: tensor(0.1116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2339/4345]: training_loss: tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2340/4345]: training_loss: tensor(0.1439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2341/4345]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2342/4345]: training_loss: tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2343/4345]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2344/4345]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2345/4345]: training_loss: tensor(0.1731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2346/4345]: training_loss: tensor(0.2826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2347/4345]: training_loss: tensor(0.2475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2348/4345]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2349/4345]: training_loss: tensor(0.2409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2350/4345]: training_loss: tensor(0.1334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2351/4345]: training_loss: tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2352/4345]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2353/4345]: training_loss: tensor(0.0354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2354/4345]: training_loss: tensor(0.1487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2355/4345]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2356/4345]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2357/4345]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2358/4345]: training_loss: tensor(0.1192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2359/4345]: training_loss: tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2360/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2361/4345]: training_loss: tensor(0.4236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2362/4345]: training_loss: tensor(0.1305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2363/4345]: training_loss: tensor(0.2122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2364/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2365/4345]: training_loss: tensor(0.1556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2366/4345]: training_loss: tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2367/4345]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2368/4345]: training_loss: tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2369/4345]: training_loss: tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2370/4345]: training_loss: tensor(0.1277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2371/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2372/4345]: training_loss: tensor(0.1282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2373/4345]: training_loss: tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2374/4345]: training_loss: tensor(0.0507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2375/4345]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2376/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2377/4345]: training_loss: tensor(0.1506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2378/4345]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2379/4345]: training_loss: tensor(0.2908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2380/4345]: training_loss: tensor(0.1711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2381/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2382/4345]: training_loss: tensor(0.1985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2383/4345]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2384/4345]: training_loss: tensor(0.1856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2385/4345]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2386/4345]: training_loss: tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2387/4345]: training_loss: tensor(0.1549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2388/4345]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2389/4345]: training_loss: tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2390/4345]: training_loss: tensor(0.2725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2391/4345]: training_loss: tensor(0.1277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2392/4345]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2393/4345]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2394/4345]: training_loss: tensor(0.1794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2395/4345]: training_loss: tensor(0.2550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2396/4345]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2397/4345]: training_loss: tensor(0.4198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2398/4345]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2399/4345]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2400/4345]: training_loss: tensor(0.3203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2401/4345]: training_loss: tensor(0.3351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2402/4345]: training_loss: tensor(0.5210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2403/4345]: training_loss: tensor(0.1699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2404/4345]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2405/4345]: training_loss: tensor(0.2881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2406/4345]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2407/4345]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2408/4345]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2409/4345]: training_loss: tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2410/4345]: training_loss: tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2411/4345]: training_loss: tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2412/4345]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2413/4345]: training_loss: tensor(0.1283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2414/4345]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2415/4345]: training_loss: tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2416/4345]: training_loss: tensor(0.3178, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2417/4345]: training_loss: tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2418/4345]: training_loss: tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2419/4345]: training_loss: tensor(0.0436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2420/4345]: training_loss: tensor(0.4355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2421/4345]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2422/4345]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2423/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2424/4345]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2425/4345]: training_loss: tensor(0.0486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2426/4345]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2427/4345]: training_loss: tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2428/4345]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2429/4345]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2430/4345]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2431/4345]: training_loss: tensor(0.1314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2432/4345]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2433/4345]: training_loss: tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2434/4345]: training_loss: tensor(0.1185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2435/4345]: training_loss: tensor(0.1059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2436/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2437/4345]: training_loss: tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2438/4345]: training_loss: tensor(0.2767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2439/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2440/4345]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2441/4345]: training_loss: tensor(0.1266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2442/4345]: training_loss: tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2443/4345]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2444/4345]: training_loss: tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2445/4345]: training_loss: tensor(0.1085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2446/4345]: training_loss: tensor(0.1671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2447/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2448/4345]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2449/4345]: training_loss: tensor(0.2422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2450/4345]: training_loss: tensor(0.1842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2451/4345]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2452/4345]: training_loss: tensor(0.1097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2453/4345]: training_loss: tensor(0.2043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2454/4345]: training_loss: tensor(0.1561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2455/4345]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2456/4345]: training_loss: tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2457/4345]: training_loss: tensor(0.1618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2458/4345]: training_loss: tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2459/4345]: training_loss: tensor(0.0439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2460/4345]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2461/4345]: training_loss: tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2462/4345]: training_loss: tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2463/4345]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2464/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2465/4345]: training_loss: tensor(0.1462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2466/4345]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2467/4345]: training_loss: tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2468/4345]: training_loss: tensor(0.1949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2469/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2470/4345]: training_loss: tensor(0.1195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2471/4345]: training_loss: tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2472/4345]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2473/4345]: training_loss: tensor(0.1752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2474/4345]: training_loss: tensor(0.3775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2475/4345]: training_loss: tensor(0.1130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2476/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2477/4345]: training_loss: tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2478/4345]: training_loss: tensor(0.1672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2479/4345]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2480/4345]: training_loss: tensor(0.4273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2481/4345]: training_loss: tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2482/4345]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2483/4345]: training_loss: tensor(0.0566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2484/4345]: training_loss: tensor(0.2317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2485/4345]: training_loss: tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2486/4345]: training_loss: tensor(0.2651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2487/4345]: training_loss: tensor(0.1580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2488/4345]: training_loss: tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2489/4345]: training_loss: tensor(0.3105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2490/4345]: training_loss: tensor(0.1409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2491/4345]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2492/4345]: training_loss: tensor(0.1254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2493/4345]: training_loss: tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2494/4345]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2495/4345]: training_loss: tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2496/4345]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2497/4345]: training_loss: tensor(0.4125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2498/4345]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2499/4345]: training_loss: tensor(0.4449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2500/4345]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2501/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2502/4345]: training_loss: tensor(0.3651, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2503/4345]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2504/4345]: training_loss: tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2505/4345]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2506/4345]: training_loss: tensor(0.1513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2507/4345]: training_loss: tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2508/4345]: training_loss: tensor(0.2255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2509/4345]: training_loss: tensor(0.1098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2510/4345]: training_loss: tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2511/4345]: training_loss: tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2512/4345]: training_loss: tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2513/4345]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2514/4345]: training_loss: tensor(0.2881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2515/4345]: training_loss: tensor(0.3385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2516/4345]: training_loss: tensor(0.0469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2517/4345]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2518/4345]: training_loss: tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2519/4345]: training_loss: tensor(0.1893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2520/4345]: training_loss: tensor(0.1337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2521/4345]: training_loss: tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2522/4345]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2523/4345]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2524/4345]: training_loss: tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2525/4345]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2526/4345]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2527/4345]: training_loss: tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2528/4345]: training_loss: tensor(0.2073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2529/4345]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2530/4345]: training_loss: tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2531/4345]: training_loss: tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2532/4345]: training_loss: tensor(0.0468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2533/4345]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2534/4345]: training_loss: tensor(0.1794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2535/4345]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2536/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2537/4345]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2538/4345]: training_loss: tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2539/4345]: training_loss: tensor(0.1538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2540/4345]: training_loss: tensor(0.1661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2541/4345]: training_loss: tensor(0.1503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2542/4345]: training_loss: tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2543/4345]: training_loss: tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2544/4345]: training_loss: tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2545/4345]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2546/4345]: training_loss: tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2547/4345]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2548/4345]: training_loss: tensor(0.2759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2549/4345]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2550/4345]: training_loss: tensor(0.1491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2551/4345]: training_loss: tensor(0.4968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2552/4345]: training_loss: tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2553/4345]: training_loss: tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2554/4345]: training_loss: tensor(0.1616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2555/4345]: training_loss: tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2556/4345]: training_loss: tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2557/4345]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2558/4345]: training_loss: tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2559/4345]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2560/4345]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2561/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2562/4345]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2563/4345]: training_loss: tensor(0.1286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2564/4345]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2565/4345]: training_loss: tensor(0.1336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2566/4345]: training_loss: tensor(0.4519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2567/4345]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2568/4345]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2569/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2570/4345]: training_loss: tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2571/4345]: training_loss: tensor(0.3693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2572/4345]: training_loss: tensor(0.1993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2573/4345]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2574/4345]: training_loss: tensor(0.0547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2575/4345]: training_loss: tensor(0.1168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2576/4345]: training_loss: tensor(0.2257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2577/4345]: training_loss: tensor(0.1563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2578/4345]: training_loss: tensor(0.2772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2579/4345]: training_loss: tensor(0.3177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2580/4345]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2581/4345]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2582/4345]: training_loss: tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2583/4345]: training_loss: tensor(0.1432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2584/4345]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2585/4345]: training_loss: tensor(0.4180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2586/4345]: training_loss: tensor(0.1467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2587/4345]: training_loss: tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2588/4345]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2589/4345]: training_loss: tensor(0.1437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2590/4345]: training_loss: tensor(0.1482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2591/4345]: training_loss: tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2592/4345]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2593/4345]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2594/4345]: training_loss: tensor(0.0456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2595/4345]: training_loss: tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2596/4345]: training_loss: tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2597/4345]: training_loss: tensor(0.2984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2598/4345]: training_loss: tensor(0.2186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2599/4345]: training_loss: tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2600/4345]: training_loss: tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2601/4345]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2602/4345]: training_loss: tensor(0.2712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2603/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2604/4345]: training_loss: tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2605/4345]: training_loss: tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2606/4345]: training_loss: tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2607/4345]: training_loss: tensor(0.4049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2608/4345]: training_loss: tensor(0.1776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2609/4345]: training_loss: tensor(0.2163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2610/4345]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2611/4345]: training_loss: tensor(0.0579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2612/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2613/4345]: training_loss: tensor(0.2028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2614/4345]: training_loss: tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2615/4345]: training_loss: tensor(0.1732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2616/4345]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2617/4345]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2618/4345]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2619/4345]: training_loss: tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2620/4345]: training_loss: tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2621/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2622/4345]: training_loss: tensor(0.2097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2623/4345]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2624/4345]: training_loss: tensor(0.1498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2625/4345]: training_loss: tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2626/4345]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2627/4345]: training_loss: tensor(0.1125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2628/4345]: training_loss: tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2629/4345]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2630/4345]: training_loss: tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2631/4345]: training_loss: tensor(0.0413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2632/4345]: training_loss: tensor(0.2843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2633/4345]: training_loss: tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2634/4345]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2635/4345]: training_loss: tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2636/4345]: training_loss: tensor(0.2129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2637/4345]: training_loss: tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2638/4345]: training_loss: tensor(0.2483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2639/4345]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2640/4345]: training_loss: tensor(0.1538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2641/4345]: training_loss: tensor(0.2140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2642/4345]: training_loss: tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2643/4345]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2644/4345]: training_loss: tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2645/4345]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2646/4345]: training_loss: tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2647/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2648/4345]: training_loss: tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2649/4345]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2650/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2651/4345]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2652/4345]: training_loss: tensor(0.1857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2653/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2654/4345]: training_loss: tensor(0.1239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2655/4345]: training_loss: tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2656/4345]: training_loss: tensor(0.2074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2657/4345]: training_loss: tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2658/4345]: training_loss: tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2659/4345]: training_loss: tensor(0.1920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2660/4345]: training_loss: tensor(0.3454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2661/4345]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2662/4345]: training_loss: tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2663/4345]: training_loss: tensor(0.4128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2664/4345]: training_loss: tensor(0.1712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2665/4345]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2666/4345]: training_loss: tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2667/4345]: training_loss: tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2668/4345]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2669/4345]: training_loss: tensor(0.1769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2670/4345]: training_loss: tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2671/4345]: training_loss: tensor(0.1419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2672/4345]: training_loss: tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2673/4345]: training_loss: tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2674/4345]: training_loss: tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2675/4345]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2676/4345]: training_loss: tensor(0.1609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2677/4345]: training_loss: tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2678/4345]: training_loss: tensor(0.2739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2679/4345]: training_loss: tensor(0.2110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2680/4345]: training_loss: tensor(0.1408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2681/4345]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2682/4345]: training_loss: tensor(0.1936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2683/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2684/4345]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2685/4345]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2686/4345]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2687/4345]: training_loss: tensor(0.4087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2688/4345]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2689/4345]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2690/4345]: training_loss: tensor(0.5148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2691/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2692/4345]: training_loss: tensor(0.2976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2693/4345]: training_loss: tensor(0.2183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2694/4345]: training_loss: tensor(0.1402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2695/4345]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2696/4345]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2697/4345]: training_loss: tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2698/4345]: training_loss: tensor(0.1897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2699/4345]: training_loss: tensor(0.2332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2700/4345]: training_loss: tensor(0.1222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2701/4345]: training_loss: tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2702/4345]: training_loss: tensor(0.1076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2703/4345]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2704/4345]: training_loss: tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2705/4345]: training_loss: tensor(0.3762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2706/4345]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2707/4345]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2708/4345]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2709/4345]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2710/4345]: training_loss: tensor(0.1417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2711/4345]: training_loss: tensor(0.2293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2712/4345]: training_loss: tensor(0.0342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2713/4345]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2714/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2715/4345]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2716/4345]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2717/4345]: training_loss: tensor(0.2114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2718/4345]: training_loss: tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2719/4345]: training_loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2720/4345]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2721/4345]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2722/4345]: training_loss: tensor(0.2097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2723/4345]: training_loss: tensor(0.2101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2724/4345]: training_loss: tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2725/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2726/4345]: training_loss: tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2727/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2728/4345]: training_loss: tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2729/4345]: training_loss: tensor(0.1714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2730/4345]: training_loss: tensor(0.0492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2731/4345]: training_loss: tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2732/4345]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2733/4345]: training_loss: tensor(0.0445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2734/4345]: training_loss: tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2735/4345]: training_loss: tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2736/4345]: training_loss: tensor(0.0354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2737/4345]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2738/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2739/4345]: training_loss: tensor(0.4458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2740/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2741/4345]: training_loss: tensor(0.0545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2742/4345]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2743/4345]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2744/4345]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2745/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2746/4345]: training_loss: tensor(0.9571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2747/4345]: training_loss: tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2748/4345]: training_loss: tensor(0.7723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2749/4345]: training_loss: tensor(0.0404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2750/4345]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2751/4345]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2752/4345]: training_loss: tensor(0.1408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2753/4345]: training_loss: tensor(0.0475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2754/4345]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2755/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2756/4345]: training_loss: tensor(0.1763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2757/4345]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2758/4345]: training_loss: tensor(0.4935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2759/4345]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2760/4345]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2761/4345]: training_loss: tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2762/4345]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2763/4345]: training_loss: tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2764/4345]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2765/4345]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2766/4345]: training_loss: tensor(0.2634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2767/4345]: training_loss: tensor(0.2075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2768/4345]: training_loss: tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2769/4345]: training_loss: tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2770/4345]: training_loss: tensor(0.4422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2771/4345]: training_loss: tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2772/4345]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2773/4345]: training_loss: tensor(0.2351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2774/4345]: training_loss: tensor(0.1533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2775/4345]: training_loss: tensor(0.2374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2776/4345]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2777/4345]: training_loss: tensor(0.1761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2778/4345]: training_loss: tensor(0.1634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2779/4345]: training_loss: tensor(0.1396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2780/4345]: training_loss: tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2781/4345]: training_loss: tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2782/4345]: training_loss: tensor(0.3174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2783/4345]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2784/4345]: training_loss: tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2785/4345]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2786/4345]: training_loss: tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2787/4345]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2788/4345]: training_loss: tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2789/4345]: training_loss: tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2790/4345]: training_loss: tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2791/4345]: training_loss: tensor(0.2444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2792/4345]: training_loss: tensor(0.2039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2793/4345]: training_loss: tensor(0.1368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2794/4345]: training_loss: tensor(0.3506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2795/4345]: training_loss: tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2796/4345]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2797/4345]: training_loss: tensor(0.0384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2798/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2799/4345]: training_loss: tensor(0.3973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2800/4345]: training_loss: tensor(0.2926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2801/4345]: training_loss: tensor(0.1982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2802/4345]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2803/4345]: training_loss: tensor(0.2592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2804/4345]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2805/4345]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2806/4345]: training_loss: tensor(0.2129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2807/4345]: training_loss: tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2808/4345]: training_loss: tensor(0.1928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2809/4345]: training_loss: tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2810/4345]: training_loss: tensor(0.1361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2811/4345]: training_loss: tensor(0.1424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2812/4345]: training_loss: tensor(0.0480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2813/4345]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2814/4345]: training_loss: tensor(0.1499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2815/4345]: training_loss: tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2816/4345]: training_loss: tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2817/4345]: training_loss: tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2818/4345]: training_loss: tensor(0.1342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2819/4345]: training_loss: tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2820/4345]: training_loss: tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2821/4345]: training_loss: tensor(0.1335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2822/4345]: training_loss: tensor(0.2329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2823/4345]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2824/4345]: training_loss: tensor(0.1777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2825/4345]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2826/4345]: training_loss: tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2827/4345]: training_loss: tensor(0.1902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2828/4345]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2829/4345]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2830/4345]: training_loss: tensor(0.1120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2831/4345]: training_loss: tensor(0.2222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2832/4345]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2833/4345]: training_loss: tensor(0.2424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2834/4345]: training_loss: tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2835/4345]: training_loss: tensor(0.1267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2836/4345]: training_loss: tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2837/4345]: training_loss: tensor(0.3716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2838/4345]: training_loss: tensor(0.3279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2839/4345]: training_loss: tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2840/4345]: training_loss: tensor(0.2510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2841/4345]: training_loss: tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2842/4345]: training_loss: tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2843/4345]: training_loss: tensor(0.1204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2844/4345]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2845/4345]: training_loss: tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2846/4345]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2847/4345]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2848/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2849/4345]: training_loss: tensor(0.2502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2850/4345]: training_loss: tensor(0.1625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2851/4345]: training_loss: tensor(0.0650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2852/4345]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2853/4345]: training_loss: tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2854/4345]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2855/4345]: training_loss: tensor(0.1064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2856/4345]: training_loss: tensor(0.4270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2857/4345]: training_loss: tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2858/4345]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2859/4345]: training_loss: tensor(0.1267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2860/4345]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2861/4345]: training_loss: tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2862/4345]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2863/4345]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2864/4345]: training_loss: tensor(0.2846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2865/4345]: training_loss: tensor(0.1678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2866/4345]: training_loss: tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2867/4345]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2868/4345]: training_loss: tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2869/4345]: training_loss: tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2870/4345]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2871/4345]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2872/4345]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2873/4345]: training_loss: tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2874/4345]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2875/4345]: training_loss: tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2876/4345]: training_loss: tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2877/4345]: training_loss: tensor(0.1217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2878/4345]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2879/4345]: training_loss: tensor(0.2881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2880/4345]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2881/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2882/4345]: training_loss: tensor(0.2042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2883/4345]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2884/4345]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2885/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2886/4345]: training_loss: tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2887/4345]: training_loss: tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2888/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2889/4345]: training_loss: tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2890/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2891/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2892/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2893/4345]: training_loss: tensor(0.2400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2894/4345]: training_loss: tensor(0.2587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2895/4345]: training_loss: tensor(0.5414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2896/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2897/4345]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2898/4345]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2899/4345]: training_loss: tensor(0.0555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2900/4345]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2901/4345]: training_loss: tensor(0.0507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2902/4345]: training_loss: tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2903/4345]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2904/4345]: training_loss: tensor(0.1351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2905/4345]: training_loss: tensor(0.0547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2906/4345]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2907/4345]: training_loss: tensor(0.2711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2908/4345]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2909/4345]: training_loss: tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2910/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2911/4345]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2912/4345]: training_loss: tensor(0.2716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2913/4345]: training_loss: tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2914/4345]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2915/4345]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2916/4345]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2917/4345]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2918/4345]: training_loss: tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2919/4345]: training_loss: tensor(0.1623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2920/4345]: training_loss: tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2921/4345]: training_loss: tensor(0.4689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2922/4345]: training_loss: tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2923/4345]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2924/4345]: training_loss: tensor(0.1374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2925/4345]: training_loss: tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2926/4345]: training_loss: tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2927/4345]: training_loss: tensor(0.1640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2928/4345]: training_loss: tensor(0.1313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2929/4345]: training_loss: tensor(0.2763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2930/4345]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2931/4345]: training_loss: tensor(0.1114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2932/4345]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2933/4345]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2934/4345]: training_loss: tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2935/4345]: training_loss: tensor(0.3410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2936/4345]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2937/4345]: training_loss: tensor(0.1758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2938/4345]: training_loss: tensor(0.1107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2939/4345]: training_loss: tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2940/4345]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2941/4345]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2942/4345]: training_loss: tensor(0.0456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2943/4345]: training_loss: tensor(0.1949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2944/4345]: training_loss: tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2945/4345]: training_loss: tensor(0.1958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2946/4345]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2947/4345]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2948/4345]: training_loss: tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2949/4345]: training_loss: tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2950/4345]: training_loss: tensor(0.0505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2951/4345]: training_loss: tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2952/4345]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2953/4345]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2954/4345]: training_loss: tensor(0.2358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2955/4345]: training_loss: tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2956/4345]: training_loss: tensor(0.1751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2957/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2958/4345]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2959/4345]: training_loss: tensor(0.1681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2960/4345]: training_loss: tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2961/4345]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2962/4345]: training_loss: tensor(0.3780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2963/4345]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2964/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2965/4345]: training_loss: tensor(0.2026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2966/4345]: training_loss: tensor(0.1334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2967/4345]: training_loss: tensor(0.1700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2968/4345]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2969/4345]: training_loss: tensor(0.1837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2970/4345]: training_loss: tensor(0.1250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2971/4345]: training_loss: tensor(0.0629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2972/4345]: training_loss: tensor(0.1282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2973/4345]: training_loss: tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2974/4345]: training_loss: tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2975/4345]: training_loss: tensor(0.7014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2976/4345]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2977/4345]: training_loss: tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2978/4345]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2979/4345]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2980/4345]: training_loss: tensor(0.3519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2981/4345]: training_loss: tensor(0.0442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2982/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2983/4345]: training_loss: tensor(0.3576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2984/4345]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2985/4345]: training_loss: tensor(0.1385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2986/4345]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2987/4345]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2988/4345]: training_loss: tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2989/4345]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2990/4345]: training_loss: tensor(0.1360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2991/4345]: training_loss: tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2992/4345]: training_loss: tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2993/4345]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2994/4345]: training_loss: tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2995/4345]: training_loss: tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2996/4345]: training_loss: tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2997/4345]: training_loss: tensor(0.1655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2998/4345]: training_loss: tensor(0.1911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2999/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3000/4345]: training_loss: tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3001/4345]: training_loss: tensor(0.2437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3002/4345]: training_loss: tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3003/4345]: training_loss: tensor(0.1674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3004/4345]: training_loss: tensor(0.2836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3005/4345]: training_loss: tensor(0.2645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3006/4345]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3007/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3008/4345]: training_loss: tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3009/4345]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3010/4345]: training_loss: tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3011/4345]: training_loss: tensor(0.1941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3012/4345]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3013/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3014/4345]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3015/4345]: training_loss: tensor(0.2262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3016/4345]: training_loss: tensor(0.2513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3017/4345]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3018/4345]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3019/4345]: training_loss: tensor(0.4975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3020/4345]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3021/4345]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3022/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3023/4345]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3024/4345]: training_loss: tensor(0.1655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3025/4345]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3026/4345]: training_loss: tensor(0.5720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3027/4345]: training_loss: tensor(0.0676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3028/4345]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3029/4345]: training_loss: tensor(0.1985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3030/4345]: training_loss: tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3031/4345]: training_loss: tensor(0.1543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3032/4345]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3033/4345]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3034/4345]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3035/4345]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3036/4345]: training_loss: tensor(0.1106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3037/4345]: training_loss: tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3038/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3039/4345]: training_loss: tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3040/4345]: training_loss: tensor(0.1276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3041/4345]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3042/4345]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3043/4345]: training_loss: tensor(0.1552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3044/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3045/4345]: training_loss: tensor(0.2180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3046/4345]: training_loss: tensor(0.4804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3047/4345]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3048/4345]: training_loss: tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3049/4345]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3050/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3051/4345]: training_loss: tensor(0.3469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3052/4345]: training_loss: tensor(0.1588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3053/4345]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3054/4345]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3055/4345]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3056/4345]: training_loss: tensor(0.2364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3057/4345]: training_loss: tensor(0.0305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3058/4345]: training_loss: tensor(0.1952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3059/4345]: training_loss: tensor(0.1196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3060/4345]: training_loss: tensor(0.2871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3061/4345]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3062/4345]: training_loss: tensor(0.2874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3063/4345]: training_loss: tensor(0.1858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3064/4345]: training_loss: tensor(0.0603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3065/4345]: training_loss: tensor(0.3232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3066/4345]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3067/4345]: training_loss: tensor(0.2471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3068/4345]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3069/4345]: training_loss: tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3070/4345]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3071/4345]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3072/4345]: training_loss: tensor(0.1211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3073/4345]: training_loss: tensor(0.0442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3074/4345]: training_loss: tensor(0.2786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3075/4345]: training_loss: tensor(0.1722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3076/4345]: training_loss: tensor(0.1780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3077/4345]: training_loss: tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3078/4345]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3079/4345]: training_loss: tensor(0.1194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3080/4345]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3081/4345]: training_loss: tensor(0.2969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3082/4345]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3083/4345]: training_loss: tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3084/4345]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3085/4345]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3086/4345]: training_loss: tensor(0.2320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3087/4345]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3088/4345]: training_loss: tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3089/4345]: training_loss: tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3090/4345]: training_loss: tensor(0.1515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3091/4345]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3092/4345]: training_loss: tensor(0.3744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3093/4345]: training_loss: tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3094/4345]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3095/4345]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3096/4345]: training_loss: tensor(0.1457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3097/4345]: training_loss: tensor(0.1868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3098/4345]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3099/4345]: training_loss: tensor(0.2563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3100/4345]: training_loss: tensor(0.1635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3101/4345]: training_loss: tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3102/4345]: training_loss: tensor(0.1324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3103/4345]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3104/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3105/4345]: training_loss: tensor(0.1503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3106/4345]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3107/4345]: training_loss: tensor(0.2291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3108/4345]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3109/4345]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3110/4345]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3111/4345]: training_loss: tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3112/4345]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3113/4345]: training_loss: tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3114/4345]: training_loss: tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3115/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3116/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3117/4345]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3118/4345]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3119/4345]: training_loss: tensor(0.1488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3120/4345]: training_loss: tensor(0.1473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3121/4345]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3122/4345]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3123/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3124/4345]: training_loss: tensor(0.2751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3125/4345]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3126/4345]: training_loss: tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3127/4345]: training_loss: tensor(0.3907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3128/4345]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3129/4345]: training_loss: tensor(0.1914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3130/4345]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3131/4345]: training_loss: tensor(0.0587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3132/4345]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3133/4345]: training_loss: tensor(0.4387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3134/4345]: training_loss: tensor(0.2215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3135/4345]: training_loss: tensor(0.1492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3136/4345]: training_loss: tensor(0.1431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3137/4345]: training_loss: tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3138/4345]: training_loss: tensor(0.3330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3139/4345]: training_loss: tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3140/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3141/4345]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3142/4345]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3143/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3144/4345]: training_loss: tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3145/4345]: training_loss: tensor(0.1838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3146/4345]: training_loss: tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3147/4345]: training_loss: tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3148/4345]: training_loss: tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3149/4345]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3150/4345]: training_loss: tensor(0.1197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3151/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3152/4345]: training_loss: tensor(0.2797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3153/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3154/4345]: training_loss: tensor(0.2017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3155/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3156/4345]: training_loss: tensor(0.3565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3157/4345]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3158/4345]: training_loss: tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3159/4345]: training_loss: tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3160/4345]: training_loss: tensor(0.3392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3161/4345]: training_loss: tensor(0.2700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3162/4345]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3163/4345]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3164/4345]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3165/4345]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3166/4345]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3167/4345]: training_loss: tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3168/4345]: training_loss: tensor(0.0423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3169/4345]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3170/4345]: training_loss: tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3171/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3172/4345]: training_loss: tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3173/4345]: training_loss: tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3174/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3175/4345]: training_loss: tensor(0.2246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3176/4345]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3177/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3178/4345]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3179/4345]: training_loss: tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3180/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3181/4345]: training_loss: tensor(0.2600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3182/4345]: training_loss: tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3183/4345]: training_loss: tensor(0.1619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3184/4345]: training_loss: tensor(0.1177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3185/4345]: training_loss: tensor(0.1640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3186/4345]: training_loss: tensor(0.1162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3187/4345]: training_loss: tensor(0.2303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3188/4345]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3189/4345]: training_loss: tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3190/4345]: training_loss: tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3191/4345]: training_loss: tensor(0.2947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3192/4345]: training_loss: tensor(0.0621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3193/4345]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3194/4345]: training_loss: tensor(0.1437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3195/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3196/4345]: training_loss: tensor(0.2853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3197/4345]: training_loss: tensor(0.1214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3198/4345]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3199/4345]: training_loss: tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3200/4345]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3201/4345]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3202/4345]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3203/4345]: training_loss: tensor(0.2157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3204/4345]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3205/4345]: training_loss: tensor(0.3166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3206/4345]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3207/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3208/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3209/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3210/4345]: training_loss: tensor(0.1159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3211/4345]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3212/4345]: training_loss: tensor(0.1169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3213/4345]: training_loss: tensor(0.1155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3214/4345]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3215/4345]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3216/4345]: training_loss: tensor(0.2399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3217/4345]: training_loss: tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3218/4345]: training_loss: tensor(0.2149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3219/4345]: training_loss: tensor(0.3414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3220/4345]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3221/4345]: training_loss: tensor(0.3734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3222/4345]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3223/4345]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3224/4345]: training_loss: tensor(0.1687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3225/4345]: training_loss: tensor(0.2229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3226/4345]: training_loss: tensor(0.1869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3227/4345]: training_loss: tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3228/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3229/4345]: training_loss: tensor(0.2981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3230/4345]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3231/4345]: training_loss: tensor(0.2216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3232/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3233/4345]: training_loss: tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3234/4345]: training_loss: tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3235/4345]: training_loss: tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3236/4345]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3237/4345]: training_loss: tensor(0.1755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3238/4345]: training_loss: tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3239/4345]: training_loss: tensor(0.4311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3240/4345]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3241/4345]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3242/4345]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3243/4345]: training_loss: tensor(0.1123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3244/4345]: training_loss: tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3245/4345]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3246/4345]: training_loss: tensor(0.2801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3247/4345]: training_loss: tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3248/4345]: training_loss: tensor(0.3901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3249/4345]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3250/4345]: training_loss: tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3251/4345]: training_loss: tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3252/4345]: training_loss: tensor(0.1690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3253/4345]: training_loss: tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3254/4345]: training_loss: tensor(0.0502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3255/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3256/4345]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3257/4345]: training_loss: tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3258/4345]: training_loss: tensor(0.2303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3259/4345]: training_loss: tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3260/4345]: training_loss: tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3261/4345]: training_loss: tensor(0.0635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3262/4345]: training_loss: tensor(0.7126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3263/4345]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3264/4345]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3265/4345]: training_loss: tensor(0.1381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3266/4345]: training_loss: tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3267/4345]: training_loss: tensor(0.2338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3268/4345]: training_loss: tensor(0.0492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3269/4345]: training_loss: tensor(0.0616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3270/4345]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3271/4345]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3272/4345]: training_loss: tensor(0.1121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3273/4345]: training_loss: tensor(0.0439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3274/4345]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3275/4345]: training_loss: tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3276/4345]: training_loss: tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3277/4345]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3278/4345]: training_loss: tensor(0.0635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3279/4345]: training_loss: tensor(0.1893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3280/4345]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3281/4345]: training_loss: tensor(0.1574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3282/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3283/4345]: training_loss: tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3284/4345]: training_loss: tensor(0.1658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3285/4345]: training_loss: tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3286/4345]: training_loss: tensor(0.0606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3287/4345]: training_loss: tensor(0.0459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3288/4345]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3289/4345]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3290/4345]: training_loss: tensor(0.3368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3291/4345]: training_loss: tensor(0.1911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3292/4345]: training_loss: tensor(0.2592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3293/4345]: training_loss: tensor(0.1413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3294/4345]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3295/4345]: training_loss: tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3296/4345]: training_loss: tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3297/4345]: training_loss: tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3298/4345]: training_loss: tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3299/4345]: training_loss: tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3300/4345]: training_loss: tensor(0.1368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3301/4345]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3302/4345]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3303/4345]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3304/4345]: training_loss: tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3305/4345]: training_loss: tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3306/4345]: training_loss: tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3307/4345]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3308/4345]: training_loss: tensor(0.3247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3309/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3310/4345]: training_loss: tensor(0.1276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3311/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3312/4345]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3313/4345]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3314/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3315/4345]: training_loss: tensor(0.2242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3316/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3317/4345]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3318/4345]: training_loss: tensor(0.2184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3319/4345]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3320/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3321/4345]: training_loss: tensor(0.2493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3322/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3323/4345]: training_loss: tensor(0.0648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3324/4345]: training_loss: tensor(0.1552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3325/4345]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3326/4345]: training_loss: tensor(0.3353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3327/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3328/4345]: training_loss: tensor(0.2318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3329/4345]: training_loss: tensor(0.2269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3330/4345]: training_loss: tensor(0.3935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3331/4345]: training_loss: tensor(0.3049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3332/4345]: training_loss: tensor(0.5251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3333/4345]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3334/4345]: training_loss: tensor(0.1887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3335/4345]: training_loss: tensor(0.3304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3336/4345]: training_loss: tensor(0.1128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3337/4345]: training_loss: tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3338/4345]: training_loss: tensor(0.2276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3339/4345]: training_loss: tensor(0.1349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3340/4345]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3341/4345]: training_loss: tensor(0.2487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3342/4345]: training_loss: tensor(0.1391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3343/4345]: training_loss: tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3344/4345]: training_loss: tensor(0.1295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3345/4345]: training_loss: tensor(0.2195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3346/4345]: training_loss: tensor(0.1139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3347/4345]: training_loss: tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3348/4345]: training_loss: tensor(0.1808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3349/4345]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3350/4345]: training_loss: tensor(0.4784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3351/4345]: training_loss: tensor(0.1426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3352/4345]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3353/4345]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3354/4345]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3355/4345]: training_loss: tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3356/4345]: training_loss: tensor(0.1838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3357/4345]: training_loss: tensor(0.1298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3358/4345]: training_loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3359/4345]: training_loss: tensor(0.1633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3360/4345]: training_loss: tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3361/4345]: training_loss: tensor(0.1724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3362/4345]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3363/4345]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3364/4345]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3365/4345]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3366/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3367/4345]: training_loss: tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3368/4345]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3369/4345]: training_loss: tensor(0.1861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3370/4345]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3371/4345]: training_loss: tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3372/4345]: training_loss: tensor(0.1722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3373/4345]: training_loss: tensor(0.2830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3374/4345]: training_loss: tensor(0.2682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3375/4345]: training_loss: tensor(0.3388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3376/4345]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3377/4345]: training_loss: tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3378/4345]: training_loss: tensor(0.0493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3379/4345]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3380/4345]: training_loss: tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3381/4345]: training_loss: tensor(0.1664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3382/4345]: training_loss: tensor(0.2069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3383/4345]: training_loss: tensor(0.1459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3384/4345]: training_loss: tensor(0.2126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3385/4345]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3386/4345]: training_loss: tensor(0.1168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3387/4345]: training_loss: tensor(0.1608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3388/4345]: training_loss: tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3389/4345]: training_loss: tensor(0.1680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3390/4345]: training_loss: tensor(0.1130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3391/4345]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3392/4345]: training_loss: tensor(0.2880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3393/4345]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3394/4345]: training_loss: tensor(0.0410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3395/4345]: training_loss: tensor(0.1432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3396/4345]: training_loss: tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3397/4345]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3398/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3399/4345]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3400/4345]: training_loss: tensor(0.4468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3401/4345]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3402/4345]: training_loss: tensor(0.2197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3403/4345]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3404/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3405/4345]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3406/4345]: training_loss: tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3407/4345]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3408/4345]: training_loss: tensor(0.4048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3409/4345]: training_loss: tensor(0.3430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3410/4345]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3411/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3412/4345]: training_loss: tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3413/4345]: training_loss: tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3414/4345]: training_loss: tensor(0.1609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3415/4345]: training_loss: tensor(0.1963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3416/4345]: training_loss: tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3417/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3418/4345]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3419/4345]: training_loss: tensor(0.2635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3420/4345]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3421/4345]: training_loss: tensor(0.1332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3422/4345]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3423/4345]: training_loss: tensor(0.4114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3424/4345]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3425/4345]: training_loss: tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3426/4345]: training_loss: tensor(0.1129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3427/4345]: training_loss: tensor(0.0384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3428/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3429/4345]: training_loss: tensor(0.1313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3430/4345]: training_loss: tensor(0.2806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3431/4345]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3432/4345]: training_loss: tensor(0.2946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3433/4345]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3434/4345]: training_loss: tensor(0.0364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3435/4345]: training_loss: tensor(0.0587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3436/4345]: training_loss: tensor(0.2320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3437/4345]: training_loss: tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3438/4345]: training_loss: tensor(0.1439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3439/4345]: training_loss: tensor(0.1951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3440/4345]: training_loss: tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3441/4345]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3442/4345]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3443/4345]: training_loss: tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3444/4345]: training_loss: tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3445/4345]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3446/4345]: training_loss: tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3447/4345]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3448/4345]: training_loss: tensor(0.1414, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3449/4345]: training_loss: tensor(0.1391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3450/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3451/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3452/4345]: training_loss: tensor(0.1244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3453/4345]: training_loss: tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3454/4345]: training_loss: tensor(0.1436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3455/4345]: training_loss: tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3456/4345]: training_loss: tensor(0.2820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3457/4345]: training_loss: tensor(0.1111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3458/4345]: training_loss: tensor(0.1352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3459/4345]: training_loss: tensor(0.4419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3460/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3461/4345]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3462/4345]: training_loss: tensor(0.3492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3463/4345]: training_loss: tensor(0.1461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3464/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3465/4345]: training_loss: tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3466/4345]: training_loss: tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3467/4345]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3468/4345]: training_loss: tensor(0.1432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3469/4345]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3470/4345]: training_loss: tensor(0.1310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3471/4345]: training_loss: tensor(0.3330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3472/4345]: training_loss: tensor(0.1493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3473/4345]: training_loss: tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3474/4345]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3475/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3476/4345]: training_loss: tensor(0.1606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3477/4345]: training_loss: tensor(0.1577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3478/4345]: training_loss: tensor(0.3043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3479/4345]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3480/4345]: training_loss: tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3481/4345]: training_loss: tensor(0.1981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3482/4345]: training_loss: tensor(0.0648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3483/4345]: training_loss: tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3484/4345]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3485/4345]: training_loss: tensor(0.1213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3486/4345]: training_loss: tensor(0.0648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3487/4345]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3488/4345]: training_loss: tensor(0.2534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3489/4345]: training_loss: tensor(0.2855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3490/4345]: training_loss: tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3491/4345]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3492/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3493/4345]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3494/4345]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3495/4345]: training_loss: tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3496/4345]: training_loss: tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3497/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3498/4345]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3499/4345]: training_loss: tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3500/4345]: training_loss: tensor(0.2925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3501/4345]: training_loss: tensor(0.3324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3502/4345]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3503/4345]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3504/4345]: training_loss: tensor(0.1512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3505/4345]: training_loss: tensor(0.3486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3506/4345]: training_loss: tensor(0.3178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3507/4345]: training_loss: tensor(0.1186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3508/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3509/4345]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3510/4345]: training_loss: tensor(0.2130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3511/4345]: training_loss: tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3512/4345]: training_loss: tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3513/4345]: training_loss: tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3514/4345]: training_loss: tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3515/4345]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3516/4345]: training_loss: tensor(0.2832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3517/4345]: training_loss: tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3518/4345]: training_loss: tensor(0.1864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3519/4345]: training_loss: tensor(0.4087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3520/4345]: training_loss: tensor(0.1077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3521/4345]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3522/4345]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3523/4345]: training_loss: tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3524/4345]: training_loss: tensor(0.2802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3525/4345]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3526/4345]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3527/4345]: training_loss: tensor(0.1674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3528/4345]: training_loss: tensor(0.1714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3529/4345]: training_loss: tensor(0.2235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3530/4345]: training_loss: tensor(0.1897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3531/4345]: training_loss: tensor(0.1671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3532/4345]: training_loss: tensor(0.1043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3533/4345]: training_loss: tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3534/4345]: training_loss: tensor(0.1776, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3535/4345]: training_loss: tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3536/4345]: training_loss: tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3537/4345]: training_loss: tensor(0.1673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3538/4345]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3539/4345]: training_loss: tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3540/4345]: training_loss: tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3541/4345]: training_loss: tensor(0.2386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3542/4345]: training_loss: tensor(0.1372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3543/4345]: training_loss: tensor(0.3483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3544/4345]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3545/4345]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3546/4345]: training_loss: tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3547/4345]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3548/4345]: training_loss: tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3549/4345]: training_loss: tensor(0.0579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3550/4345]: training_loss: tensor(0.3238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3551/4345]: training_loss: tensor(0.1562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3552/4345]: training_loss: tensor(0.1598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3553/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3554/4345]: training_loss: tensor(0.5256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3555/4345]: training_loss: tensor(0.2131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3556/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3557/4345]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3558/4345]: training_loss: tensor(0.2580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3559/4345]: training_loss: tensor(0.2112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3560/4345]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3561/4345]: training_loss: tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3562/4345]: training_loss: tensor(0.1946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3563/4345]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3564/4345]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3565/4345]: training_loss: tensor(0.1085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3566/4345]: training_loss: tensor(0.1809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3567/4345]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3568/4345]: training_loss: tensor(0.0313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3569/4345]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3570/4345]: training_loss: tensor(0.0573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3571/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3572/4345]: training_loss: tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3573/4345]: training_loss: tensor(0.1184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3574/4345]: training_loss: tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3575/4345]: training_loss: tensor(0.2612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3576/4345]: training_loss: tensor(0.3756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3577/4345]: training_loss: tensor(0.1447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3578/4345]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3579/4345]: training_loss: tensor(0.1272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3580/4345]: training_loss: tensor(0.3697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3581/4345]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3582/4345]: training_loss: tensor(0.1422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3583/4345]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3584/4345]: training_loss: tensor(0.2598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3585/4345]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3586/4345]: training_loss: tensor(0.2562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3587/4345]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3588/4345]: training_loss: tensor(0.1449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3589/4345]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3590/4345]: training_loss: tensor(0.1190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3591/4345]: training_loss: tensor(0.2046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3592/4345]: training_loss: tensor(0.1091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3593/4345]: training_loss: tensor(0.2391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3594/4345]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3595/4345]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3596/4345]: training_loss: tensor(0.1810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3597/4345]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3598/4345]: training_loss: tensor(0.1677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3599/4345]: training_loss: tensor(0.1809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3600/4345]: training_loss: tensor(0.2208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3601/4345]: training_loss: tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3602/4345]: training_loss: tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3603/4345]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3604/4345]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3605/4345]: training_loss: tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3606/4345]: training_loss: tensor(0.0606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3607/4345]: training_loss: tensor(0.2602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3608/4345]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3609/4345]: training_loss: tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3610/4345]: training_loss: tensor(0.0666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3611/4345]: training_loss: tensor(0.1859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3612/4345]: training_loss: tensor(0.3846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3613/4345]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3614/4345]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3615/4345]: training_loss: tensor(0.3595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3616/4345]: training_loss: tensor(0.1415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3617/4345]: training_loss: tensor(0.1532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3618/4345]: training_loss: tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3619/4345]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3620/4345]: training_loss: tensor(0.2035, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3621/4345]: training_loss: tensor(0.1191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3622/4345]: training_loss: tensor(0.1388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3623/4345]: training_loss: tensor(0.4974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3624/4345]: training_loss: tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3625/4345]: training_loss: tensor(0.1798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3626/4345]: training_loss: tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3627/4345]: training_loss: tensor(0.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3628/4345]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3629/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3630/4345]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3631/4345]: training_loss: tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3632/4345]: training_loss: tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3633/4345]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3634/4345]: training_loss: tensor(0.1263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3635/4345]: training_loss: tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3636/4345]: training_loss: tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3637/4345]: training_loss: tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3638/4345]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3639/4345]: training_loss: tensor(0.2847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3640/4345]: training_loss: tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3641/4345]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3642/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3643/4345]: training_loss: tensor(0.1573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3644/4345]: training_loss: tensor(0.4429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3645/4345]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3646/4345]: training_loss: tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3647/4345]: training_loss: tensor(0.3601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3648/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3649/4345]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3650/4345]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3651/4345]: training_loss: tensor(0.1923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3652/4345]: training_loss: tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3653/4345]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3654/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3655/4345]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3656/4345]: training_loss: tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3657/4345]: training_loss: tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3658/4345]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3659/4345]: training_loss: tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3660/4345]: training_loss: tensor(0.3407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3661/4345]: training_loss: tensor(0.0364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3662/4345]: training_loss: tensor(0.2033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3663/4345]: training_loss: tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3664/4345]: training_loss: tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3665/4345]: training_loss: tensor(0.1069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3666/4345]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3667/4345]: training_loss: tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3668/4345]: training_loss: tensor(0.1478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3669/4345]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3670/4345]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3671/4345]: training_loss: tensor(0.3280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3672/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3673/4345]: training_loss: tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3674/4345]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3675/4345]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3676/4345]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3677/4345]: training_loss: tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3678/4345]: training_loss: tensor(0.0492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3679/4345]: training_loss: tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3680/4345]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3681/4345]: training_loss: tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3682/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3683/4345]: training_loss: tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3684/4345]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3685/4345]: training_loss: tensor(0.1699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3686/4345]: training_loss: tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3687/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3688/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3689/4345]: training_loss: tensor(0.1197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3690/4345]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3691/4345]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3692/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3693/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3694/4345]: training_loss: tensor(0.0545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3695/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3696/4345]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3697/4345]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3698/4345]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3699/4345]: training_loss: tensor(0.1418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3700/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3701/4345]: training_loss: tensor(0.1332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3702/4345]: training_loss: tensor(0.6603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3703/4345]: training_loss: tensor(0.2942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3704/4345]: training_loss: tensor(0.3320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3705/4345]: training_loss: tensor(0.4630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3706/4345]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3707/4345]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3708/4345]: training_loss: tensor(0.1786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3709/4345]: training_loss: tensor(0.3862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3710/4345]: training_loss: tensor(0.1687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3711/4345]: training_loss: tensor(0.1464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3712/4345]: training_loss: tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3713/4345]: training_loss: tensor(0.2404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3714/4345]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3715/4345]: training_loss: tensor(0.4321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3716/4345]: training_loss: tensor(0.1919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3717/4345]: training_loss: tensor(0.3865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3718/4345]: training_loss: tensor(0.1299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3719/4345]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3720/4345]: training_loss: tensor(0.3981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3721/4345]: training_loss: tensor(0.1348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3722/4345]: training_loss: tensor(0.4237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3723/4345]: training_loss: tensor(0.1820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3724/4345]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3725/4345]: training_loss: tensor(0.1418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3726/4345]: training_loss: tensor(0.2851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3727/4345]: training_loss: tensor(0.3387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3728/4345]: training_loss: tensor(0.1523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3729/4345]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3730/4345]: training_loss: tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3731/4345]: training_loss: tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3732/4345]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3733/4345]: training_loss: tensor(0.2320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3734/4345]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3735/4345]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3736/4345]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3737/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3738/4345]: training_loss: tensor(0.2037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3739/4345]: training_loss: tensor(0.1899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3740/4345]: training_loss: tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3741/4345]: training_loss: tensor(0.1719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3742/4345]: training_loss: tensor(0.4589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3743/4345]: training_loss: tensor(0.1376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3744/4345]: training_loss: tensor(0.1520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3745/4345]: training_loss: tensor(0.1733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3746/4345]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3747/4345]: training_loss: tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3748/4345]: training_loss: tensor(0.3209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3749/4345]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3750/4345]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3751/4345]: training_loss: tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3752/4345]: training_loss: tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3753/4345]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3754/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3755/4345]: training_loss: tensor(0.0616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3756/4345]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3757/4345]: training_loss: tensor(0.1260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3758/4345]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3759/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3760/4345]: training_loss: tensor(0.1157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3761/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3762/4345]: training_loss: tensor(0.2740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3763/4345]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3764/4345]: training_loss: tensor(0.6081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3765/4345]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3766/4345]: training_loss: tensor(0.3483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3767/4345]: training_loss: tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3768/4345]: training_loss: tensor(0.0494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3769/4345]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3770/4345]: training_loss: tensor(0.1861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3771/4345]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3772/4345]: training_loss: tensor(0.1848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3773/4345]: training_loss: tensor(0.1505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3774/4345]: training_loss: tensor(0.2805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3775/4345]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3776/4345]: training_loss: tensor(0.3910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3777/4345]: training_loss: tensor(0.2215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3778/4345]: training_loss: tensor(0.2720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3779/4345]: training_loss: tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3780/4345]: training_loss: tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3781/4345]: training_loss: tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3782/4345]: training_loss: tensor(0.1499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3783/4345]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3784/4345]: training_loss: tensor(0.1362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3785/4345]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3786/4345]: training_loss: tensor(0.1533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3787/4345]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3788/4345]: training_loss: tensor(0.0413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3789/4345]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3790/4345]: training_loss: tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3791/4345]: training_loss: tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3792/4345]: training_loss: tensor(0.1407, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3793/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3794/4345]: training_loss: tensor(0.3229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3795/4345]: training_loss: tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3796/4345]: training_loss: tensor(0.9295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3797/4345]: training_loss: tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3798/4345]: training_loss: tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3799/4345]: training_loss: tensor(0.0469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3800/4345]: training_loss: tensor(0.3247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3801/4345]: training_loss: tensor(0.2866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3802/4345]: training_loss: tensor(0.1415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3803/4345]: training_loss: tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3804/4345]: training_loss: tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3805/4345]: training_loss: tensor(0.3178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3806/4345]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3807/4345]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3808/4345]: training_loss: tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3809/4345]: training_loss: tensor(0.1945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3810/4345]: training_loss: tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3811/4345]: training_loss: tensor(0.1889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3812/4345]: training_loss: tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3813/4345]: training_loss: tensor(0.1608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3814/4345]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3815/4345]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3816/4345]: training_loss: tensor(0.2119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3817/4345]: training_loss: tensor(0.1280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3818/4345]: training_loss: tensor(0.1768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3819/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3820/4345]: training_loss: tensor(0.3598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3821/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3822/4345]: training_loss: tensor(0.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3823/4345]: training_loss: tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3824/4345]: training_loss: tensor(0.2436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3825/4345]: training_loss: tensor(0.1072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3826/4345]: training_loss: tensor(0.2428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3827/4345]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3828/4345]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3829/4345]: training_loss: tensor(0.1779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3830/4345]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3831/4345]: training_loss: tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3832/4345]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3833/4345]: training_loss: tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3834/4345]: training_loss: tensor(0.2465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3835/4345]: training_loss: tensor(0.1711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3836/4345]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3837/4345]: training_loss: tensor(0.3308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3838/4345]: training_loss: tensor(0.3372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3839/4345]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3840/4345]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3841/4345]: training_loss: tensor(0.0501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3842/4345]: training_loss: tensor(0.0645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3843/4345]: training_loss: tensor(0.3103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3844/4345]: training_loss: tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3845/4345]: training_loss: tensor(0.1093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3846/4345]: training_loss: tensor(0.1735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3847/4345]: training_loss: tensor(0.2331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3848/4345]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3849/4345]: training_loss: tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3850/4345]: training_loss: tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3851/4345]: training_loss: tensor(0.3223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3852/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3853/4345]: training_loss: tensor(0.1197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3854/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3855/4345]: training_loss: tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3856/4345]: training_loss: tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3857/4345]: training_loss: tensor(0.3607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3858/4345]: training_loss: tensor(0.2024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3859/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3860/4345]: training_loss: tensor(0.1356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3861/4345]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3862/4345]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3863/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3864/4345]: training_loss: tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3865/4345]: training_loss: tensor(0.1781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3866/4345]: training_loss: tensor(0.3299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3867/4345]: training_loss: tensor(0.2078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3868/4345]: training_loss: tensor(0.1526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3869/4345]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3870/4345]: training_loss: tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3871/4345]: training_loss: tensor(0.2788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3872/4345]: training_loss: tensor(0.2484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3873/4345]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3874/4345]: training_loss: tensor(0.1165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3875/4345]: training_loss: tensor(0.2298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3876/4345]: training_loss: tensor(0.1108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3877/4345]: training_loss: tensor(0.1111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3878/4345]: training_loss: tensor(0.2192, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3879/4345]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3880/4345]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3881/4345]: training_loss: tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3882/4345]: training_loss: tensor(0.2143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3883/4345]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3884/4345]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3885/4345]: training_loss: tensor(0.1099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3886/4345]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3887/4345]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3888/4345]: training_loss: tensor(0.2941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3889/4345]: training_loss: tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3890/4345]: training_loss: tensor(0.2599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3891/4345]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3892/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3893/4345]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3894/4345]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3895/4345]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3896/4345]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3897/4345]: training_loss: tensor(0.2450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3898/4345]: training_loss: tensor(0.1861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3899/4345]: training_loss: tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3900/4345]: training_loss: tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3901/4345]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3902/4345]: training_loss: tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3903/4345]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3904/4345]: training_loss: tensor(0.1357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3905/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3906/4345]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3907/4345]: training_loss: tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3908/4345]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3909/4345]: training_loss: tensor(0.1433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3910/4345]: training_loss: tensor(0.1347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3911/4345]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3912/4345]: training_loss: tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3913/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3914/4345]: training_loss: tensor(0.2234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3915/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3916/4345]: training_loss: tensor(0.1339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3917/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3918/4345]: training_loss: tensor(0.1780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3919/4345]: training_loss: tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3920/4345]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3921/4345]: training_loss: tensor(0.1970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3922/4345]: training_loss: tensor(0.1099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3923/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3924/4345]: training_loss: tensor(0.1417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3925/4345]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3926/4345]: training_loss: tensor(0.2923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3927/4345]: training_loss: tensor(0.0555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3928/4345]: training_loss: tensor(0.1112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3929/4345]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3930/4345]: training_loss: tensor(0.1332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3931/4345]: training_loss: tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3932/4345]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3933/4345]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3934/4345]: training_loss: tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3935/4345]: training_loss: tensor(0.1502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3936/4345]: training_loss: tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3937/4345]: training_loss: tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3938/4345]: training_loss: tensor(0.5514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3939/4345]: training_loss: tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3940/4345]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3941/4345]: training_loss: tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3942/4345]: training_loss: tensor(0.2828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3943/4345]: training_loss: tensor(0.3811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3944/4345]: training_loss: tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3945/4345]: training_loss: tensor(0.5699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3946/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3947/4345]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3948/4345]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3949/4345]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3950/4345]: training_loss: tensor(0.3097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3951/4345]: training_loss: tensor(0.1137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3952/4345]: training_loss: tensor(0.6511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3953/4345]: training_loss: tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3954/4345]: training_loss: tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3955/4345]: training_loss: tensor(0.1111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3956/4345]: training_loss: tensor(0.2403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3957/4345]: training_loss: tensor(0.1465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3958/4345]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3959/4345]: training_loss: tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3960/4345]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3961/4345]: training_loss: tensor(0.2925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3962/4345]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3963/4345]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3964/4345]: training_loss: tensor(0.2085, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3965/4345]: training_loss: tensor(0.1947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3966/4345]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3967/4345]: training_loss: tensor(0.1974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3968/4345]: training_loss: tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3969/4345]: training_loss: tensor(0.1566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3970/4345]: training_loss: tensor(0.1941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3971/4345]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3972/4345]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3973/4345]: training_loss: tensor(0.3286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3974/4345]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3975/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3976/4345]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3977/4345]: training_loss: tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3978/4345]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3979/4345]: training_loss: tensor(0.1502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3980/4345]: training_loss: tensor(0.1742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3981/4345]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3982/4345]: training_loss: tensor(0.1504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3983/4345]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3984/4345]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3985/4345]: training_loss: tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3986/4345]: training_loss: tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3987/4345]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3988/4345]: training_loss: tensor(0.3981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3989/4345]: training_loss: tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3990/4345]: training_loss: tensor(0.5361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3991/4345]: training_loss: tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3992/4345]: training_loss: tensor(0.1239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3993/4345]: training_loss: tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3994/4345]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3995/4345]: training_loss: tensor(0.2489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3996/4345]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3997/4345]: training_loss: tensor(0.1590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3998/4345]: training_loss: tensor(0.0475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3999/4345]: training_loss: tensor(0.2377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4000/4345]: training_loss: tensor(0.5304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4001/4345]: training_loss: tensor(0.1197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4002/4345]: training_loss: tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4003/4345]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4004/4345]: training_loss: tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4005/4345]: training_loss: tensor(0.2059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4006/4345]: training_loss: tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4007/4345]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4008/4345]: training_loss: tensor(0.2801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4009/4345]: training_loss: tensor(0.3720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4010/4345]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4011/4345]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4012/4345]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4013/4345]: training_loss: tensor(0.1843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4014/4345]: training_loss: tensor(0.1631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4015/4345]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4016/4345]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4017/4345]: training_loss: tensor(0.2938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4018/4345]: training_loss: tensor(0.2812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4019/4345]: training_loss: tensor(0.2641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4020/4345]: training_loss: tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4021/4345]: training_loss: tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4022/4345]: training_loss: tensor(0.5080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4023/4345]: training_loss: tensor(0.3294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4024/4345]: training_loss: tensor(0.2487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4025/4345]: training_loss: tensor(0.1440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4026/4345]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4027/4345]: training_loss: tensor(0.1095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4028/4345]: training_loss: tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4029/4345]: training_loss: tensor(0.1234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4030/4345]: training_loss: tensor(0.1167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4031/4345]: training_loss: tensor(0.1409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4032/4345]: training_loss: tensor(0.1287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4033/4345]: training_loss: tensor(0.0459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4034/4345]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4035/4345]: training_loss: tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4036/4345]: training_loss: tensor(0.1225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4037/4345]: training_loss: tensor(0.3996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4038/4345]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4039/4345]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4040/4345]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4041/4345]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4042/4345]: training_loss: tensor(0.1566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4043/4345]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4044/4345]: training_loss: tensor(0.1504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4045/4345]: training_loss: tensor(0.5120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4046/4345]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4047/4345]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4048/4345]: training_loss: tensor(0.2912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4049/4345]: training_loss: tensor(0.1377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4050/4345]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4051/4345]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4052/4345]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4053/4345]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4054/4345]: training_loss: tensor(0.1309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4055/4345]: training_loss: tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4056/4345]: training_loss: tensor(0.1378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4057/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4058/4345]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4059/4345]: training_loss: tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4060/4345]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4061/4345]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4062/4345]: training_loss: tensor(0.1935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4063/4345]: training_loss: tensor(0.1494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4064/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4065/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4066/4345]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4067/4345]: training_loss: tensor(0.2122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4068/4345]: training_loss: tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4069/4345]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4070/4345]: training_loss: tensor(0.2228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4071/4345]: training_loss: tensor(0.1907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4072/4345]: training_loss: tensor(0.4621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4073/4345]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4074/4345]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4075/4345]: training_loss: tensor(0.1884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4076/4345]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4077/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4078/4345]: training_loss: tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4079/4345]: training_loss: tensor(0.2454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4080/4345]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4081/4345]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4082/4345]: training_loss: tensor(0.2226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4083/4345]: training_loss: tensor(0.2335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4084/4345]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4085/4345]: training_loss: tensor(0.1825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4086/4345]: training_loss: tensor(0.1355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4087/4345]: training_loss: tensor(0.1229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4088/4345]: training_loss: tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4089/4345]: training_loss: tensor(0.1430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4090/4345]: training_loss: tensor(0.1364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4091/4345]: training_loss: tensor(0.1349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4092/4345]: training_loss: tensor(0.2325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4093/4345]: training_loss: tensor(0.1103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4094/4345]: training_loss: tensor(0.0486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4095/4345]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4096/4345]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4097/4345]: training_loss: tensor(0.3127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4098/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4099/4345]: training_loss: tensor(0.2696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4100/4345]: training_loss: tensor(0.3285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4101/4345]: training_loss: tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4102/4345]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4103/4345]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4104/4345]: training_loss: tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4105/4345]: training_loss: tensor(0.1689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4106/4345]: training_loss: tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4107/4345]: training_loss: tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4108/4345]: training_loss: tensor(0.1291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4109/4345]: training_loss: tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4110/4345]: training_loss: tensor(0.0592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4111/4345]: training_loss: tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4112/4345]: training_loss: tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4113/4345]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4114/4345]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4115/4345]: training_loss: tensor(0.2158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4116/4345]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4117/4345]: training_loss: tensor(0.1707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4118/4345]: training_loss: tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4119/4345]: training_loss: tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4120/4345]: training_loss: tensor(0.3241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4121/4345]: training_loss: tensor(0.2357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4122/4345]: training_loss: tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4123/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4124/4345]: training_loss: tensor(0.2065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4125/4345]: training_loss: tensor(0.3780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4126/4345]: training_loss: tensor(0.1597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4127/4345]: training_loss: tensor(0.2434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4128/4345]: training_loss: tensor(0.1421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4129/4345]: training_loss: tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4130/4345]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4131/4345]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4132/4345]: training_loss: tensor(0.1790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4133/4345]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4134/4345]: training_loss: tensor(0.2218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4135/4345]: training_loss: tensor(0.2119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4136/4345]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4137/4345]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4138/4345]: training_loss: tensor(0.1292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4139/4345]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4140/4345]: training_loss: tensor(0.1350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4141/4345]: training_loss: tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4142/4345]: training_loss: tensor(0.1335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4143/4345]: training_loss: tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4144/4345]: training_loss: tensor(0.5459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4145/4345]: training_loss: tensor(0.2188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4146/4345]: training_loss: tensor(0.3871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4147/4345]: training_loss: tensor(0.3833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4148/4345]: training_loss: tensor(0.2080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4149/4345]: training_loss: tensor(0.5572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4150/4345]: training_loss: tensor(0.1570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4151/4345]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4152/4345]: training_loss: tensor(0.1302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4153/4345]: training_loss: tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4154/4345]: training_loss: tensor(0.1345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4155/4345]: training_loss: tensor(0.1118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4156/4345]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4157/4345]: training_loss: tensor(0.1360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4158/4345]: training_loss: tensor(0.1903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4159/4345]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4160/4345]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4161/4345]: training_loss: tensor(0.1525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4162/4345]: training_loss: tensor(0.3062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4163/4345]: training_loss: tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4164/4345]: training_loss: tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4165/4345]: training_loss: tensor(0.1233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4166/4345]: training_loss: tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4167/4345]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4168/4345]: training_loss: tensor(0.3784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4169/4345]: training_loss: tensor(0.1895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4170/4345]: training_loss: tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4171/4345]: training_loss: tensor(0.1392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4172/4345]: training_loss: tensor(0.3774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4173/4345]: training_loss: tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4174/4345]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4175/4345]: training_loss: tensor(0.1999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4176/4345]: training_loss: tensor(0.1526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4177/4345]: training_loss: tensor(0.1520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4178/4345]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4179/4345]: training_loss: tensor(0.2164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4180/4345]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4181/4345]: training_loss: tensor(0.1495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4182/4345]: training_loss: tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4183/4345]: training_loss: tensor(0.2311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4184/4345]: training_loss: tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4185/4345]: training_loss: tensor(0.2032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4186/4345]: training_loss: tensor(0.5471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4187/4345]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4188/4345]: training_loss: tensor(0.2124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4189/4345]: training_loss: tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4190/4345]: training_loss: tensor(0.3334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4191/4345]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4192/4345]: training_loss: tensor(0.1578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4193/4345]: training_loss: tensor(0.2134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4194/4345]: training_loss: tensor(0.3424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4195/4345]: training_loss: tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4196/4345]: training_loss: tensor(0.1799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4197/4345]: training_loss: tensor(0.2569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4198/4345]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4199/4345]: training_loss: tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4200/4345]: training_loss: tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4201/4345]: training_loss: tensor(0.2485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4202/4345]: training_loss: tensor(0.2855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4203/4345]: training_loss: tensor(0.2972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4204/4345]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4205/4345]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4206/4345]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4207/4345]: training_loss: tensor(0.1957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4208/4345]: training_loss: tensor(0.4258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4209/4345]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4210/4345]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4211/4345]: training_loss: tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4212/4345]: training_loss: tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4213/4345]: training_loss: tensor(0.1120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4214/4345]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4215/4345]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4216/4345]: training_loss: tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4217/4345]: training_loss: tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4218/4345]: training_loss: tensor(0.3789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4219/4345]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4220/4345]: training_loss: tensor(0.3061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4221/4345]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4222/4345]: training_loss: tensor(0.3298, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4223/4345]: training_loss: tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4224/4345]: training_loss: tensor(0.1538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4225/4345]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4226/4345]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4227/4345]: training_loss: tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4228/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4229/4345]: training_loss: tensor(0.1881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4230/4345]: training_loss: tensor(0.3458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4231/4345]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4232/4345]: training_loss: tensor(0.3694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4233/4345]: training_loss: tensor(0.1985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4234/4345]: training_loss: tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4235/4345]: training_loss: tensor(0.4058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4236/4345]: training_loss: tensor(0.2144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4237/4345]: training_loss: tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4238/4345]: training_loss: tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4239/4345]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4240/4345]: training_loss: tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4241/4345]: training_loss: tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4242/4345]: training_loss: tensor(0.3652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4243/4345]: training_loss: tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4244/4345]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4245/4345]: training_loss: tensor(0.2215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4246/4345]: training_loss: tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4247/4345]: training_loss: tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4248/4345]: training_loss: tensor(0.3253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4249/4345]: training_loss: tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4250/4345]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4251/4345]: training_loss: tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4252/4345]: training_loss: tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4253/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4254/4345]: training_loss: tensor(0.3172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4255/4345]: training_loss: tensor(0.2565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4256/4345]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4257/4345]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4258/4345]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4259/4345]: training_loss: tensor(0.3206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4260/4345]: training_loss: tensor(0.1410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4261/4345]: training_loss: tensor(0.1870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4262/4345]: training_loss: tensor(0.1279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4263/4345]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4264/4345]: training_loss: tensor(0.4030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4265/4345]: training_loss: tensor(0.0480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4266/4345]: training_loss: tensor(0.7112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4267/4345]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4268/4345]: training_loss: tensor(0.2389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4269/4345]: training_loss: tensor(0.2765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4270/4345]: training_loss: tensor(0.2932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4271/4345]: training_loss: tensor(0.0645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4272/4345]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4273/4345]: training_loss: tensor(0.2726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4274/4345]: training_loss: tensor(0.1655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4275/4345]: training_loss: tensor(0.2498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4276/4345]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4277/4345]: training_loss: tensor(0.2669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4278/4345]: training_loss: tensor(0.1442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4279/4345]: training_loss: tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4280/4345]: training_loss: tensor(0.2537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4281/4345]: training_loss: tensor(0.3377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4282/4345]: training_loss: tensor(0.1812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4283/4345]: training_loss: tensor(0.2178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4284/4345]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4285/4345]: training_loss: tensor(0.3784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4286/4345]: training_loss: tensor(0.1630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4287/4345]: training_loss: tensor(0.1430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4288/4345]: training_loss: tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4289/4345]: training_loss: tensor(0.3124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4290/4345]: training_loss: tensor(0.1624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4291/4345]: training_loss: tensor(0.2878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4292/4345]: training_loss: tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4293/4345]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4294/4345]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4295/4345]: training_loss: tensor(0.1345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4296/4345]: training_loss: tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4297/4345]: training_loss: tensor(0.2304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4298/4345]: training_loss: tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4299/4345]: training_loss: tensor(0.1599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4300/4345]: training_loss: tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4301/4345]: training_loss: tensor(0.2392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4302/4345]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4303/4345]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4304/4345]: training_loss: tensor(0.0469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4305/4345]: training_loss: tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4306/4345]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4307/4345]: training_loss: tensor(0.1329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4308/4345]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4309/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4310/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4311/4345]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4312/4345]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4313/4345]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4314/4345]: training_loss: tensor(0.5918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4315/4345]: training_loss: tensor(0.1536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4316/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4317/4345]: training_loss: tensor(0.4460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4318/4345]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4319/4345]: training_loss: tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4320/4345]: training_loss: tensor(0.1943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4321/4345]: training_loss: tensor(0.1567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4322/4345]: training_loss: tensor(0.2739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4323/4345]: training_loss: tensor(0.2917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4324/4345]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4325/4345]: training_loss: tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4326/4345]: training_loss: tensor(0.2533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4327/4345]: training_loss: tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4328/4345]: training_loss: tensor(0.1867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4329/4345]: training_loss: tensor(0.3113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4330/4345]: training_loss: tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4331/4345]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4332/4345]: training_loss: tensor(0.1506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4333/4345]: training_loss: tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4334/4345]: training_loss: tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4335/4345]: training_loss: tensor(0.1071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4336/4345]: training_loss: tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4337/4345]: training_loss: tensor(0.2981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4338/4345]: training_loss: tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4339/4345]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4340/4345]: training_loss: tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4341/4345]: training_loss: tensor(0.1188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4342/4345]: training_loss: tensor(0.1286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4343/4345]: training_loss: tensor(0.4654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4344/4345]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4345/4345]: training_loss: tensor(0.3957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4346/4345]: training_loss: tensor(0.3006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [1/5], Step [4346/21730], Train Loss: 0.1224, Valid Loss: 0.1194\n",
      "Model saved to ==> Model/model.pt\n",
      "Model saved to ==> Model/metrics.pt\n",
      "batch_no [1/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [5/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [6/4345]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [7/4345]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [8/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [9/4345]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [10/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [11/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [12/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [13/4345]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [14/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [15/4345]: training_loss: tensor(0.1493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [16/4345]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [17/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [18/4345]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [19/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [20/4345]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [21/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [22/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [23/4345]: training_loss: tensor(0.2899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [24/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [25/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [26/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [27/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [28/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [29/4345]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [30/4345]: training_loss: tensor(0.2284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [31/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [32/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [33/4345]: training_loss: tensor(0.1820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [34/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [35/4345]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [36/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [37/4345]: training_loss: tensor(0.2568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [38/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [39/4345]: training_loss: tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [40/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [41/4345]: training_loss: tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [42/4345]: training_loss: tensor(0.1458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [43/4345]: training_loss: tensor(0.2080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [44/4345]: training_loss: tensor(0.3020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [45/4345]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [46/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [47/4345]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [48/4345]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [49/4345]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [50/4345]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [51/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [52/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [53/4345]: training_loss: tensor(0.1464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [54/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [55/4345]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [56/4345]: training_loss: tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [57/4345]: training_loss: tensor(0.0389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [58/4345]: training_loss: tensor(0.2551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [59/4345]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [60/4345]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [61/4345]: training_loss: tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [62/4345]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [63/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [64/4345]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [65/4345]: training_loss: tensor(0.1850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [66/4345]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [67/4345]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [68/4345]: training_loss: tensor(0.4730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [69/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [70/4345]: training_loss: tensor(0.1543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [71/4345]: training_loss: tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [72/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [73/4345]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [74/4345]: training_loss: tensor(0.6778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [75/4345]: training_loss: tensor(0.1805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [76/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [77/4345]: training_loss: tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [78/4345]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [79/4345]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [80/4345]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [81/4345]: training_loss: tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [82/4345]: training_loss: tensor(0.1733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [83/4345]: training_loss: tensor(0.1258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [84/4345]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [85/4345]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [86/4345]: training_loss: tensor(0.1495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [87/4345]: training_loss: tensor(0.0342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [88/4345]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [89/4345]: training_loss: tensor(0.0389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [90/4345]: training_loss: tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [91/4345]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [92/4345]: training_loss: tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [93/4345]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [94/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [95/4345]: training_loss: tensor(0.1399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [96/4345]: training_loss: tensor(0.3800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [97/4345]: training_loss: tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [98/4345]: training_loss: tensor(0.1692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [99/4345]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [100/4345]: training_loss: tensor(0.2814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [101/4345]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [102/4345]: training_loss: tensor(0.3881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [103/4345]: training_loss: tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [104/4345]: training_loss: tensor(0.3002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [105/4345]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [106/4345]: training_loss: tensor(0.1167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [107/4345]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [108/4345]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [109/4345]: training_loss: tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [110/4345]: training_loss: tensor(0.1494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [111/4345]: training_loss: tensor(0.1883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [112/4345]: training_loss: tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [113/4345]: training_loss: tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [114/4345]: training_loss: tensor(0.1667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [115/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [116/4345]: training_loss: tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [117/4345]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [118/4345]: training_loss: tensor(0.3015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [119/4345]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [120/4345]: training_loss: tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [121/4345]: training_loss: tensor(0.0487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [122/4345]: training_loss: tensor(0.1643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [123/4345]: training_loss: tensor(0.5809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [124/4345]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [125/4345]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [126/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [127/4345]: training_loss: tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [128/4345]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [129/4345]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [130/4345]: training_loss: tensor(0.1191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [131/4345]: training_loss: tensor(0.2765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [132/4345]: training_loss: tensor(0.1440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [133/4345]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [134/4345]: training_loss: tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [135/4345]: training_loss: tensor(0.2970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [136/4345]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [137/4345]: training_loss: tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [138/4345]: training_loss: tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [139/4345]: training_loss: tensor(0.2339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [140/4345]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [141/4345]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [142/4345]: training_loss: tensor(0.1614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [143/4345]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [144/4345]: training_loss: tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [145/4345]: training_loss: tensor(0.3103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [146/4345]: training_loss: tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [147/4345]: training_loss: tensor(0.1139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [148/4345]: training_loss: tensor(0.1101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [149/4345]: training_loss: tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [150/4345]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [151/4345]: training_loss: tensor(0.0582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [152/4345]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [153/4345]: training_loss: tensor(0.2072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [154/4345]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [155/4345]: training_loss: tensor(0.2155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [156/4345]: training_loss: tensor(0.3219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [157/4345]: training_loss: tensor(0.2399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [158/4345]: training_loss: tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [159/4345]: training_loss: tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [160/4345]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [161/4345]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [162/4345]: training_loss: tensor(0.0462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [163/4345]: training_loss: tensor(0.0579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [164/4345]: training_loss: tensor(0.1471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [165/4345]: training_loss: tensor(0.2513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [166/4345]: training_loss: tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [167/4345]: training_loss: tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [168/4345]: training_loss: tensor(0.2495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [169/4345]: training_loss: tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [170/4345]: training_loss: tensor(0.1205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [171/4345]: training_loss: tensor(0.3727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [172/4345]: training_loss: tensor(0.2773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [173/4345]: training_loss: tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [174/4345]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [175/4345]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [176/4345]: training_loss: tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [177/4345]: training_loss: tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [178/4345]: training_loss: tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [179/4345]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [180/4345]: training_loss: tensor(0.2541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [181/4345]: training_loss: tensor(0.1085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [182/4345]: training_loss: tensor(0.2914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [183/4345]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [184/4345]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [185/4345]: training_loss: tensor(0.2635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [186/4345]: training_loss: tensor(0.1999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [187/4345]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [188/4345]: training_loss: tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [189/4345]: training_loss: tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [190/4345]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [191/4345]: training_loss: tensor(0.2501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [192/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [193/4345]: training_loss: tensor(0.1605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [194/4345]: training_loss: tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [195/4345]: training_loss: tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [196/4345]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [197/4345]: training_loss: tensor(0.4064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [198/4345]: training_loss: tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [199/4345]: training_loss: tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [200/4345]: training_loss: tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [201/4345]: training_loss: tensor(0.1783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [202/4345]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [203/4345]: training_loss: tensor(0.1559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [204/4345]: training_loss: tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [205/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [206/4345]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [207/4345]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [208/4345]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [209/4345]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [210/4345]: training_loss: tensor(0.0439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [211/4345]: training_loss: tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [212/4345]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [213/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [214/4345]: training_loss: tensor(0.2237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [215/4345]: training_loss: tensor(0.1849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [216/4345]: training_loss: tensor(0.1577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [217/4345]: training_loss: tensor(0.3636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [218/4345]: training_loss: tensor(0.3760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [219/4345]: training_loss: tensor(0.1503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [220/4345]: training_loss: tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [221/4345]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [222/4345]: training_loss: tensor(0.4431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [223/4345]: training_loss: tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [224/4345]: training_loss: tensor(0.2791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [225/4345]: training_loss: tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [226/4345]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [227/4345]: training_loss: tensor(0.2305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [228/4345]: training_loss: tensor(0.0545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [229/4345]: training_loss: tensor(0.1836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [230/4345]: training_loss: tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [231/4345]: training_loss: tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [232/4345]: training_loss: tensor(0.6131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [233/4345]: training_loss: tensor(0.1182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [234/4345]: training_loss: tensor(0.0541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [235/4345]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [236/4345]: training_loss: tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [237/4345]: training_loss: tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [238/4345]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [239/4345]: training_loss: tensor(0.0525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [240/4345]: training_loss: tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [241/4345]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [242/4345]: training_loss: tensor(0.2893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [243/4345]: training_loss: tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [244/4345]: training_loss: tensor(0.1444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [245/4345]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [246/4345]: training_loss: tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [247/4345]: training_loss: tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [248/4345]: training_loss: tensor(0.2174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [249/4345]: training_loss: tensor(0.1132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [250/4345]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [251/4345]: training_loss: tensor(0.2559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [252/4345]: training_loss: tensor(0.4116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [253/4345]: training_loss: tensor(0.2030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [254/4345]: training_loss: tensor(0.1504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [255/4345]: training_loss: tensor(0.1427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [256/4345]: training_loss: tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [257/4345]: training_loss: tensor(0.0603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [258/4345]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [259/4345]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [260/4345]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [261/4345]: training_loss: tensor(0.1902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [262/4345]: training_loss: tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [263/4345]: training_loss: tensor(0.2541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [264/4345]: training_loss: tensor(0.2685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [265/4345]: training_loss: tensor(0.1127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [266/4345]: training_loss: tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [267/4345]: training_loss: tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [268/4345]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [269/4345]: training_loss: tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [270/4345]: training_loss: tensor(0.5216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [271/4345]: training_loss: tensor(0.1432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [272/4345]: training_loss: tensor(0.1792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [273/4345]: training_loss: tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [274/4345]: training_loss: tensor(0.1627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [275/4345]: training_loss: tensor(0.1801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [276/4345]: training_loss: tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [277/4345]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [278/4345]: training_loss: tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [279/4345]: training_loss: tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [280/4345]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [281/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [282/4345]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [283/4345]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [284/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [285/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [286/4345]: training_loss: tensor(0.1201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [287/4345]: training_loss: tensor(0.2158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [288/4345]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [289/4345]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [290/4345]: training_loss: tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [291/4345]: training_loss: tensor(0.1908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [292/4345]: training_loss: tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [293/4345]: training_loss: tensor(0.0354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [294/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [295/4345]: training_loss: tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [296/4345]: training_loss: tensor(0.1944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [297/4345]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [298/4345]: training_loss: tensor(0.1131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [299/4345]: training_loss: tensor(0.2030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [300/4345]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [301/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [302/4345]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [303/4345]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [304/4345]: training_loss: tensor(0.2204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [305/4345]: training_loss: tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [306/4345]: training_loss: tensor(0.1692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [307/4345]: training_loss: tensor(0.1600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [308/4345]: training_loss: tensor(0.4198, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [309/4345]: training_loss: tensor(0.1809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [310/4345]: training_loss: tensor(0.3268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [311/4345]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [312/4345]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [313/4345]: training_loss: tensor(0.2385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [314/4345]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [315/4345]: training_loss: tensor(0.2232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [316/4345]: training_loss: tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [317/4345]: training_loss: tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [318/4345]: training_loss: tensor(0.1497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [319/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [320/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [321/4345]: training_loss: tensor(0.1341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [322/4345]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [323/4345]: training_loss: tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [324/4345]: training_loss: tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [325/4345]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [326/4345]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [327/4345]: training_loss: tensor(0.4348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [328/4345]: training_loss: tensor(0.1886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [329/4345]: training_loss: tensor(0.3370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [330/4345]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [331/4345]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [332/4345]: training_loss: tensor(0.1600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [333/4345]: training_loss: tensor(0.2803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [334/4345]: training_loss: tensor(0.2101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [335/4345]: training_loss: tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [336/4345]: training_loss: tensor(0.5881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [337/4345]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [338/4345]: training_loss: tensor(0.1538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [339/4345]: training_loss: tensor(0.1716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [340/4345]: training_loss: tensor(0.3322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [341/4345]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [342/4345]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [343/4345]: training_loss: tensor(0.1201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [344/4345]: training_loss: tensor(0.3155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [345/4345]: training_loss: tensor(0.0683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [346/4345]: training_loss: tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [347/4345]: training_loss: tensor(0.2477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [348/4345]: training_loss: tensor(0.1642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [349/4345]: training_loss: tensor(0.1632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [350/4345]: training_loss: tensor(0.1633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [351/4345]: training_loss: tensor(0.1791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [352/4345]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [353/4345]: training_loss: tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [354/4345]: training_loss: tensor(0.2189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [355/4345]: training_loss: tensor(0.2717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [356/4345]: training_loss: tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [357/4345]: training_loss: tensor(0.2014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [358/4345]: training_loss: tensor(0.1692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [359/4345]: training_loss: tensor(0.1541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [360/4345]: training_loss: tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [361/4345]: training_loss: tensor(0.0534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [362/4345]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [363/4345]: training_loss: tensor(0.0587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [364/4345]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [365/4345]: training_loss: tensor(0.2191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [366/4345]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [367/4345]: training_loss: tensor(0.1071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [368/4345]: training_loss: tensor(0.1743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [369/4345]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [370/4345]: training_loss: tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [371/4345]: training_loss: tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [372/4345]: training_loss: tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [373/4345]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [374/4345]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [375/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [376/4345]: training_loss: tensor(0.1076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [377/4345]: training_loss: tensor(0.3084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [378/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [379/4345]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [380/4345]: training_loss: tensor(0.3392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [381/4345]: training_loss: tensor(0.4475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [382/4345]: training_loss: tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [383/4345]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [384/4345]: training_loss: tensor(0.2113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [385/4345]: training_loss: tensor(0.0313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [386/4345]: training_loss: tensor(0.3514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [387/4345]: training_loss: tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [388/4345]: training_loss: tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [389/4345]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [390/4345]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [391/4345]: training_loss: tensor(0.2037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [392/4345]: training_loss: tensor(0.1592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [393/4345]: training_loss: tensor(0.0462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [394/4345]: training_loss: tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [395/4345]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [396/4345]: training_loss: tensor(0.1812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [397/4345]: training_loss: tensor(0.0615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [398/4345]: training_loss: tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [399/4345]: training_loss: tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [400/4345]: training_loss: tensor(0.1542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [401/4345]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [402/4345]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [403/4345]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [404/4345]: training_loss: tensor(0.0621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [405/4345]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [406/4345]: training_loss: tensor(0.1289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [407/4345]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [408/4345]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [409/4345]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [410/4345]: training_loss: tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [411/4345]: training_loss: tensor(0.1516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [412/4345]: training_loss: tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [413/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [414/4345]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [415/4345]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [416/4345]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [417/4345]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [418/4345]: training_loss: tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [419/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [420/4345]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [421/4345]: training_loss: tensor(0.1516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [422/4345]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [423/4345]: training_loss: tensor(0.2487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [424/4345]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [425/4345]: training_loss: tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [426/4345]: training_loss: tensor(0.2343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [427/4345]: training_loss: tensor(0.1356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [428/4345]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [429/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [430/4345]: training_loss: tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [431/4345]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [432/4345]: training_loss: tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [433/4345]: training_loss: tensor(0.1127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [434/4345]: training_loss: tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [435/4345]: training_loss: tensor(0.5148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [436/4345]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [437/4345]: training_loss: tensor(0.3929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [438/4345]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [439/4345]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [440/4345]: training_loss: tensor(0.2170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [441/4345]: training_loss: tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [442/4345]: training_loss: tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [443/4345]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [444/4345]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [445/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [446/4345]: training_loss: tensor(0.1945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [447/4345]: training_loss: tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [448/4345]: training_loss: tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [449/4345]: training_loss: tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [450/4345]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [451/4345]: training_loss: tensor(0.1470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [452/4345]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [453/4345]: training_loss: tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [454/4345]: training_loss: tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [455/4345]: training_loss: tensor(0.4318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [456/4345]: training_loss: tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [457/4345]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [458/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [459/4345]: training_loss: tensor(0.1978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [460/4345]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [461/4345]: training_loss: tensor(0.1542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [462/4345]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [463/4345]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [464/4345]: training_loss: tensor(0.1121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [465/4345]: training_loss: tensor(0.1160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [466/4345]: training_loss: tensor(0.0493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [467/4345]: training_loss: tensor(0.1489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [468/4345]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [469/4345]: training_loss: tensor(0.1576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [470/4345]: training_loss: tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [471/4345]: training_loss: tensor(0.3184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [472/4345]: training_loss: tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [473/4345]: training_loss: tensor(0.0404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [474/4345]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [475/4345]: training_loss: tensor(0.4915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [476/4345]: training_loss: tensor(0.1653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [477/4345]: training_loss: tensor(0.3380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [478/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [479/4345]: training_loss: tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [480/4345]: training_loss: tensor(0.2956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [481/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [482/4345]: training_loss: tensor(0.1494, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [483/4345]: training_loss: tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [484/4345]: training_loss: tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [485/4345]: training_loss: tensor(0.2418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [486/4345]: training_loss: tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [487/4345]: training_loss: tensor(0.1555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [488/4345]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [489/4345]: training_loss: tensor(0.1363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [490/4345]: training_loss: tensor(0.0354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [491/4345]: training_loss: tensor(0.1661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [492/4345]: training_loss: tensor(0.1540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [493/4345]: training_loss: tensor(0.2092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [494/4345]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [495/4345]: training_loss: tensor(0.2618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [496/4345]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [497/4345]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [498/4345]: training_loss: tensor(0.1379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [499/4345]: training_loss: tensor(0.1758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [500/4345]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [501/4345]: training_loss: tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [502/4345]: training_loss: tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [503/4345]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [504/4345]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [505/4345]: training_loss: tensor(0.5427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [506/4345]: training_loss: tensor(0.1222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [507/4345]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [508/4345]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [509/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [510/4345]: training_loss: tensor(0.1202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [511/4345]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [512/4345]: training_loss: tensor(0.1855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [513/4345]: training_loss: tensor(0.1197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [514/4345]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [515/4345]: training_loss: tensor(0.2964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [516/4345]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [517/4345]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [518/4345]: training_loss: tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [519/4345]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [520/4345]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [521/4345]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [522/4345]: training_loss: tensor(0.2104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [523/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [524/4345]: training_loss: tensor(0.1417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [525/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [526/4345]: training_loss: tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [527/4345]: training_loss: tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [528/4345]: training_loss: tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [529/4345]: training_loss: tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [530/4345]: training_loss: tensor(0.0342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [531/4345]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [532/4345]: training_loss: tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [533/4345]: training_loss: tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [534/4345]: training_loss: tensor(0.2039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [535/4345]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [536/4345]: training_loss: tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [537/4345]: training_loss: tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [538/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [539/4345]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [540/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [541/4345]: training_loss: tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [542/4345]: training_loss: tensor(0.4377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [543/4345]: training_loss: tensor(0.4288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [544/4345]: training_loss: tensor(0.1878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [545/4345]: training_loss: tensor(0.1381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [546/4345]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [547/4345]: training_loss: tensor(0.0576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [548/4345]: training_loss: tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [549/4345]: training_loss: tensor(0.2083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [550/4345]: training_loss: tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [551/4345]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [552/4345]: training_loss: tensor(0.1420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [553/4345]: training_loss: tensor(0.1879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [554/4345]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [555/4345]: training_loss: tensor(0.2560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [556/4345]: training_loss: tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [557/4345]: training_loss: tensor(0.1456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [558/4345]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [559/4345]: training_loss: tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [560/4345]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [561/4345]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [562/4345]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [563/4345]: training_loss: tensor(0.1456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [564/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [565/4345]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [566/4345]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [567/4345]: training_loss: tensor(0.2357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [568/4345]: training_loss: tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [569/4345]: training_loss: tensor(0.1426, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [570/4345]: training_loss: tensor(0.1177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [571/4345]: training_loss: tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [572/4345]: training_loss: tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [573/4345]: training_loss: tensor(0.2831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [574/4345]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [575/4345]: training_loss: tensor(0.2724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [576/4345]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [577/4345]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [578/4345]: training_loss: tensor(0.1721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [579/4345]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [580/4345]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [581/4345]: training_loss: tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [582/4345]: training_loss: tensor(0.1603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [583/4345]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [584/4345]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [585/4345]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [586/4345]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [587/4345]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [588/4345]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [589/4345]: training_loss: tensor(0.1509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [590/4345]: training_loss: tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [591/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [592/4345]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [593/4345]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [594/4345]: training_loss: tensor(0.2887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [595/4345]: training_loss: tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [596/4345]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [597/4345]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [598/4345]: training_loss: tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [599/4345]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [600/4345]: training_loss: tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [601/4345]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [602/4345]: training_loss: tensor(0.6179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [603/4345]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [604/4345]: training_loss: tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [605/4345]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [606/4345]: training_loss: tensor(0.1572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [607/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [608/4345]: training_loss: tensor(0.1352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [609/4345]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [610/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [611/4345]: training_loss: tensor(0.1902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [612/4345]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [613/4345]: training_loss: tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [614/4345]: training_loss: tensor(0.1742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [615/4345]: training_loss: tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [616/4345]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [617/4345]: training_loss: tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [618/4345]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [619/4345]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [620/4345]: training_loss: tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [621/4345]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [622/4345]: training_loss: tensor(0.3722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [623/4345]: training_loss: tensor(0.3664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [624/4345]: training_loss: tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [625/4345]: training_loss: tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [626/4345]: training_loss: tensor(0.1660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [627/4345]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [628/4345]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [629/4345]: training_loss: tensor(0.2822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [630/4345]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [631/4345]: training_loss: tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [632/4345]: training_loss: tensor(0.3404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [633/4345]: training_loss: tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [634/4345]: training_loss: tensor(0.2357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [635/4345]: training_loss: tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [636/4345]: training_loss: tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [637/4345]: training_loss: tensor(0.1796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [638/4345]: training_loss: tensor(0.0603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [639/4345]: training_loss: tensor(0.3212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [640/4345]: training_loss: tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [641/4345]: training_loss: tensor(0.1408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [642/4345]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [643/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [644/4345]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [645/4345]: training_loss: tensor(0.2826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [646/4345]: training_loss: tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [647/4345]: training_loss: tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [648/4345]: training_loss: tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [649/4345]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [650/4345]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [651/4345]: training_loss: tensor(0.4264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [652/4345]: training_loss: tensor(0.3435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [653/4345]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [654/4345]: training_loss: tensor(0.1193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [655/4345]: training_loss: tensor(0.2352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [656/4345]: training_loss: tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [657/4345]: training_loss: tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [658/4345]: training_loss: tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [659/4345]: training_loss: tensor(0.1098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [660/4345]: training_loss: tensor(0.1750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [661/4345]: training_loss: tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [662/4345]: training_loss: tensor(0.1942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [663/4345]: training_loss: tensor(0.1880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [664/4345]: training_loss: tensor(0.2140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [665/4345]: training_loss: tensor(0.1312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [666/4345]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [667/4345]: training_loss: tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [668/4345]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [669/4345]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [670/4345]: training_loss: tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [671/4345]: training_loss: tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [672/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [673/4345]: training_loss: tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [674/4345]: training_loss: tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [675/4345]: training_loss: tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [676/4345]: training_loss: tensor(0.3324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [677/4345]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [678/4345]: training_loss: tensor(0.3001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [679/4345]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [680/4345]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [681/4345]: training_loss: tensor(0.1450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [682/4345]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [683/4345]: training_loss: tensor(0.2067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [684/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [685/4345]: training_loss: tensor(0.2595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [686/4345]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [687/4345]: training_loss: tensor(0.3709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [688/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [689/4345]: training_loss: tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [690/4345]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [691/4345]: training_loss: tensor(0.1316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [692/4345]: training_loss: tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [693/4345]: training_loss: tensor(0.1656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [694/4345]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [695/4345]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [696/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [697/4345]: training_loss: tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [698/4345]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [699/4345]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [700/4345]: training_loss: tensor(0.1442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [701/4345]: training_loss: tensor(0.1894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [702/4345]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [703/4345]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [704/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [705/4345]: training_loss: tensor(0.1430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [706/4345]: training_loss: tensor(0.1593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [707/4345]: training_loss: tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [708/4345]: training_loss: tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [709/4345]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [710/4345]: training_loss: tensor(0.3076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [711/4345]: training_loss: tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [712/4345]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [713/4345]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [714/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [715/4345]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [716/4345]: training_loss: tensor(0.1503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [717/4345]: training_loss: tensor(0.0541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [718/4345]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [719/4345]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [720/4345]: training_loss: tensor(0.2129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [721/4345]: training_loss: tensor(0.1709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [722/4345]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [723/4345]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [724/4345]: training_loss: tensor(0.2941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [725/4345]: training_loss: tensor(0.1601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [726/4345]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [727/4345]: training_loss: tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [728/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [729/4345]: training_loss: tensor(0.1989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [730/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [731/4345]: training_loss: tensor(0.4233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [732/4345]: training_loss: tensor(0.1782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [733/4345]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [734/4345]: training_loss: tensor(0.0615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [735/4345]: training_loss: tensor(0.0364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [736/4345]: training_loss: tensor(0.0603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [737/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [738/4345]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [739/4345]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [740/4345]: training_loss: tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [741/4345]: training_loss: tensor(0.0475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [742/4345]: training_loss: tensor(0.1201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [743/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [744/4345]: training_loss: tensor(0.1394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [745/4345]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [746/4345]: training_loss: tensor(0.2680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [747/4345]: training_loss: tensor(0.1259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [748/4345]: training_loss: tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [749/4345]: training_loss: tensor(0.0494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [750/4345]: training_loss: tensor(0.1153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [751/4345]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [752/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [753/4345]: training_loss: tensor(0.0629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [754/4345]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [755/4345]: training_loss: tensor(0.1843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [756/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [757/4345]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [758/4345]: training_loss: tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [759/4345]: training_loss: tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [760/4345]: training_loss: tensor(0.1919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [761/4345]: training_loss: tensor(0.1171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [762/4345]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [763/4345]: training_loss: tensor(0.1127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [764/4345]: training_loss: tensor(0.2037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [765/4345]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [766/4345]: training_loss: tensor(0.2427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [767/4345]: training_loss: tensor(0.2371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [768/4345]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [769/4345]: training_loss: tensor(0.1729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [770/4345]: training_loss: tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [771/4345]: training_loss: tensor(0.3568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [772/4345]: training_loss: tensor(0.1737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [773/4345]: training_loss: tensor(0.6144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [774/4345]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [775/4345]: training_loss: tensor(0.3509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [776/4345]: training_loss: tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [777/4345]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [778/4345]: training_loss: tensor(0.2644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [779/4345]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [780/4345]: training_loss: tensor(0.2486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [781/4345]: training_loss: tensor(0.1128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [782/4345]: training_loss: tensor(0.1823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [783/4345]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [784/4345]: training_loss: tensor(0.2509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [785/4345]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [786/4345]: training_loss: tensor(0.1133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [787/4345]: training_loss: tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [788/4345]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [789/4345]: training_loss: tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [790/4345]: training_loss: tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [791/4345]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [792/4345]: training_loss: tensor(0.1581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [793/4345]: training_loss: tensor(0.1782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [794/4345]: training_loss: tensor(0.2353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [795/4345]: training_loss: tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [796/4345]: training_loss: tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [797/4345]: training_loss: tensor(0.1373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [798/4345]: training_loss: tensor(0.2385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [799/4345]: training_loss: tensor(0.2639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [800/4345]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [801/4345]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [802/4345]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [803/4345]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [804/4345]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [805/4345]: training_loss: tensor(0.1471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [806/4345]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [807/4345]: training_loss: tensor(0.4845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [808/4345]: training_loss: tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [809/4345]: training_loss: tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [810/4345]: training_loss: tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [811/4345]: training_loss: tensor(0.0573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [812/4345]: training_loss: tensor(0.1793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [813/4345]: training_loss: tensor(0.1910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [814/4345]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [815/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [816/4345]: training_loss: tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [817/4345]: training_loss: tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [818/4345]: training_loss: tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [819/4345]: training_loss: tensor(0.2466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [820/4345]: training_loss: tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [821/4345]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [822/4345]: training_loss: tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [823/4345]: training_loss: tensor(0.0507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [824/4345]: training_loss: tensor(0.1784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [825/4345]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [826/4345]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [827/4345]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [828/4345]: training_loss: tensor(0.1436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [829/4345]: training_loss: tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [830/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [831/4345]: training_loss: tensor(0.3776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [832/4345]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [833/4345]: training_loss: tensor(0.0342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [834/4345]: training_loss: tensor(0.3074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [835/4345]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [836/4345]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [837/4345]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [838/4345]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [839/4345]: training_loss: tensor(0.2613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [840/4345]: training_loss: tensor(0.1499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [841/4345]: training_loss: tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [842/4345]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [843/4345]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [844/4345]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [845/4345]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [846/4345]: training_loss: tensor(0.1127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [847/4345]: training_loss: tensor(0.3475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [848/4345]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [849/4345]: training_loss: tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [850/4345]: training_loss: tensor(0.1396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [851/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [852/4345]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [853/4345]: training_loss: tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [854/4345]: training_loss: tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [855/4345]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [856/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [857/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [858/4345]: training_loss: tensor(0.6443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [859/4345]: training_loss: tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [860/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [861/4345]: training_loss: tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [862/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [863/4345]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [864/4345]: training_loss: tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [865/4345]: training_loss: tensor(0.2661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [866/4345]: training_loss: tensor(0.4982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [867/4345]: training_loss: tensor(0.2558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [868/4345]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [869/4345]: training_loss: tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [870/4345]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [871/4345]: training_loss: tensor(0.3770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [872/4345]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [873/4345]: training_loss: tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [874/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [875/4345]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [876/4345]: training_loss: tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [877/4345]: training_loss: tensor(0.0573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [878/4345]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [879/4345]: training_loss: tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [880/4345]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [881/4345]: training_loss: tensor(0.3049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [882/4345]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [883/4345]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [884/4345]: training_loss: tensor(0.1285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [885/4345]: training_loss: tensor(0.1374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [886/4345]: training_loss: tensor(0.2029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [887/4345]: training_loss: tensor(0.1143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [888/4345]: training_loss: tensor(0.1842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [889/4345]: training_loss: tensor(0.3542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [890/4345]: training_loss: tensor(0.1904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [891/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [892/4345]: training_loss: tensor(0.1542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [893/4345]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [894/4345]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [895/4345]: training_loss: tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [896/4345]: training_loss: tensor(0.2007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [897/4345]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [898/4345]: training_loss: tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [899/4345]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [900/4345]: training_loss: tensor(0.1512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [901/4345]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [902/4345]: training_loss: tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [903/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [904/4345]: training_loss: tensor(0.3970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [905/4345]: training_loss: tensor(0.0404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [906/4345]: training_loss: tensor(0.3977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [907/4345]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [908/4345]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [909/4345]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [910/4345]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [911/4345]: training_loss: tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [912/4345]: training_loss: tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [913/4345]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [914/4345]: training_loss: tensor(0.1690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [915/4345]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [916/4345]: training_loss: tensor(0.5900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [917/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [918/4345]: training_loss: tensor(0.1580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [919/4345]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [920/4345]: training_loss: tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [921/4345]: training_loss: tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [922/4345]: training_loss: tensor(0.1563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [923/4345]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [924/4345]: training_loss: tensor(0.1379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [925/4345]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [926/4345]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [927/4345]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [928/4345]: training_loss: tensor(0.0502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [929/4345]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [930/4345]: training_loss: tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [931/4345]: training_loss: tensor(0.2044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [932/4345]: training_loss: tensor(0.1280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [933/4345]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [934/4345]: training_loss: tensor(0.1567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [935/4345]: training_loss: tensor(0.1758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [936/4345]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [937/4345]: training_loss: tensor(0.1186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [938/4345]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [939/4345]: training_loss: tensor(0.4388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [940/4345]: training_loss: tensor(0.2964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [941/4345]: training_loss: tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [942/4345]: training_loss: tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [943/4345]: training_loss: tensor(0.1230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [944/4345]: training_loss: tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [945/4345]: training_loss: tensor(0.2526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [946/4345]: training_loss: tensor(0.1754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [947/4345]: training_loss: tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [948/4345]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [949/4345]: training_loss: tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [950/4345]: training_loss: tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [951/4345]: training_loss: tensor(0.2577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [952/4345]: training_loss: tensor(0.2232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [953/4345]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [954/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [955/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [956/4345]: training_loss: tensor(0.2068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [957/4345]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [958/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [959/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [960/4345]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [961/4345]: training_loss: tensor(0.1657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [962/4345]: training_loss: tensor(0.3149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [963/4345]: training_loss: tensor(0.1892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [964/4345]: training_loss: tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [965/4345]: training_loss: tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [966/4345]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [967/4345]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [968/4345]: training_loss: tensor(0.1142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [969/4345]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [970/4345]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [971/4345]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [972/4345]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [973/4345]: training_loss: tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [974/4345]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [975/4345]: training_loss: tensor(0.1731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [976/4345]: training_loss: tensor(0.1378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [977/4345]: training_loss: tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [978/4345]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [979/4345]: training_loss: tensor(0.2943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [980/4345]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [981/4345]: training_loss: tensor(0.2338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [982/4345]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [983/4345]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [984/4345]: training_loss: tensor(0.1607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [985/4345]: training_loss: tensor(0.1423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [986/4345]: training_loss: tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [987/4345]: training_loss: tensor(0.2084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [988/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [989/4345]: training_loss: tensor(0.2091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [990/4345]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [991/4345]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [992/4345]: training_loss: tensor(0.2154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [993/4345]: training_loss: tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [994/4345]: training_loss: tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [995/4345]: training_loss: tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [996/4345]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [997/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [998/4345]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [999/4345]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1000/4345]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1001/4345]: training_loss: tensor(0.1527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1002/4345]: training_loss: tensor(0.1384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1003/4345]: training_loss: tensor(0.1384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1004/4345]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1005/4345]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/4345]: training_loss: tensor(0.2583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/4345]: training_loss: tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/4345]: training_loss: tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/4345]: training_loss: tensor(0.2171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/4345]: training_loss: tensor(0.1564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/4345]: training_loss: tensor(0.1937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/4345]: training_loss: tensor(0.0354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/4345]: training_loss: tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/4345]: training_loss: tensor(0.1108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/4345]: training_loss: tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/4345]: training_loss: tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/4345]: training_loss: tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1019/4345]: training_loss: tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/4345]: training_loss: tensor(0.0486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/4345]: training_loss: tensor(0.1308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/4345]: training_loss: tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1023/4345]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/4345]: training_loss: tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/4345]: training_loss: tensor(0.0445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/4345]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/4345]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/4345]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/4345]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/4345]: training_loss: tensor(0.2128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/4345]: training_loss: tensor(0.4204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/4345]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/4345]: training_loss: tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/4345]: training_loss: tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1040/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/4345]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1043/4345]: training_loss: tensor(0.1521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/4345]: training_loss: tensor(0.1321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/4345]: training_loss: tensor(0.4537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/4345]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/4345]: training_loss: tensor(0.3958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/4345]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/4345]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/4345]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/4345]: training_loss: tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/4345]: training_loss: tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/4345]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/4345]: training_loss: tensor(0.2519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/4345]: training_loss: tensor(0.1668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/4345]: training_loss: tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/4345]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/4345]: training_loss: tensor(0.1480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/4345]: training_loss: tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/4345]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/4345]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/4345]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/4345]: training_loss: tensor(0.0507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/4345]: training_loss: tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/4345]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/4345]: training_loss: tensor(0.2126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/4345]: training_loss: tensor(0.1149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/4345]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/4345]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/4345]: training_loss: tensor(0.1471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/4345]: training_loss: tensor(0.0517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/4345]: training_loss: tensor(0.1483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/4345]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/4345]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/4345]: training_loss: tensor(0.3146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/4345]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/4345]: training_loss: tensor(0.3039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/4345]: training_loss: tensor(0.1352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/4345]: training_loss: tensor(0.2341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/4345]: training_loss: tensor(0.2154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/4345]: training_loss: tensor(0.0446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/4345]: training_loss: tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/4345]: training_loss: tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/4345]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/4345]: training_loss: tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/4345]: training_loss: tensor(0.2164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/4345]: training_loss: tensor(0.2502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/4345]: training_loss: tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1091/4345]: training_loss: tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/4345]: training_loss: tensor(0.1434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/4345]: training_loss: tensor(0.1667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/4345]: training_loss: tensor(0.1113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/4345]: training_loss: tensor(0.6222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/4345]: training_loss: tensor(0.3196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/4345]: training_loss: tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/4345]: training_loss: tensor(0.1535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1100/4345]: training_loss: tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/4345]: training_loss: tensor(0.3867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/4345]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/4345]: training_loss: tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/4345]: training_loss: tensor(0.1449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/4345]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/4345]: training_loss: tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/4345]: training_loss: tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/4345]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/4345]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/4345]: training_loss: tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/4345]: training_loss: tensor(0.1697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/4345]: training_loss: tensor(0.1319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/4345]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/4345]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/4345]: training_loss: tensor(0.1132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/4345]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/4345]: training_loss: tensor(0.1134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/4345]: training_loss: tensor(0.1924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/4345]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/4345]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/4345]: training_loss: tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/4345]: training_loss: tensor(0.2054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/4345]: training_loss: tensor(0.1729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/4345]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1129/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/4345]: training_loss: tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/4345]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/4345]: training_loss: tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/4345]: training_loss: tensor(0.4227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/4345]: training_loss: tensor(0.2432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/4345]: training_loss: tensor(0.1694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/4345]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/4345]: training_loss: tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/4345]: training_loss: tensor(0.1363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/4345]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/4345]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/4345]: training_loss: tensor(0.4052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/4345]: training_loss: tensor(0.3078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/4345]: training_loss: tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/4345]: training_loss: tensor(0.2428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/4345]: training_loss: tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/4345]: training_loss: tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/4345]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/4345]: training_loss: tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/4345]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/4345]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/4345]: training_loss: tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/4345]: training_loss: tensor(0.0502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/4345]: training_loss: tensor(0.2149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/4345]: training_loss: tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/4345]: training_loss: tensor(0.1267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/4345]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/4345]: training_loss: tensor(0.1561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/4345]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/4345]: training_loss: tensor(0.1956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/4345]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/4345]: training_loss: tensor(0.1859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/4345]: training_loss: tensor(0.1139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/4345]: training_loss: tensor(0.3111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/4345]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/4345]: training_loss: tensor(0.1195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/4345]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/4345]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/4345]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/4345]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/4345]: training_loss: tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/4345]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/4345]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/4345]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/4345]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1177/4345]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/4345]: training_loss: tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/4345]: training_loss: tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/4345]: training_loss: tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/4345]: training_loss: tensor(0.2248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/4345]: training_loss: tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/4345]: training_loss: tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/4345]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/4345]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/4345]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/4345]: training_loss: tensor(0.1956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/4345]: training_loss: tensor(0.2248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/4345]: training_loss: tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/4345]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/4345]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/4345]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/4345]: training_loss: tensor(0.1918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/4345]: training_loss: tensor(0.2314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/4345]: training_loss: tensor(0.2636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/4345]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/4345]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/4345]: training_loss: tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/4345]: training_loss: tensor(0.1728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/4345]: training_loss: tensor(0.1389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/4345]: training_loss: tensor(0.1209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/4345]: training_loss: tensor(0.4278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/4345]: training_loss: tensor(0.3681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/4345]: training_loss: tensor(0.1987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/4345]: training_loss: tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/4345]: training_loss: tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1213/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1215/4345]: training_loss: tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/4345]: training_loss: tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/4345]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/4345]: training_loss: tensor(0.7246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/4345]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/4345]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/4345]: training_loss: tensor(0.0383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/4345]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/4345]: training_loss: tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/4345]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/4345]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/4345]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/4345]: training_loss: tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/4345]: training_loss: tensor(0.1423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/4345]: training_loss: tensor(0.1683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/4345]: training_loss: tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/4345]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/4345]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/4345]: training_loss: tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/4345]: training_loss: tensor(0.1829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/4345]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/4345]: training_loss: tensor(0.1402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/4345]: training_loss: tensor(0.2055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/4345]: training_loss: tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/4345]: training_loss: tensor(0.0596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/4345]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/4345]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/4345]: training_loss: tensor(0.1888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/4345]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/4345]: training_loss: tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/4345]: training_loss: tensor(0.1787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/4345]: training_loss: tensor(0.1475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/4345]: training_loss: tensor(0.1418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/4345]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/4345]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/4345]: training_loss: tensor(0.0621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/4345]: training_loss: tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/4345]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/4345]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/4345]: training_loss: tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/4345]: training_loss: tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/4345]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/4345]: training_loss: tensor(0.3053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/4345]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1263/4345]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/4345]: training_loss: tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/4345]: training_loss: tensor(0.3121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/4345]: training_loss: tensor(0.2170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/4345]: training_loss: tensor(0.1317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/4345]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/4345]: training_loss: tensor(0.5237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/4345]: training_loss: tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/4345]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/4345]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/4345]: training_loss: tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/4345]: training_loss: tensor(0.1691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/4345]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/4345]: training_loss: tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/4345]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/4345]: training_loss: tensor(0.2622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/4345]: training_loss: tensor(0.1546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/4345]: training_loss: tensor(0.0570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/4345]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/4345]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/4345]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/4345]: training_loss: tensor(0.1503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/4345]: training_loss: tensor(0.1133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/4345]: training_loss: tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/4345]: training_loss: tensor(0.1436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/4345]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/4345]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/4345]: training_loss: tensor(0.1489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/4345]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/4345]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/4345]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1299/4345]: training_loss: tensor(0.3193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/4345]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/4345]: training_loss: tensor(0.1954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/4345]: training_loss: tensor(0.2648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/4345]: training_loss: tensor(0.1636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/4345]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/4345]: training_loss: tensor(0.2549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/4345]: training_loss: tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/4345]: training_loss: tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/4345]: training_loss: tensor(0.3310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/4345]: training_loss: tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/4345]: training_loss: tensor(0.1812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/4345]: training_loss: tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/4345]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/4345]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/4345]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/4345]: training_loss: tensor(0.1169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/4345]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/4345]: training_loss: tensor(0.1270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/4345]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/4345]: training_loss: tensor(0.4695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/4345]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/4345]: training_loss: tensor(0.0384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/4345]: training_loss: tensor(0.1820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/4345]: training_loss: tensor(0.3214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/4345]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/4345]: training_loss: tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/4345]: training_loss: tensor(0.0480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/4345]: training_loss: tensor(0.2169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/4345]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/4345]: training_loss: tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/4345]: training_loss: tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/4345]: training_loss: tensor(0.0445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/4345]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/4345]: training_loss: tensor(0.1299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/4345]: training_loss: tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/4345]: training_loss: tensor(0.2488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/4345]: training_loss: tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/4345]: training_loss: tensor(0.0446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/4345]: training_loss: tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/4345]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/4345]: training_loss: tensor(0.2646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/4345]: training_loss: tensor(0.1458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/4345]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1349/4345]: training_loss: tensor(0.1250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/4345]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/4345]: training_loss: tensor(0.1911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/4345]: training_loss: tensor(0.2409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/4345]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/4345]: training_loss: tensor(0.0459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/4345]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/4345]: training_loss: tensor(0.2080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/4345]: training_loss: tensor(0.1823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/4345]: training_loss: tensor(0.3620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/4345]: training_loss: tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/4345]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1361/4345]: training_loss: tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/4345]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1363/4345]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/4345]: training_loss: tensor(0.3135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/4345]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/4345]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/4345]: training_loss: tensor(0.1818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/4345]: training_loss: tensor(0.1059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/4345]: training_loss: tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/4345]: training_loss: tensor(0.1555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/4345]: training_loss: tensor(0.3495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/4345]: training_loss: tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/4345]: training_loss: tensor(0.1308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/4345]: training_loss: tensor(0.2537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/4345]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/4345]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/4345]: training_loss: tensor(0.1500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/4345]: training_loss: tensor(0.2678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/4345]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/4345]: training_loss: tensor(0.0423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/4345]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1385/4345]: training_loss: tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1386/4345]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1387/4345]: training_loss: tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1388/4345]: training_loss: tensor(0.2041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1389/4345]: training_loss: tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1390/4345]: training_loss: tensor(0.1059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1391/4345]: training_loss: tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1392/4345]: training_loss: tensor(0.1867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1393/4345]: training_loss: tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1394/4345]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1395/4345]: training_loss: tensor(0.3200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1396/4345]: training_loss: tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1397/4345]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1398/4345]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1399/4345]: training_loss: tensor(0.1639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1400/4345]: training_loss: tensor(0.0384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1401/4345]: training_loss: tensor(0.1210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1402/4345]: training_loss: tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1403/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1404/4345]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1405/4345]: training_loss: tensor(0.2249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1406/4345]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1407/4345]: training_loss: tensor(0.2973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1408/4345]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1409/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1410/4345]: training_loss: tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1411/4345]: training_loss: tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1412/4345]: training_loss: tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1413/4345]: training_loss: tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1414/4345]: training_loss: tensor(0.1321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1415/4345]: training_loss: tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1416/4345]: training_loss: tensor(0.4206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1417/4345]: training_loss: tensor(0.4483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1418/4345]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1419/4345]: training_loss: tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1420/4345]: training_loss: tensor(0.2142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1421/4345]: training_loss: tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1422/4345]: training_loss: tensor(0.1591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1423/4345]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1424/4345]: training_loss: tensor(0.1066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1425/4345]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1426/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1427/4345]: training_loss: tensor(0.1125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1428/4345]: training_loss: tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1429/4345]: training_loss: tensor(0.1294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1430/4345]: training_loss: tensor(0.1401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1431/4345]: training_loss: tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1432/4345]: training_loss: tensor(0.3229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1433/4345]: training_loss: tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1434/4345]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1435/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1436/4345]: training_loss: tensor(0.1169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1437/4345]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1438/4345]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1439/4345]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1440/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1441/4345]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1442/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1443/4345]: training_loss: tensor(0.3292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1444/4345]: training_loss: tensor(0.2690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1445/4345]: training_loss: tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1446/4345]: training_loss: tensor(0.2797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1447/4345]: training_loss: tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1448/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1449/4345]: training_loss: tensor(0.1476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1450/4345]: training_loss: tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1451/4345]: training_loss: tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1452/4345]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1453/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1454/4345]: training_loss: tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1455/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1456/4345]: training_loss: tensor(0.2064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1457/4345]: training_loss: tensor(0.1629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1458/4345]: training_loss: tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1459/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1460/4345]: training_loss: tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1461/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1462/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1463/4345]: training_loss: tensor(0.1327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1464/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1465/4345]: training_loss: tensor(0.3066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1466/4345]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1467/4345]: training_loss: tensor(0.3599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1468/4345]: training_loss: tensor(0.1525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1469/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1470/4345]: training_loss: tensor(0.1500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1471/4345]: training_loss: tensor(0.1765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1472/4345]: training_loss: tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1473/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1474/4345]: training_loss: tensor(0.2069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1475/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1476/4345]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1477/4345]: training_loss: tensor(0.2375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1478/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1479/4345]: training_loss: tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1480/4345]: training_loss: tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1481/4345]: training_loss: tensor(0.0492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1482/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1483/4345]: training_loss: tensor(0.0362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1484/4345]: training_loss: tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1485/4345]: training_loss: tensor(0.2211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1486/4345]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1487/4345]: training_loss: tensor(0.1331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1488/4345]: training_loss: tensor(0.3723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1489/4345]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1490/4345]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1491/4345]: training_loss: tensor(0.1118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1492/4345]: training_loss: tensor(0.1120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1493/4345]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1494/4345]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1495/4345]: training_loss: tensor(0.1307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1496/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1497/4345]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1498/4345]: training_loss: tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1499/4345]: training_loss: tensor(0.3809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1500/4345]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1501/4345]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1502/4345]: training_loss: tensor(0.2027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1503/4345]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1504/4345]: training_loss: tensor(0.2025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1505/4345]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1506/4345]: training_loss: tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1507/4345]: training_loss: tensor(0.1197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1508/4345]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1509/4345]: training_loss: tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1510/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1511/4345]: training_loss: tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1512/4345]: training_loss: tensor(0.1986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1513/4345]: training_loss: tensor(0.5349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1514/4345]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1515/4345]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1516/4345]: training_loss: tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1517/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1518/4345]: training_loss: tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1519/4345]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1520/4345]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1521/4345]: training_loss: tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1522/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1523/4345]: training_loss: tensor(0.1958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1524/4345]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1525/4345]: training_loss: tensor(0.1684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1526/4345]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1527/4345]: training_loss: tensor(0.1117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1528/4345]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1529/4345]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1530/4345]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1531/4345]: training_loss: tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1532/4345]: training_loss: tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1533/4345]: training_loss: tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1534/4345]: training_loss: tensor(0.3501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1535/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1536/4345]: training_loss: tensor(0.0543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1537/4345]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1538/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1539/4345]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1540/4345]: training_loss: tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1541/4345]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1542/4345]: training_loss: tensor(0.2198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1543/4345]: training_loss: tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1544/4345]: training_loss: tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1545/4345]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1546/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1547/4345]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1548/4345]: training_loss: tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1549/4345]: training_loss: tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1550/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1551/4345]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1552/4345]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1553/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1554/4345]: training_loss: tensor(0.0616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1555/4345]: training_loss: tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1556/4345]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1557/4345]: training_loss: tensor(0.3047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1558/4345]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1559/4345]: training_loss: tensor(0.3627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1560/4345]: training_loss: tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1561/4345]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1562/4345]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1563/4345]: training_loss: tensor(0.2160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1564/4345]: training_loss: tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1565/4345]: training_loss: tensor(0.2295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1566/4345]: training_loss: tensor(0.2788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1567/4345]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1568/4345]: training_loss: tensor(0.1621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1569/4345]: training_loss: tensor(0.5741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1570/4345]: training_loss: tensor(0.1139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1571/4345]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1572/4345]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1573/4345]: training_loss: tensor(0.1361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1574/4345]: training_loss: tensor(0.1121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1575/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1576/4345]: training_loss: tensor(0.1574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1577/4345]: training_loss: tensor(0.1614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1578/4345]: training_loss: tensor(0.1319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1579/4345]: training_loss: tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1580/4345]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1581/4345]: training_loss: tensor(0.2565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1582/4345]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1583/4345]: training_loss: tensor(0.1781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1584/4345]: training_loss: tensor(0.0439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1585/4345]: training_loss: tensor(0.2323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1586/4345]: training_loss: tensor(0.1940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1587/4345]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1588/4345]: training_loss: tensor(0.2117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1589/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1590/4345]: training_loss: tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1591/4345]: training_loss: tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1592/4345]: training_loss: tensor(0.1821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1593/4345]: training_loss: tensor(0.2560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1594/4345]: training_loss: tensor(0.1130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1595/4345]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1596/4345]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1597/4345]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1598/4345]: training_loss: tensor(0.1559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1599/4345]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1600/4345]: training_loss: tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1601/4345]: training_loss: tensor(0.2904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1602/4345]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1603/4345]: training_loss: tensor(0.2205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1604/4345]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1605/4345]: training_loss: tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1606/4345]: training_loss: tensor(0.1469, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1607/4345]: training_loss: tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1608/4345]: training_loss: tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1609/4345]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1610/4345]: training_loss: tensor(0.1661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1611/4345]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1612/4345]: training_loss: tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1613/4345]: training_loss: tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1614/4345]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1615/4345]: training_loss: tensor(0.2143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1616/4345]: training_loss: tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1617/4345]: training_loss: tensor(0.4906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1618/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1619/4345]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1620/4345]: training_loss: tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1621/4345]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1622/4345]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1623/4345]: training_loss: tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1624/4345]: training_loss: tensor(0.1441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1625/4345]: training_loss: tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1626/4345]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1627/4345]: training_loss: tensor(0.1909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1628/4345]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1629/4345]: training_loss: tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1630/4345]: training_loss: tensor(0.1756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1631/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1632/4345]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1633/4345]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1634/4345]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1635/4345]: training_loss: tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1636/4345]: training_loss: tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1637/4345]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1638/4345]: training_loss: tensor(0.2031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1639/4345]: training_loss: tensor(0.2602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1640/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1641/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1642/4345]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1643/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1644/4345]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1645/4345]: training_loss: tensor(0.1667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1646/4345]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1647/4345]: training_loss: tensor(0.1200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1648/4345]: training_loss: tensor(0.2609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1649/4345]: training_loss: tensor(0.1854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1650/4345]: training_loss: tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1651/4345]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1652/4345]: training_loss: tensor(0.1723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1653/4345]: training_loss: tensor(0.2907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1654/4345]: training_loss: tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1655/4345]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1656/4345]: training_loss: tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1657/4345]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1658/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1659/4345]: training_loss: tensor(0.1446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1660/4345]: training_loss: tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1661/4345]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1662/4345]: training_loss: tensor(0.0410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1663/4345]: training_loss: tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1664/4345]: training_loss: tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1665/4345]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1666/4345]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1667/4345]: training_loss: tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1668/4345]: training_loss: tensor(0.2272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1669/4345]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1670/4345]: training_loss: tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1671/4345]: training_loss: tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1672/4345]: training_loss: tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1673/4345]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1674/4345]: training_loss: tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1675/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1676/4345]: training_loss: tensor(0.3602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1677/4345]: training_loss: tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1678/4345]: training_loss: tensor(0.0290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1679/4345]: training_loss: tensor(0.0469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1680/4345]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1681/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1682/4345]: training_loss: tensor(0.0637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1683/4345]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1684/4345]: training_loss: tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1685/4345]: training_loss: tensor(0.1777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1686/4345]: training_loss: tensor(0.5048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1687/4345]: training_loss: tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1688/4345]: training_loss: tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1689/4345]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1690/4345]: training_loss: tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1691/4345]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1692/4345]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1693/4345]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1694/4345]: training_loss: tensor(0.5514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1695/4345]: training_loss: tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1696/4345]: training_loss: tensor(0.1539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1697/4345]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1698/4345]: training_loss: tensor(0.1474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1699/4345]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1700/4345]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1701/4345]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1702/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1703/4345]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1704/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1705/4345]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1706/4345]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1707/4345]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1708/4345]: training_loss: tensor(0.2596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1709/4345]: training_loss: tensor(0.3640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1710/4345]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1711/4345]: training_loss: tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1712/4345]: training_loss: tensor(0.5064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1713/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1714/4345]: training_loss: tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1715/4345]: training_loss: tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1716/4345]: training_loss: tensor(0.1555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1717/4345]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1718/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1719/4345]: training_loss: tensor(0.1404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1720/4345]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1721/4345]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1722/4345]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1723/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1724/4345]: training_loss: tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1725/4345]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1726/4345]: training_loss: tensor(0.0594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1727/4345]: training_loss: tensor(0.1574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1728/4345]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1729/4345]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1730/4345]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1731/4345]: training_loss: tensor(0.1078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1732/4345]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1733/4345]: training_loss: tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1734/4345]: training_loss: tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1735/4345]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1736/4345]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1737/4345]: training_loss: tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1738/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1739/4345]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1740/4345]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1741/4345]: training_loss: tensor(0.2952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1742/4345]: training_loss: tensor(0.4805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1743/4345]: training_loss: tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1744/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1745/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1746/4345]: training_loss: tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1747/4345]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1748/4345]: training_loss: tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1749/4345]: training_loss: tensor(0.1811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1750/4345]: training_loss: tensor(0.2247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1751/4345]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1752/4345]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1753/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1754/4345]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1755/4345]: training_loss: tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1756/4345]: training_loss: tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1757/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1758/4345]: training_loss: tensor(0.1362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1759/4345]: training_loss: tensor(0.0313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1760/4345]: training_loss: tensor(0.1741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1761/4345]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1762/4345]: training_loss: tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1763/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1764/4345]: training_loss: tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1765/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1766/4345]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1767/4345]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1768/4345]: training_loss: tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1769/4345]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1770/4345]: training_loss: tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1771/4345]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1772/4345]: training_loss: tensor(0.1585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1773/4345]: training_loss: tensor(0.2218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1774/4345]: training_loss: tensor(0.4671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1775/4345]: training_loss: tensor(0.4625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1776/4345]: training_loss: tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1777/4345]: training_loss: tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1778/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1779/4345]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1780/4345]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1781/4345]: training_loss: tensor(0.1638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1782/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1783/4345]: training_loss: tensor(0.1875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1784/4345]: training_loss: tensor(0.1453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1785/4345]: training_loss: tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1786/4345]: training_loss: tensor(0.3011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1787/4345]: training_loss: tensor(0.1588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1788/4345]: training_loss: tensor(0.1237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1789/4345]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1790/4345]: training_loss: tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1791/4345]: training_loss: tensor(0.1191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1792/4345]: training_loss: tensor(0.2090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1793/4345]: training_loss: tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1794/4345]: training_loss: tensor(0.1078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1795/4345]: training_loss: tensor(0.3973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1796/4345]: training_loss: tensor(0.1336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1797/4345]: training_loss: tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1798/4345]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1799/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1800/4345]: training_loss: tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1801/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1802/4345]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1803/4345]: training_loss: tensor(0.1572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1804/4345]: training_loss: tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1805/4345]: training_loss: tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1806/4345]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1807/4345]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1808/4345]: training_loss: tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1809/4345]: training_loss: tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1810/4345]: training_loss: tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1811/4345]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1812/4345]: training_loss: tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1813/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1814/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1815/4345]: training_loss: tensor(0.1881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1816/4345]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1817/4345]: training_loss: tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1818/4345]: training_loss: tensor(0.1600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1819/4345]: training_loss: tensor(0.1876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1820/4345]: training_loss: tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1821/4345]: training_loss: tensor(0.4016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1822/4345]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1823/4345]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1824/4345]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1825/4345]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1826/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1827/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1828/4345]: training_loss: tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1829/4345]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1830/4345]: training_loss: tensor(0.1351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1831/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1832/4345]: training_loss: tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1833/4345]: training_loss: tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1834/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1835/4345]: training_loss: tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1836/4345]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1837/4345]: training_loss: tensor(0.1616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1838/4345]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1839/4345]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1840/4345]: training_loss: tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1841/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1842/4345]: training_loss: tensor(0.1793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1843/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1844/4345]: training_loss: tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1845/4345]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1846/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1847/4345]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1848/4345]: training_loss: tensor(0.5898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1849/4345]: training_loss: tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1850/4345]: training_loss: tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1851/4345]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1852/4345]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1853/4345]: training_loss: tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1854/4345]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1855/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1856/4345]: training_loss: tensor(0.2458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1857/4345]: training_loss: tensor(0.1863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1858/4345]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1859/4345]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1860/4345]: training_loss: tensor(0.1773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1861/4345]: training_loss: tensor(0.2793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1862/4345]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1863/4345]: training_loss: tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1864/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1865/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1866/4345]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1867/4345]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1868/4345]: training_loss: tensor(0.3095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1869/4345]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1870/4345]: training_loss: tensor(0.1738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1871/4345]: training_loss: tensor(0.1528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1872/4345]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1873/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1874/4345]: training_loss: tensor(0.3287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1875/4345]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1876/4345]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1877/4345]: training_loss: tensor(0.2449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1878/4345]: training_loss: tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1879/4345]: training_loss: tensor(0.3080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1880/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1881/4345]: training_loss: tensor(0.3079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1882/4345]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1883/4345]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1884/4345]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1885/4345]: training_loss: tensor(0.1794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1886/4345]: training_loss: tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1887/4345]: training_loss: tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1888/4345]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1889/4345]: training_loss: tensor(0.0572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1890/4345]: training_loss: tensor(0.1633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1891/4345]: training_loss: tensor(0.3271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1892/4345]: training_loss: tensor(0.2552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1893/4345]: training_loss: tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1894/4345]: training_loss: tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1895/4345]: training_loss: tensor(0.1187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1896/4345]: training_loss: tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1897/4345]: training_loss: tensor(0.1945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1898/4345]: training_loss: tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1899/4345]: training_loss: tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1900/4345]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1901/4345]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1902/4345]: training_loss: tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1903/4345]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1904/4345]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1905/4345]: training_loss: tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1906/4345]: training_loss: tensor(0.1608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1907/4345]: training_loss: tensor(0.2179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1908/4345]: training_loss: tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1909/4345]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1910/4345]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1911/4345]: training_loss: tensor(0.1489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1912/4345]: training_loss: tensor(0.1226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1913/4345]: training_loss: tensor(0.1103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1914/4345]: training_loss: tensor(0.1154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1915/4345]: training_loss: tensor(0.2415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1916/4345]: training_loss: tensor(0.1684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1917/4345]: training_loss: tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1918/4345]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1919/4345]: training_loss: tensor(0.1837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1920/4345]: training_loss: tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1921/4345]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1922/4345]: training_loss: tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1923/4345]: training_loss: tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1924/4345]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1925/4345]: training_loss: tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1926/4345]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1927/4345]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1928/4345]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1929/4345]: training_loss: tensor(0.2160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1930/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1931/4345]: training_loss: tensor(0.4003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1932/4345]: training_loss: tensor(0.1676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1933/4345]: training_loss: tensor(0.2301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1934/4345]: training_loss: tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1935/4345]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1936/4345]: training_loss: tensor(0.1230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1937/4345]: training_loss: tensor(0.2580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1938/4345]: training_loss: tensor(0.2241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1939/4345]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1940/4345]: training_loss: tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1941/4345]: training_loss: tensor(0.1619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1942/4345]: training_loss: tensor(0.1629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1943/4345]: training_loss: tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1944/4345]: training_loss: tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1945/4345]: training_loss: tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1946/4345]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1947/4345]: training_loss: tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1948/4345]: training_loss: tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1949/4345]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1950/4345]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1951/4345]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1952/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1953/4345]: training_loss: tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1954/4345]: training_loss: tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1955/4345]: training_loss: tensor(0.2675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1956/4345]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1957/4345]: training_loss: tensor(0.1993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1958/4345]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1959/4345]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1960/4345]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1961/4345]: training_loss: tensor(0.0389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1962/4345]: training_loss: tensor(0.0493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1963/4345]: training_loss: tensor(0.4234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1964/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1965/4345]: training_loss: tensor(0.2647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1966/4345]: training_loss: tensor(0.1262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1967/4345]: training_loss: tensor(0.1304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1968/4345]: training_loss: tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1969/4345]: training_loss: tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1970/4345]: training_loss: tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1971/4345]: training_loss: tensor(0.0637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1972/4345]: training_loss: tensor(0.2133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1973/4345]: training_loss: tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1974/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1975/4345]: training_loss: tensor(0.1760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1976/4345]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1977/4345]: training_loss: tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1978/4345]: training_loss: tensor(0.3060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1979/4345]: training_loss: tensor(0.2599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1980/4345]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1981/4345]: training_loss: tensor(0.1723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1982/4345]: training_loss: tensor(0.1879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1983/4345]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1984/4345]: training_loss: tensor(0.0427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1985/4345]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1986/4345]: training_loss: tensor(0.1713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1987/4345]: training_loss: tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1988/4345]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1989/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1990/4345]: training_loss: tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1991/4345]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1992/4345]: training_loss: tensor(0.2131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1993/4345]: training_loss: tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1994/4345]: training_loss: tensor(0.1313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1995/4345]: training_loss: tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1996/4345]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1997/4345]: training_loss: tensor(0.1139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1998/4345]: training_loss: tensor(0.3247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1999/4345]: training_loss: tensor(0.2135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2000/4345]: training_loss: tensor(0.1093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2001/4345]: training_loss: tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2002/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2003/4345]: training_loss: tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2004/4345]: training_loss: tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2005/4345]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2006/4345]: training_loss: tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2007/4345]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2008/4345]: training_loss: tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2009/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2010/4345]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2011/4345]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2012/4345]: training_loss: tensor(0.2560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2013/4345]: training_loss: tensor(0.5591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2014/4345]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2015/4345]: training_loss: tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2016/4345]: training_loss: tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2017/4345]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2018/4345]: training_loss: tensor(0.5485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2019/4345]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2020/4345]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2021/4345]: training_loss: tensor(0.4850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2022/4345]: training_loss: tensor(0.1553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2023/4345]: training_loss: tensor(0.1447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2024/4345]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2025/4345]: training_loss: tensor(0.2119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2026/4345]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2027/4345]: training_loss: tensor(0.1730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2028/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2029/4345]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2030/4345]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2031/4345]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2032/4345]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2033/4345]: training_loss: tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2034/4345]: training_loss: tensor(0.1320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2035/4345]: training_loss: tensor(0.2723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2036/4345]: training_loss: tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2037/4345]: training_loss: tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2038/4345]: training_loss: tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2039/4345]: training_loss: tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2040/4345]: training_loss: tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2041/4345]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2042/4345]: training_loss: tensor(0.1112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2043/4345]: training_loss: tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2044/4345]: training_loss: tensor(0.1103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2045/4345]: training_loss: tensor(0.1710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2046/4345]: training_loss: tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2047/4345]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2048/4345]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2049/4345]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2050/4345]: training_loss: tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2051/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2052/4345]: training_loss: tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2053/4345]: training_loss: tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2054/4345]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2055/4345]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2056/4345]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2057/4345]: training_loss: tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2058/4345]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2059/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2060/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2061/4345]: training_loss: tensor(0.1836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2062/4345]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2063/4345]: training_loss: tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2064/4345]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2065/4345]: training_loss: tensor(0.0354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2066/4345]: training_loss: tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2067/4345]: training_loss: tensor(0.3022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2068/4345]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2069/4345]: training_loss: tensor(0.0573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2070/4345]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2071/4345]: training_loss: tensor(0.1751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2072/4345]: training_loss: tensor(0.2628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2073/4345]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2074/4345]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2075/4345]: training_loss: tensor(0.1071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2076/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2077/4345]: training_loss: tensor(0.0582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2078/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2079/4345]: training_loss: tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2080/4345]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2081/4345]: training_loss: tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2082/4345]: training_loss: tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2083/4345]: training_loss: tensor(0.1301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2084/4345]: training_loss: tensor(0.1746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2085/4345]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2086/4345]: training_loss: tensor(0.1442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2087/4345]: training_loss: tensor(0.4556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2088/4345]: training_loss: tensor(0.1213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2089/4345]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2090/4345]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2091/4345]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2092/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2093/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2094/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2095/4345]: training_loss: tensor(0.1297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2096/4345]: training_loss: tensor(0.0650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2097/4345]: training_loss: tensor(0.2051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2098/4345]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2099/4345]: training_loss: tensor(0.1446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2100/4345]: training_loss: tensor(0.1690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2101/4345]: training_loss: tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2102/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2103/4345]: training_loss: tensor(0.1689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2104/4345]: training_loss: tensor(0.1883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2105/4345]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2106/4345]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2107/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2108/4345]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2109/4345]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2110/4345]: training_loss: tensor(0.1554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2111/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2112/4345]: training_loss: tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2113/4345]: training_loss: tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2114/4345]: training_loss: tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2115/4345]: training_loss: tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2116/4345]: training_loss: tensor(0.0423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2117/4345]: training_loss: tensor(0.1297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2118/4345]: training_loss: tensor(0.2483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2119/4345]: training_loss: tensor(0.1909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2120/4345]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2121/4345]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2122/4345]: training_loss: tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2123/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2124/4345]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2125/4345]: training_loss: tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2126/4345]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2127/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2128/4345]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2129/4345]: training_loss: tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2130/4345]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2131/4345]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2132/4345]: training_loss: tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2133/4345]: training_loss: tensor(0.1513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2134/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2135/4345]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2136/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2137/4345]: training_loss: tensor(0.2131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2138/4345]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2139/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2140/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2141/4345]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2142/4345]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2143/4345]: training_loss: tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2144/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2145/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2146/4345]: training_loss: tensor(0.1572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2147/4345]: training_loss: tensor(0.0410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2148/4345]: training_loss: tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2149/4345]: training_loss: tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2150/4345]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2151/4345]: training_loss: tensor(0.1230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2152/4345]: training_loss: tensor(0.1357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2153/4345]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2154/4345]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2155/4345]: training_loss: tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2156/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2157/4345]: training_loss: tensor(0.1782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2158/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2159/4345]: training_loss: tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2160/4345]: training_loss: tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2161/4345]: training_loss: tensor(0.1353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2162/4345]: training_loss: tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2163/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2164/4345]: training_loss: tensor(0.3716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2165/4345]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2166/4345]: training_loss: tensor(0.1257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2167/4345]: training_loss: tensor(0.2676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2168/4345]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2169/4345]: training_loss: tensor(0.1268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2170/4345]: training_loss: tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2171/4345]: training_loss: tensor(0.2714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2172/4345]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2173/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [2/5], Step [6519/21730], Train Loss: 0.1027, Valid Loss: 0.1316\n",
      "batch_no [2174/4345]: training_loss: tensor(0.1239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2175/4345]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2176/4345]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2177/4345]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2178/4345]: training_loss: tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2179/4345]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2180/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2181/4345]: training_loss: tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2182/4345]: training_loss: tensor(0.0501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2183/4345]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2184/4345]: training_loss: tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2185/4345]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2186/4345]: training_loss: tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2187/4345]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2188/4345]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2189/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2190/4345]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2191/4345]: training_loss: tensor(0.4428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2192/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2193/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2194/4345]: training_loss: tensor(0.1643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2195/4345]: training_loss: tensor(0.2342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2196/4345]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2197/4345]: training_loss: tensor(0.4578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2198/4345]: training_loss: tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2199/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2200/4345]: training_loss: tensor(0.1145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2201/4345]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2202/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2203/4345]: training_loss: tensor(0.1554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2204/4345]: training_loss: tensor(0.2070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2205/4345]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2206/4345]: training_loss: tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2207/4345]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2208/4345]: training_loss: tensor(0.2799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2209/4345]: training_loss: tensor(0.1724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2210/4345]: training_loss: tensor(0.6011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2211/4345]: training_loss: tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2212/4345]: training_loss: tensor(0.1175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2213/4345]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2214/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2215/4345]: training_loss: tensor(0.0476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2216/4345]: training_loss: tensor(0.1603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2217/4345]: training_loss: tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2218/4345]: training_loss: tensor(0.2179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2219/4345]: training_loss: tensor(0.1116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2220/4345]: training_loss: tensor(0.2014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2221/4345]: training_loss: tensor(0.1799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2222/4345]: training_loss: tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2223/4345]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2224/4345]: training_loss: tensor(0.1282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2225/4345]: training_loss: tensor(0.4522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2226/4345]: training_loss: tensor(0.1629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2227/4345]: training_loss: tensor(0.1836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2228/4345]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2229/4345]: training_loss: tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2230/4345]: training_loss: tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2231/4345]: training_loss: tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2232/4345]: training_loss: tensor(0.1339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2233/4345]: training_loss: tensor(0.2243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2234/4345]: training_loss: tensor(0.1619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2235/4345]: training_loss: tensor(0.2270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2236/4345]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2237/4345]: training_loss: tensor(0.2528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2238/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2239/4345]: training_loss: tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2240/4345]: training_loss: tensor(0.1156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2241/4345]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2242/4345]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2243/4345]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2244/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2245/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2246/4345]: training_loss: tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2247/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2248/4345]: training_loss: tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2249/4345]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2250/4345]: training_loss: tensor(0.0587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2251/4345]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2252/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2253/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2254/4345]: training_loss: tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2255/4345]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2256/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2257/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2258/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2259/4345]: training_loss: tensor(0.3175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2260/4345]: training_loss: tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2261/4345]: training_loss: tensor(0.2154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2262/4345]: training_loss: tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2263/4345]: training_loss: tensor(0.1404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2264/4345]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2265/4345]: training_loss: tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2266/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2267/4345]: training_loss: tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2268/4345]: training_loss: tensor(0.2447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2269/4345]: training_loss: tensor(0.1110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2270/4345]: training_loss: tensor(0.1169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2271/4345]: training_loss: tensor(0.1857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2272/4345]: training_loss: tensor(0.3203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2273/4345]: training_loss: tensor(0.1078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2274/4345]: training_loss: tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2275/4345]: training_loss: tensor(0.1344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2276/4345]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2277/4345]: training_loss: tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2278/4345]: training_loss: tensor(0.0534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2279/4345]: training_loss: tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2280/4345]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2281/4345]: training_loss: tensor(0.0468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2282/4345]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2283/4345]: training_loss: tensor(0.1093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2284/4345]: training_loss: tensor(0.1997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2285/4345]: training_loss: tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2286/4345]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2287/4345]: training_loss: tensor(0.1537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2288/4345]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2289/4345]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2290/4345]: training_loss: tensor(0.0635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2291/4345]: training_loss: tensor(0.1647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2292/4345]: training_loss: tensor(0.1390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2293/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2294/4345]: training_loss: tensor(0.1935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2295/4345]: training_loss: tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2296/4345]: training_loss: tensor(0.1111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2297/4345]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2298/4345]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2299/4345]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2300/4345]: training_loss: tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2301/4345]: training_loss: tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2302/4345]: training_loss: tensor(0.1581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2303/4345]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2304/4345]: training_loss: tensor(0.1154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2305/4345]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2306/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2307/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2308/4345]: training_loss: tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2309/4345]: training_loss: tensor(0.1610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2310/4345]: training_loss: tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2311/4345]: training_loss: tensor(0.1321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2312/4345]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2313/4345]: training_loss: tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2314/4345]: training_loss: tensor(0.1554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2315/4345]: training_loss: tensor(0.1858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2316/4345]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2317/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2318/4345]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2319/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2320/4345]: training_loss: tensor(0.1466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2321/4345]: training_loss: tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2322/4345]: training_loss: tensor(0.2244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2323/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2324/4345]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2325/4345]: training_loss: tensor(0.1155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2326/4345]: training_loss: tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2327/4345]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2328/4345]: training_loss: tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2329/4345]: training_loss: tensor(0.1507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2330/4345]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2331/4345]: training_loss: tensor(0.2306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2332/4345]: training_loss: tensor(0.1216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2333/4345]: training_loss: tensor(0.2027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2334/4345]: training_loss: tensor(0.2984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2335/4345]: training_loss: tensor(0.4171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2336/4345]: training_loss: tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2337/4345]: training_loss: tensor(0.1596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2338/4345]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2339/4345]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2340/4345]: training_loss: tensor(0.0579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2341/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2342/4345]: training_loss: tensor(0.0389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2343/4345]: training_loss: tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2344/4345]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2345/4345]: training_loss: tensor(0.1193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2346/4345]: training_loss: tensor(0.2644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2347/4345]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2348/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2349/4345]: training_loss: tensor(0.2029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2350/4345]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2351/4345]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2352/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2353/4345]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2354/4345]: training_loss: tensor(0.2134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2355/4345]: training_loss: tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2356/4345]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2357/4345]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2358/4345]: training_loss: tensor(0.1759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2359/4345]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2360/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2361/4345]: training_loss: tensor(0.2252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2362/4345]: training_loss: tensor(0.1887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2363/4345]: training_loss: tensor(0.1517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2364/4345]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2365/4345]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2366/4345]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2367/4345]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2368/4345]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2369/4345]: training_loss: tensor(0.1628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2370/4345]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2371/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2372/4345]: training_loss: tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2373/4345]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2374/4345]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2375/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2376/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2377/4345]: training_loss: tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2378/4345]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2379/4345]: training_loss: tensor(0.1752, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2380/4345]: training_loss: tensor(0.1353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2381/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2382/4345]: training_loss: tensor(0.1351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2383/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2384/4345]: training_loss: tensor(0.1479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2385/4345]: training_loss: tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2386/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2387/4345]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2388/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2389/4345]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2390/4345]: training_loss: tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2391/4345]: training_loss: tensor(0.1091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2392/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2393/4345]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2394/4345]: training_loss: tensor(0.2513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2395/4345]: training_loss: tensor(0.1853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2396/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2397/4345]: training_loss: tensor(0.3833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2398/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2399/4345]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2400/4345]: training_loss: tensor(0.1883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2401/4345]: training_loss: tensor(0.3072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2402/4345]: training_loss: tensor(0.4193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2403/4345]: training_loss: tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2404/4345]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2405/4345]: training_loss: tensor(0.2239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2406/4345]: training_loss: tensor(0.0427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2407/4345]: training_loss: tensor(0.1285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2408/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2409/4345]: training_loss: tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2410/4345]: training_loss: tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2411/4345]: training_loss: tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2412/4345]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2413/4345]: training_loss: tensor(0.1777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2414/4345]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2415/4345]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2416/4345]: training_loss: tensor(0.2322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2417/4345]: training_loss: tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2418/4345]: training_loss: tensor(0.2001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2419/4345]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2420/4345]: training_loss: tensor(0.3315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2421/4345]: training_loss: tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2422/4345]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2423/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2424/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2425/4345]: training_loss: tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2426/4345]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2427/4345]: training_loss: tensor(0.1935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2428/4345]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2429/4345]: training_loss: tensor(0.0494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2430/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2431/4345]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2432/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2433/4345]: training_loss: tensor(0.0364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2434/4345]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2435/4345]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2436/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2437/4345]: training_loss: tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2438/4345]: training_loss: tensor(0.1536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2439/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2440/4345]: training_loss: tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2441/4345]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2442/4345]: training_loss: tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2443/4345]: training_loss: tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2444/4345]: training_loss: tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2445/4345]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2446/4345]: training_loss: tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2447/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2448/4345]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2449/4345]: training_loss: tensor(0.1194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2450/4345]: training_loss: tensor(0.1220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2451/4345]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2452/4345]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2453/4345]: training_loss: tensor(0.1483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2454/4345]: training_loss: tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2455/4345]: training_loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2456/4345]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2457/4345]: training_loss: tensor(0.1375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2458/4345]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2459/4345]: training_loss: tensor(0.0501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2460/4345]: training_loss: tensor(0.1241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2461/4345]: training_loss: tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2462/4345]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2463/4345]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2464/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2465/4345]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2466/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2467/4345]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2468/4345]: training_loss: tensor(0.1541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2469/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2470/4345]: training_loss: tensor(0.1415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2471/4345]: training_loss: tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2472/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2473/4345]: training_loss: tensor(0.1071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2474/4345]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2475/4345]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2476/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2477/4345]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2478/4345]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2479/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2480/4345]: training_loss: tensor(0.3875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2481/4345]: training_loss: tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2482/4345]: training_loss: tensor(0.0468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2483/4345]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2484/4345]: training_loss: tensor(0.2925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2485/4345]: training_loss: tensor(0.0404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2486/4345]: training_loss: tensor(0.2111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2487/4345]: training_loss: tensor(0.1657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2488/4345]: training_loss: tensor(0.0579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2489/4345]: training_loss: tensor(0.2974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2490/4345]: training_loss: tensor(0.0442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2491/4345]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2492/4345]: training_loss: tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2493/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2494/4345]: training_loss: tensor(0.1483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2495/4345]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2496/4345]: training_loss: tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2497/4345]: training_loss: tensor(0.3993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2498/4345]: training_loss: tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2499/4345]: training_loss: tensor(0.3557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2500/4345]: training_loss: tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2501/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2502/4345]: training_loss: tensor(0.3122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2503/4345]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2504/4345]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2505/4345]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2506/4345]: training_loss: tensor(0.1059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2507/4345]: training_loss: tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2508/4345]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2509/4345]: training_loss: tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2510/4345]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2511/4345]: training_loss: tensor(0.0383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2512/4345]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2513/4345]: training_loss: tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2514/4345]: training_loss: tensor(0.2896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2515/4345]: training_loss: tensor(0.1116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2516/4345]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2517/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2518/4345]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2519/4345]: training_loss: tensor(0.1348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2520/4345]: training_loss: tensor(0.1728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2521/4345]: training_loss: tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2522/4345]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2523/4345]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2524/4345]: training_loss: tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2525/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2526/4345]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2527/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2528/4345]: training_loss: tensor(0.1589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2529/4345]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2530/4345]: training_loss: tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2531/4345]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2532/4345]: training_loss: tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2533/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2534/4345]: training_loss: tensor(0.1443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2535/4345]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2536/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2537/4345]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2538/4345]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2539/4345]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2540/4345]: training_loss: tensor(0.1559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2541/4345]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2542/4345]: training_loss: tensor(0.0361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2543/4345]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2544/4345]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2545/4345]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2546/4345]: training_loss: tensor(0.0686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2547/4345]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2548/4345]: training_loss: tensor(0.2221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2549/4345]: training_loss: tensor(0.0446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2550/4345]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2551/4345]: training_loss: tensor(0.5195, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2552/4345]: training_loss: tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2553/4345]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2554/4345]: training_loss: tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2555/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2556/4345]: training_loss: tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2557/4345]: training_loss: tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2558/4345]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2559/4345]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2560/4345]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2561/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2562/4345]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2563/4345]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2564/4345]: training_loss: tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2565/4345]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2566/4345]: training_loss: tensor(0.3913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2567/4345]: training_loss: tensor(0.0606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2568/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2569/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2570/4345]: training_loss: tensor(0.1348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2571/4345]: training_loss: tensor(0.2497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2572/4345]: training_loss: tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2573/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2574/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2575/4345]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2576/4345]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2577/4345]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2578/4345]: training_loss: tensor(0.1587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2579/4345]: training_loss: tensor(0.1907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2580/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2581/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2582/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2583/4345]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2584/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2585/4345]: training_loss: tensor(0.3810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2586/4345]: training_loss: tensor(0.0495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2587/4345]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2588/4345]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2589/4345]: training_loss: tensor(0.0606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2590/4345]: training_loss: tensor(0.1684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2591/4345]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2592/4345]: training_loss: tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2593/4345]: training_loss: tensor(0.0459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2594/4345]: training_loss: tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2595/4345]: training_loss: tensor(0.0476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2596/4345]: training_loss: tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2597/4345]: training_loss: tensor(0.1139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2598/4345]: training_loss: tensor(0.3188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2599/4345]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2600/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2601/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2602/4345]: training_loss: tensor(0.1350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2603/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2604/4345]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2605/4345]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2606/4345]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2607/4345]: training_loss: tensor(0.2535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2608/4345]: training_loss: tensor(0.1774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2609/4345]: training_loss: tensor(0.2952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2610/4345]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2611/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2612/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2613/4345]: training_loss: tensor(0.2269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2614/4345]: training_loss: tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2615/4345]: training_loss: tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2616/4345]: training_loss: tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2617/4345]: training_loss: tensor(0.1469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2618/4345]: training_loss: tensor(0.1474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2619/4345]: training_loss: tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2620/4345]: training_loss: tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2621/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2622/4345]: training_loss: tensor(0.1722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2623/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2624/4345]: training_loss: tensor(0.1847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2625/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2626/4345]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2627/4345]: training_loss: tensor(0.1107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2628/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2629/4345]: training_loss: tensor(0.1213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2630/4345]: training_loss: tensor(0.0637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2631/4345]: training_loss: tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2632/4345]: training_loss: tensor(0.2639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2633/4345]: training_loss: tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2634/4345]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2635/4345]: training_loss: tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2636/4345]: training_loss: tensor(0.1279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2637/4345]: training_loss: tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2638/4345]: training_loss: tensor(0.1422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2639/4345]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2640/4345]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2641/4345]: training_loss: tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2642/4345]: training_loss: tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2643/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2644/4345]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2645/4345]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2646/4345]: training_loss: tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2647/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2648/4345]: training_loss: tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2649/4345]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2650/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2651/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2652/4345]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2653/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2654/4345]: training_loss: tensor(0.1449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2655/4345]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2656/4345]: training_loss: tensor(0.1981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2657/4345]: training_loss: tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2658/4345]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2659/4345]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2660/4345]: training_loss: tensor(0.1870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2661/4345]: training_loss: tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2662/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2663/4345]: training_loss: tensor(0.4582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2664/4345]: training_loss: tensor(0.1536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2665/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2666/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2667/4345]: training_loss: tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2668/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2669/4345]: training_loss: tensor(0.2255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2670/4345]: training_loss: tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2671/4345]: training_loss: tensor(0.1978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2672/4345]: training_loss: tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2673/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2674/4345]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2675/4345]: training_loss: tensor(0.1267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2676/4345]: training_loss: tensor(0.2155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2677/4345]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2678/4345]: training_loss: tensor(0.2279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2679/4345]: training_loss: tensor(0.1212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2680/4345]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2681/4345]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2682/4345]: training_loss: tensor(0.1195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2683/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2684/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2685/4345]: training_loss: tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2686/4345]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2687/4345]: training_loss: tensor(0.2986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2688/4345]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2689/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2690/4345]: training_loss: tensor(0.3853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2691/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2692/4345]: training_loss: tensor(0.2416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2693/4345]: training_loss: tensor(0.1153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2694/4345]: training_loss: tensor(0.1844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2695/4345]: training_loss: tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2696/4345]: training_loss: tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2697/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2698/4345]: training_loss: tensor(0.1629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2699/4345]: training_loss: tensor(0.1876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2700/4345]: training_loss: tensor(0.1098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2701/4345]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2702/4345]: training_loss: tensor(0.1322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2703/4345]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2704/4345]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2705/4345]: training_loss: tensor(0.3366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2706/4345]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2707/4345]: training_loss: tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2708/4345]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2709/4345]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2710/4345]: training_loss: tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2711/4345]: training_loss: tensor(0.1560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2712/4345]: training_loss: tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2713/4345]: training_loss: tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2714/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2715/4345]: training_loss: tensor(0.1287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2716/4345]: training_loss: tensor(0.0676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2717/4345]: training_loss: tensor(0.1603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2718/4345]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2719/4345]: training_loss: tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2720/4345]: training_loss: tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2721/4345]: training_loss: tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2722/4345]: training_loss: tensor(0.1700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2723/4345]: training_loss: tensor(0.1315, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2724/4345]: training_loss: tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2725/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2726/4345]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2727/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2728/4345]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2729/4345]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2730/4345]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2731/4345]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2732/4345]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2733/4345]: training_loss: tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2734/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2735/4345]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2736/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2737/4345]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2738/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2739/4345]: training_loss: tensor(0.4703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2740/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2741/4345]: training_loss: tensor(0.0364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2742/4345]: training_loss: tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2743/4345]: training_loss: tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2744/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2745/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2746/4345]: training_loss: tensor(0.9657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2747/4345]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2748/4345]: training_loss: tensor(0.7901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2749/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2750/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2751/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2752/4345]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2753/4345]: training_loss: tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2754/4345]: training_loss: tensor(0.0582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2755/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2756/4345]: training_loss: tensor(0.1690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2757/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2758/4345]: training_loss: tensor(0.4970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2759/4345]: training_loss: tensor(0.0462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2760/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2761/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2762/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2763/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2764/4345]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2765/4345]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2766/4345]: training_loss: tensor(0.1396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2767/4345]: training_loss: tensor(0.1077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2768/4345]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2769/4345]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2770/4345]: training_loss: tensor(0.1851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2771/4345]: training_loss: tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2772/4345]: training_loss: tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2773/4345]: training_loss: tensor(0.1563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2774/4345]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2775/4345]: training_loss: tensor(0.3175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2776/4345]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2777/4345]: training_loss: tensor(0.1428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2778/4345]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2779/4345]: training_loss: tensor(0.1638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2780/4345]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2781/4345]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2782/4345]: training_loss: tensor(0.3671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2783/4345]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2784/4345]: training_loss: tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2785/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2786/4345]: training_loss: tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2787/4345]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2788/4345]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2789/4345]: training_loss: tensor(0.1300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2790/4345]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2791/4345]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2792/4345]: training_loss: tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2793/4345]: training_loss: tensor(0.1591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2794/4345]: training_loss: tensor(0.2073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2795/4345]: training_loss: tensor(0.1264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2796/4345]: training_loss: tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2797/4345]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2798/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2799/4345]: training_loss: tensor(0.3118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2800/4345]: training_loss: tensor(0.2465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2801/4345]: training_loss: tensor(0.2024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2802/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2803/4345]: training_loss: tensor(0.2347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2804/4345]: training_loss: tensor(0.0517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2805/4345]: training_loss: tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2806/4345]: training_loss: tensor(0.2636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2807/4345]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2808/4345]: training_loss: tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2809/4345]: training_loss: tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2810/4345]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2811/4345]: training_loss: tensor(0.1323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2812/4345]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2813/4345]: training_loss: tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2814/4345]: training_loss: tensor(0.1681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2815/4345]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2816/4345]: training_loss: tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2817/4345]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2818/4345]: training_loss: tensor(0.1577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2819/4345]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2820/4345]: training_loss: tensor(0.1392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2821/4345]: training_loss: tensor(0.1437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2822/4345]: training_loss: tensor(0.1186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2823/4345]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2824/4345]: training_loss: tensor(0.2734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2825/4345]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2826/4345]: training_loss: tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2827/4345]: training_loss: tensor(0.1438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2828/4345]: training_loss: tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2829/4345]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2830/4345]: training_loss: tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2831/4345]: training_loss: tensor(0.1619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2832/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2833/4345]: training_loss: tensor(0.2154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2834/4345]: training_loss: tensor(0.0361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2835/4345]: training_loss: tensor(0.0364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2836/4345]: training_loss: tensor(0.0475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2837/4345]: training_loss: tensor(0.3101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2838/4345]: training_loss: tensor(0.2456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2839/4345]: training_loss: tensor(0.1275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2840/4345]: training_loss: tensor(0.2579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2841/4345]: training_loss: tensor(0.1206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2842/4345]: training_loss: tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2843/4345]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2844/4345]: training_loss: tensor(0.0611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2845/4345]: training_loss: tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2846/4345]: training_loss: tensor(0.1112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2847/4345]: training_loss: tensor(0.1290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2848/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2849/4345]: training_loss: tensor(0.2924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2850/4345]: training_loss: tensor(0.1516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2851/4345]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2852/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2853/4345]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2854/4345]: training_loss: tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2855/4345]: training_loss: tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2856/4345]: training_loss: tensor(0.3141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2857/4345]: training_loss: tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2858/4345]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2859/4345]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2860/4345]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2861/4345]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2862/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2863/4345]: training_loss: tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2864/4345]: training_loss: tensor(0.3229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2865/4345]: training_loss: tensor(0.0507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2866/4345]: training_loss: tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2867/4345]: training_loss: tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2868/4345]: training_loss: tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2869/4345]: training_loss: tensor(0.0365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2870/4345]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2871/4345]: training_loss: tensor(0.1410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2872/4345]: training_loss: tensor(0.1130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2873/4345]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2874/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2875/4345]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2876/4345]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2877/4345]: training_loss: tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2878/4345]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2879/4345]: training_loss: tensor(0.1640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2880/4345]: training_loss: tensor(0.0517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2881/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2882/4345]: training_loss: tensor(0.2181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2883/4345]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2884/4345]: training_loss: tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2885/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2886/4345]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2887/4345]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2888/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2889/4345]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2890/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2891/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2892/4345]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2893/4345]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2894/4345]: training_loss: tensor(0.2995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2895/4345]: training_loss: tensor(0.3965, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2896/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2897/4345]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2898/4345]: training_loss: tensor(0.1072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2899/4345]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2900/4345]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2901/4345]: training_loss: tensor(0.0505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2902/4345]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2903/4345]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2904/4345]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2905/4345]: training_loss: tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2906/4345]: training_loss: tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2907/4345]: training_loss: tensor(0.2797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2908/4345]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2909/4345]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2910/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2911/4345]: training_loss: tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2912/4345]: training_loss: tensor(0.2133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2913/4345]: training_loss: tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2914/4345]: training_loss: tensor(0.0525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2915/4345]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2916/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2917/4345]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2918/4345]: training_loss: tensor(0.1598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2919/4345]: training_loss: tensor(0.1451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2920/4345]: training_loss: tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2921/4345]: training_loss: tensor(0.3200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2922/4345]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2923/4345]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2924/4345]: training_loss: tensor(0.1415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2925/4345]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2926/4345]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2927/4345]: training_loss: tensor(0.1356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2928/4345]: training_loss: tensor(0.1959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2929/4345]: training_loss: tensor(0.2876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2930/4345]: training_loss: tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2931/4345]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2932/4345]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2933/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2934/4345]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2935/4345]: training_loss: tensor(0.2429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2936/4345]: training_loss: tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2937/4345]: training_loss: tensor(0.1130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2938/4345]: training_loss: tensor(0.1275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2939/4345]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2940/4345]: training_loss: tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2941/4345]: training_loss: tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2942/4345]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2943/4345]: training_loss: tensor(0.1108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2944/4345]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2945/4345]: training_loss: tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2946/4345]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2947/4345]: training_loss: tensor(0.1308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2948/4345]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2949/4345]: training_loss: tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2950/4345]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2951/4345]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2952/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2953/4345]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2954/4345]: training_loss: tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2955/4345]: training_loss: tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2956/4345]: training_loss: tensor(0.0586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2957/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2958/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2959/4345]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2960/4345]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2961/4345]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2962/4345]: training_loss: tensor(0.2944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2963/4345]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2964/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2965/4345]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2966/4345]: training_loss: tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2967/4345]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2968/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2969/4345]: training_loss: tensor(0.1284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2970/4345]: training_loss: tensor(0.1519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2971/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2972/4345]: training_loss: tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2973/4345]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2974/4345]: training_loss: tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2975/4345]: training_loss: tensor(0.7199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2976/4345]: training_loss: tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2977/4345]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2978/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2979/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2980/4345]: training_loss: tensor(0.3584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2981/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2982/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2983/4345]: training_loss: tensor(0.4110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2984/4345]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2985/4345]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2986/4345]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2987/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2988/4345]: training_loss: tensor(0.2608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2989/4345]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2990/4345]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2991/4345]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2992/4345]: training_loss: tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2993/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2994/4345]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2995/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2996/4345]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2997/4345]: training_loss: tensor(0.1511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2998/4345]: training_loss: tensor(0.1286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2999/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3000/4345]: training_loss: tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3001/4345]: training_loss: tensor(0.2191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3002/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3003/4345]: training_loss: tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3004/4345]: training_loss: tensor(0.3436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3005/4345]: training_loss: tensor(0.1882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3006/4345]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3007/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3008/4345]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3009/4345]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3010/4345]: training_loss: tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3011/4345]: training_loss: tensor(0.1942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3012/4345]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3013/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3014/4345]: training_loss: tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3015/4345]: training_loss: tensor(0.1601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3016/4345]: training_loss: tensor(0.3282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3017/4345]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3018/4345]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3019/4345]: training_loss: tensor(0.3065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3020/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3021/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3022/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3023/4345]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3024/4345]: training_loss: tensor(0.1178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3025/4345]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3026/4345]: training_loss: tensor(0.4620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3027/4345]: training_loss: tensor(0.0525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3028/4345]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3029/4345]: training_loss: tensor(0.1099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3030/4345]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3031/4345]: training_loss: tensor(0.1366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3032/4345]: training_loss: tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3033/4345]: training_loss: tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3034/4345]: training_loss: tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3035/4345]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3036/4345]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3037/4345]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3038/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3039/4345]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3040/4345]: training_loss: tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3041/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3042/4345]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3043/4345]: training_loss: tensor(0.1652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3044/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3045/4345]: training_loss: tensor(0.1200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3046/4345]: training_loss: tensor(0.2323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3047/4345]: training_loss: tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3048/4345]: training_loss: tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3049/4345]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3050/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3051/4345]: training_loss: tensor(0.2211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3052/4345]: training_loss: tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3053/4345]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3054/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3055/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3056/4345]: training_loss: tensor(0.1863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3057/4345]: training_loss: tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3058/4345]: training_loss: tensor(0.1177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3059/4345]: training_loss: tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3060/4345]: training_loss: tensor(0.1230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3061/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3062/4345]: training_loss: tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3063/4345]: training_loss: tensor(0.2923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3064/4345]: training_loss: tensor(0.0606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3065/4345]: training_loss: tensor(0.2596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3066/4345]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3067/4345]: training_loss: tensor(0.2468, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3068/4345]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3069/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3070/4345]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3071/4345]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3072/4345]: training_loss: tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3073/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3074/4345]: training_loss: tensor(0.3086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3075/4345]: training_loss: tensor(0.1473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3076/4345]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3077/4345]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3078/4345]: training_loss: tensor(0.1165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3079/4345]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3080/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3081/4345]: training_loss: tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3082/4345]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3083/4345]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3084/4345]: training_loss: tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3085/4345]: training_loss: tensor(0.1175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3086/4345]: training_loss: tensor(0.1940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3087/4345]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3088/4345]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3089/4345]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3090/4345]: training_loss: tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3091/4345]: training_loss: tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3092/4345]: training_loss: tensor(0.3761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3093/4345]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3094/4345]: training_loss: tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3095/4345]: training_loss: tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3096/4345]: training_loss: tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3097/4345]: training_loss: tensor(0.1833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3098/4345]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3099/4345]: training_loss: tensor(0.1817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3100/4345]: training_loss: tensor(0.0545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3101/4345]: training_loss: tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3102/4345]: training_loss: tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3103/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3104/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3105/4345]: training_loss: tensor(0.1957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3106/4345]: training_loss: tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3107/4345]: training_loss: tensor(0.2099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3108/4345]: training_loss: tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3109/4345]: training_loss: tensor(0.1286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3110/4345]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3111/4345]: training_loss: tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3112/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3113/4345]: training_loss: tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3114/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3115/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3116/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3117/4345]: training_loss: tensor(0.0507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3118/4345]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3119/4345]: training_loss: tensor(0.1564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3120/4345]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3121/4345]: training_loss: tensor(0.0362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3122/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3123/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3124/4345]: training_loss: tensor(0.1417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3125/4345]: training_loss: tensor(0.1503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3126/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3127/4345]: training_loss: tensor(0.2450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3128/4345]: training_loss: tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3129/4345]: training_loss: tensor(0.2353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3130/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3131/4345]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3132/4345]: training_loss: tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3133/4345]: training_loss: tensor(0.3044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3134/4345]: training_loss: tensor(0.1821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3135/4345]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3136/4345]: training_loss: tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3137/4345]: training_loss: tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3138/4345]: training_loss: tensor(0.2636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3139/4345]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3140/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3141/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3142/4345]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3143/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3144/4345]: training_loss: tensor(0.0615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3145/4345]: training_loss: tensor(0.1019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3146/4345]: training_loss: tensor(0.1314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3147/4345]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3148/4345]: training_loss: tensor(0.1085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3149/4345]: training_loss: tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3150/4345]: training_loss: tensor(0.1142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3151/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3152/4345]: training_loss: tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3153/4345]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3154/4345]: training_loss: tensor(0.2309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3155/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3156/4345]: training_loss: tensor(0.2890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3157/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3158/4345]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3159/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3160/4345]: training_loss: tensor(0.3366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3161/4345]: training_loss: tensor(0.2696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3162/4345]: training_loss: tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3163/4345]: training_loss: tensor(0.1250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3164/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3165/4345]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3166/4345]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3167/4345]: training_loss: tensor(0.1214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3168/4345]: training_loss: tensor(0.0305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3169/4345]: training_loss: tensor(0.1193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3170/4345]: training_loss: tensor(0.0686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3171/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3172/4345]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3173/4345]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3174/4345]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3175/4345]: training_loss: tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3176/4345]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3177/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3178/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3179/4345]: training_loss: tensor(0.0501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3180/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3181/4345]: training_loss: tensor(0.3102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3182/4345]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3183/4345]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3184/4345]: training_loss: tensor(0.1638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3185/4345]: training_loss: tensor(0.1404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3186/4345]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3187/4345]: training_loss: tensor(0.1348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3188/4345]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3189/4345]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3190/4345]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3191/4345]: training_loss: tensor(0.1895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3192/4345]: training_loss: tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3193/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3194/4345]: training_loss: tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3195/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3196/4345]: training_loss: tensor(0.1154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3197/4345]: training_loss: tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3198/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3199/4345]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3200/4345]: training_loss: tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3201/4345]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3202/4345]: training_loss: tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3203/4345]: training_loss: tensor(0.1200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3204/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3205/4345]: training_loss: tensor(0.1354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3206/4345]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3207/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3208/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3209/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3210/4345]: training_loss: tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3211/4345]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3212/4345]: training_loss: tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3213/4345]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3214/4345]: training_loss: tensor(0.0586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3215/4345]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3216/4345]: training_loss: tensor(0.2495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3217/4345]: training_loss: tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3218/4345]: training_loss: tensor(0.2078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3219/4345]: training_loss: tensor(0.2666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3220/4345]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3221/4345]: training_loss: tensor(0.3122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3222/4345]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3223/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3224/4345]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3225/4345]: training_loss: tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3226/4345]: training_loss: tensor(0.1894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3227/4345]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3228/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3229/4345]: training_loss: tensor(0.1991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3230/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3231/4345]: training_loss: tensor(0.2502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3232/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3233/4345]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3234/4345]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3235/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3236/4345]: training_loss: tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3237/4345]: training_loss: tensor(0.2015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3238/4345]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3239/4345]: training_loss: tensor(0.4750, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3240/4345]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3241/4345]: training_loss: tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3242/4345]: training_loss: tensor(0.1604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3243/4345]: training_loss: tensor(0.1241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3244/4345]: training_loss: tensor(0.1183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3245/4345]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3246/4345]: training_loss: tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3247/4345]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3248/4345]: training_loss: tensor(0.4175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3249/4345]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3250/4345]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3251/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3252/4345]: training_loss: tensor(0.1472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3253/4345]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3254/4345]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3255/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3256/4345]: training_loss: tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3257/4345]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3258/4345]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3259/4345]: training_loss: tensor(0.1513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3260/4345]: training_loss: tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3261/4345]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3262/4345]: training_loss: tensor(0.5216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3263/4345]: training_loss: tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3264/4345]: training_loss: tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3265/4345]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3266/4345]: training_loss: tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3267/4345]: training_loss: tensor(0.2680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3268/4345]: training_loss: tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3269/4345]: training_loss: tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3270/4345]: training_loss: tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3271/4345]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3272/4345]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3273/4345]: training_loss: tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3274/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3275/4345]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3276/4345]: training_loss: tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3277/4345]: training_loss: tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3278/4345]: training_loss: tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3279/4345]: training_loss: tensor(0.1647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3280/4345]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3281/4345]: training_loss: tensor(0.1218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3282/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3283/4345]: training_loss: tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3284/4345]: training_loss: tensor(0.1470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3285/4345]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3286/4345]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3287/4345]: training_loss: tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3288/4345]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3289/4345]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3290/4345]: training_loss: tensor(0.2388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3291/4345]: training_loss: tensor(0.0494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3292/4345]: training_loss: tensor(0.3447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3293/4345]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3294/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3295/4345]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3296/4345]: training_loss: tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3297/4345]: training_loss: tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3298/4345]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3299/4345]: training_loss: tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3300/4345]: training_loss: tensor(0.1315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3301/4345]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3302/4345]: training_loss: tensor(0.1171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3303/4345]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3304/4345]: training_loss: tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3305/4345]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3306/4345]: training_loss: tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3307/4345]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3308/4345]: training_loss: tensor(0.3589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3309/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3310/4345]: training_loss: tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3311/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3312/4345]: training_loss: tensor(0.0629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3313/4345]: training_loss: tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3314/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3315/4345]: training_loss: tensor(0.1384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3316/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3317/4345]: training_loss: tensor(0.0354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3318/4345]: training_loss: tensor(0.1881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3319/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3320/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3321/4345]: training_loss: tensor(0.1771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3322/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3323/4345]: training_loss: tensor(0.0384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3324/4345]: training_loss: tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3325/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3326/4345]: training_loss: tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3327/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3328/4345]: training_loss: tensor(0.1298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3329/4345]: training_loss: tensor(0.1303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3330/4345]: training_loss: tensor(0.2682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3331/4345]: training_loss: tensor(0.3259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3332/4345]: training_loss: tensor(0.6004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3333/4345]: training_loss: tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3334/4345]: training_loss: tensor(0.1969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3335/4345]: training_loss: tensor(0.3052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3336/4345]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3337/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3338/4345]: training_loss: tensor(0.2346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3339/4345]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3340/4345]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3341/4345]: training_loss: tensor(0.2048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3342/4345]: training_loss: tensor(0.1209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3343/4345]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3344/4345]: training_loss: tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3345/4345]: training_loss: tensor(0.1423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3346/4345]: training_loss: tensor(0.0486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3347/4345]: training_loss: tensor(0.0383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3348/4345]: training_loss: tensor(0.2416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3349/4345]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3350/4345]: training_loss: tensor(0.4034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3351/4345]: training_loss: tensor(0.1177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3352/4345]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3353/4345]: training_loss: tensor(0.0354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3354/4345]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3355/4345]: training_loss: tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3356/4345]: training_loss: tensor(0.1656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3357/4345]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3358/4345]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3359/4345]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3360/4345]: training_loss: tensor(0.0543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3361/4345]: training_loss: tensor(0.1458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3362/4345]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3363/4345]: training_loss: tensor(0.0637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3364/4345]: training_loss: tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3365/4345]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3366/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3367/4345]: training_loss: tensor(0.1206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3368/4345]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3369/4345]: training_loss: tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3370/4345]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3371/4345]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3372/4345]: training_loss: tensor(0.1150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3373/4345]: training_loss: tensor(0.2312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3374/4345]: training_loss: tensor(0.1776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3375/4345]: training_loss: tensor(0.1489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3376/4345]: training_loss: tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3377/4345]: training_loss: tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3378/4345]: training_loss: tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3379/4345]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3380/4345]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3381/4345]: training_loss: tensor(0.1110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3382/4345]: training_loss: tensor(0.1846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3383/4345]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3384/4345]: training_loss: tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3385/4345]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3386/4345]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3387/4345]: training_loss: tensor(0.1242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3388/4345]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3389/4345]: training_loss: tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3390/4345]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3391/4345]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3392/4345]: training_loss: tensor(0.2576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3393/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3394/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3395/4345]: training_loss: tensor(0.1526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3396/4345]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3397/4345]: training_loss: tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3398/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3399/4345]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3400/4345]: training_loss: tensor(0.3541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3401/4345]: training_loss: tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3402/4345]: training_loss: tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3403/4345]: training_loss: tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3404/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3405/4345]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3406/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3407/4345]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3408/4345]: training_loss: tensor(0.2954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3409/4345]: training_loss: tensor(0.4612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3410/4345]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3411/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3412/4345]: training_loss: tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3413/4345]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3414/4345]: training_loss: tensor(0.1658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3415/4345]: training_loss: tensor(0.1222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3416/4345]: training_loss: tensor(0.0439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3417/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3418/4345]: training_loss: tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3419/4345]: training_loss: tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3420/4345]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3421/4345]: training_loss: tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3422/4345]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3423/4345]: training_loss: tensor(0.1956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3424/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3425/4345]: training_loss: tensor(0.0442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3426/4345]: training_loss: tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3427/4345]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3428/4345]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3429/4345]: training_loss: tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3430/4345]: training_loss: tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3431/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3432/4345]: training_loss: tensor(0.3249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3433/4345]: training_loss: tensor(0.0361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3434/4345]: training_loss: tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3435/4345]: training_loss: tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3436/4345]: training_loss: tensor(0.2258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3437/4345]: training_loss: tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3438/4345]: training_loss: tensor(0.1185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3439/4345]: training_loss: tensor(0.1981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3440/4345]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3441/4345]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3442/4345]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3443/4345]: training_loss: tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3444/4345]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3445/4345]: training_loss: tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3446/4345]: training_loss: tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3447/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3448/4345]: training_loss: tensor(0.1643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3449/4345]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3450/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3451/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3452/4345]: training_loss: tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3453/4345]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3454/4345]: training_loss: tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3455/4345]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3456/4345]: training_loss: tensor(0.2188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3457/4345]: training_loss: tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3458/4345]: training_loss: tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3459/4345]: training_loss: tensor(0.4666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3460/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3461/4345]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3462/4345]: training_loss: tensor(0.2718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3463/4345]: training_loss: tensor(0.1447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3464/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3465/4345]: training_loss: tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3466/4345]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3467/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3468/4345]: training_loss: tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3469/4345]: training_loss: tensor(0.0676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3470/4345]: training_loss: tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3471/4345]: training_loss: tensor(0.2705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3472/4345]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3473/4345]: training_loss: tensor(0.0305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3474/4345]: training_loss: tensor(0.0648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3475/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3476/4345]: training_loss: tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3477/4345]: training_loss: tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3478/4345]: training_loss: tensor(0.2658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3479/4345]: training_loss: tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3480/4345]: training_loss: tensor(0.0486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3481/4345]: training_loss: tensor(0.1468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3482/4345]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3483/4345]: training_loss: tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3484/4345]: training_loss: tensor(0.0389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3485/4345]: training_loss: tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3486/4345]: training_loss: tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3487/4345]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3488/4345]: training_loss: tensor(0.2079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3489/4345]: training_loss: tensor(0.2172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3490/4345]: training_loss: tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3491/4345]: training_loss: tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3492/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3493/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3494/4345]: training_loss: tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3495/4345]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3496/4345]: training_loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3497/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3498/4345]: training_loss: tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3499/4345]: training_loss: tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3500/4345]: training_loss: tensor(0.2018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3501/4345]: training_loss: tensor(0.1162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3502/4345]: training_loss: tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3503/4345]: training_loss: tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3504/4345]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3505/4345]: training_loss: tensor(0.2141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3506/4345]: training_loss: tensor(0.2544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3507/4345]: training_loss: tensor(0.1162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3508/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3509/4345]: training_loss: tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3510/4345]: training_loss: tensor(0.1488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3511/4345]: training_loss: tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3512/4345]: training_loss: tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3513/4345]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3514/4345]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3515/4345]: training_loss: tensor(0.1069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3516/4345]: training_loss: tensor(0.3122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3517/4345]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3518/4345]: training_loss: tensor(0.2054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3519/4345]: training_loss: tensor(0.3776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3520/4345]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3521/4345]: training_loss: tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3522/4345]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3523/4345]: training_loss: tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3524/4345]: training_loss: tensor(0.3507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3525/4345]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3526/4345]: training_loss: tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3527/4345]: training_loss: tensor(0.1191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3528/4345]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3529/4345]: training_loss: tensor(0.1559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3530/4345]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3531/4345]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3532/4345]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3533/4345]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3534/4345]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3535/4345]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3536/4345]: training_loss: tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3537/4345]: training_loss: tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3538/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3539/4345]: training_loss: tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3540/4345]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3541/4345]: training_loss: tensor(0.1996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3542/4345]: training_loss: tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3543/4345]: training_loss: tensor(0.2059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3544/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3545/4345]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3546/4345]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3547/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3548/4345]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3549/4345]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3550/4345]: training_loss: tensor(0.2837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3551/4345]: training_loss: tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3552/4345]: training_loss: tensor(0.1277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3553/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3554/4345]: training_loss: tensor(0.3577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3555/4345]: training_loss: tensor(0.1752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3556/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3557/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3558/4345]: training_loss: tensor(0.1706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3559/4345]: training_loss: tensor(0.1353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3560/4345]: training_loss: tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3561/4345]: training_loss: tensor(0.0534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3562/4345]: training_loss: tensor(0.1804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3563/4345]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3564/4345]: training_loss: tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3565/4345]: training_loss: tensor(0.1091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3566/4345]: training_loss: tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3567/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3568/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3569/4345]: training_loss: tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3570/4345]: training_loss: tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3571/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3572/4345]: training_loss: tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3573/4345]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3574/4345]: training_loss: tensor(0.0290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3575/4345]: training_loss: tensor(0.3774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3576/4345]: training_loss: tensor(0.3984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3577/4345]: training_loss: tensor(0.1574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3578/4345]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3579/4345]: training_loss: tensor(0.1605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3580/4345]: training_loss: tensor(0.3586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3581/4345]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3582/4345]: training_loss: tensor(0.1200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3583/4345]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3584/4345]: training_loss: tensor(0.2057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3585/4345]: training_loss: tensor(0.0445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3586/4345]: training_loss: tensor(0.1178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3587/4345]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3588/4345]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3589/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3590/4345]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3591/4345]: training_loss: tensor(0.2098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3592/4345]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3593/4345]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3594/4345]: training_loss: tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3595/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3596/4345]: training_loss: tensor(0.1306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3597/4345]: training_loss: tensor(0.1463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3598/4345]: training_loss: tensor(0.1287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3599/4345]: training_loss: tensor(0.1164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3600/4345]: training_loss: tensor(0.1101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3601/4345]: training_loss: tensor(0.1185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3602/4345]: training_loss: tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3603/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3604/4345]: training_loss: tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3605/4345]: training_loss: tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3606/4345]: training_loss: tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3607/4345]: training_loss: tensor(0.1987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3608/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3609/4345]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3610/4345]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3611/4345]: training_loss: tensor(0.0429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3612/4345]: training_loss: tensor(0.3023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3613/4345]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3614/4345]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3615/4345]: training_loss: tensor(0.3159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3616/4345]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3617/4345]: training_loss: tensor(0.1019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3618/4345]: training_loss: tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3619/4345]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3620/4345]: training_loss: tensor(0.1343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3621/4345]: training_loss: tensor(0.1219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3622/4345]: training_loss: tensor(0.1369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3623/4345]: training_loss: tensor(0.4464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3624/4345]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3625/4345]: training_loss: tensor(0.1309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3626/4345]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3627/4345]: training_loss: tensor(0.2745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3628/4345]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3629/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3630/4345]: training_loss: tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3631/4345]: training_loss: tensor(0.1962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3632/4345]: training_loss: tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3633/4345]: training_loss: tensor(0.1256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3634/4345]: training_loss: tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3635/4345]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3636/4345]: training_loss: tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3637/4345]: training_loss: tensor(0.1475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3638/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3639/4345]: training_loss: tensor(0.2946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3640/4345]: training_loss: tensor(0.1275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3641/4345]: training_loss: tensor(0.1487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3642/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3643/4345]: training_loss: tensor(0.1676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3644/4345]: training_loss: tensor(0.3281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3645/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3646/4345]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3647/4345]: training_loss: tensor(0.2966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3648/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3649/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3650/4345]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3651/4345]: training_loss: tensor(0.1809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3652/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3653/4345]: training_loss: tensor(0.0469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3654/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3655/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3656/4345]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3657/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3658/4345]: training_loss: tensor(0.1370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3659/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3660/4345]: training_loss: tensor(0.2562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3661/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3662/4345]: training_loss: tensor(0.2598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3663/4345]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3664/4345]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3665/4345]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3666/4345]: training_loss: tensor(0.1728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3667/4345]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3668/4345]: training_loss: tensor(0.1585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3669/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3670/4345]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3671/4345]: training_loss: tensor(0.2949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3672/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3673/4345]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3674/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3675/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3676/4345]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3677/4345]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3678/4345]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3679/4345]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3680/4345]: training_loss: tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3681/4345]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3682/4345]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3683/4345]: training_loss: tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3684/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3685/4345]: training_loss: tensor(0.1110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3686/4345]: training_loss: tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3687/4345]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3688/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3689/4345]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3690/4345]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3691/4345]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3692/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3693/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3694/4345]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3695/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3696/4345]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3697/4345]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3698/4345]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3699/4345]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3700/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3701/4345]: training_loss: tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3702/4345]: training_loss: tensor(0.2498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3703/4345]: training_loss: tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3704/4345]: training_loss: tensor(0.1498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3705/4345]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3706/4345]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3707/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3708/4345]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3709/4345]: training_loss: tensor(0.4132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3710/4345]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3711/4345]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3712/4345]: training_loss: tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3713/4345]: training_loss: tensor(0.1972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3714/4345]: training_loss: tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3715/4345]: training_loss: tensor(0.1636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3716/4345]: training_loss: tensor(0.1110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3717/4345]: training_loss: tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3718/4345]: training_loss: tensor(0.1746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3719/4345]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3720/4345]: training_loss: tensor(0.4422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3721/4345]: training_loss: tensor(0.1879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3722/4345]: training_loss: tensor(0.3881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3723/4345]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3724/4345]: training_loss: tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3725/4345]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3726/4345]: training_loss: tensor(0.3259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3727/4345]: training_loss: tensor(0.1469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3728/4345]: training_loss: tensor(0.1123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3729/4345]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3730/4345]: training_loss: tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3731/4345]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3732/4345]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3733/4345]: training_loss: tensor(0.1152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3734/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3735/4345]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3736/4345]: training_loss: tensor(0.0547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3737/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3738/4345]: training_loss: tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3739/4345]: training_loss: tensor(0.1295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3740/4345]: training_loss: tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3741/4345]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3742/4345]: training_loss: tensor(0.3589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3743/4345]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3744/4345]: training_loss: tensor(0.1291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3745/4345]: training_loss: tensor(0.1139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3746/4345]: training_loss: tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3747/4345]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3748/4345]: training_loss: tensor(0.2260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3749/4345]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3750/4345]: training_loss: tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3751/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3752/4345]: training_loss: tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3753/4345]: training_loss: tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3754/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3755/4345]: training_loss: tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3756/4345]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3757/4345]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3758/4345]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3759/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3760/4345]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3761/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3762/4345]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3763/4345]: training_loss: tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3764/4345]: training_loss: tensor(0.1573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3765/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3766/4345]: training_loss: tensor(0.1784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3767/4345]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3768/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3769/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3770/4345]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3771/4345]: training_loss: tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3772/4345]: training_loss: tensor(0.2004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3773/4345]: training_loss: tensor(0.1282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3774/4345]: training_loss: tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3775/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3776/4345]: training_loss: tensor(0.2589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3777/4345]: training_loss: tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3778/4345]: training_loss: tensor(0.2007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3779/4345]: training_loss: tensor(0.2293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3780/4345]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3781/4345]: training_loss: tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3782/4345]: training_loss: tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3783/4345]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3784/4345]: training_loss: tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3785/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3786/4345]: training_loss: tensor(0.1159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3787/4345]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3788/4345]: training_loss: tensor(0.0543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3789/4345]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3790/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3791/4345]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3792/4345]: training_loss: tensor(0.1319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3793/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3794/4345]: training_loss: tensor(0.3643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3795/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3796/4345]: training_loss: tensor(0.7404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3797/4345]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3798/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3799/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3800/4345]: training_loss: tensor(0.3420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3801/4345]: training_loss: tensor(0.3563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3802/4345]: training_loss: tensor(0.1121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3803/4345]: training_loss: tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3804/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3805/4345]: training_loss: tensor(0.3172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3806/4345]: training_loss: tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3807/4345]: training_loss: tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3808/4345]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3809/4345]: training_loss: tensor(0.1782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3810/4345]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3811/4345]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3812/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3813/4345]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3814/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3815/4345]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3816/4345]: training_loss: tensor(0.1602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3817/4345]: training_loss: tensor(0.1421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3818/4345]: training_loss: tensor(0.1616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3819/4345]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3820/4345]: training_loss: tensor(0.2681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3821/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3822/4345]: training_loss: tensor(0.1489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3823/4345]: training_loss: tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3824/4345]: training_loss: tensor(0.1930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3825/4345]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3826/4345]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3827/4345]: training_loss: tensor(0.0637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3828/4345]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3829/4345]: training_loss: tensor(0.2184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3830/4345]: training_loss: tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3831/4345]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3832/4345]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3833/4345]: training_loss: tensor(0.0480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3834/4345]: training_loss: tensor(0.1963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3835/4345]: training_loss: tensor(0.1433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3836/4345]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3837/4345]: training_loss: tensor(0.2853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3838/4345]: training_loss: tensor(0.2494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3839/4345]: training_loss: tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3840/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3841/4345]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3842/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3843/4345]: training_loss: tensor(0.1877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3844/4345]: training_loss: tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3845/4345]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3846/4345]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3847/4345]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3848/4345]: training_loss: tensor(0.0555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3849/4345]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3850/4345]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3851/4345]: training_loss: tensor(0.2692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3852/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3853/4345]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3854/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3855/4345]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3856/4345]: training_loss: tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3857/4345]: training_loss: tensor(0.2792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3858/4345]: training_loss: tensor(0.2648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3859/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3860/4345]: training_loss: tensor(0.1164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3861/4345]: training_loss: tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3862/4345]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3863/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3864/4345]: training_loss: tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3865/4345]: training_loss: tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3866/4345]: training_loss: tensor(0.1758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3867/4345]: training_loss: tensor(0.2485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3868/4345]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3869/4345]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3870/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3871/4345]: training_loss: tensor(0.1447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3872/4345]: training_loss: tensor(0.1878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3873/4345]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3874/4345]: training_loss: tensor(0.1625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3875/4345]: training_loss: tensor(0.1738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3876/4345]: training_loss: tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3877/4345]: training_loss: tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3878/4345]: training_loss: tensor(0.1099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3879/4345]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3880/4345]: training_loss: tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3881/4345]: training_loss: tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3882/4345]: training_loss: tensor(0.2060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3883/4345]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3884/4345]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3885/4345]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3886/4345]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3887/4345]: training_loss: tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3888/4345]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3889/4345]: training_loss: tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3890/4345]: training_loss: tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3891/4345]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3892/4345]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3893/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3894/4345]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3895/4345]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3896/4345]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3897/4345]: training_loss: tensor(0.1750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3898/4345]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3899/4345]: training_loss: tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3900/4345]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3901/4345]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3902/4345]: training_loss: tensor(0.1386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3903/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3904/4345]: training_loss: tensor(0.1175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3905/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3906/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3907/4345]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3908/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3909/4345]: training_loss: tensor(0.1769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3910/4345]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3911/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3912/4345]: training_loss: tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3913/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3914/4345]: training_loss: tensor(0.2847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3915/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3916/4345]: training_loss: tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3917/4345]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3918/4345]: training_loss: tensor(0.2540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3919/4345]: training_loss: tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3920/4345]: training_loss: tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3921/4345]: training_loss: tensor(0.1951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3922/4345]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3923/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3924/4345]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3925/4345]: training_loss: tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3926/4345]: training_loss: tensor(0.2680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3927/4345]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3928/4345]: training_loss: tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3929/4345]: training_loss: tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3930/4345]: training_loss: tensor(0.1150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3931/4345]: training_loss: tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3932/4345]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3933/4345]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3934/4345]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3935/4345]: training_loss: tensor(0.0456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3936/4345]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3937/4345]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3938/4345]: training_loss: tensor(0.6567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3939/4345]: training_loss: tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3940/4345]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3941/4345]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3942/4345]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3943/4345]: training_loss: tensor(0.2252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3944/4345]: training_loss: tensor(0.0361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3945/4345]: training_loss: tensor(0.1453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3946/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3947/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3948/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3949/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3950/4345]: training_loss: tensor(0.3519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3951/4345]: training_loss: tensor(0.1361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3952/4345]: training_loss: tensor(0.6406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3953/4345]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3954/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3955/4345]: training_loss: tensor(0.1224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3956/4345]: training_loss: tensor(0.2483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3957/4345]: training_loss: tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3958/4345]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3959/4345]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3960/4345]: training_loss: tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3961/4345]: training_loss: tensor(0.2984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3962/4345]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3963/4345]: training_loss: tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3964/4345]: training_loss: tensor(0.1304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3965/4345]: training_loss: tensor(0.2310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3966/4345]: training_loss: tensor(0.1192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3967/4345]: training_loss: tensor(0.1991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3968/4345]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3969/4345]: training_loss: tensor(0.1487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3970/4345]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3971/4345]: training_loss: tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3972/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3973/4345]: training_loss: tensor(0.2270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3974/4345]: training_loss: tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3975/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3976/4345]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3977/4345]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3978/4345]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3979/4345]: training_loss: tensor(0.1350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3980/4345]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3981/4345]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3982/4345]: training_loss: tensor(0.1457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3983/4345]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3984/4345]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3985/4345]: training_loss: tensor(0.1180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3986/4345]: training_loss: tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3987/4345]: training_loss: tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3988/4345]: training_loss: tensor(0.4611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3989/4345]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3990/4345]: training_loss: tensor(0.1354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3991/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3992/4345]: training_loss: tensor(0.1043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3993/4345]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3994/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3995/4345]: training_loss: tensor(0.1863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3996/4345]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3997/4345]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3998/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3999/4345]: training_loss: tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4000/4345]: training_loss: tensor(0.3080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4001/4345]: training_loss: tensor(0.0413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4002/4345]: training_loss: tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4003/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4004/4345]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4005/4345]: training_loss: tensor(0.1156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4006/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4007/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4008/4345]: training_loss: tensor(0.3205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4009/4345]: training_loss: tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4010/4345]: training_loss: tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4011/4345]: training_loss: tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4012/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4013/4345]: training_loss: tensor(0.1283, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4014/4345]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4015/4345]: training_loss: tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4016/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4017/4345]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4018/4345]: training_loss: tensor(0.3619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4019/4345]: training_loss: tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4020/4345]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4021/4345]: training_loss: tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4022/4345]: training_loss: tensor(0.5581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4023/4345]: training_loss: tensor(0.3287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4024/4345]: training_loss: tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4025/4345]: training_loss: tensor(0.1284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4026/4345]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4027/4345]: training_loss: tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4028/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4029/4345]: training_loss: tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4030/4345]: training_loss: tensor(0.1817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4031/4345]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4032/4345]: training_loss: tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4033/4345]: training_loss: tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4034/4345]: training_loss: tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4035/4345]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4036/4345]: training_loss: tensor(0.1354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4037/4345]: training_loss: tensor(0.3155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4038/4345]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4039/4345]: training_loss: tensor(0.1600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4040/4345]: training_loss: tensor(0.0637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4041/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4042/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4043/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4044/4345]: training_loss: tensor(0.1519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4045/4345]: training_loss: tensor(0.3644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4046/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4047/4345]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4048/4345]: training_loss: tensor(0.2840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4049/4345]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4050/4345]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4051/4345]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4052/4345]: training_loss: tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4053/4345]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4054/4345]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4055/4345]: training_loss: tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4056/4345]: training_loss: tensor(0.1683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4057/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4058/4345]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4059/4345]: training_loss: tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4060/4345]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4061/4345]: training_loss: tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4062/4345]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4063/4345]: training_loss: tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4064/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4065/4345]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4066/4345]: training_loss: tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4067/4345]: training_loss: tensor(0.1550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4068/4345]: training_loss: tensor(0.0621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4069/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4070/4345]: training_loss: tensor(0.1164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4071/4345]: training_loss: tensor(0.1369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4072/4345]: training_loss: tensor(0.4495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4073/4345]: training_loss: tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4074/4345]: training_loss: tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4075/4345]: training_loss: tensor(0.1680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4076/4345]: training_loss: tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4077/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4078/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4079/4345]: training_loss: tensor(0.2666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4080/4345]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4081/4345]: training_loss: tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4082/4345]: training_loss: tensor(0.2084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4083/4345]: training_loss: tensor(0.1765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4084/4345]: training_loss: tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4085/4345]: training_loss: tensor(0.2088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4086/4345]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4087/4345]: training_loss: tensor(0.0620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4088/4345]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4089/4345]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4090/4345]: training_loss: tensor(0.1811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4091/4345]: training_loss: tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4092/4345]: training_loss: tensor(0.1554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4093/4345]: training_loss: tensor(0.1059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4094/4345]: training_loss: tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4095/4345]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4096/4345]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4097/4345]: training_loss: tensor(0.2133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4098/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4099/4345]: training_loss: tensor(0.2332, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4100/4345]: training_loss: tensor(0.2409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4101/4345]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4102/4345]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4103/4345]: training_loss: tensor(0.1193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4104/4345]: training_loss: tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4105/4345]: training_loss: tensor(0.1414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4106/4345]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4107/4345]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4108/4345]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4109/4345]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4110/4345]: training_loss: tensor(0.0507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4111/4345]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4112/4345]: training_loss: tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4113/4345]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4114/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4115/4345]: training_loss: tensor(0.1310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4116/4345]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4117/4345]: training_loss: tensor(0.2302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4118/4345]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4119/4345]: training_loss: tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4120/4345]: training_loss: tensor(0.2394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4121/4345]: training_loss: tensor(0.1695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4122/4345]: training_loss: tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4123/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4124/4345]: training_loss: tensor(0.2141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4125/4345]: training_loss: tensor(0.2963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4126/4345]: training_loss: tensor(0.1536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4127/4345]: training_loss: tensor(0.1942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4128/4345]: training_loss: tensor(0.1238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4129/4345]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4130/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4131/4345]: training_loss: tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4132/4345]: training_loss: tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4133/4345]: training_loss: tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4134/4345]: training_loss: tensor(0.1261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4135/4345]: training_loss: tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4136/4345]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4137/4345]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4138/4345]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4139/4345]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4140/4345]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4141/4345]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4142/4345]: training_loss: tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4143/4345]: training_loss: tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4144/4345]: training_loss: tensor(0.3774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4145/4345]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4146/4345]: training_loss: tensor(0.1999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4147/4345]: training_loss: tensor(0.2762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4148/4345]: training_loss: tensor(0.3700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4149/4345]: training_loss: tensor(0.2611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4150/4345]: training_loss: tensor(0.2333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4151/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4152/4345]: training_loss: tensor(0.2617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4153/4345]: training_loss: tensor(0.1105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4154/4345]: training_loss: tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4155/4345]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4156/4345]: training_loss: tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4157/4345]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4158/4345]: training_loss: tensor(0.2045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4159/4345]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4160/4345]: training_loss: tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4161/4345]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4162/4345]: training_loss: tensor(0.1773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4163/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4164/4345]: training_loss: tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4165/4345]: training_loss: tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4166/4345]: training_loss: tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4167/4345]: training_loss: tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4168/4345]: training_loss: tensor(0.2805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4169/4345]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4170/4345]: training_loss: tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4171/4345]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4172/4345]: training_loss: tensor(0.4354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4173/4345]: training_loss: tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4174/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4175/4345]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4176/4345]: training_loss: tensor(0.0572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4177/4345]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4178/4345]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4179/4345]: training_loss: tensor(0.2112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4180/4345]: training_loss: tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4181/4345]: training_loss: tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4182/4345]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4183/4345]: training_loss: tensor(0.2039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4184/4345]: training_loss: tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4185/4345]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4186/4345]: training_loss: tensor(0.5321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4187/4345]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4188/4345]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4189/4345]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4190/4345]: training_loss: tensor(0.2601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4191/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4192/4345]: training_loss: tensor(0.1369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4193/4345]: training_loss: tensor(0.2254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4194/4345]: training_loss: tensor(0.2568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4195/4345]: training_loss: tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4196/4345]: training_loss: tensor(0.1760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4197/4345]: training_loss: tensor(0.1748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4198/4345]: training_loss: tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4199/4345]: training_loss: tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4200/4345]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4201/4345]: training_loss: tensor(0.1627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4202/4345]: training_loss: tensor(0.2943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4203/4345]: training_loss: tensor(0.2303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4204/4345]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4205/4345]: training_loss: tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4206/4345]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4207/4345]: training_loss: tensor(0.1309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4208/4345]: training_loss: tensor(0.1453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4209/4345]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4210/4345]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4211/4345]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4212/4345]: training_loss: tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4213/4345]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4214/4345]: training_loss: tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4215/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4216/4345]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4217/4345]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4218/4345]: training_loss: tensor(0.4568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4219/4345]: training_loss: tensor(0.1412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4220/4345]: training_loss: tensor(0.2468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4221/4345]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4222/4345]: training_loss: tensor(0.3096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4223/4345]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4224/4345]: training_loss: tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4225/4345]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4226/4345]: training_loss: tensor(0.0501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4227/4345]: training_loss: tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4228/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4229/4345]: training_loss: tensor(0.1710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4230/4345]: training_loss: tensor(0.4346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4231/4345]: training_loss: tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4232/4345]: training_loss: tensor(0.3421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4233/4345]: training_loss: tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4234/4345]: training_loss: tensor(0.0436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4235/4345]: training_loss: tensor(0.3313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4236/4345]: training_loss: tensor(0.1386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4237/4345]: training_loss: tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4238/4345]: training_loss: tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4239/4345]: training_loss: tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4240/4345]: training_loss: tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4241/4345]: training_loss: tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4242/4345]: training_loss: tensor(0.2426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4243/4345]: training_loss: tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4244/4345]: training_loss: tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4245/4345]: training_loss: tensor(0.1563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4246/4345]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4247/4345]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4248/4345]: training_loss: tensor(0.2271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4249/4345]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4250/4345]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4251/4345]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4252/4345]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4253/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4254/4345]: training_loss: tensor(0.2727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4255/4345]: training_loss: tensor(0.2701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4256/4345]: training_loss: tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4257/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4258/4345]: training_loss: tensor(0.1108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4259/4345]: training_loss: tensor(0.2334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4260/4345]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4261/4345]: training_loss: tensor(0.1340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4262/4345]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4263/4345]: training_loss: tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4264/4345]: training_loss: tensor(0.3466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4265/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4266/4345]: training_loss: tensor(0.5440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4267/4345]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4268/4345]: training_loss: tensor(0.1408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4269/4345]: training_loss: tensor(0.2949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4270/4345]: training_loss: tensor(0.1489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4271/4345]: training_loss: tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4272/4345]: training_loss: tensor(0.0650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4273/4345]: training_loss: tensor(0.2049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4274/4345]: training_loss: tensor(0.1191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4275/4345]: training_loss: tensor(0.1251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4276/4345]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4277/4345]: training_loss: tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4278/4345]: training_loss: tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4279/4345]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4280/4345]: training_loss: tensor(0.2343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4281/4345]: training_loss: tensor(0.3229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4282/4345]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4283/4345]: training_loss: tensor(0.1666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4284/4345]: training_loss: tensor(0.0534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4285/4345]: training_loss: tensor(0.3783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4286/4345]: training_loss: tensor(0.4171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4287/4345]: training_loss: tensor(0.2119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4288/4345]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4289/4345]: training_loss: tensor(0.2561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4290/4345]: training_loss: tensor(0.1303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4291/4345]: training_loss: tensor(0.2171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4292/4345]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4293/4345]: training_loss: tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4294/4345]: training_loss: tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4295/4345]: training_loss: tensor(0.1132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4296/4345]: training_loss: tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4297/4345]: training_loss: tensor(0.1527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4298/4345]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4299/4345]: training_loss: tensor(0.1610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4300/4345]: training_loss: tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4301/4345]: training_loss: tensor(0.1456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4302/4345]: training_loss: tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4303/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4304/4345]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4305/4345]: training_loss: tensor(0.0555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4306/4345]: training_loss: tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4307/4345]: training_loss: tensor(0.1144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4308/4345]: training_loss: tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4309/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4310/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4311/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4312/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4313/4345]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4314/4345]: training_loss: tensor(0.5394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4315/4345]: training_loss: tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4316/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4317/4345]: training_loss: tensor(0.4411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4318/4345]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4319/4345]: training_loss: tensor(0.1736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4320/4345]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4321/4345]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4322/4345]: training_loss: tensor(0.2204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4323/4345]: training_loss: tensor(0.1788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4324/4345]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4325/4345]: training_loss: tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4326/4345]: training_loss: tensor(0.2089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4327/4345]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4328/4345]: training_loss: tensor(0.0587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4329/4345]: training_loss: tensor(0.3351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4330/4345]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4331/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4332/4345]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4333/4345]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4334/4345]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4335/4345]: training_loss: tensor(0.1770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4336/4345]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4337/4345]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4338/4345]: training_loss: tensor(0.1831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4339/4345]: training_loss: tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4340/4345]: training_loss: tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4341/4345]: training_loss: tensor(0.2267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4342/4345]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4343/4345]: training_loss: tensor(0.2808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4344/4345]: training_loss: tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4345/4345]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4346/4345]: training_loss: tensor(0.2018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [2/5], Step [8692/21730], Train Loss: 0.0930, Valid Loss: 0.1350\n",
      "batch_no [1/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2/4345]: training_loss: tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [5/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [6/4345]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [7/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [8/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [9/4345]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [10/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [11/4345]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [12/4345]: training_loss: tensor(0.0364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [13/4345]: training_loss: tensor(0.1239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [14/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [15/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [16/4345]: training_loss: tensor(0.2644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [17/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [18/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [19/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [20/4345]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [21/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [22/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [23/4345]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [24/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [25/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [26/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [27/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [28/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [29/4345]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [30/4345]: training_loss: tensor(0.3405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [31/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [32/4345]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [33/4345]: training_loss: tensor(0.1416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [34/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [35/4345]: training_loss: tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [36/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [37/4345]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [38/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [39/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [40/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [41/4345]: training_loss: tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [42/4345]: training_loss: tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [43/4345]: training_loss: tensor(0.1603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [44/4345]: training_loss: tensor(0.4214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [45/4345]: training_loss: tensor(0.2109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [46/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [47/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [48/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [49/4345]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [50/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [51/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [52/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [53/4345]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [54/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [55/4345]: training_loss: tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [56/4345]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [57/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [58/4345]: training_loss: tensor(0.2939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [59/4345]: training_loss: tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [60/4345]: training_loss: tensor(0.1651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [61/4345]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [62/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [63/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [64/4345]: training_loss: tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [65/4345]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [66/4345]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [67/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [68/4345]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [69/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [70/4345]: training_loss: tensor(0.1869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [71/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [72/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [73/4345]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [74/4345]: training_loss: tensor(0.7556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [75/4345]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [76/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [77/4345]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [78/4345]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [79/4345]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [80/4345]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [81/4345]: training_loss: tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [82/4345]: training_loss: tensor(0.1413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [83/4345]: training_loss: tensor(0.1201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [84/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [85/4345]: training_loss: tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [86/4345]: training_loss: tensor(0.1407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [87/4345]: training_loss: tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [88/4345]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [89/4345]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [90/4345]: training_loss: tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [91/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [92/4345]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [93/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [94/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [95/4345]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [96/4345]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [97/4345]: training_loss: tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [98/4345]: training_loss: tensor(0.1336, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [99/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [100/4345]: training_loss: tensor(0.1739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [101/4345]: training_loss: tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [102/4345]: training_loss: tensor(0.1800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [103/4345]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [104/4345]: training_loss: tensor(0.3333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [105/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [106/4345]: training_loss: tensor(0.1392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [107/4345]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [108/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [109/4345]: training_loss: tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [110/4345]: training_loss: tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [111/4345]: training_loss: tensor(0.1543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [112/4345]: training_loss: tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [113/4345]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [114/4345]: training_loss: tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [115/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [116/4345]: training_loss: tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [117/4345]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [118/4345]: training_loss: tensor(0.2474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [119/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [120/4345]: training_loss: tensor(0.0290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [121/4345]: training_loss: tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [122/4345]: training_loss: tensor(0.1927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [123/4345]: training_loss: tensor(0.5019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [124/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [125/4345]: training_loss: tensor(0.1566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [126/4345]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [127/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [128/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [129/4345]: training_loss: tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [130/4345]: training_loss: tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [131/4345]: training_loss: tensor(0.2297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [132/4345]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [133/4345]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [134/4345]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [135/4345]: training_loss: tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [136/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [137/4345]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [138/4345]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [139/4345]: training_loss: tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [140/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [141/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [142/4345]: training_loss: tensor(0.1900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [143/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [144/4345]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [145/4345]: training_loss: tensor(0.1129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [146/4345]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [147/4345]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [148/4345]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [149/4345]: training_loss: tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [150/4345]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [151/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [152/4345]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [153/4345]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [154/4345]: training_loss: tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [155/4345]: training_loss: tensor(0.3044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [156/4345]: training_loss: tensor(0.1784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [157/4345]: training_loss: tensor(0.1304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [158/4345]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [159/4345]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [160/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [161/4345]: training_loss: tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [162/4345]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [163/4345]: training_loss: tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [164/4345]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [165/4345]: training_loss: tensor(0.1533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [166/4345]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [167/4345]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [168/4345]: training_loss: tensor(0.1103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [169/4345]: training_loss: tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [170/4345]: training_loss: tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [171/4345]: training_loss: tensor(0.2820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [172/4345]: training_loss: tensor(0.1068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [173/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [174/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [175/4345]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [176/4345]: training_loss: tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [177/4345]: training_loss: tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [178/4345]: training_loss: tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [179/4345]: training_loss: tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [180/4345]: training_loss: tensor(0.2067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [181/4345]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [182/4345]: training_loss: tensor(0.2472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [183/4345]: training_loss: tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [184/4345]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [185/4345]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [186/4345]: training_loss: tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [187/4345]: training_loss: tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [188/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [189/4345]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [190/4345]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [191/4345]: training_loss: tensor(0.1728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [192/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [193/4345]: training_loss: tensor(0.2981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [194/4345]: training_loss: tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [195/4345]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [196/4345]: training_loss: tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [197/4345]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [198/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [199/4345]: training_loss: tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [200/4345]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [201/4345]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [202/4345]: training_loss: tensor(0.1815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [203/4345]: training_loss: tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [204/4345]: training_loss: tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [205/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [206/4345]: training_loss: tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [207/4345]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [208/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [209/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [210/4345]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [211/4345]: training_loss: tensor(0.1255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [212/4345]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [213/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [214/4345]: training_loss: tensor(0.2743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [215/4345]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [216/4345]: training_loss: tensor(0.2523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [217/4345]: training_loss: tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [218/4345]: training_loss: tensor(0.1557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [219/4345]: training_loss: tensor(0.2043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [220/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [221/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [222/4345]: training_loss: tensor(0.3730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [223/4345]: training_loss: tensor(0.0410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [224/4345]: training_loss: tensor(0.3593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [225/4345]: training_loss: tensor(0.1111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [226/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [227/4345]: training_loss: tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [228/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [229/4345]: training_loss: tensor(0.2108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [230/4345]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [231/4345]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [232/4345]: training_loss: tensor(0.3907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [233/4345]: training_loss: tensor(0.1447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [234/4345]: training_loss: tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [235/4345]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [236/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [237/4345]: training_loss: tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [238/4345]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [239/4345]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [240/4345]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [241/4345]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [242/4345]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [243/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [244/4345]: training_loss: tensor(0.2064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [245/4345]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [246/4345]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [247/4345]: training_loss: tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [248/4345]: training_loss: tensor(0.2341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [249/4345]: training_loss: tensor(0.1935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [250/4345]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [251/4345]: training_loss: tensor(0.1173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [252/4345]: training_loss: tensor(0.2686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [253/4345]: training_loss: tensor(0.1681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [254/4345]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [255/4345]: training_loss: tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [256/4345]: training_loss: tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [257/4345]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [258/4345]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [259/4345]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [260/4345]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [261/4345]: training_loss: tensor(0.2269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [262/4345]: training_loss: tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [263/4345]: training_loss: tensor(0.2178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [264/4345]: training_loss: tensor(0.2338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [265/4345]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [266/4345]: training_loss: tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [267/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [268/4345]: training_loss: tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [269/4345]: training_loss: tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [270/4345]: training_loss: tensor(0.2329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [271/4345]: training_loss: tensor(0.1401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [272/4345]: training_loss: tensor(0.1398, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [273/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [274/4345]: training_loss: tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [275/4345]: training_loss: tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [276/4345]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [277/4345]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [278/4345]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [279/4345]: training_loss: tensor(0.1282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [280/4345]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [281/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [282/4345]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [283/4345]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [284/4345]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [285/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [286/4345]: training_loss: tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [287/4345]: training_loss: tensor(0.2052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [288/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [289/4345]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [290/4345]: training_loss: tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [291/4345]: training_loss: tensor(0.1822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [292/4345]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [293/4345]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [294/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [295/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [296/4345]: training_loss: tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [297/4345]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [298/4345]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [299/4345]: training_loss: tensor(0.1463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [300/4345]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [301/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [302/4345]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [303/4345]: training_loss: tensor(0.0462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [304/4345]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [305/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [306/4345]: training_loss: tensor(0.0582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [307/4345]: training_loss: tensor(0.1256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [308/4345]: training_loss: tensor(0.2455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [309/4345]: training_loss: tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [310/4345]: training_loss: tensor(0.2277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [311/4345]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [312/4345]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [313/4345]: training_loss: tensor(0.1955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [314/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [315/4345]: training_loss: tensor(0.2702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [316/4345]: training_loss: tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [317/4345]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [318/4345]: training_loss: tensor(0.1385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [319/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [320/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [321/4345]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [322/4345]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [323/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [324/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [325/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [326/4345]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [327/4345]: training_loss: tensor(0.3543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [328/4345]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [329/4345]: training_loss: tensor(0.2046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [330/4345]: training_loss: tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [331/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [332/4345]: training_loss: tensor(0.0666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [333/4345]: training_loss: tensor(0.1725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [334/4345]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [335/4345]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [336/4345]: training_loss: tensor(0.3921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [337/4345]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [338/4345]: training_loss: tensor(0.1064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [339/4345]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [340/4345]: training_loss: tensor(0.1451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [341/4345]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [342/4345]: training_loss: tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [343/4345]: training_loss: tensor(0.1784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [344/4345]: training_loss: tensor(0.2663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [345/4345]: training_loss: tensor(0.1595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [346/4345]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [347/4345]: training_loss: tensor(0.2575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [348/4345]: training_loss: tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [349/4345]: training_loss: tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [350/4345]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [351/4345]: training_loss: tensor(0.1808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [352/4345]: training_loss: tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [353/4345]: training_loss: tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [354/4345]: training_loss: tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [355/4345]: training_loss: tensor(0.1159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [356/4345]: training_loss: tensor(0.0354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [357/4345]: training_loss: tensor(0.1622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [358/4345]: training_loss: tensor(0.1824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [359/4345]: training_loss: tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [360/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [361/4345]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [362/4345]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [363/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [364/4345]: training_loss: tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [365/4345]: training_loss: tensor(0.2555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [366/4345]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [367/4345]: training_loss: tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [368/4345]: training_loss: tensor(0.1344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [369/4345]: training_loss: tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [370/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [371/4345]: training_loss: tensor(0.0507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [372/4345]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [373/4345]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [374/4345]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [375/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [376/4345]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [377/4345]: training_loss: tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [378/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [379/4345]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [380/4345]: training_loss: tensor(0.2596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [381/4345]: training_loss: tensor(0.1516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [382/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [383/4345]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [384/4345]: training_loss: tensor(0.1580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [385/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [386/4345]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [387/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [388/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [389/4345]: training_loss: tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [390/4345]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [391/4345]: training_loss: tensor(0.1287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [392/4345]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [393/4345]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [394/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [395/4345]: training_loss: tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [396/4345]: training_loss: tensor(0.1143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [397/4345]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [398/4345]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [399/4345]: training_loss: tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [400/4345]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [401/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [402/4345]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [403/4345]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [404/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [405/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [406/4345]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [407/4345]: training_loss: tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [408/4345]: training_loss: tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [409/4345]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [410/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [411/4345]: training_loss: tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [412/4345]: training_loss: tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [413/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [414/4345]: training_loss: tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [415/4345]: training_loss: tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [416/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [417/4345]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [418/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [419/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [420/4345]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [421/4345]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [422/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [423/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [424/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [425/4345]: training_loss: tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [426/4345]: training_loss: tensor(0.2116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [427/4345]: training_loss: tensor(0.1638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [428/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [429/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [430/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [431/4345]: training_loss: tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [432/4345]: training_loss: tensor(0.3626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [433/4345]: training_loss: tensor(0.1823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [434/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [435/4345]: training_loss: tensor(0.5172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [436/4345]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [437/4345]: training_loss: tensor(0.2516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [438/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [439/4345]: training_loss: tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [440/4345]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [441/4345]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [442/4345]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [443/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [444/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [445/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [446/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [447/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [448/4345]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [449/4345]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [450/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [451/4345]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [452/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [453/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [454/4345]: training_loss: tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [455/4345]: training_loss: tensor(0.3418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [456/4345]: training_loss: tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [457/4345]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [458/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [459/4345]: training_loss: tensor(0.1484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [460/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [461/4345]: training_loss: tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [462/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [463/4345]: training_loss: tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [464/4345]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [465/4345]: training_loss: tensor(0.1090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [466/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [467/4345]: training_loss: tensor(0.1131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [468/4345]: training_loss: tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [469/4345]: training_loss: tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [470/4345]: training_loss: tensor(0.0543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [471/4345]: training_loss: tensor(0.2864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [472/4345]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [473/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [474/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [475/4345]: training_loss: tensor(0.3902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [476/4345]: training_loss: tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [477/4345]: training_loss: tensor(0.2023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [478/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [479/4345]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [480/4345]: training_loss: tensor(0.4282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [481/4345]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [482/4345]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [483/4345]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [484/4345]: training_loss: tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [485/4345]: training_loss: tensor(0.2396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [486/4345]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [487/4345]: training_loss: tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [488/4345]: training_loss: tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [489/4345]: training_loss: tensor(0.1401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [490/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [491/4345]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [492/4345]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [493/4345]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [494/4345]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [495/4345]: training_loss: tensor(0.1851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [496/4345]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [497/4345]: training_loss: tensor(0.1117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [498/4345]: training_loss: tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [499/4345]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [500/4345]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [501/4345]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [502/4345]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [503/4345]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [504/4345]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [505/4345]: training_loss: tensor(0.3783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [506/4345]: training_loss: tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [507/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [508/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [509/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [510/4345]: training_loss: tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [511/4345]: training_loss: tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [512/4345]: training_loss: tensor(0.1591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [513/4345]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [514/4345]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [515/4345]: training_loss: tensor(0.1618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [516/4345]: training_loss: tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [517/4345]: training_loss: tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [518/4345]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [519/4345]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [520/4345]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [521/4345]: training_loss: tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [522/4345]: training_loss: tensor(0.1473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [523/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [524/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [525/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [526/4345]: training_loss: tensor(0.0410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [527/4345]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [528/4345]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [529/4345]: training_loss: tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [530/4345]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [531/4345]: training_loss: tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [532/4345]: training_loss: tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [533/4345]: training_loss: tensor(0.0582, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [534/4345]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [535/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [536/4345]: training_loss: tensor(0.1114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [537/4345]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [538/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [539/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [540/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [541/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [542/4345]: training_loss: tensor(0.3818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [543/4345]: training_loss: tensor(0.2525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [544/4345]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [545/4345]: training_loss: tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [546/4345]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [547/4345]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [548/4345]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [549/4345]: training_loss: tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [550/4345]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [551/4345]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [552/4345]: training_loss: tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [553/4345]: training_loss: tensor(0.1706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [554/4345]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [555/4345]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [556/4345]: training_loss: tensor(0.1297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [557/4345]: training_loss: tensor(0.2678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [558/4345]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [559/4345]: training_loss: tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [560/4345]: training_loss: tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [561/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [562/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [563/4345]: training_loss: tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [564/4345]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [565/4345]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [566/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [567/4345]: training_loss: tensor(0.1718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [568/4345]: training_loss: tensor(0.1152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [569/4345]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [570/4345]: training_loss: tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [571/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [572/4345]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [573/4345]: training_loss: tensor(0.1654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [574/4345]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [575/4345]: training_loss: tensor(0.2166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [576/4345]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [577/4345]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [578/4345]: training_loss: tensor(0.2357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [579/4345]: training_loss: tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [580/4345]: training_loss: tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [581/4345]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [582/4345]: training_loss: tensor(0.1711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [583/4345]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [584/4345]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [585/4345]: training_loss: tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [586/4345]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [587/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [588/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [589/4345]: training_loss: tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [590/4345]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [591/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [592/4345]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [593/4345]: training_loss: tensor(0.0413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [594/4345]: training_loss: tensor(0.1629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [595/4345]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [596/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [597/4345]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [598/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [599/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [600/4345]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [601/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [602/4345]: training_loss: tensor(0.2972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [603/4345]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [604/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [605/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [606/4345]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [607/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [608/4345]: training_loss: tensor(0.2088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [609/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [610/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [611/4345]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [612/4345]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [613/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [614/4345]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [615/4345]: training_loss: tensor(0.0362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [616/4345]: training_loss: tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [617/4345]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [618/4345]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [619/4345]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [620/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [621/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [622/4345]: training_loss: tensor(0.3177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [623/4345]: training_loss: tensor(0.3531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [624/4345]: training_loss: tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [625/4345]: training_loss: tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [626/4345]: training_loss: tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [627/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [628/4345]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [629/4345]: training_loss: tensor(0.2407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [630/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [631/4345]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [632/4345]: training_loss: tensor(0.3363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [633/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [634/4345]: training_loss: tensor(0.2576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [635/4345]: training_loss: tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [636/4345]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [637/4345]: training_loss: tensor(0.1216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [638/4345]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [639/4345]: training_loss: tensor(0.2901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [640/4345]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [641/4345]: training_loss: tensor(0.0305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [642/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [643/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [644/4345]: training_loss: tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [645/4345]: training_loss: tensor(0.1850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [646/4345]: training_loss: tensor(0.0413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [647/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [648/4345]: training_loss: tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [649/4345]: training_loss: tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [650/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [651/4345]: training_loss: tensor(0.3183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [652/4345]: training_loss: tensor(0.1758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [653/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [654/4345]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [655/4345]: training_loss: tensor(0.1268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [656/4345]: training_loss: tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [657/4345]: training_loss: tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [658/4345]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [659/4345]: training_loss: tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [660/4345]: training_loss: tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [661/4345]: training_loss: tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [662/4345]: training_loss: tensor(0.1758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [663/4345]: training_loss: tensor(0.1112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [664/4345]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [665/4345]: training_loss: tensor(0.1385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [666/4345]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [667/4345]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [668/4345]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [669/4345]: training_loss: tensor(0.1165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [670/4345]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [671/4345]: training_loss: tensor(0.0495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [672/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [673/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [674/4345]: training_loss: tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [675/4345]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [676/4345]: training_loss: tensor(0.3227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [677/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [678/4345]: training_loss: tensor(0.1858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [679/4345]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [680/4345]: training_loss: tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [681/4345]: training_loss: tensor(0.0586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [682/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [683/4345]: training_loss: tensor(0.1811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [684/4345]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [685/4345]: training_loss: tensor(0.2819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [686/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [687/4345]: training_loss: tensor(0.3664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [688/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [689/4345]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [690/4345]: training_loss: tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [691/4345]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [692/4345]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [693/4345]: training_loss: tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [694/4345]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [695/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [696/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [697/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [698/4345]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [699/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [700/4345]: training_loss: tensor(0.0468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [701/4345]: training_loss: tensor(0.1405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [702/4345]: training_loss: tensor(0.2816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [703/4345]: training_loss: tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [704/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [705/4345]: training_loss: tensor(0.0505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [706/4345]: training_loss: tensor(0.1961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [707/4345]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [708/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [709/4345]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [710/4345]: training_loss: tensor(0.2613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [711/4345]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [712/4345]: training_loss: tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [713/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [714/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [715/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [716/4345]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [717/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [718/4345]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [719/4345]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [720/4345]: training_loss: tensor(0.1933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [721/4345]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [722/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [723/4345]: training_loss: tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [724/4345]: training_loss: tensor(0.1476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [725/4345]: training_loss: tensor(0.1822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [726/4345]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [727/4345]: training_loss: tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [728/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [729/4345]: training_loss: tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [730/4345]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [731/4345]: training_loss: tensor(0.1845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [732/4345]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [733/4345]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [734/4345]: training_loss: tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [735/4345]: training_loss: tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [736/4345]: training_loss: tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [737/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [738/4345]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [739/4345]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [740/4345]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [741/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [742/4345]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [743/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [744/4345]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [745/4345]: training_loss: tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [746/4345]: training_loss: tensor(0.1220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [747/4345]: training_loss: tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [748/4345]: training_loss: tensor(0.1677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [749/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [750/4345]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [751/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [752/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [753/4345]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [754/4345]: training_loss: tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [755/4345]: training_loss: tensor(0.1456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [756/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [757/4345]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [758/4345]: training_loss: tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [759/4345]: training_loss: tensor(0.0442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [760/4345]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [761/4345]: training_loss: tensor(0.1263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [762/4345]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [763/4345]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [764/4345]: training_loss: tensor(0.2839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [765/4345]: training_loss: tensor(0.1286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [766/4345]: training_loss: tensor(0.1401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [767/4345]: training_loss: tensor(0.1436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [768/4345]: training_loss: tensor(0.1437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [769/4345]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [770/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [771/4345]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [772/4345]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [773/4345]: training_loss: tensor(0.3873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [774/4345]: training_loss: tensor(0.1305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [775/4345]: training_loss: tensor(0.2009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [776/4345]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [777/4345]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [778/4345]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [779/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [780/4345]: training_loss: tensor(0.2259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [781/4345]: training_loss: tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [782/4345]: training_loss: tensor(0.1243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [783/4345]: training_loss: tensor(0.2119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [784/4345]: training_loss: tensor(0.2324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [785/4345]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [786/4345]: training_loss: tensor(0.0413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [787/4345]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [788/4345]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [789/4345]: training_loss: tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [790/4345]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [791/4345]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [792/4345]: training_loss: tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [793/4345]: training_loss: tensor(0.1433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [794/4345]: training_loss: tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [795/4345]: training_loss: tensor(0.0476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [796/4345]: training_loss: tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [797/4345]: training_loss: tensor(0.2171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [798/4345]: training_loss: tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [799/4345]: training_loss: tensor(0.3146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [800/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [801/4345]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [802/4345]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [803/4345]: training_loss: tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [804/4345]: training_loss: tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [805/4345]: training_loss: tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [806/4345]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [807/4345]: training_loss: tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [808/4345]: training_loss: tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [809/4345]: training_loss: tensor(0.0517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [810/4345]: training_loss: tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [811/4345]: training_loss: tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [812/4345]: training_loss: tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [813/4345]: training_loss: tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [814/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [815/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [816/4345]: training_loss: tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [817/4345]: training_loss: tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [818/4345]: training_loss: tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [819/4345]: training_loss: tensor(0.2573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [820/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [821/4345]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [822/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [823/4345]: training_loss: tensor(0.1139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [824/4345]: training_loss: tensor(0.1480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [825/4345]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [826/4345]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [827/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [828/4345]: training_loss: tensor(0.0640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [829/4345]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [830/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [831/4345]: training_loss: tensor(0.3477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [832/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [833/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [834/4345]: training_loss: tensor(0.3072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [835/4345]: training_loss: tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [836/4345]: training_loss: tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [837/4345]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [838/4345]: training_loss: tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [839/4345]: training_loss: tensor(0.2696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [840/4345]: training_loss: tensor(0.1513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [841/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [842/4345]: training_loss: tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [843/4345]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [844/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [845/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [846/4345]: training_loss: tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [847/4345]: training_loss: tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [848/4345]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [849/4345]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [850/4345]: training_loss: tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [851/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [852/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [853/4345]: training_loss: tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [854/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [855/4345]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [856/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [857/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [858/4345]: training_loss: tensor(0.4982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [859/4345]: training_loss: tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [860/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [861/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [862/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [863/4345]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [864/4345]: training_loss: tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [865/4345]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [866/4345]: training_loss: tensor(0.1337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [867/4345]: training_loss: tensor(0.2509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [868/4345]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [869/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [870/4345]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [871/4345]: training_loss: tensor(0.3672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [872/4345]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [873/4345]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [874/4345]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [875/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [876/4345]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [877/4345]: training_loss: tensor(0.1323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [878/4345]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [879/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [880/4345]: training_loss: tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [881/4345]: training_loss: tensor(0.3868, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [882/4345]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [883/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [884/4345]: training_loss: tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [885/4345]: training_loss: tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [886/4345]: training_loss: tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [887/4345]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [888/4345]: training_loss: tensor(0.3417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [889/4345]: training_loss: tensor(0.5473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [890/4345]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [891/4345]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [892/4345]: training_loss: tensor(0.4551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [893/4345]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [894/4345]: training_loss: tensor(0.1393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [895/4345]: training_loss: tensor(0.1118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [896/4345]: training_loss: tensor(0.1653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [897/4345]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [898/4345]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [899/4345]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [900/4345]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [901/4345]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [902/4345]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [903/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [904/4345]: training_loss: tensor(0.3910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [905/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [906/4345]: training_loss: tensor(0.1634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [907/4345]: training_loss: tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [908/4345]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [909/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [910/4345]: training_loss: tensor(0.0676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [911/4345]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [912/4345]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [913/4345]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [914/4345]: training_loss: tensor(0.3199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [915/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [916/4345]: training_loss: tensor(0.5685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [917/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [918/4345]: training_loss: tensor(0.1938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [919/4345]: training_loss: tensor(0.5242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [920/4345]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [921/4345]: training_loss: tensor(0.2240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [922/4345]: training_loss: tensor(0.1458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [923/4345]: training_loss: tensor(0.1385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [924/4345]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [925/4345]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [926/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [927/4345]: training_loss: tensor(0.1152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [928/4345]: training_loss: tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [929/4345]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [930/4345]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [931/4345]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [932/4345]: training_loss: tensor(0.1262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [933/4345]: training_loss: tensor(0.0615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [934/4345]: training_loss: tensor(0.2032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [935/4345]: training_loss: tensor(0.1407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [936/4345]: training_loss: tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [937/4345]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [938/4345]: training_loss: tensor(0.0410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [939/4345]: training_loss: tensor(0.2641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [940/4345]: training_loss: tensor(0.2336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [941/4345]: training_loss: tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [942/4345]: training_loss: tensor(0.1527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [943/4345]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [944/4345]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [945/4345]: training_loss: tensor(0.1904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [946/4345]: training_loss: tensor(0.0456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [947/4345]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [948/4345]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [949/4345]: training_loss: tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [950/4345]: training_loss: tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [951/4345]: training_loss: tensor(0.1072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [952/4345]: training_loss: tensor(0.1230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [953/4345]: training_loss: tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [954/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [955/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [956/4345]: training_loss: tensor(0.1411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [957/4345]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [958/4345]: training_loss: tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [959/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [960/4345]: training_loss: tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [961/4345]: training_loss: tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [962/4345]: training_loss: tensor(0.3001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [963/4345]: training_loss: tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [964/4345]: training_loss: tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [965/4345]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [966/4345]: training_loss: tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [967/4345]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [968/4345]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [969/4345]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [970/4345]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [971/4345]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [972/4345]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [973/4345]: training_loss: tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [974/4345]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [975/4345]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [976/4345]: training_loss: tensor(0.1344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [977/4345]: training_loss: tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [978/4345]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [979/4345]: training_loss: tensor(0.2648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [980/4345]: training_loss: tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [981/4345]: training_loss: tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [982/4345]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [983/4345]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [984/4345]: training_loss: tensor(0.1091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [985/4345]: training_loss: tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [986/4345]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [987/4345]: training_loss: tensor(0.2325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [988/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [989/4345]: training_loss: tensor(0.1409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [990/4345]: training_loss: tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [991/4345]: training_loss: tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [992/4345]: training_loss: tensor(0.2401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [993/4345]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [994/4345]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [995/4345]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [996/4345]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [997/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [998/4345]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [999/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1000/4345]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1001/4345]: training_loss: tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1002/4345]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1003/4345]: training_loss: tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1004/4345]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1005/4345]: training_loss: tensor(0.0445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/4345]: training_loss: tensor(0.2225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/4345]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/4345]: training_loss: tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/4345]: training_loss: tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/4345]: training_loss: tensor(0.1577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/4345]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/4345]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/4345]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/4345]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/4345]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1019/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/4345]: training_loss: tensor(0.1273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/4345]: training_loss: tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1023/4345]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/4345]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/4345]: training_loss: tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/4345]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/4345]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/4345]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/4345]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/4345]: training_loss: tensor(0.4704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/4345]: training_loss: tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/4345]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1040/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1043/4345]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/4345]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/4345]: training_loss: tensor(0.3543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/4345]: training_loss: tensor(0.2783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/4345]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/4345]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/4345]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/4345]: training_loss: tensor(0.0362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/4345]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/4345]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/4345]: training_loss: tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1055/4345]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/4345]: training_loss: tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/4345]: training_loss: tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/4345]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/4345]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/4345]: training_loss: tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/4345]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/4345]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/4345]: training_loss: tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/4345]: training_loss: tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/4345]: training_loss: tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/4345]: training_loss: tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/4345]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/4345]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/4345]: training_loss: tensor(0.1996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/4345]: training_loss: tensor(0.2259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/4345]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/4345]: training_loss: tensor(0.1649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/4345]: training_loss: tensor(0.4510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/4345]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/4345]: training_loss: tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/4345]: training_loss: tensor(0.1839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/4345]: training_loss: tensor(0.2078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/4345]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/4345]: training_loss: tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/4345]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/4345]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/4345]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/4345]: training_loss: tensor(0.2439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/4345]: training_loss: tensor(0.2181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/4345]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/4345]: training_loss: tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1100/4345]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/4345]: training_loss: tensor(0.3743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/4345]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/4345]: training_loss: tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/4345]: training_loss: tensor(0.0468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/4345]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/4345]: training_loss: tensor(0.0462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/4345]: training_loss: tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/4345]: training_loss: tensor(0.1516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/4345]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/4345]: training_loss: tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/4345]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/4345]: training_loss: tensor(0.1202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/4345]: training_loss: tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/4345]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/4345]: training_loss: tensor(0.1486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/4345]: training_loss: tensor(0.0413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/4345]: training_loss: tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/4345]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/4345]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/4345]: training_loss: tensor(0.2205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1129/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/4345]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/4345]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/4345]: training_loss: tensor(0.0501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/4345]: training_loss: tensor(0.2775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/4345]: training_loss: tensor(0.2032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/4345]: training_loss: tensor(0.1123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/4345]: training_loss: tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/4345]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/4345]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/4345]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1141/4345]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/4345]: training_loss: tensor(0.3757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/4345]: training_loss: tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/4345]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/4345]: training_loss: tensor(0.1774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/4345]: training_loss: tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/4345]: training_loss: tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/4345]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/4345]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/4345]: training_loss: tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/4345]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/4345]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/4345]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/4345]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/4345]: training_loss: tensor(0.1558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/4345]: training_loss: tensor(0.0305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/4345]: training_loss: tensor(0.2490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/4345]: training_loss: tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/4345]: training_loss: tensor(0.3880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/4345]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/4345]: training_loss: tensor(0.0362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/4345]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/4345]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/4345]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/4345]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/4345]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/4345]: training_loss: tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/4345]: training_loss: tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/4345]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/4345]: training_loss: tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/4345]: training_loss: tensor(0.1069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/4345]: training_loss: tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/4345]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/4345]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/4345]: training_loss: tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/4345]: training_loss: tensor(0.2145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/4345]: training_loss: tensor(0.0362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/4345]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/4345]: training_loss: tensor(0.1596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/4345]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/4345]: training_loss: tensor(0.2108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/4345]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/4345]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/4345]: training_loss: tensor(0.1288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/4345]: training_loss: tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/4345]: training_loss: tensor(0.1772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/4345]: training_loss: tensor(0.1951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/4345]: training_loss: tensor(0.1541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/4345]: training_loss: tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/4345]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1213/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1215/4345]: training_loss: tensor(0.1433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/4345]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/4345]: training_loss: tensor(0.4968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/4345]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/4345]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/4345]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/4345]: training_loss: tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1227/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/4345]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/4345]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/4345]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/4345]: training_loss: tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/4345]: training_loss: tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/4345]: training_loss: tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/4345]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/4345]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/4345]: training_loss: tensor(0.1222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/4345]: training_loss: tensor(0.1821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/4345]: training_loss: tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/4345]: training_loss: tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/4345]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/4345]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/4345]: training_loss: tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/4345]: training_loss: tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/4345]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/4345]: training_loss: tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/4345]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/4345]: training_loss: tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/4345]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/4345]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/4345]: training_loss: tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/4345]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/4345]: training_loss: tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/4345]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/4345]: training_loss: tensor(0.2672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/4345]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/4345]: training_loss: tensor(0.3403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/4345]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/4345]: training_loss: tensor(0.2751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/4345]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/4345]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/4345]: training_loss: tensor(0.1340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/4345]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/4345]: training_loss: tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/4345]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/4345]: training_loss: tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/4345]: training_loss: tensor(0.1682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/4345]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/4345]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/4345]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/4345]: training_loss: tensor(0.1422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/4345]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/4345]: training_loss: tensor(0.1974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/4345]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/4345]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/4345]: training_loss: tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/4345]: training_loss: tensor(0.0383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1299/4345]: training_loss: tensor(0.1438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/4345]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/4345]: training_loss: tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/4345]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/4345]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/4345]: training_loss: tensor(0.1687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/4345]: training_loss: tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/4345]: training_loss: tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/4345]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/4345]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/4345]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/4345]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1313/4345]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/4345]: training_loss: tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/4345]: training_loss: tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/4345]: training_loss: tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/4345]: training_loss: tensor(0.1440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/4345]: training_loss: tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/4345]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/4345]: training_loss: tensor(0.3331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/4345]: training_loss: tensor(0.1413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/4345]: training_loss: tensor(0.3255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/4345]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/4345]: training_loss: tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/4345]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/4345]: training_loss: tensor(0.1721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/4345]: training_loss: tensor(0.1787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/4345]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/4345]: training_loss: tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/4345]: training_loss: tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/4345]: training_loss: tensor(0.1150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/4345]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/4345]: training_loss: tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/4345]: training_loss: tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/4345]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/4345]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/4345]: training_loss: tensor(0.2268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/4345]: training_loss: tensor(0.0413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/4345]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/4345]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/4345]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/4345]: training_loss: tensor(0.1345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/4345]: training_loss: tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/4345]: training_loss: tensor(0.2067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/4345]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/4345]: training_loss: tensor(0.2508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/4345]: training_loss: tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1361/4345]: training_loss: tensor(0.0442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1363/4345]: training_loss: tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/4345]: training_loss: tensor(0.1691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/4345]: training_loss: tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/4345]: training_loss: tensor(0.1205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/4345]: training_loss: tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/4345]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/4345]: training_loss: tensor(0.1452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/4345]: training_loss: tensor(0.1735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/4345]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/4345]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/4345]: training_loss: tensor(0.1617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/4345]: training_loss: tensor(0.1410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/4345]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/4345]: training_loss: tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1385/4345]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1386/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1387/4345]: training_loss: tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1388/4345]: training_loss: tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1389/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1390/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1391/4345]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1392/4345]: training_loss: tensor(0.1439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1393/4345]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1394/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1395/4345]: training_loss: tensor(0.2332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1396/4345]: training_loss: tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1397/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1398/4345]: training_loss: tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1399/4345]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1400/4345]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1401/4345]: training_loss: tensor(0.0547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1402/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1403/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1404/4345]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1405/4345]: training_loss: tensor(0.2869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1406/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1407/4345]: training_loss: tensor(0.3470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1408/4345]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1409/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1410/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1411/4345]: training_loss: tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1412/4345]: training_loss: tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1413/4345]: training_loss: tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1414/4345]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1415/4345]: training_loss: tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1416/4345]: training_loss: tensor(0.2649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1417/4345]: training_loss: tensor(0.4678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1418/4345]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1419/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1420/4345]: training_loss: tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1421/4345]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1422/4345]: training_loss: tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1423/4345]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1424/4345]: training_loss: tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1425/4345]: training_loss: tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1426/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1427/4345]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1428/4345]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1429/4345]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1430/4345]: training_loss: tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1431/4345]: training_loss: tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1432/4345]: training_loss: tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1433/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1434/4345]: training_loss: tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1435/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1436/4345]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1437/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1438/4345]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1439/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1440/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1441/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1442/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1443/4345]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1444/4345]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1445/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1446/4345]: training_loss: tensor(0.1352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1447/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1448/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1449/4345]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1450/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1451/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1452/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1453/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1454/4345]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1455/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1456/4345]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1457/4345]: training_loss: tensor(0.1229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1458/4345]: training_loss: tensor(0.0507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1459/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1460/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1461/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1462/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1463/4345]: training_loss: tensor(0.1508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1464/4345]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1465/4345]: training_loss: tensor(0.3683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1466/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1467/4345]: training_loss: tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1468/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1469/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1470/4345]: training_loss: tensor(0.2205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1471/4345]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1472/4345]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1473/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1474/4345]: training_loss: tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1475/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1476/4345]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1477/4345]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1478/4345]: training_loss: tensor(0.1409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1479/4345]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1480/4345]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1481/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1482/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1483/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1484/4345]: training_loss: tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1485/4345]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1486/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1487/4345]: training_loss: tensor(0.3177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1488/4345]: training_loss: tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1489/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1490/4345]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1491/4345]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1492/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1493/4345]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1494/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1495/4345]: training_loss: tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1496/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1497/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1498/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1499/4345]: training_loss: tensor(0.3937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1500/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1501/4345]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1502/4345]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1503/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1504/4345]: training_loss: tensor(0.1107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1505/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1506/4345]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1507/4345]: training_loss: tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1508/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1509/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1510/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1511/4345]: training_loss: tensor(0.2109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1512/4345]: training_loss: tensor(0.2026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1513/4345]: training_loss: tensor(0.1778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1514/4345]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1515/4345]: training_loss: tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1516/4345]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1517/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1518/4345]: training_loss: tensor(0.0383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1519/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1520/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1521/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1522/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1523/4345]: training_loss: tensor(0.0666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1524/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1525/4345]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1526/4345]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1527/4345]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1528/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1529/4345]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1530/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1531/4345]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1532/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1533/4345]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1534/4345]: training_loss: tensor(0.1793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1535/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1536/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1537/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1538/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1539/4345]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1540/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1541/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1542/4345]: training_loss: tensor(0.1476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1543/4345]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1544/4345]: training_loss: tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1545/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1546/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1547/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1548/4345]: training_loss: tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1549/4345]: training_loss: tensor(0.0576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1550/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1551/4345]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1552/4345]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1553/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1554/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1555/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1556/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1557/4345]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1558/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1559/4345]: training_loss: tensor(0.3740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1560/4345]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1561/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1562/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1563/4345]: training_loss: tensor(0.1275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1564/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1565/4345]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1566/4345]: training_loss: tensor(0.3327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1567/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1568/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1569/4345]: training_loss: tensor(0.1851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1570/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1571/4345]: training_loss: tensor(0.1643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1572/4345]: training_loss: tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1573/4345]: training_loss: tensor(0.1735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1574/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1575/4345]: training_loss: tensor(0.2135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1576/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1577/4345]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1578/4345]: training_loss: tensor(0.0615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1579/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1580/4345]: training_loss: tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1581/4345]: training_loss: tensor(0.2324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1582/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1583/4345]: training_loss: tensor(0.1720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1584/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1585/4345]: training_loss: tensor(0.1884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1586/4345]: training_loss: tensor(0.2383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1587/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1588/4345]: training_loss: tensor(0.2206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1589/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1590/4345]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1591/4345]: training_loss: tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1592/4345]: training_loss: tensor(0.1487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1593/4345]: training_loss: tensor(0.1725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1594/4345]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1595/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1596/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1597/4345]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1598/4345]: training_loss: tensor(0.1897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1599/4345]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1600/4345]: training_loss: tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1601/4345]: training_loss: tensor(0.1693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1602/4345]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1603/4345]: training_loss: tensor(0.1636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1604/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1605/4345]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1606/4345]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1607/4345]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1608/4345]: training_loss: tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1609/4345]: training_loss: tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1610/4345]: training_loss: tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1611/4345]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1612/4345]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1613/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1614/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1615/4345]: training_loss: tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1616/4345]: training_loss: tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1617/4345]: training_loss: tensor(0.4689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1618/4345]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1619/4345]: training_loss: tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1620/4345]: training_loss: tensor(0.0501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1621/4345]: training_loss: tensor(0.1301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1622/4345]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1623/4345]: training_loss: tensor(0.2107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1624/4345]: training_loss: tensor(0.1111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1625/4345]: training_loss: tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1626/4345]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1627/4345]: training_loss: tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1628/4345]: training_loss: tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1629/4345]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1630/4345]: training_loss: tensor(0.1420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1631/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1632/4345]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1633/4345]: training_loss: tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1634/4345]: training_loss: tensor(0.1639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1635/4345]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1636/4345]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1637/4345]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1638/4345]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1639/4345]: training_loss: tensor(0.2169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1640/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1641/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1642/4345]: training_loss: tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1643/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1644/4345]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1645/4345]: training_loss: tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1646/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1647/4345]: training_loss: tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1648/4345]: training_loss: tensor(0.2257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1649/4345]: training_loss: tensor(0.1644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1650/4345]: training_loss: tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1651/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1652/4345]: training_loss: tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1653/4345]: training_loss: tensor(0.4887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1654/4345]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1655/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1656/4345]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1657/4345]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1658/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1659/4345]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1660/4345]: training_loss: tensor(0.1098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1661/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1662/4345]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1663/4345]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1664/4345]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1665/4345]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1666/4345]: training_loss: tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1667/4345]: training_loss: tensor(0.1860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1668/4345]: training_loss: tensor(0.2181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1669/4345]: training_loss: tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1670/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1671/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1672/4345]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1673/4345]: training_loss: tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1674/4345]: training_loss: tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1675/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1676/4345]: training_loss: tensor(0.3056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1677/4345]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1678/4345]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1679/4345]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1680/4345]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1681/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1682/4345]: training_loss: tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1683/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1684/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1685/4345]: training_loss: tensor(0.1723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1686/4345]: training_loss: tensor(0.2082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1687/4345]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1688/4345]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1689/4345]: training_loss: tensor(0.1167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1690/4345]: training_loss: tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1691/4345]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1692/4345]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1693/4345]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1694/4345]: training_loss: tensor(0.6164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1695/4345]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1696/4345]: training_loss: tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1697/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1698/4345]: training_loss: tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1699/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1700/4345]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1701/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1702/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1703/4345]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1704/4345]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1705/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1706/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1707/4345]: training_loss: tensor(0.1413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1708/4345]: training_loss: tensor(0.2570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1709/4345]: training_loss: tensor(0.2192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1710/4345]: training_loss: tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1711/4345]: training_loss: tensor(0.0534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1712/4345]: training_loss: tensor(0.3336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1713/4345]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1714/4345]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1715/4345]: training_loss: tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1716/4345]: training_loss: tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1717/4345]: training_loss: tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1718/4345]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1719/4345]: training_loss: tensor(0.1072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1720/4345]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1721/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1722/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1723/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1724/4345]: training_loss: tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1725/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1726/4345]: training_loss: tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1727/4345]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1728/4345]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1729/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1730/4345]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1731/4345]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1732/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1733/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1734/4345]: training_loss: tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1735/4345]: training_loss: tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1736/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1737/4345]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1738/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1739/4345]: training_loss: tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1740/4345]: training_loss: tensor(0.1238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1741/4345]: training_loss: tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1742/4345]: training_loss: tensor(0.4319, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1743/4345]: training_loss: tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1744/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1745/4345]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1746/4345]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1747/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1748/4345]: training_loss: tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1749/4345]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1750/4345]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1751/4345]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1752/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1753/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1754/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1755/4345]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1756/4345]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1757/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1758/4345]: training_loss: tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1759/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1760/4345]: training_loss: tensor(0.2419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1761/4345]: training_loss: tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1762/4345]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1763/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1764/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1765/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1766/4345]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1767/4345]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1768/4345]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1769/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1770/4345]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1771/4345]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1772/4345]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1773/4345]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1774/4345]: training_loss: tensor(0.4405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1775/4345]: training_loss: tensor(0.4731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1776/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1777/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1778/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1779/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1780/4345]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1781/4345]: training_loss: tensor(0.1210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1782/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1783/4345]: training_loss: tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1784/4345]: training_loss: tensor(0.1607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1785/4345]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1786/4345]: training_loss: tensor(0.3226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1787/4345]: training_loss: tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1788/4345]: training_loss: tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1789/4345]: training_loss: tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1790/4345]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1791/4345]: training_loss: tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1792/4345]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1793/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1794/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1795/4345]: training_loss: tensor(0.4207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1796/4345]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1797/4345]: training_loss: tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1798/4345]: training_loss: tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1799/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1800/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1801/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1802/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1803/4345]: training_loss: tensor(0.1332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1804/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1805/4345]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1806/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1807/4345]: training_loss: tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1808/4345]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1809/4345]: training_loss: tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1810/4345]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1811/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1812/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1813/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1814/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1815/4345]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1816/4345]: training_loss: tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1817/4345]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1818/4345]: training_loss: tensor(0.2085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1819/4345]: training_loss: tensor(0.2854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1820/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1821/4345]: training_loss: tensor(0.1952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1822/4345]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1823/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1824/4345]: training_loss: tensor(0.0361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1825/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1826/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1827/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1828/4345]: training_loss: tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1829/4345]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1830/4345]: training_loss: tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1831/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1832/4345]: training_loss: tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1833/4345]: training_loss: tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1834/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1835/4345]: training_loss: tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1836/4345]: training_loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1837/4345]: training_loss: tensor(0.0468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1838/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1839/4345]: training_loss: tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1840/4345]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1841/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1842/4345]: training_loss: tensor(0.1770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1843/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1844/4345]: training_loss: tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1845/4345]: training_loss: tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1846/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1847/4345]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1848/4345]: training_loss: tensor(0.4543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1849/4345]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1850/4345]: training_loss: tensor(0.1292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1851/4345]: training_loss: tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1852/4345]: training_loss: tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1853/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1854/4345]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1855/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1856/4345]: training_loss: tensor(0.2226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1857/4345]: training_loss: tensor(0.1449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1858/4345]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1859/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1860/4345]: training_loss: tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1861/4345]: training_loss: tensor(0.3263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1862/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1863/4345]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1864/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1865/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1866/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1867/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1868/4345]: training_loss: tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1869/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1870/4345]: training_loss: tensor(0.1675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1871/4345]: training_loss: tensor(0.1871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1872/4345]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1873/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1874/4345]: training_loss: tensor(0.1659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1875/4345]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1876/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1877/4345]: training_loss: tensor(0.1751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1878/4345]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1879/4345]: training_loss: tensor(0.3591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1880/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1881/4345]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1882/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1883/4345]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1884/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1885/4345]: training_loss: tensor(0.1529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1886/4345]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1887/4345]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1888/4345]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1889/4345]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1890/4345]: training_loss: tensor(0.1422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1891/4345]: training_loss: tensor(0.2990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1892/4345]: training_loss: tensor(0.1303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1893/4345]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1894/4345]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1895/4345]: training_loss: tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1896/4345]: training_loss: tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1897/4345]: training_loss: tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1898/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1899/4345]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1900/4345]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1901/4345]: training_loss: tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1902/4345]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1903/4345]: training_loss: tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1904/4345]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1905/4345]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1906/4345]: training_loss: tensor(0.1451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1907/4345]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1908/4345]: training_loss: tensor(0.0492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1909/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1910/4345]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1911/4345]: training_loss: tensor(0.1419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1912/4345]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1913/4345]: training_loss: tensor(0.1112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1914/4345]: training_loss: tensor(0.1781, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1915/4345]: training_loss: tensor(0.2228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1916/4345]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1917/4345]: training_loss: tensor(0.1393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1918/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1919/4345]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1920/4345]: training_loss: tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1921/4345]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1922/4345]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1923/4345]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1924/4345]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1925/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1926/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1927/4345]: training_loss: tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1928/4345]: training_loss: tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1929/4345]: training_loss: tensor(0.2627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1930/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1931/4345]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1932/4345]: training_loss: tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1933/4345]: training_loss: tensor(0.1044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1934/4345]: training_loss: tensor(0.1237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1935/4345]: training_loss: tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1936/4345]: training_loss: tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1937/4345]: training_loss: tensor(0.1273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1938/4345]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1939/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1940/4345]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1941/4345]: training_loss: tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1942/4345]: training_loss: tensor(0.0447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1943/4345]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1944/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1945/4345]: training_loss: tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1946/4345]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1947/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1948/4345]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1949/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1950/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1951/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1952/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1953/4345]: training_loss: tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1954/4345]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1955/4345]: training_loss: tensor(0.1636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1956/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1957/4345]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1958/4345]: training_loss: tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1959/4345]: training_loss: tensor(0.1825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1960/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1961/4345]: training_loss: tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1962/4345]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1963/4345]: training_loss: tensor(0.3686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1964/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1965/4345]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1966/4345]: training_loss: tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1967/4345]: training_loss: tensor(0.2115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1968/4345]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1969/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1970/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1971/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1972/4345]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1973/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1974/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1975/4345]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1976/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1977/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1978/4345]: training_loss: tensor(0.1285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1979/4345]: training_loss: tensor(0.2196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1980/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1981/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1982/4345]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1983/4345]: training_loss: tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1984/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1985/4345]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1986/4345]: training_loss: tensor(0.2172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1987/4345]: training_loss: tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1988/4345]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1989/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1990/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1991/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1992/4345]: training_loss: tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1993/4345]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1994/4345]: training_loss: tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1995/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1996/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1997/4345]: training_loss: tensor(0.1370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1998/4345]: training_loss: tensor(0.1429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1999/4345]: training_loss: tensor(0.1521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2000/4345]: training_loss: tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2001/4345]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2002/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2003/4345]: training_loss: tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2004/4345]: training_loss: tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2005/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2006/4345]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2007/4345]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2008/4345]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2009/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2010/4345]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2011/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2012/4345]: training_loss: tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2013/4345]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2014/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2015/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2016/4345]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2017/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2018/4345]: training_loss: tensor(0.4272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2019/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2020/4345]: training_loss: tensor(0.2476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2021/4345]: training_loss: tensor(0.4919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2022/4345]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2023/4345]: training_loss: tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2024/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2025/4345]: training_loss: tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2026/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2027/4345]: training_loss: tensor(0.1289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2028/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2029/4345]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2030/4345]: training_loss: tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2031/4345]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2032/4345]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2033/4345]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2034/4345]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2035/4345]: training_loss: tensor(0.1958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2036/4345]: training_loss: tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2037/4345]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2038/4345]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2039/4345]: training_loss: tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2040/4345]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2041/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2042/4345]: training_loss: tensor(0.1224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2043/4345]: training_loss: tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2044/4345]: training_loss: tensor(0.1231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2045/4345]: training_loss: tensor(0.1320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2046/4345]: training_loss: tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2047/4345]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2048/4345]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2049/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2050/4345]: training_loss: tensor(0.1259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2051/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2052/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2053/4345]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2054/4345]: training_loss: tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2055/4345]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2056/4345]: training_loss: tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2057/4345]: training_loss: tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2058/4345]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2059/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2060/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2061/4345]: training_loss: tensor(0.1835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2062/4345]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2063/4345]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2064/4345]: training_loss: tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2065/4345]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2066/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2067/4345]: training_loss: tensor(0.2000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2068/4345]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2069/4345]: training_loss: tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2070/4345]: training_loss: tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2071/4345]: training_loss: tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2072/4345]: training_loss: tensor(0.2177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2073/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2074/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2075/4345]: training_loss: tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2076/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2077/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2078/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2079/4345]: training_loss: tensor(0.2639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2080/4345]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2081/4345]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2082/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2083/4345]: training_loss: tensor(0.0629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2084/4345]: training_loss: tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2085/4345]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2086/4345]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2087/4345]: training_loss: tensor(0.2599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2088/4345]: training_loss: tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2089/4345]: training_loss: tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2090/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2091/4345]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2092/4345]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2093/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2094/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2095/4345]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2096/4345]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2097/4345]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2098/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2099/4345]: training_loss: tensor(0.0501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2100/4345]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2101/4345]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2102/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2103/4345]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2104/4345]: training_loss: tensor(0.1625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2105/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2106/4345]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2107/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2108/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2109/4345]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2110/4345]: training_loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2111/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2112/4345]: training_loss: tensor(0.0575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2113/4345]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2114/4345]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2115/4345]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2116/4345]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2117/4345]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2118/4345]: training_loss: tensor(0.4965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2119/4345]: training_loss: tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2120/4345]: training_loss: tensor(0.1827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2121/4345]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2122/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2123/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2124/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2125/4345]: training_loss: tensor(0.1251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2126/4345]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2127/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2128/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2129/4345]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2130/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2131/4345]: training_loss: tensor(0.1433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2132/4345]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2133/4345]: training_loss: tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2134/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2135/4345]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2136/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2137/4345]: training_loss: tensor(0.1955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2138/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2139/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2140/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2141/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2142/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2143/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2144/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2145/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2146/4345]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2147/4345]: training_loss: tensor(0.1951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2148/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2149/4345]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2150/4345]: training_loss: tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2151/4345]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2152/4345]: training_loss: tensor(0.0575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2153/4345]: training_loss: tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2154/4345]: training_loss: tensor(0.0290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2155/4345]: training_loss: tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2156/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2157/4345]: training_loss: tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2158/4345]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2159/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2160/4345]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2161/4345]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2162/4345]: training_loss: tensor(0.1199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2163/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2164/4345]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2165/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2166/4345]: training_loss: tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2167/4345]: training_loss: tensor(0.1476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2168/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2169/4345]: training_loss: tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2170/4345]: training_loss: tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2171/4345]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2172/4345]: training_loss: tensor(0.1509, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2173/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [3/5], Step [10865/21730], Train Loss: 0.0668, Valid Loss: 0.1686\n",
      "batch_no [2174/4345]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2175/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2176/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2177/4345]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2178/4345]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2179/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2180/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2181/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2182/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2183/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2184/4345]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2185/4345]: training_loss: tensor(0.1142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2186/4345]: training_loss: tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2187/4345]: training_loss: tensor(0.1388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2188/4345]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2189/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2190/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2191/4345]: training_loss: tensor(0.1996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2192/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2193/4345]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2194/4345]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2195/4345]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2196/4345]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2197/4345]: training_loss: tensor(0.1278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2198/4345]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2199/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2200/4345]: training_loss: tensor(0.1407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2201/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2202/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2203/4345]: training_loss: tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2204/4345]: training_loss: tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2205/4345]: training_loss: tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2206/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2207/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2208/4345]: training_loss: tensor(0.3395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2209/4345]: training_loss: tensor(0.1366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2210/4345]: training_loss: tensor(0.3815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2211/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2212/4345]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2213/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2214/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2215/4345]: training_loss: tensor(0.0413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2216/4345]: training_loss: tensor(0.1683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2217/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2218/4345]: training_loss: tensor(0.2902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2219/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2220/4345]: training_loss: tensor(0.2403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2221/4345]: training_loss: tensor(0.2753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2222/4345]: training_loss: tensor(0.1352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2223/4345]: training_loss: tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2224/4345]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2225/4345]: training_loss: tensor(0.2669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2226/4345]: training_loss: tensor(0.0476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2227/4345]: training_loss: tensor(0.1287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2228/4345]: training_loss: tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2229/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2230/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2231/4345]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2232/4345]: training_loss: tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2233/4345]: training_loss: tensor(0.1897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2234/4345]: training_loss: tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2235/4345]: training_loss: tensor(0.3612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2236/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2237/4345]: training_loss: tensor(0.3069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2238/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2239/4345]: training_loss: tensor(0.0445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2240/4345]: training_loss: tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2241/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2242/4345]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2243/4345]: training_loss: tensor(0.1258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2244/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2245/4345]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2246/4345]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2247/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2248/4345]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2249/4345]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2250/4345]: training_loss: tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2251/4345]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2252/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2253/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2254/4345]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2255/4345]: training_loss: tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2256/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2257/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2258/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2259/4345]: training_loss: tensor(0.1844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2260/4345]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2261/4345]: training_loss: tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2262/4345]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2263/4345]: training_loss: tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2264/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2265/4345]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2266/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2267/4345]: training_loss: tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2268/4345]: training_loss: tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2269/4345]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2270/4345]: training_loss: tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2271/4345]: training_loss: tensor(0.1716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2272/4345]: training_loss: tensor(0.2217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2273/4345]: training_loss: tensor(0.0611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2274/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2275/4345]: training_loss: tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2276/4345]: training_loss: tensor(0.1562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2277/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2278/4345]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2279/4345]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2280/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2281/4345]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2282/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2283/4345]: training_loss: tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2284/4345]: training_loss: tensor(0.1516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2285/4345]: training_loss: tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2286/4345]: training_loss: tensor(0.4247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2287/4345]: training_loss: tensor(0.1535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2288/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2289/4345]: training_loss: tensor(0.1494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2290/4345]: training_loss: tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2291/4345]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2292/4345]: training_loss: tensor(0.1663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2293/4345]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2294/4345]: training_loss: tensor(0.1641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2295/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2296/4345]: training_loss: tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2297/4345]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2298/4345]: training_loss: tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2299/4345]: training_loss: tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2300/4345]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2301/4345]: training_loss: tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2302/4345]: training_loss: tensor(0.1144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2303/4345]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2304/4345]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2305/4345]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2306/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2307/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2308/4345]: training_loss: tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2309/4345]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2310/4345]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2311/4345]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2312/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2313/4345]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2314/4345]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2315/4345]: training_loss: tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2316/4345]: training_loss: tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2317/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2318/4345]: training_loss: tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2319/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2320/4345]: training_loss: tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2321/4345]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2322/4345]: training_loss: tensor(0.1877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2323/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2324/4345]: training_loss: tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2325/4345]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2326/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2327/4345]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2328/4345]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2329/4345]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2330/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2331/4345]: training_loss: tensor(0.2280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2332/4345]: training_loss: tensor(0.1936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2333/4345]: training_loss: tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2334/4345]: training_loss: tensor(0.0570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2335/4345]: training_loss: tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2336/4345]: training_loss: tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2337/4345]: training_loss: tensor(0.1417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2338/4345]: training_loss: tensor(0.0459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2339/4345]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2340/4345]: training_loss: tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2341/4345]: training_loss: tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2342/4345]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2343/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2344/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2345/4345]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2346/4345]: training_loss: tensor(0.3120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2347/4345]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2348/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2349/4345]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2350/4345]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2351/4345]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2352/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2353/4345]: training_loss: tensor(0.1133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2354/4345]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2355/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2356/4345]: training_loss: tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2357/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2358/4345]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2359/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2360/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2361/4345]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2362/4345]: training_loss: tensor(0.1865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2363/4345]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2364/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2365/4345]: training_loss: tensor(0.0586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2366/4345]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2367/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2368/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2369/4345]: training_loss: tensor(0.1187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2370/4345]: training_loss: tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2371/4345]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2372/4345]: training_loss: tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2373/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2374/4345]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2375/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2376/4345]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2377/4345]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2378/4345]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2379/4345]: training_loss: tensor(0.3307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2380/4345]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2381/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2382/4345]: training_loss: tensor(0.1828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2383/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2384/4345]: training_loss: tensor(0.2769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2385/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2386/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2387/4345]: training_loss: tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2388/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2389/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2390/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2391/4345]: training_loss: tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2392/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2393/4345]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2394/4345]: training_loss: tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2395/4345]: training_loss: tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2396/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2397/4345]: training_loss: tensor(0.3521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2398/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2399/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2400/4345]: training_loss: tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2401/4345]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2402/4345]: training_loss: tensor(0.2182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2403/4345]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2404/4345]: training_loss: tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2405/4345]: training_loss: tensor(0.1485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2406/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2407/4345]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2408/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2409/4345]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2410/4345]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2411/4345]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2412/4345]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2413/4345]: training_loss: tensor(0.2026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2414/4345]: training_loss: tensor(0.0543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2415/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2416/4345]: training_loss: tensor(0.2060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2417/4345]: training_loss: tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2418/4345]: training_loss: tensor(0.2666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2419/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2420/4345]: training_loss: tensor(0.3106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2421/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2422/4345]: training_loss: tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2423/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2424/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2425/4345]: training_loss: tensor(0.0342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2426/4345]: training_loss: tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2427/4345]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2428/4345]: training_loss: tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2429/4345]: training_loss: tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2430/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2431/4345]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2432/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2433/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2434/4345]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2435/4345]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2436/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2437/4345]: training_loss: tensor(0.2614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2438/4345]: training_loss: tensor(0.2365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2439/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2440/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2441/4345]: training_loss: tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2442/4345]: training_loss: tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2443/4345]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2444/4345]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2445/4345]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2446/4345]: training_loss: tensor(0.0342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2447/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2448/4345]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2449/4345]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2450/4345]: training_loss: tensor(0.1815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2451/4345]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2452/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2453/4345]: training_loss: tensor(0.0439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2454/4345]: training_loss: tensor(0.1197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2455/4345]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2456/4345]: training_loss: tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2457/4345]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2458/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2459/4345]: training_loss: tensor(0.0410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2460/4345]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2461/4345]: training_loss: tensor(0.0570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2462/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2463/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2464/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2465/4345]: training_loss: tensor(0.1098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2466/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2467/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2468/4345]: training_loss: tensor(0.1274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2469/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2470/4345]: training_loss: tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2471/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2472/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2473/4345]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2474/4345]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2475/4345]: training_loss: tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2476/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2477/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2478/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2479/4345]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2480/4345]: training_loss: tensor(0.2927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2481/4345]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2482/4345]: training_loss: tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2483/4345]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2484/4345]: training_loss: tensor(0.3201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2485/4345]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2486/4345]: training_loss: tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2487/4345]: training_loss: tensor(0.1990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2488/4345]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2489/4345]: training_loss: tensor(0.2046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2490/4345]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2491/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2492/4345]: training_loss: tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2493/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2494/4345]: training_loss: tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2495/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2496/4345]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2497/4345]: training_loss: tensor(0.3869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2498/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2499/4345]: training_loss: tensor(0.2888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2500/4345]: training_loss: tensor(0.0429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2501/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2502/4345]: training_loss: tensor(0.2087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2503/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2504/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2505/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2506/4345]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2507/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2508/4345]: training_loss: tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2509/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2510/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2511/4345]: training_loss: tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2512/4345]: training_loss: tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2513/4345]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2514/4345]: training_loss: tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2515/4345]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2516/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2517/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2518/4345]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2519/4345]: training_loss: tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2520/4345]: training_loss: tensor(0.1940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2521/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2522/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2523/4345]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2524/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2525/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2526/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2527/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2528/4345]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2529/4345]: training_loss: tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2530/4345]: training_loss: tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2531/4345]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2532/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2533/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2534/4345]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2535/4345]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2536/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2537/4345]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2538/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2539/4345]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2540/4345]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2541/4345]: training_loss: tensor(0.1659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2542/4345]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2543/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2544/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2545/4345]: training_loss: tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2546/4345]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2547/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2548/4345]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2549/4345]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2550/4345]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2551/4345]: training_loss: tensor(0.5095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2552/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2553/4345]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2554/4345]: training_loss: tensor(0.1408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2555/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2556/4345]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2557/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2558/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2559/4345]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2560/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2561/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2562/4345]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2563/4345]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2564/4345]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2565/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2566/4345]: training_loss: tensor(0.2444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2567/4345]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2568/4345]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2569/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2570/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2571/4345]: training_loss: tensor(0.0570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2572/4345]: training_loss: tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2573/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2574/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2575/4345]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2576/4345]: training_loss: tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2577/4345]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2578/4345]: training_loss: tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2579/4345]: training_loss: tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2580/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2581/4345]: training_loss: tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2582/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2583/4345]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2584/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2585/4345]: training_loss: tensor(0.2970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2586/4345]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2587/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2588/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2589/4345]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2590/4345]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2591/4345]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2592/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2593/4345]: training_loss: tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2594/4345]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2595/4345]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2596/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2597/4345]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2598/4345]: training_loss: tensor(0.1718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2599/4345]: training_loss: tensor(0.0456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2600/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2601/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2602/4345]: training_loss: tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2603/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2604/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2605/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2606/4345]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2607/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2608/4345]: training_loss: tensor(0.1914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2609/4345]: training_loss: tensor(0.2159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2610/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2611/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2612/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2613/4345]: training_loss: tensor(0.1886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2614/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2615/4345]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2616/4345]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2617/4345]: training_loss: tensor(0.1240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2618/4345]: training_loss: tensor(0.1654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2619/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2620/4345]: training_loss: tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2621/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2622/4345]: training_loss: tensor(0.2360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2623/4345]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2624/4345]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2625/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2626/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2627/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2628/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2629/4345]: training_loss: tensor(0.1891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2630/4345]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2631/4345]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2632/4345]: training_loss: tensor(0.1480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2633/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2634/4345]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2635/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2636/4345]: training_loss: tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2637/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2638/4345]: training_loss: tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2639/4345]: training_loss: tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2640/4345]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2641/4345]: training_loss: tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2642/4345]: training_loss: tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2643/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2644/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2645/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2646/4345]: training_loss: tensor(0.2069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2647/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2648/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2649/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2650/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2651/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2652/4345]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2653/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2654/4345]: training_loss: tensor(0.1209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2655/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2656/4345]: training_loss: tensor(0.1588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2657/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2658/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2659/4345]: training_loss: tensor(0.2696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2660/4345]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2661/4345]: training_loss: tensor(0.1193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2662/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2663/4345]: training_loss: tensor(0.3055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2664/4345]: training_loss: tensor(0.1983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2665/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2666/4345]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2667/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2668/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2669/4345]: training_loss: tensor(0.3938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2670/4345]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2671/4345]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2672/4345]: training_loss: tensor(0.1146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2673/4345]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2674/4345]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2675/4345]: training_loss: tensor(0.1347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2676/4345]: training_loss: tensor(0.1625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2677/4345]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2678/4345]: training_loss: tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2679/4345]: training_loss: tensor(0.1159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2680/4345]: training_loss: tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2681/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2682/4345]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2683/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2684/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2685/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2686/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2687/4345]: training_loss: tensor(0.3087, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2688/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2689/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2690/4345]: training_loss: tensor(0.4029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2691/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2692/4345]: training_loss: tensor(0.1260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2693/4345]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2694/4345]: training_loss: tensor(0.0384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2695/4345]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2696/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2697/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2698/4345]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2699/4345]: training_loss: tensor(0.1491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2700/4345]: training_loss: tensor(0.2150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2701/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2702/4345]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2703/4345]: training_loss: tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2704/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2705/4345]: training_loss: tensor(0.1106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2706/4345]: training_loss: tensor(0.0475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2707/4345]: training_loss: tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2708/4345]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2709/4345]: training_loss: tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2710/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2711/4345]: training_loss: tensor(0.1600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2712/4345]: training_loss: tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2713/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2714/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2715/4345]: training_loss: tensor(0.1145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2716/4345]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2717/4345]: training_loss: tensor(0.1809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2718/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2719/4345]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2720/4345]: training_loss: tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2721/4345]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2722/4345]: training_loss: tensor(0.1107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2723/4345]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2724/4345]: training_loss: tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2725/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2726/4345]: training_loss: tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2727/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2728/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2729/4345]: training_loss: tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2730/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2731/4345]: training_loss: tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2732/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2733/4345]: training_loss: tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2734/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2735/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2736/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2737/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2738/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2739/4345]: training_loss: tensor(0.3848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2740/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2741/4345]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2742/4345]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2743/4345]: training_loss: tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2744/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2745/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2746/4345]: training_loss: tensor(0.7811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2747/4345]: training_loss: tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2748/4345]: training_loss: tensor(0.4976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2749/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2750/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2751/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2752/4345]: training_loss: tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2753/4345]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2754/4345]: training_loss: tensor(0.0582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2755/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2756/4345]: training_loss: tensor(0.1338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2757/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2758/4345]: training_loss: tensor(0.2815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2759/4345]: training_loss: tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2760/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2761/4345]: training_loss: tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2762/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2763/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2764/4345]: training_loss: tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2765/4345]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2766/4345]: training_loss: tensor(0.1618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2767/4345]: training_loss: tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2768/4345]: training_loss: tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2769/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2770/4345]: training_loss: tensor(0.4413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2771/4345]: training_loss: tensor(0.0404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2772/4345]: training_loss: tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2773/4345]: training_loss: tensor(0.1320, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2774/4345]: training_loss: tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2775/4345]: training_loss: tensor(0.2248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2776/4345]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2777/4345]: training_loss: tensor(0.1466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2778/4345]: training_loss: tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2779/4345]: training_loss: tensor(0.1440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2780/4345]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2781/4345]: training_loss: tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2782/4345]: training_loss: tensor(0.2465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2783/4345]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2784/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2785/4345]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2786/4345]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2787/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2788/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2789/4345]: training_loss: tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2790/4345]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2791/4345]: training_loss: tensor(0.0365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2792/4345]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2793/4345]: training_loss: tensor(0.2014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2794/4345]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2795/4345]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2796/4345]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2797/4345]: training_loss: tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2798/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2799/4345]: training_loss: tensor(0.2429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2800/4345]: training_loss: tensor(0.2256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2801/4345]: training_loss: tensor(0.2214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2802/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2803/4345]: training_loss: tensor(0.1383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2804/4345]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2805/4345]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2806/4345]: training_loss: tensor(0.2159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2807/4345]: training_loss: tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2808/4345]: training_loss: tensor(0.1498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2809/4345]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2810/4345]: training_loss: tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2811/4345]: training_loss: tensor(0.0534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2812/4345]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2813/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2814/4345]: training_loss: tensor(0.1177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2815/4345]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2816/4345]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2817/4345]: training_loss: tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2818/4345]: training_loss: tensor(0.1217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2819/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2820/4345]: training_loss: tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2821/4345]: training_loss: tensor(0.1519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2822/4345]: training_loss: tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2823/4345]: training_loss: tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2824/4345]: training_loss: tensor(0.1931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2825/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2826/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2827/4345]: training_loss: tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2828/4345]: training_loss: tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2829/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2830/4345]: training_loss: tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2831/4345]: training_loss: tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2832/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2833/4345]: training_loss: tensor(0.1535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2834/4345]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2835/4345]: training_loss: tensor(0.0354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2836/4345]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2837/4345]: training_loss: tensor(0.2688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2838/4345]: training_loss: tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2839/4345]: training_loss: tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2840/4345]: training_loss: tensor(0.1904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2841/4345]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2842/4345]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2843/4345]: training_loss: tensor(0.1177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2844/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2845/4345]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2846/4345]: training_loss: tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2847/4345]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2848/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2849/4345]: training_loss: tensor(0.1417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2850/4345]: training_loss: tensor(0.1367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2851/4345]: training_loss: tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2852/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2853/4345]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2854/4345]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2855/4345]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2856/4345]: training_loss: tensor(0.1470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2857/4345]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2858/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2859/4345]: training_loss: tensor(0.1287, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2860/4345]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2861/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2862/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2863/4345]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2864/4345]: training_loss: tensor(0.2158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2865/4345]: training_loss: tensor(0.1421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2866/4345]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2867/4345]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2868/4345]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2869/4345]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2870/4345]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2871/4345]: training_loss: tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2872/4345]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2873/4345]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2874/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2875/4345]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2876/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2877/4345]: training_loss: tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2878/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2879/4345]: training_loss: tensor(0.1474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2880/4345]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2881/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2882/4345]: training_loss: tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2883/4345]: training_loss: tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2884/4345]: training_loss: tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2885/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2886/4345]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2887/4345]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2888/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2889/4345]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2890/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2891/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2892/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2893/4345]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2894/4345]: training_loss: tensor(0.2010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2895/4345]: training_loss: tensor(0.3631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2896/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2897/4345]: training_loss: tensor(0.0587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2898/4345]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2899/4345]: training_loss: tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2900/4345]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2901/4345]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2902/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2903/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2904/4345]: training_loss: tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2905/4345]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2906/4345]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2907/4345]: training_loss: tensor(0.2427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2908/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2909/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2910/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2911/4345]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2912/4345]: training_loss: tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2913/4345]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2914/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2915/4345]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2916/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2917/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2918/4345]: training_loss: tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2919/4345]: training_loss: tensor(0.1112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2920/4345]: training_loss: tensor(0.0579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2921/4345]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2922/4345]: training_loss: tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2923/4345]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2924/4345]: training_loss: tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2925/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2926/4345]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2927/4345]: training_loss: tensor(0.2017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2928/4345]: training_loss: tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2929/4345]: training_loss: tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2930/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2931/4345]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2932/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2933/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2934/4345]: training_loss: tensor(0.0445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2935/4345]: training_loss: tensor(0.1196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2936/4345]: training_loss: tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2937/4345]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2938/4345]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2939/4345]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2940/4345]: training_loss: tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2941/4345]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2942/4345]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2943/4345]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2944/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2945/4345]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2946/4345]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2947/4345]: training_loss: tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2948/4345]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2949/4345]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2950/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2951/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2952/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2953/4345]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2954/4345]: training_loss: tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2955/4345]: training_loss: tensor(0.0645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2956/4345]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2957/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2958/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2959/4345]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2960/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2961/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2962/4345]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2963/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2964/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2965/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2966/4345]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2967/4345]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2968/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2969/4345]: training_loss: tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2970/4345]: training_loss: tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2971/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2972/4345]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2973/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2974/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2975/4345]: training_loss: tensor(0.5028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2976/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2977/4345]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2978/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2979/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2980/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2981/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2982/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2983/4345]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2984/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2985/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2986/4345]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2987/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2988/4345]: training_loss: tensor(0.1672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2989/4345]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2990/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2991/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2992/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2993/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2994/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2995/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2996/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2997/4345]: training_loss: tensor(0.1251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2998/4345]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2999/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3000/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3001/4345]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3002/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3003/4345]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3004/4345]: training_loss: tensor(0.1420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3005/4345]: training_loss: tensor(0.1890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3006/4345]: training_loss: tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3007/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3008/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3009/4345]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3010/4345]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3011/4345]: training_loss: tensor(0.2238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3012/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3013/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3014/4345]: training_loss: tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3015/4345]: training_loss: tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3016/4345]: training_loss: tensor(0.3296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3017/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3018/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3019/4345]: training_loss: tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3020/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3021/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3022/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3023/4345]: training_loss: tensor(0.1913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3024/4345]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3025/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3026/4345]: training_loss: tensor(0.4056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3027/4345]: training_loss: tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3028/4345]: training_loss: tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3029/4345]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3030/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3031/4345]: training_loss: tensor(0.0541, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3032/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3033/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3034/4345]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3035/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3036/4345]: training_loss: tensor(0.1852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3037/4345]: training_loss: tensor(0.1640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3038/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3039/4345]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3040/4345]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3041/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3042/4345]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3043/4345]: training_loss: tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3044/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3045/4345]: training_loss: tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3046/4345]: training_loss: tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3047/4345]: training_loss: tensor(0.1519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3048/4345]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3049/4345]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3050/4345]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3051/4345]: training_loss: tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3052/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3053/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3054/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3055/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3056/4345]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3057/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3058/4345]: training_loss: tensor(0.4750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3059/4345]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3060/4345]: training_loss: tensor(0.1774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3061/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3062/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3063/4345]: training_loss: tensor(0.2879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3064/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3065/4345]: training_loss: tensor(0.2957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3066/4345]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3067/4345]: training_loss: tensor(0.1263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3068/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3069/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3070/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3071/4345]: training_loss: tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3072/4345]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3073/4345]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3074/4345]: training_loss: tensor(0.2576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3075/4345]: training_loss: tensor(0.1306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3076/4345]: training_loss: tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3077/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3078/4345]: training_loss: tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3079/4345]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3080/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3081/4345]: training_loss: tensor(0.1391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3082/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3083/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3084/4345]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3085/4345]: training_loss: tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3086/4345]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3087/4345]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3088/4345]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3089/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3090/4345]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3091/4345]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3092/4345]: training_loss: tensor(0.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3093/4345]: training_loss: tensor(0.1356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3094/4345]: training_loss: tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3095/4345]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3096/4345]: training_loss: tensor(0.1570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3097/4345]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3098/4345]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3099/4345]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3100/4345]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3101/4345]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3102/4345]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3103/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3104/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3105/4345]: training_loss: tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3106/4345]: training_loss: tensor(0.1234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3107/4345]: training_loss: tensor(0.3448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3108/4345]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3109/4345]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3110/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3111/4345]: training_loss: tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3112/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3113/4345]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3114/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3115/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3116/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3117/4345]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3118/4345]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3119/4345]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3120/4345]: training_loss: tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3121/4345]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3122/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3123/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3124/4345]: training_loss: tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3125/4345]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3126/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3127/4345]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3128/4345]: training_loss: tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3129/4345]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3130/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3131/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3132/4345]: training_loss: tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3133/4345]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3134/4345]: training_loss: tensor(0.1790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3135/4345]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3136/4345]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3137/4345]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3138/4345]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3139/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3140/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3141/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3142/4345]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3143/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3144/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3145/4345]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3146/4345]: training_loss: tensor(0.1956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3147/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3148/4345]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3149/4345]: training_loss: tensor(0.1678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3150/4345]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3151/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3152/4345]: training_loss: tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3153/4345]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3154/4345]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3155/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3156/4345]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3157/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3158/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3159/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3160/4345]: training_loss: tensor(0.3233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3161/4345]: training_loss: tensor(0.2317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3162/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3163/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3164/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3165/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3166/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3167/4345]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3168/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3169/4345]: training_loss: tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3170/4345]: training_loss: tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3171/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3172/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3173/4345]: training_loss: tensor(0.1241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3174/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3175/4345]: training_loss: tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3176/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3177/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3178/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3179/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3180/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3181/4345]: training_loss: tensor(0.2756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3182/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3183/4345]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3184/4345]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3185/4345]: training_loss: tensor(0.1642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3186/4345]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3187/4345]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3188/4345]: training_loss: tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3189/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3190/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3191/4345]: training_loss: tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3192/4345]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3193/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3194/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3195/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3196/4345]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3197/4345]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3198/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3199/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3200/4345]: training_loss: tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3201/4345]: training_loss: tensor(0.1446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3202/4345]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3203/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3204/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3205/4345]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3206/4345]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3207/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3208/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3209/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3210/4345]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3211/4345]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3212/4345]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3213/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3214/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3215/4345]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3216/4345]: training_loss: tensor(0.1206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3217/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3218/4345]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3219/4345]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3220/4345]: training_loss: tensor(0.1423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3221/4345]: training_loss: tensor(0.4746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3222/4345]: training_loss: tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3223/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3224/4345]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3225/4345]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3226/4345]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3227/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3228/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3229/4345]: training_loss: tensor(0.0439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3230/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3231/4345]: training_loss: tensor(0.1728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3232/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3233/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3234/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3235/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3236/4345]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3237/4345]: training_loss: tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3238/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3239/4345]: training_loss: tensor(0.4374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3240/4345]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3241/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3242/4345]: training_loss: tensor(0.0683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3243/4345]: training_loss: tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3244/4345]: training_loss: tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3245/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3246/4345]: training_loss: tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3247/4345]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3248/4345]: training_loss: tensor(0.3487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3249/4345]: training_loss: tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3250/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3251/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3252/4345]: training_loss: tensor(0.4092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3253/4345]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3254/4345]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3255/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3256/4345]: training_loss: tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3257/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3258/4345]: training_loss: tensor(0.1778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3259/4345]: training_loss: tensor(0.1663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3260/4345]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3261/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3262/4345]: training_loss: tensor(0.2703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3263/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3264/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3265/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3266/4345]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3267/4345]: training_loss: tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3268/4345]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3269/4345]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3270/4345]: training_loss: tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3271/4345]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3272/4345]: training_loss: tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3273/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3274/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3275/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3276/4345]: training_loss: tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3277/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3278/4345]: training_loss: tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3279/4345]: training_loss: tensor(0.1690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3280/4345]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3281/4345]: training_loss: tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3282/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3283/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3284/4345]: training_loss: tensor(0.1923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3285/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3286/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3287/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3288/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3289/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3290/4345]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3291/4345]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3292/4345]: training_loss: tensor(0.2494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3293/4345]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3294/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3295/4345]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3296/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3297/4345]: training_loss: tensor(0.1494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3298/4345]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3299/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3300/4345]: training_loss: tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3301/4345]: training_loss: tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3302/4345]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3303/4345]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3304/4345]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3305/4345]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3306/4345]: training_loss: tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3307/4345]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3308/4345]: training_loss: tensor(0.2263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3309/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3310/4345]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3311/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3312/4345]: training_loss: tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3313/4345]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3314/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3315/4345]: training_loss: tensor(0.1101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3316/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3317/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3318/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3319/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3320/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3321/4345]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3322/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3323/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3324/4345]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3325/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3326/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3327/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3328/4345]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3329/4345]: training_loss: tensor(0.2056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3330/4345]: training_loss: tensor(0.1414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3331/4345]: training_loss: tensor(0.2362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3332/4345]: training_loss: tensor(0.5318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3333/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3334/4345]: training_loss: tensor(0.1974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3335/4345]: training_loss: tensor(0.0354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3336/4345]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3337/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3338/4345]: training_loss: tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3339/4345]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3340/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3341/4345]: training_loss: tensor(0.2651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3342/4345]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3343/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3344/4345]: training_loss: tensor(0.1504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3345/4345]: training_loss: tensor(0.1848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3346/4345]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3347/4345]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3348/4345]: training_loss: tensor(0.3196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3349/4345]: training_loss: tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3350/4345]: training_loss: tensor(0.2596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3351/4345]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3352/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3353/4345]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3354/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3355/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3356/4345]: training_loss: tensor(0.1730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3357/4345]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3358/4345]: training_loss: tensor(0.1301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3359/4345]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3360/4345]: training_loss: tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3361/4345]: training_loss: tensor(0.0501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3362/4345]: training_loss: tensor(0.1262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3363/4345]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3364/4345]: training_loss: tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3365/4345]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3366/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3367/4345]: training_loss: tensor(0.0545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3368/4345]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3369/4345]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3370/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3371/4345]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3372/4345]: training_loss: tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3373/4345]: training_loss: tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3374/4345]: training_loss: tensor(0.2024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3375/4345]: training_loss: tensor(0.2059, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3376/4345]: training_loss: tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3377/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3378/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3379/4345]: training_loss: tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3380/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3381/4345]: training_loss: tensor(0.1225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3382/4345]: training_loss: tensor(0.1443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3383/4345]: training_loss: tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3384/4345]: training_loss: tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3385/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3386/4345]: training_loss: tensor(0.0501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3387/4345]: training_loss: tensor(0.1860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3388/4345]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3389/4345]: training_loss: tensor(0.1165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3390/4345]: training_loss: tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3391/4345]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3392/4345]: training_loss: tensor(0.1383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3393/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3394/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3395/4345]: training_loss: tensor(0.2610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3396/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3397/4345]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3398/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3399/4345]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3400/4345]: training_loss: tensor(0.1852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3401/4345]: training_loss: tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3402/4345]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3403/4345]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3404/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3405/4345]: training_loss: tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3406/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3407/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3408/4345]: training_loss: tensor(0.2189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3409/4345]: training_loss: tensor(0.3236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3410/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3411/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3412/4345]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3413/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3414/4345]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3415/4345]: training_loss: tensor(0.1857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3416/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3417/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3418/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3419/4345]: training_loss: tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3420/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3421/4345]: training_loss: tensor(0.2167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3422/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3423/4345]: training_loss: tensor(0.1368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3424/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3425/4345]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3426/4345]: training_loss: tensor(0.1149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3427/4345]: training_loss: tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3428/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3429/4345]: training_loss: tensor(0.1213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3430/4345]: training_loss: tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3431/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3432/4345]: training_loss: tensor(0.2772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3433/4345]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3434/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3435/4345]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3436/4345]: training_loss: tensor(0.2251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3437/4345]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3438/4345]: training_loss: tensor(0.1293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3439/4345]: training_loss: tensor(0.1683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3440/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3441/4345]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3442/4345]: training_loss: tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3443/4345]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3444/4345]: training_loss: tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3445/4345]: training_loss: tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3446/4345]: training_loss: tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3447/4345]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3448/4345]: training_loss: tensor(0.1837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3449/4345]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3450/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3451/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3452/4345]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3453/4345]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3454/4345]: training_loss: tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3455/4345]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3456/4345]: training_loss: tensor(0.1265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3457/4345]: training_loss: tensor(0.0475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3458/4345]: training_loss: tensor(0.0364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3459/4345]: training_loss: tensor(0.4671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3460/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3461/4345]: training_loss: tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3462/4345]: training_loss: tensor(0.2162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3463/4345]: training_loss: tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3464/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3465/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3466/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3467/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3468/4345]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3469/4345]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3470/4345]: training_loss: tensor(0.1290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3471/4345]: training_loss: tensor(0.2433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3472/4345]: training_loss: tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3473/4345]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3474/4345]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3475/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3476/4345]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3477/4345]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3478/4345]: training_loss: tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3479/4345]: training_loss: tensor(0.1407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3480/4345]: training_loss: tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3481/4345]: training_loss: tensor(0.1728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3482/4345]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3483/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3484/4345]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3485/4345]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3486/4345]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3487/4345]: training_loss: tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3488/4345]: training_loss: tensor(0.1474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3489/4345]: training_loss: tensor(0.1450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3490/4345]: training_loss: tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3491/4345]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3492/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3493/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3494/4345]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3495/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3496/4345]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3497/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3498/4345]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3499/4345]: training_loss: tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3500/4345]: training_loss: tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3501/4345]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3502/4345]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3503/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3504/4345]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3505/4345]: training_loss: tensor(0.1536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3506/4345]: training_loss: tensor(0.1635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3507/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3508/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3509/4345]: training_loss: tensor(0.1083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3510/4345]: training_loss: tensor(0.1874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3511/4345]: training_loss: tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3512/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3513/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3514/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3515/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3516/4345]: training_loss: tensor(0.2416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3517/4345]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3518/4345]: training_loss: tensor(0.1257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3519/4345]: training_loss: tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3520/4345]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3521/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3522/4345]: training_loss: tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3523/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3524/4345]: training_loss: tensor(0.2994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3525/4345]: training_loss: tensor(0.1243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3526/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3527/4345]: training_loss: tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3528/4345]: training_loss: tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3529/4345]: training_loss: tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3530/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3531/4345]: training_loss: tensor(0.0429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3532/4345]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3533/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3534/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3535/4345]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3536/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3537/4345]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3538/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3539/4345]: training_loss: tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3540/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3541/4345]: training_loss: tensor(0.2846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3542/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3543/4345]: training_loss: tensor(0.2140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3544/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3545/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3546/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3547/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3548/4345]: training_loss: tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3549/4345]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3550/4345]: training_loss: tensor(0.2317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3551/4345]: training_loss: tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3552/4345]: training_loss: tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3553/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3554/4345]: training_loss: tensor(0.2566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3555/4345]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3556/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3557/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3558/4345]: training_loss: tensor(0.1826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3559/4345]: training_loss: tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3560/4345]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3561/4345]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3562/4345]: training_loss: tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3563/4345]: training_loss: tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3564/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3565/4345]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3566/4345]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3567/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3568/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3569/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3570/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3571/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3572/4345]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3573/4345]: training_loss: tensor(0.1421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3574/4345]: training_loss: tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3575/4345]: training_loss: tensor(0.1787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3576/4345]: training_loss: tensor(0.3774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3577/4345]: training_loss: tensor(0.1278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3578/4345]: training_loss: tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3579/4345]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3580/4345]: training_loss: tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3581/4345]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3582/4345]: training_loss: tensor(0.1134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3583/4345]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3584/4345]: training_loss: tensor(0.1587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3585/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3586/4345]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3587/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3588/4345]: training_loss: tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3589/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3590/4345]: training_loss: tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3591/4345]: training_loss: tensor(0.1829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3592/4345]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3593/4345]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3594/4345]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3595/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3596/4345]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3597/4345]: training_loss: tensor(0.1611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3598/4345]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3599/4345]: training_loss: tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3600/4345]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3601/4345]: training_loss: tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3602/4345]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3603/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3604/4345]: training_loss: tensor(0.0413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3605/4345]: training_loss: tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3606/4345]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3607/4345]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3608/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3609/4345]: training_loss: tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3610/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3611/4345]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3612/4345]: training_loss: tensor(0.3223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3613/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3614/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3615/4345]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3616/4345]: training_loss: tensor(0.1098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3617/4345]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3618/4345]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3619/4345]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3620/4345]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3621/4345]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3622/4345]: training_loss: tensor(0.1566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3623/4345]: training_loss: tensor(0.3764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3624/4345]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3625/4345]: training_loss: tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3626/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3627/4345]: training_loss: tensor(0.2485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3628/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3629/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3630/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3631/4345]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3632/4345]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3633/4345]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3634/4345]: training_loss: tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3635/4345]: training_loss: tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3636/4345]: training_loss: tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3637/4345]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3638/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3639/4345]: training_loss: tensor(0.2490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3640/4345]: training_loss: tensor(0.1266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3641/4345]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3642/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3643/4345]: training_loss: tensor(0.0342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3644/4345]: training_loss: tensor(0.2082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3645/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3646/4345]: training_loss: tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3647/4345]: training_loss: tensor(0.1962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3648/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3649/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3650/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3651/4345]: training_loss: tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3652/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3653/4345]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3654/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3655/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3656/4345]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3657/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3658/4345]: training_loss: tensor(0.1202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3659/4345]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3660/4345]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3661/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3662/4345]: training_loss: tensor(0.2603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3663/4345]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3664/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3665/4345]: training_loss: tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3666/4345]: training_loss: tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3667/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3668/4345]: training_loss: tensor(0.1232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3669/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3670/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3671/4345]: training_loss: tensor(0.2565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3672/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3673/4345]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3674/4345]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3675/4345]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3676/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3677/4345]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3678/4345]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3679/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3680/4345]: training_loss: tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3681/4345]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3682/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3683/4345]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3684/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3685/4345]: training_loss: tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3686/4345]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3687/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3688/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3689/4345]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3690/4345]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3691/4345]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3692/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3693/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3694/4345]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3695/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3696/4345]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3697/4345]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3698/4345]: training_loss: tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3699/4345]: training_loss: tensor(0.0364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3700/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3701/4345]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3702/4345]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3703/4345]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3704/4345]: training_loss: tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3705/4345]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3706/4345]: training_loss: tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3707/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3708/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3709/4345]: training_loss: tensor(0.2807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3710/4345]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3711/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3712/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3713/4345]: training_loss: tensor(0.1296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3714/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3715/4345]: training_loss: tensor(0.2061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3716/4345]: training_loss: tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3717/4345]: training_loss: tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3718/4345]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3719/4345]: training_loss: tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3720/4345]: training_loss: tensor(0.1145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3721/4345]: training_loss: tensor(0.4687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3722/4345]: training_loss: tensor(0.2658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3723/4345]: training_loss: tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3724/4345]: training_loss: tensor(0.0342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3725/4345]: training_loss: tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3726/4345]: training_loss: tensor(0.2694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3727/4345]: training_loss: tensor(0.1586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3728/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3729/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3730/4345]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3731/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3732/4345]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3733/4345]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3734/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3735/4345]: training_loss: tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3736/4345]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3737/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3738/4345]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3739/4345]: training_loss: tensor(0.1587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3740/4345]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3741/4345]: training_loss: tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3742/4345]: training_loss: tensor(0.1557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3743/4345]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3744/4345]: training_loss: tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3745/4345]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3746/4345]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3747/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3748/4345]: training_loss: tensor(0.1402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3749/4345]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3750/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3751/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3752/4345]: training_loss: tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3753/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3754/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3755/4345]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3756/4345]: training_loss: tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3757/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3758/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3759/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3760/4345]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3761/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3762/4345]: training_loss: tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3763/4345]: training_loss: tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3764/4345]: training_loss: tensor(0.2161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3765/4345]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3766/4345]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3767/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3768/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3769/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3770/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3771/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3772/4345]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3773/4345]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3774/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3775/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3776/4345]: training_loss: tensor(0.0362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3777/4345]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3778/4345]: training_loss: tensor(0.2279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3779/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3780/4345]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3781/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3782/4345]: training_loss: tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3783/4345]: training_loss: tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3784/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3785/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3786/4345]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3787/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3788/4345]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3789/4345]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3790/4345]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3791/4345]: training_loss: tensor(0.1206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3792/4345]: training_loss: tensor(0.2275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3793/4345]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3794/4345]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3795/4345]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3796/4345]: training_loss: tensor(0.5086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3797/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3798/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3799/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3800/4345]: training_loss: tensor(0.3462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3801/4345]: training_loss: tensor(0.3589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3802/4345]: training_loss: tensor(0.1703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3803/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3804/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3805/4345]: training_loss: tensor(0.4887, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3806/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3807/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3808/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3809/4345]: training_loss: tensor(0.1766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3810/4345]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3811/4345]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3812/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3813/4345]: training_loss: tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3814/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3815/4345]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3816/4345]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3817/4345]: training_loss: tensor(0.1194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3818/4345]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3819/4345]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3820/4345]: training_loss: tensor(0.1562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3821/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3822/4345]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3823/4345]: training_loss: tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3824/4345]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3825/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3826/4345]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3827/4345]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3828/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3829/4345]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3830/4345]: training_loss: tensor(0.1418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3831/4345]: training_loss: tensor(0.1183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3832/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3833/4345]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3834/4345]: training_loss: tensor(0.0313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3835/4345]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3836/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3837/4345]: training_loss: tensor(0.2752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3838/4345]: training_loss: tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3839/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3840/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3841/4345]: training_loss: tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3842/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3843/4345]: training_loss: tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3844/4345]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3845/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3846/4345]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3847/4345]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3848/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3849/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3850/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3851/4345]: training_loss: tensor(0.5077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3852/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3853/4345]: training_loss: tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3854/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3855/4345]: training_loss: tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3856/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3857/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3858/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3859/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3860/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3861/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3862/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3863/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3864/4345]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3865/4345]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3866/4345]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3867/4345]: training_loss: tensor(0.2957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3868/4345]: training_loss: tensor(0.1816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3869/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3870/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3871/4345]: training_loss: tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3872/4345]: training_loss: tensor(0.1167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3873/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3874/4345]: training_loss: tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3875/4345]: training_loss: tensor(0.1150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3876/4345]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3877/4345]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3878/4345]: training_loss: tensor(0.1639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3879/4345]: training_loss: tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3880/4345]: training_loss: tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3881/4345]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3882/4345]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3883/4345]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3884/4345]: training_loss: tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3885/4345]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3886/4345]: training_loss: tensor(0.1194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3887/4345]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3888/4345]: training_loss: tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3889/4345]: training_loss: tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3890/4345]: training_loss: tensor(0.1761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3891/4345]: training_loss: tensor(0.0427, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3892/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3893/4345]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3894/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3895/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3896/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3897/4345]: training_loss: tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3898/4345]: training_loss: tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3899/4345]: training_loss: tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3900/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3901/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3902/4345]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3903/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3904/4345]: training_loss: tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3905/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3906/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3907/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3908/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3909/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3910/4345]: training_loss: tensor(0.1899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3911/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3912/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3913/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3914/4345]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3915/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3916/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3917/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3918/4345]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3919/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3920/4345]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3921/4345]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3922/4345]: training_loss: tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3923/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3924/4345]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3925/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3926/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3927/4345]: training_loss: tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3928/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3929/4345]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3930/4345]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3931/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3932/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3933/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3934/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3935/4345]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3936/4345]: training_loss: tensor(0.0305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3937/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3938/4345]: training_loss: tensor(0.5574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3939/4345]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3940/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3941/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3942/4345]: training_loss: tensor(0.4203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3943/4345]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3944/4345]: training_loss: tensor(0.2197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3945/4345]: training_loss: tensor(0.1303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3946/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3947/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3948/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3949/4345]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3950/4345]: training_loss: tensor(0.1910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3951/4345]: training_loss: tensor(0.1636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3952/4345]: training_loss: tensor(0.4121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3953/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3954/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3955/4345]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3956/4345]: training_loss: tensor(0.2058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3957/4345]: training_loss: tensor(0.2061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3958/4345]: training_loss: tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3959/4345]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3960/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3961/4345]: training_loss: tensor(0.2977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3962/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3963/4345]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3964/4345]: training_loss: tensor(0.1414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3965/4345]: training_loss: tensor(0.1169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3966/4345]: training_loss: tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3967/4345]: training_loss: tensor(0.1947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3968/4345]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3969/4345]: training_loss: tensor(0.1799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3970/4345]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3971/4345]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3972/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3973/4345]: training_loss: tensor(0.2258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3974/4345]: training_loss: tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3975/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3976/4345]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3977/4345]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3978/4345]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3979/4345]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3980/4345]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3981/4345]: training_loss: tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3982/4345]: training_loss: tensor(0.1186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3983/4345]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3984/4345]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3985/4345]: training_loss: tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3986/4345]: training_loss: tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3987/4345]: training_loss: tensor(0.1458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3988/4345]: training_loss: tensor(0.3126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3989/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3990/4345]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3991/4345]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3992/4345]: training_loss: tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3993/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3994/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3995/4345]: training_loss: tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3996/4345]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3997/4345]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3998/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3999/4345]: training_loss: tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4000/4345]: training_loss: tensor(0.1634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4001/4345]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4002/4345]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4003/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4004/4345]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4005/4345]: training_loss: tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4006/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4007/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4008/4345]: training_loss: tensor(0.1229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4009/4345]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4010/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4011/4345]: training_loss: tensor(0.1064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4012/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4013/4345]: training_loss: tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4014/4345]: training_loss: tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4015/4345]: training_loss: tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4016/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4017/4345]: training_loss: tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4018/4345]: training_loss: tensor(0.3060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4019/4345]: training_loss: tensor(0.1206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4020/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4021/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4022/4345]: training_loss: tensor(0.3481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4023/4345]: training_loss: tensor(0.3398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4024/4345]: training_loss: tensor(0.1928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4025/4345]: training_loss: tensor(0.1202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4026/4345]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4027/4345]: training_loss: tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4028/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4029/4345]: training_loss: tensor(0.2489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4030/4345]: training_loss: tensor(0.2617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4031/4345]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4032/4345]: training_loss: tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4033/4345]: training_loss: tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4034/4345]: training_loss: tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4035/4345]: training_loss: tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4036/4345]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4037/4345]: training_loss: tensor(0.1152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4038/4345]: training_loss: tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4039/4345]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4040/4345]: training_loss: tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4041/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4042/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4043/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4044/4345]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4045/4345]: training_loss: tensor(0.2437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4046/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4047/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4048/4345]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4049/4345]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4050/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4051/4345]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4052/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4053/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4054/4345]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4055/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4056/4345]: training_loss: tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4057/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4058/4345]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4059/4345]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4060/4345]: training_loss: tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4061/4345]: training_loss: tensor(0.1297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4062/4345]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4063/4345]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4064/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4065/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4066/4345]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4067/4345]: training_loss: tensor(0.1099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4068/4345]: training_loss: tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4069/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4070/4345]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4071/4345]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4072/4345]: training_loss: tensor(0.3318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4073/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4074/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4075/4345]: training_loss: tensor(0.1118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4076/4345]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4077/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4078/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4079/4345]: training_loss: tensor(0.1656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4080/4345]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4081/4345]: training_loss: tensor(0.0313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4082/4345]: training_loss: tensor(0.1933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4083/4345]: training_loss: tensor(0.1651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4084/4345]: training_loss: tensor(0.0495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4085/4345]: training_loss: tensor(0.1614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4086/4345]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4087/4345]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4088/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4089/4345]: training_loss: tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4090/4345]: training_loss: tensor(0.1765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4091/4345]: training_loss: tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4092/4345]: training_loss: tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4093/4345]: training_loss: tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4094/4345]: training_loss: tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4095/4345]: training_loss: tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4096/4345]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4097/4345]: training_loss: tensor(0.1378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4098/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4099/4345]: training_loss: tensor(0.0541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4100/4345]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4101/4345]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4102/4345]: training_loss: tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4103/4345]: training_loss: tensor(0.1640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4104/4345]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4105/4345]: training_loss: tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4106/4345]: training_loss: tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4107/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4108/4345]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4109/4345]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4110/4345]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4111/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4112/4345]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4113/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4114/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4115/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4116/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4117/4345]: training_loss: tensor(0.1689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4118/4345]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4119/4345]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4120/4345]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4121/4345]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4122/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4123/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4124/4345]: training_loss: tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4125/4345]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4126/4345]: training_loss: tensor(0.2019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4127/4345]: training_loss: tensor(0.1329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4128/4345]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4129/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4130/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4131/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4132/4345]: training_loss: tensor(0.1369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4133/4345]: training_loss: tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4134/4345]: training_loss: tensor(0.2984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4135/4345]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4136/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4137/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4138/4345]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4139/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4140/4345]: training_loss: tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4141/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4142/4345]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4143/4345]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4144/4345]: training_loss: tensor(0.1714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4145/4345]: training_loss: tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4146/4345]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4147/4345]: training_loss: tensor(0.4321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4148/4345]: training_loss: tensor(0.3488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4149/4345]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4150/4345]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4151/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4152/4345]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4153/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4154/4345]: training_loss: tensor(0.2329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4155/4345]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4156/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4157/4345]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4158/4345]: training_loss: tensor(0.1405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4159/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4160/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4161/4345]: training_loss: tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4162/4345]: training_loss: tensor(0.1352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4163/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4164/4345]: training_loss: tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4165/4345]: training_loss: tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4166/4345]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4167/4345]: training_loss: tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4168/4345]: training_loss: tensor(0.2819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4169/4345]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4170/4345]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4171/4345]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4172/4345]: training_loss: tensor(0.3414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4173/4345]: training_loss: tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4174/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4175/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4176/4345]: training_loss: tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4177/4345]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4178/4345]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4179/4345]: training_loss: tensor(0.1296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4180/4345]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4181/4345]: training_loss: tensor(0.0442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4182/4345]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4183/4345]: training_loss: tensor(0.1358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4184/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4185/4345]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4186/4345]: training_loss: tensor(0.4046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4187/4345]: training_loss: tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4188/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4189/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4190/4345]: training_loss: tensor(0.1766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4191/4345]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4192/4345]: training_loss: tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4193/4345]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4194/4345]: training_loss: tensor(0.1607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4195/4345]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4196/4345]: training_loss: tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4197/4345]: training_loss: tensor(0.1855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4198/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4199/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4200/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4201/4345]: training_loss: tensor(0.1268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4202/4345]: training_loss: tensor(0.1935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4203/4345]: training_loss: tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4204/4345]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4205/4345]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4206/4345]: training_loss: tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4207/4345]: training_loss: tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4208/4345]: training_loss: tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4209/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4210/4345]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4211/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4212/4345]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4213/4345]: training_loss: tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4214/4345]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4215/4345]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4216/4345]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4217/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4218/4345]: training_loss: tensor(0.1255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4219/4345]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4220/4345]: training_loss: tensor(0.1560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4221/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4222/4345]: training_loss: tensor(0.1710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4223/4345]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4224/4345]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4225/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4226/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4227/4345]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4228/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4229/4345]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4230/4345]: training_loss: tensor(0.1912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4231/4345]: training_loss: tensor(0.1209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4232/4345]: training_loss: tensor(0.3761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4233/4345]: training_loss: tensor(0.1301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4234/4345]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4235/4345]: training_loss: tensor(0.1524, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4236/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4237/4345]: training_loss: tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4238/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4239/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4240/4345]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4241/4345]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4242/4345]: training_loss: tensor(0.3299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4243/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4244/4345]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4245/4345]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4246/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4247/4345]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4248/4345]: training_loss: tensor(0.2122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4249/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4250/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4251/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4252/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4253/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4254/4345]: training_loss: tensor(0.2254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4255/4345]: training_loss: tensor(0.1946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4256/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4257/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4258/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4259/4345]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4260/4345]: training_loss: tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4261/4345]: training_loss: tensor(0.2177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4262/4345]: training_loss: tensor(0.1800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4263/4345]: training_loss: tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4264/4345]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4265/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4266/4345]: training_loss: tensor(0.1767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4267/4345]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4268/4345]: training_loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4269/4345]: training_loss: tensor(0.2841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4270/4345]: training_loss: tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4271/4345]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4272/4345]: training_loss: tensor(0.1266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4273/4345]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4274/4345]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4275/4345]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4276/4345]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4277/4345]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4278/4345]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4279/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4280/4345]: training_loss: tensor(0.2209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4281/4345]: training_loss: tensor(0.1672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4282/4345]: training_loss: tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4283/4345]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4284/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4285/4345]: training_loss: tensor(0.3664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4286/4345]: training_loss: tensor(0.3213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4287/4345]: training_loss: tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4288/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4289/4345]: training_loss: tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4290/4345]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4291/4345]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4292/4345]: training_loss: tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4293/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4294/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4295/4345]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4296/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4297/4345]: training_loss: tensor(0.1309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4298/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4299/4345]: training_loss: tensor(0.1466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4300/4345]: training_loss: tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4301/4345]: training_loss: tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4302/4345]: training_loss: tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4303/4345]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4304/4345]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4305/4345]: training_loss: tensor(0.3849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4306/4345]: training_loss: tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4307/4345]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4308/4345]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4309/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4310/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4311/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4312/4345]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4313/4345]: training_loss: tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4314/4345]: training_loss: tensor(0.3519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4315/4345]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4316/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4317/4345]: training_loss: tensor(0.4186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4318/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4319/4345]: training_loss: tensor(0.2503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4320/4345]: training_loss: tensor(0.1448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4321/4345]: training_loss: tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4322/4345]: training_loss: tensor(0.3147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4323/4345]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4324/4345]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4325/4345]: training_loss: tensor(0.2003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4326/4345]: training_loss: tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4327/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4328/4345]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4329/4345]: training_loss: tensor(0.3158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4330/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4331/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4332/4345]: training_loss: tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4333/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4334/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4335/4345]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4336/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4337/4345]: training_loss: tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4338/4345]: training_loss: tensor(0.2080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4339/4345]: training_loss: tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4340/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4341/4345]: training_loss: tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4342/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4343/4345]: training_loss: tensor(0.2657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4344/4345]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4345/4345]: training_loss: tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4346/4345]: training_loss: tensor(0.1838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [3/5], Step [13038/21730], Train Loss: 0.0615, Valid Loss: 0.1512\n",
      "batch_no [1/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [5/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [6/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [7/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [8/4345]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [9/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [10/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [11/4345]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [12/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [13/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [14/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [15/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [16/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [17/4345]: training_loss: tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [18/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [19/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [20/4345]: training_loss: tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [21/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [22/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [23/4345]: training_loss: tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [24/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [25/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [26/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [27/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [28/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [29/4345]: training_loss: tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [30/4345]: training_loss: tensor(0.2720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [31/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [32/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [33/4345]: training_loss: tensor(0.1205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [34/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [35/4345]: training_loss: tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [36/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [37/4345]: training_loss: tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [38/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [39/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [40/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [41/4345]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [42/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [43/4345]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [44/4345]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [45/4345]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [46/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [47/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [48/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [49/4345]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [50/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [51/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [52/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [53/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [54/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [55/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [56/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [57/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [58/4345]: training_loss: tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [59/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [60/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [61/4345]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [62/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [63/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [64/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [65/4345]: training_loss: tensor(0.2537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [66/4345]: training_loss: tensor(0.4886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [67/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [68/4345]: training_loss: tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [69/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [70/4345]: training_loss: tensor(0.3176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [71/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [72/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [73/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [74/4345]: training_loss: tensor(0.3496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [75/4345]: training_loss: tensor(0.1358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [76/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [77/4345]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [78/4345]: training_loss: tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [79/4345]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [80/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [81/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [82/4345]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [83/4345]: training_loss: tensor(0.1831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [84/4345]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [85/4345]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [86/4345]: training_loss: tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [87/4345]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [88/4345]: training_loss: tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [89/4345]: training_loss: tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [90/4345]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [91/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [92/4345]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [93/4345]: training_loss: tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [94/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [95/4345]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [96/4345]: training_loss: tensor(0.2738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [97/4345]: training_loss: tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [98/4345]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [99/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [100/4345]: training_loss: tensor(0.3137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [101/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [102/4345]: training_loss: tensor(0.3000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [103/4345]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [104/4345]: training_loss: tensor(0.1613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [105/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [106/4345]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [107/4345]: training_loss: tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [108/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [109/4345]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [110/4345]: training_loss: tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [111/4345]: training_loss: tensor(0.0575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [112/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [113/4345]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [114/4345]: training_loss: tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [115/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [116/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [117/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [118/4345]: training_loss: tensor(0.2662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [119/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [120/4345]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [121/4345]: training_loss: tensor(0.1187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [122/4345]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [123/4345]: training_loss: tensor(0.3866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [124/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [125/4345]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [126/4345]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [127/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [128/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [129/4345]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [130/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [131/4345]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [132/4345]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [133/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [134/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [135/4345]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [136/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [137/4345]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [138/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [139/4345]: training_loss: tensor(0.0404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [140/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [141/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [142/4345]: training_loss: tensor(0.1259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [143/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [144/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [145/4345]: training_loss: tensor(0.0445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [146/4345]: training_loss: tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [147/4345]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [148/4345]: training_loss: tensor(0.1451, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [149/4345]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [150/4345]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [151/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [152/4345]: training_loss: tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [153/4345]: training_loss: tensor(0.2413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [154/4345]: training_loss: tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [155/4345]: training_loss: tensor(0.1498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [156/4345]: training_loss: tensor(0.1776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [157/4345]: training_loss: tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [158/4345]: training_loss: tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [159/4345]: training_loss: tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [160/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [161/4345]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [162/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [163/4345]: training_loss: tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [164/4345]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [165/4345]: training_loss: tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [166/4345]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [167/4345]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [168/4345]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [169/4345]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [170/4345]: training_loss: tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [171/4345]: training_loss: tensor(0.2496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [172/4345]: training_loss: tensor(0.1626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [173/4345]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [174/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [175/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [176/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [177/4345]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [178/4345]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [179/4345]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [180/4345]: training_loss: tensor(0.2693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [181/4345]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [182/4345]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [183/4345]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [184/4345]: training_loss: tensor(0.1519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [185/4345]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [186/4345]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [187/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [188/4345]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [189/4345]: training_loss: tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [190/4345]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [191/4345]: training_loss: tensor(0.1059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [192/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [193/4345]: training_loss: tensor(0.1948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [194/4345]: training_loss: tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [195/4345]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [196/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [197/4345]: training_loss: tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [198/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [199/4345]: training_loss: tensor(0.1224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [200/4345]: training_loss: tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [201/4345]: training_loss: tensor(0.0629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [202/4345]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [203/4345]: training_loss: tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [204/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [205/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [206/4345]: training_loss: tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [207/4345]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [208/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [209/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [210/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [211/4345]: training_loss: tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [212/4345]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [213/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [214/4345]: training_loss: tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [215/4345]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [216/4345]: training_loss: tensor(0.1647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [217/4345]: training_loss: tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [218/4345]: training_loss: tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [219/4345]: training_loss: tensor(0.1832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [220/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [221/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [222/4345]: training_loss: tensor(0.2399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [223/4345]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [224/4345]: training_loss: tensor(0.2415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [225/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [226/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [227/4345]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [228/4345]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [229/4345]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [230/4345]: training_loss: tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [231/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [232/4345]: training_loss: tensor(0.3865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [233/4345]: training_loss: tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [234/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [235/4345]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [236/4345]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [237/4345]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [238/4345]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [239/4345]: training_loss: tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [240/4345]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [241/4345]: training_loss: tensor(0.1513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [242/4345]: training_loss: tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [243/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [244/4345]: training_loss: tensor(0.1544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [245/4345]: training_loss: tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [246/4345]: training_loss: tensor(0.0342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [247/4345]: training_loss: tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [248/4345]: training_loss: tensor(0.1328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [249/4345]: training_loss: tensor(0.1598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [250/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [251/4345]: training_loss: tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [252/4345]: training_loss: tensor(0.1243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [253/4345]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [254/4345]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [255/4345]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [256/4345]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [257/4345]: training_loss: tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [258/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [259/4345]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [260/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [261/4345]: training_loss: tensor(0.2345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [262/4345]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [263/4345]: training_loss: tensor(0.1667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [264/4345]: training_loss: tensor(0.2365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [265/4345]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [266/4345]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [267/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [268/4345]: training_loss: tensor(0.0410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [269/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [270/4345]: training_loss: tensor(0.3790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [271/4345]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [272/4345]: training_loss: tensor(0.1729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [273/4345]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [274/4345]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [275/4345]: training_loss: tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [276/4345]: training_loss: tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [277/4345]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [278/4345]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [279/4345]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [280/4345]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [281/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [282/4345]: training_loss: tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [283/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [284/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [285/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [286/4345]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [287/4345]: training_loss: tensor(0.2209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [288/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [289/4345]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [290/4345]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [291/4345]: training_loss: tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [292/4345]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [293/4345]: training_loss: tensor(0.0365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [294/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [295/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [296/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [297/4345]: training_loss: tensor(0.3256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [298/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [299/4345]: training_loss: tensor(0.2069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [300/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [301/4345]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [302/4345]: training_loss: tensor(0.1443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [303/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [304/4345]: training_loss: tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [305/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [306/4345]: training_loss: tensor(0.0365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [307/4345]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [308/4345]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [309/4345]: training_loss: tensor(0.0573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [310/4345]: training_loss: tensor(0.1105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [311/4345]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [312/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [313/4345]: training_loss: tensor(0.1771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [314/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [315/4345]: training_loss: tensor(0.2171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [316/4345]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [317/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [318/4345]: training_loss: tensor(0.1601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [319/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [320/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [321/4345]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [322/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [323/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [324/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [325/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [326/4345]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [327/4345]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [328/4345]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [329/4345]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [330/4345]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [331/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [332/4345]: training_loss: tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [333/4345]: training_loss: tensor(0.2919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [334/4345]: training_loss: tensor(0.0446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [335/4345]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [336/4345]: training_loss: tensor(0.4317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [337/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [338/4345]: training_loss: tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [339/4345]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [340/4345]: training_loss: tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [341/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [342/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [343/4345]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [344/4345]: training_loss: tensor(0.2753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [345/4345]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [346/4345]: training_loss: tensor(0.1390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [347/4345]: training_loss: tensor(0.1871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [348/4345]: training_loss: tensor(0.2039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [349/4345]: training_loss: tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [350/4345]: training_loss: tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [351/4345]: training_loss: tensor(0.2548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [352/4345]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [353/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [354/4345]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [355/4345]: training_loss: tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [356/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [357/4345]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [358/4345]: training_loss: tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [359/4345]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [360/4345]: training_loss: tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [361/4345]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [362/4345]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [363/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [364/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [365/4345]: training_loss: tensor(0.2112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [366/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [367/4345]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [368/4345]: training_loss: tensor(0.3688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [369/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [370/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [371/4345]: training_loss: tensor(0.1768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [372/4345]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [373/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [374/4345]: training_loss: tensor(0.0429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [375/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [376/4345]: training_loss: tensor(0.1389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [377/4345]: training_loss: tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [378/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [379/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [380/4345]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [381/4345]: training_loss: tensor(0.3722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [382/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [383/4345]: training_loss: tensor(0.0423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [384/4345]: training_loss: tensor(0.1707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [385/4345]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [386/4345]: training_loss: tensor(0.1167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [387/4345]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [388/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [389/4345]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [390/4345]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [391/4345]: training_loss: tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [392/4345]: training_loss: tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [393/4345]: training_loss: tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [394/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [395/4345]: training_loss: tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [396/4345]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [397/4345]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [398/4345]: training_loss: tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [399/4345]: training_loss: tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [400/4345]: training_loss: tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [401/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [402/4345]: training_loss: tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [403/4345]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [404/4345]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [405/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [406/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [407/4345]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [408/4345]: training_loss: tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [409/4345]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [410/4345]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [411/4345]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [412/4345]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [413/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [414/4345]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [415/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [416/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [417/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [418/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [419/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [420/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [421/4345]: training_loss: tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [422/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [423/4345]: training_loss: tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [424/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [425/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [426/4345]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [427/4345]: training_loss: tensor(0.0666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [428/4345]: training_loss: tensor(0.1105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [429/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [430/4345]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [431/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [432/4345]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [433/4345]: training_loss: tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [434/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [435/4345]: training_loss: tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [436/4345]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [437/4345]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [438/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [439/4345]: training_loss: tensor(0.1343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [440/4345]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [441/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [442/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [443/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [444/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [445/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [446/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [447/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [448/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [449/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [450/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [451/4345]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [452/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [453/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [454/4345]: training_loss: tensor(0.2281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [455/4345]: training_loss: tensor(0.3524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [456/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [457/4345]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [458/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [459/4345]: training_loss: tensor(0.1772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [460/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [461/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [462/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [463/4345]: training_loss: tensor(0.1265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [464/4345]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [465/4345]: training_loss: tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [466/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [467/4345]: training_loss: tensor(0.2407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [468/4345]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [469/4345]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [470/4345]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [471/4345]: training_loss: tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [472/4345]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [473/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [474/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [475/4345]: training_loss: tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [476/4345]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [477/4345]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [478/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [479/4345]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [480/4345]: training_loss: tensor(0.2302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [481/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [482/4345]: training_loss: tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [483/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [484/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [485/4345]: training_loss: tensor(0.1308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [486/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [487/4345]: training_loss: tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [488/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [489/4345]: training_loss: tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [490/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [491/4345]: training_loss: tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [492/4345]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [493/4345]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [494/4345]: training_loss: tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [495/4345]: training_loss: tensor(0.1095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [496/4345]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [497/4345]: training_loss: tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [498/4345]: training_loss: tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [499/4345]: training_loss: tensor(0.1471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [500/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [501/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [502/4345]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [503/4345]: training_loss: tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [504/4345]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [505/4345]: training_loss: tensor(0.4195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [506/4345]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [507/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [508/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [509/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [510/4345]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [511/4345]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [512/4345]: training_loss: tensor(0.1527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [513/4345]: training_loss: tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [514/4345]: training_loss: tensor(0.2249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [515/4345]: training_loss: tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [516/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [517/4345]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [518/4345]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [519/4345]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [520/4345]: training_loss: tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [521/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [522/4345]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [523/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [524/4345]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [525/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [526/4345]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [527/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [528/4345]: training_loss: tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [529/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [530/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [531/4345]: training_loss: tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [532/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [533/4345]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [534/4345]: training_loss: tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [535/4345]: training_loss: tensor(0.1315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [536/4345]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [537/4345]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [538/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [539/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [540/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [541/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [542/4345]: training_loss: tensor(0.1387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [543/4345]: training_loss: tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [544/4345]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [545/4345]: training_loss: tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [546/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [547/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [548/4345]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [549/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [550/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [551/4345]: training_loss: tensor(0.2636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [552/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [553/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [554/4345]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [555/4345]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [556/4345]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [557/4345]: training_loss: tensor(0.1895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [558/4345]: training_loss: tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [559/4345]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [560/4345]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [561/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [562/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [563/4345]: training_loss: tensor(0.1591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [564/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [565/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [566/4345]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [567/4345]: training_loss: tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [568/4345]: training_loss: tensor(0.0502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [569/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [570/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [571/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [572/4345]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [573/4345]: training_loss: tensor(0.0494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [574/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [575/4345]: training_loss: tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [576/4345]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [577/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [578/4345]: training_loss: tensor(0.1422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [579/4345]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [580/4345]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [581/4345]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [582/4345]: training_loss: tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [583/4345]: training_loss: tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [584/4345]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [585/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [586/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [587/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [588/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [589/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [590/4345]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [591/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [592/4345]: training_loss: tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [593/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [594/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [595/4345]: training_loss: tensor(0.2071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [596/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [597/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [598/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [599/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [600/4345]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [601/4345]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [602/4345]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [603/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [604/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [605/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [606/4345]: training_loss: tensor(0.1256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [607/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [608/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [609/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [610/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [611/4345]: training_loss: tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [612/4345]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [613/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [614/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [615/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [616/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [617/4345]: training_loss: tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [618/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [619/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [620/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [621/4345]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [622/4345]: training_loss: tensor(0.2039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [623/4345]: training_loss: tensor(0.3013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [624/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [625/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [626/4345]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [627/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [628/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [629/4345]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [630/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [631/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [632/4345]: training_loss: tensor(0.2743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [633/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [634/4345]: training_loss: tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [635/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [636/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [637/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [638/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [639/4345]: training_loss: tensor(0.1468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [640/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [641/4345]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [642/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [643/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [644/4345]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [645/4345]: training_loss: tensor(0.1064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [646/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [647/4345]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [648/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [649/4345]: training_loss: tensor(0.2221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [650/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [651/4345]: training_loss: tensor(0.1497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [652/4345]: training_loss: tensor(0.1217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [653/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [654/4345]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [655/4345]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [656/4345]: training_loss: tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [657/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [658/4345]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [659/4345]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [660/4345]: training_loss: tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [661/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [662/4345]: training_loss: tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [663/4345]: training_loss: tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [664/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [665/4345]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [666/4345]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [667/4345]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [668/4345]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [669/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [670/4345]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [671/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [672/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [673/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [674/4345]: training_loss: tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [675/4345]: training_loss: tensor(0.1922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [676/4345]: training_loss: tensor(0.1950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [677/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [678/4345]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [679/4345]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [680/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [681/4345]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [682/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [683/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [684/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [685/4345]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [686/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [687/4345]: training_loss: tensor(0.4013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [688/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [689/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [690/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [691/4345]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [692/4345]: training_loss: tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [693/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [694/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [695/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [696/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [697/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [698/4345]: training_loss: tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [699/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [700/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [701/4345]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [702/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [703/4345]: training_loss: tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [704/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [705/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [706/4345]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [707/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [708/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [709/4345]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [710/4345]: training_loss: tensor(0.3012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [711/4345]: training_loss: tensor(0.0621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [712/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [713/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [714/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [715/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [716/4345]: training_loss: tensor(0.1320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [717/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [718/4345]: training_loss: tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [719/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [720/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [721/4345]: training_loss: tensor(0.1670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [722/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [723/4345]: training_loss: tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [724/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [725/4345]: training_loss: tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [726/4345]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [727/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [728/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [729/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [730/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [731/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [732/4345]: training_loss: tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [733/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [734/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [735/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [736/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [737/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [738/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [739/4345]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [740/4345]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [741/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [742/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [743/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [744/4345]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [745/4345]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [746/4345]: training_loss: tensor(0.0683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [747/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [748/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [749/4345]: training_loss: tensor(0.1533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [750/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [751/4345]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [752/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [753/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [754/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [755/4345]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [756/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [757/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [758/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [759/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [760/4345]: training_loss: tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [761/4345]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [762/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [763/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [764/4345]: training_loss: tensor(0.3006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [765/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [766/4345]: training_loss: tensor(0.1402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [767/4345]: training_loss: tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [768/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [769/4345]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [770/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [771/4345]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [772/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [773/4345]: training_loss: tensor(0.3223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [774/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [775/4345]: training_loss: tensor(0.3807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [776/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [777/4345]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [778/4345]: training_loss: tensor(0.2594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [779/4345]: training_loss: tensor(0.1334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [780/4345]: training_loss: tensor(0.1415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [781/4345]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [782/4345]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [783/4345]: training_loss: tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [784/4345]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [785/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [786/4345]: training_loss: tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [787/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [788/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [789/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [790/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [791/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [792/4345]: training_loss: tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [793/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [794/4345]: training_loss: tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [795/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [796/4345]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [797/4345]: training_loss: tensor(0.3005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [798/4345]: training_loss: tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [799/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [800/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [801/4345]: training_loss: tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [802/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [803/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [804/4345]: training_loss: tensor(0.0525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [805/4345]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [806/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [807/4345]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [808/4345]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [809/4345]: training_loss: tensor(0.1367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [810/4345]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [811/4345]: training_loss: tensor(0.1751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [812/4345]: training_loss: tensor(0.1619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [813/4345]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [814/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [815/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [816/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [817/4345]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [818/4345]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [819/4345]: training_loss: tensor(0.2231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [820/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [821/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [822/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [823/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [824/4345]: training_loss: tensor(0.1117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [825/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [826/4345]: training_loss: tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [827/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [828/4345]: training_loss: tensor(0.1083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [829/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [830/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [831/4345]: training_loss: tensor(0.1898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [832/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [833/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [834/4345]: training_loss: tensor(0.2518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [835/4345]: training_loss: tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [836/4345]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [837/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [838/4345]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [839/4345]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [840/4345]: training_loss: tensor(0.1644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [841/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [842/4345]: training_loss: tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [843/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [844/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [845/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [846/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [847/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [848/4345]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [849/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [850/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [851/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [852/4345]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [853/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [854/4345]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [855/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [856/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [857/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [858/4345]: training_loss: tensor(0.4265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [859/4345]: training_loss: tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [860/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [861/4345]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [862/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [863/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [864/4345]: training_loss: tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [865/4345]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [866/4345]: training_loss: tensor(0.1370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [867/4345]: training_loss: tensor(0.2182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [868/4345]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [869/4345]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [870/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [871/4345]: training_loss: tensor(0.0487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [872/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [873/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [874/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [875/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [876/4345]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [877/4345]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [878/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [879/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [880/4345]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [881/4345]: training_loss: tensor(0.3149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [882/4345]: training_loss: tensor(0.1556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [883/4345]: training_loss: tensor(0.1733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [884/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [885/4345]: training_loss: tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [886/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [887/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [888/4345]: training_loss: tensor(0.1866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [889/4345]: training_loss: tensor(0.1784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [890/4345]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [891/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [892/4345]: training_loss: tensor(0.1604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [893/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [894/4345]: training_loss: tensor(0.1467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [895/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [896/4345]: training_loss: tensor(0.1773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [897/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [898/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [899/4345]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [900/4345]: training_loss: tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [901/4345]: training_loss: tensor(0.1304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [902/4345]: training_loss: tensor(0.1132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [903/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [904/4345]: training_loss: tensor(0.2566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [905/4345]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [906/4345]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [907/4345]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [908/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [909/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [910/4345]: training_loss: tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [911/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [912/4345]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [913/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [914/4345]: training_loss: tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [915/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [916/4345]: training_loss: tensor(0.4806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [917/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [918/4345]: training_loss: tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [919/4345]: training_loss: tensor(0.1794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [920/4345]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [921/4345]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [922/4345]: training_loss: tensor(0.1613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [923/4345]: training_loss: tensor(0.1488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [924/4345]: training_loss: tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [925/4345]: training_loss: tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [926/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [927/4345]: training_loss: tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [928/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [929/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [930/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [931/4345]: training_loss: tensor(0.1139, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [932/4345]: training_loss: tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [933/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [934/4345]: training_loss: tensor(0.1914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [935/4345]: training_loss: tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [936/4345]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [937/4345]: training_loss: tensor(0.0594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [938/4345]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [939/4345]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [940/4345]: training_loss: tensor(0.1762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [941/4345]: training_loss: tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [942/4345]: training_loss: tensor(0.1296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [943/4345]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [944/4345]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [945/4345]: training_loss: tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [946/4345]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [947/4345]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [948/4345]: training_loss: tensor(0.2606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [949/4345]: training_loss: tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [950/4345]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [951/4345]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [952/4345]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [953/4345]: training_loss: tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [954/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [955/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [956/4345]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [957/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [958/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [959/4345]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [960/4345]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [961/4345]: training_loss: tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [962/4345]: training_loss: tensor(0.2820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [963/4345]: training_loss: tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [964/4345]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [965/4345]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [966/4345]: training_loss: tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [967/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [968/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [969/4345]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [970/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [971/4345]: training_loss: tensor(0.0596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [972/4345]: training_loss: tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [973/4345]: training_loss: tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [974/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [975/4345]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [976/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [977/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [978/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [979/4345]: training_loss: tensor(0.1212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [980/4345]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [981/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [982/4345]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [983/4345]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [984/4345]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [985/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [986/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [987/4345]: training_loss: tensor(0.1655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [988/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [989/4345]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [990/4345]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [991/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [992/4345]: training_loss: tensor(0.1506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [993/4345]: training_loss: tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [994/4345]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [995/4345]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [996/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [997/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [998/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [999/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1000/4345]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1001/4345]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1002/4345]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1003/4345]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1004/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1005/4345]: training_loss: tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/4345]: training_loss: tensor(0.2063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/4345]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/4345]: training_loss: tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/4345]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/4345]: training_loss: tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/4345]: training_loss: tensor(0.0446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/4345]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/4345]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/4345]: training_loss: tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1019/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/4345]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1023/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/4345]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/4345]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/4345]: training_loss: tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/4345]: training_loss: tensor(0.4557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/4345]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1040/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1043/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/4345]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/4345]: training_loss: tensor(0.2699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/4345]: training_loss: tensor(0.1286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/4345]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/4345]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/4345]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/4345]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/4345]: training_loss: tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/4345]: training_loss: tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/4345]: training_loss: tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/4345]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/4345]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/4345]: training_loss: tensor(0.0423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/4345]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/4345]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/4345]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/4345]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/4345]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/4345]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/4345]: training_loss: tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/4345]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/4345]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/4345]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/4345]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/4345]: training_loss: tensor(0.0570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/4345]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/4345]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1100/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/4345]: training_loss: tensor(0.2130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/4345]: training_loss: tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1105/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/4345]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/4345]: training_loss: tensor(0.2193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/4345]: training_loss: tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/4345]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/4345]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/4345]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/4345]: training_loss: tensor(0.1439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/4345]: training_loss: tensor(0.2581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1129/4345]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/4345]: training_loss: tensor(0.1294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/4345]: training_loss: tensor(0.1172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/4345]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/4345]: training_loss: tensor(0.2228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/4345]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/4345]: training_loss: tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/4345]: training_loss: tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/4345]: training_loss: tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/4345]: training_loss: tensor(0.2309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/4345]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/4345]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/4345]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/4345]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/4345]: training_loss: tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/4345]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/4345]: training_loss: tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/4345]: training_loss: tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/4345]: training_loss: tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/4345]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/4345]: training_loss: tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/4345]: training_loss: tensor(0.2555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/4345]: training_loss: tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/4345]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/4345]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/4345]: training_loss: tensor(0.0462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/4345]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/4345]: training_loss: tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/4345]: training_loss: tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/4345]: training_loss: tensor(0.1512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/4345]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/4345]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/4345]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/4345]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/4345]: training_loss: tensor(0.1367, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1191/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/4345]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/4345]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/4345]: training_loss: tensor(0.1925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/4345]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/4345]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/4345]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/4345]: training_loss: tensor(0.1862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/4345]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1213/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1215/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/4345]: training_loss: tensor(0.3137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/4345]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/4345]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/4345]: training_loss: tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/4345]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/4345]: training_loss: tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/4345]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/4345]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/4345]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/4345]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/4345]: training_loss: tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/4345]: training_loss: tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/4345]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/4345]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/4345]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/4345]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/4345]: training_loss: tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/4345]: training_loss: tensor(0.1842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/4345]: training_loss: tensor(0.4390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/4345]: training_loss: tensor(0.2416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/4345]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/4345]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1277/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/4345]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/4345]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/4345]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/4345]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/4345]: training_loss: tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1299/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/4345]: training_loss: tensor(0.0436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/4345]: training_loss: tensor(0.3723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/4345]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/4345]: training_loss: tensor(0.1664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/4345]: training_loss: tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/4345]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/4345]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/4345]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/4345]: training_loss: tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/4345]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/4345]: training_loss: tensor(0.1620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/4345]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/4345]: training_loss: tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/4345]: training_loss: tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/4345]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/4345]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/4345]: training_loss: tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/4345]: training_loss: tensor(0.1355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/4345]: training_loss: tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/4345]: training_loss: tensor(0.1362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/4345]: training_loss: tensor(0.0572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/4345]: training_loss: tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/4345]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/4345]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/4345]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/4345]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/4345]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/4345]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/4345]: training_loss: tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/4345]: training_loss: tensor(0.0342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1361/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1363/4345]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/4345]: training_loss: tensor(0.1154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/4345]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/4345]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/4345]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/4345]: training_loss: tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/4345]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/4345]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/4345]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/4345]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/4345]: training_loss: tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/4345]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1385/4345]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1386/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1387/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1388/4345]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1389/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1390/4345]: training_loss: tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1391/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1392/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1393/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1394/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1395/4345]: training_loss: tensor(0.1834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1396/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1397/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1398/4345]: training_loss: tensor(0.1095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1399/4345]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1400/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1401/4345]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1402/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1403/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1404/4345]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1405/4345]: training_loss: tensor(0.1752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1406/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1407/4345]: training_loss: tensor(0.3945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1408/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1409/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1410/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1411/4345]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1412/4345]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1413/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1414/4345]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1415/4345]: training_loss: tensor(0.1584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1416/4345]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1417/4345]: training_loss: tensor(0.2015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1418/4345]: training_loss: tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1419/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1420/4345]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1421/4345]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1422/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1423/4345]: training_loss: tensor(0.1957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1424/4345]: training_loss: tensor(0.1260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1425/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1426/4345]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1427/4345]: training_loss: tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1428/4345]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1429/4345]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1430/4345]: training_loss: tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1431/4345]: training_loss: tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1432/4345]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1433/4345]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1434/4345]: training_loss: tensor(0.1265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1435/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1436/4345]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1437/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1438/4345]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1439/4345]: training_loss: tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1440/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1441/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1442/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1443/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1444/4345]: training_loss: tensor(0.1667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1445/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1446/4345]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1447/4345]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1448/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1449/4345]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1450/4345]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1451/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1452/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1453/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1454/4345]: training_loss: tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1455/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1456/4345]: training_loss: tensor(0.2057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1457/4345]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1458/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1459/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1460/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1461/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1462/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1463/4345]: training_loss: tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1464/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1465/4345]: training_loss: tensor(0.2286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1466/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1467/4345]: training_loss: tensor(0.1324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1468/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1469/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1470/4345]: training_loss: tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1471/4345]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1472/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1473/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1474/4345]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1475/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1476/4345]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1477/4345]: training_loss: tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1478/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1479/4345]: training_loss: tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1480/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1481/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1482/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1483/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1484/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1485/4345]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1486/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1487/4345]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1488/4345]: training_loss: tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1489/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1490/4345]: training_loss: tensor(0.0534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1491/4345]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1492/4345]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1493/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1494/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1495/4345]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1496/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1497/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1498/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1499/4345]: training_loss: tensor(0.3535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1500/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1501/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1502/4345]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1503/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1504/4345]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1505/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1506/4345]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1507/4345]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1508/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1509/4345]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1510/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1511/4345]: training_loss: tensor(0.1866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1512/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1513/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1514/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1515/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1516/4345]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1517/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1518/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1519/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1520/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1521/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1522/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1523/4345]: training_loss: tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1524/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1525/4345]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1526/4345]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1527/4345]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1528/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1529/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1530/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1531/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1532/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1533/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1534/4345]: training_loss: tensor(0.1478, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1535/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1536/4345]: training_loss: tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1537/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1538/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1539/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1540/4345]: training_loss: tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1541/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1542/4345]: training_loss: tensor(0.1539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1543/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1544/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1545/4345]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1546/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1547/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1548/4345]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1549/4345]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1550/4345]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1551/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1552/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1553/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1554/4345]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1555/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1556/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1557/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1558/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1559/4345]: training_loss: tensor(0.3883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1560/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1561/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1562/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1563/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1564/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1565/4345]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1566/4345]: training_loss: tensor(0.2648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1567/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1568/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1569/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1570/4345]: training_loss: tensor(0.0650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1571/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1572/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1573/4345]: training_loss: tensor(0.2715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1574/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1575/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1576/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1577/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1578/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1579/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1580/4345]: training_loss: tensor(0.0543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1581/4345]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1582/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1583/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1584/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1585/4345]: training_loss: tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1586/4345]: training_loss: tensor(0.2247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1587/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1588/4345]: training_loss: tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1589/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1590/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1591/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1592/4345]: training_loss: tensor(0.1473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1593/4345]: training_loss: tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1594/4345]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1595/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1596/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1597/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1598/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1599/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1600/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1601/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1602/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1603/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1604/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1605/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1606/4345]: training_loss: tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1607/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1608/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1609/4345]: training_loss: tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1610/4345]: training_loss: tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1611/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1612/4345]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1613/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1614/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1615/4345]: training_loss: tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1616/4345]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1617/4345]: training_loss: tensor(0.3256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1618/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1619/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1620/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1621/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1622/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1623/4345]: training_loss: tensor(0.1725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1624/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1625/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1626/4345]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1627/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1628/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1629/4345]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1630/4345]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1631/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1632/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1633/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1634/4345]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1635/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1636/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1637/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1638/4345]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1639/4345]: training_loss: tensor(0.3402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1640/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1641/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1642/4345]: training_loss: tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1643/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1644/4345]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1645/4345]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1646/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1647/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1648/4345]: training_loss: tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1649/4345]: training_loss: tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1650/4345]: training_loss: tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1651/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1652/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1653/4345]: training_loss: tensor(0.2305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1654/4345]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1655/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1656/4345]: training_loss: tensor(0.1212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1657/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1658/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1659/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1660/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1661/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1662/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1663/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1664/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1665/4345]: training_loss: tensor(0.0290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1666/4345]: training_loss: tensor(0.3122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1667/4345]: training_loss: tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1668/4345]: training_loss: tensor(0.2467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1669/4345]: training_loss: tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1670/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1671/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1672/4345]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1673/4345]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1674/4345]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1675/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1676/4345]: training_loss: tensor(0.2197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1677/4345]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1678/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1679/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1680/4345]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1681/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1682/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1683/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1684/4345]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1685/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1686/4345]: training_loss: tensor(0.1733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1687/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1688/4345]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1689/4345]: training_loss: tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1690/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1691/4345]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1692/4345]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1693/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1694/4345]: training_loss: tensor(0.2176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1695/4345]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1696/4345]: training_loss: tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1697/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1698/4345]: training_loss: tensor(0.2248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1699/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1700/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1701/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1702/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1703/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1704/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1705/4345]: training_loss: tensor(0.1275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1706/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1707/4345]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1708/4345]: training_loss: tensor(0.3557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1709/4345]: training_loss: tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1710/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1711/4345]: training_loss: tensor(0.1548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1712/4345]: training_loss: tensor(0.2967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1713/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1714/4345]: training_loss: tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1715/4345]: training_loss: tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1716/4345]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1717/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1718/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1719/4345]: training_loss: tensor(0.1414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1720/4345]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1721/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1722/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1723/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1724/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1725/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1726/4345]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1727/4345]: training_loss: tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1728/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1729/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1730/4345]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1731/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1732/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1733/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1734/4345]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1735/4345]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1736/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1737/4345]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1738/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1739/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1740/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1741/4345]: training_loss: tensor(0.0492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1742/4345]: training_loss: tensor(0.2014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1743/4345]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1744/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1745/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1746/4345]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1747/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1748/4345]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1749/4345]: training_loss: tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1750/4345]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1751/4345]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1752/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1753/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1754/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1755/4345]: training_loss: tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1756/4345]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1757/4345]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1758/4345]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1759/4345]: training_loss: tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1760/4345]: training_loss: tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1761/4345]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1762/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1763/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1764/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1765/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1766/4345]: training_loss: tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1767/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1768/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1769/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1770/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1771/4345]: training_loss: tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1772/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1773/4345]: training_loss: tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1774/4345]: training_loss: tensor(0.3543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1775/4345]: training_loss: tensor(0.3079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1776/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1777/4345]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1778/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1779/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1780/4345]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1781/4345]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1782/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1783/4345]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1784/4345]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1785/4345]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1786/4345]: training_loss: tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1787/4345]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1788/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1789/4345]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1790/4345]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1791/4345]: training_loss: tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1792/4345]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1793/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1794/4345]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1795/4345]: training_loss: tensor(0.2858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1796/4345]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1797/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1798/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1799/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1800/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1801/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1802/4345]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1803/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1804/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1805/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1806/4345]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1807/4345]: training_loss: tensor(0.0410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1808/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1809/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1810/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1811/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1812/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1813/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1814/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1815/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1816/4345]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1817/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1818/4345]: training_loss: tensor(0.2215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1819/4345]: training_loss: tensor(0.0650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1820/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1821/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1822/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1823/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1824/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1825/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1826/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1827/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1828/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1829/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1830/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1831/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1832/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1833/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1834/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1835/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1836/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1837/4345]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1838/4345]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1839/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1840/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1841/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1842/4345]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1843/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1844/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1845/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1846/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1847/4345]: training_loss: tensor(0.1983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1848/4345]: training_loss: tensor(0.3840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1849/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1850/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1851/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1852/4345]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1853/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1854/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1855/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1856/4345]: training_loss: tensor(0.0493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1857/4345]: training_loss: tensor(0.1360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1858/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1859/4345]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1860/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1861/4345]: training_loss: tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1862/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1863/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1864/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1865/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1866/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1867/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1868/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1869/4345]: training_loss: tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1870/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1871/4345]: training_loss: tensor(0.1947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1872/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1873/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1874/4345]: training_loss: tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1875/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1876/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1877/4345]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1878/4345]: training_loss: tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1879/4345]: training_loss: tensor(0.1699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1880/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1881/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1882/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1883/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1884/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1885/4345]: training_loss: tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1886/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1887/4345]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1888/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1889/4345]: training_loss: tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1890/4345]: training_loss: tensor(0.1572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1891/4345]: training_loss: tensor(0.3167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1892/4345]: training_loss: tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1893/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1894/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1895/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1896/4345]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1897/4345]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1898/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1899/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1900/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1901/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1902/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1903/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1904/4345]: training_loss: tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1905/4345]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1906/4345]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1907/4345]: training_loss: tensor(0.1165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1908/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1909/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1910/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1911/4345]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1912/4345]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1913/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1914/4345]: training_loss: tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1915/4345]: training_loss: tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1916/4345]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1917/4345]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1918/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1919/4345]: training_loss: tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1920/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1921/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1922/4345]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1923/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1924/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1925/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1926/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1927/4345]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1928/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1929/4345]: training_loss: tensor(0.2582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1930/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1931/4345]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1932/4345]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1933/4345]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1934/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1935/4345]: training_loss: tensor(0.1660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1936/4345]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1937/4345]: training_loss: tensor(0.0342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1938/4345]: training_loss: tensor(0.1562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1939/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1940/4345]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1941/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1942/4345]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1943/4345]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1944/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1945/4345]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1946/4345]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1947/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1948/4345]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1949/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1950/4345]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1951/4345]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1952/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1953/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1954/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1955/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1956/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1957/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1958/4345]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1959/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1960/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1961/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1962/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1963/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1964/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1965/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1966/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1967/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1968/4345]: training_loss: tensor(0.1292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1969/4345]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1970/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1971/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1972/4345]: training_loss: tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1973/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1974/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1975/4345]: training_loss: tensor(0.1532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1976/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1977/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1978/4345]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1979/4345]: training_loss: tensor(0.2594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1980/4345]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1981/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1982/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1983/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1984/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1985/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1986/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1987/4345]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1988/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1989/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1990/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1991/4345]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1992/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1993/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1994/4345]: training_loss: tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1995/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1996/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1997/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1998/4345]: training_loss: tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1999/4345]: training_loss: tensor(0.0475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2000/4345]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2001/4345]: training_loss: tensor(0.2002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2002/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2003/4345]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2004/4345]: training_loss: tensor(0.1442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2005/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2006/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2007/4345]: training_loss: tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2008/4345]: training_loss: tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2009/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2010/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2011/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2012/4345]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2013/4345]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2014/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2015/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2016/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2017/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2018/4345]: training_loss: tensor(0.2925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2019/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2020/4345]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2021/4345]: training_loss: tensor(0.3121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2022/4345]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2023/4345]: training_loss: tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2024/4345]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2025/4345]: training_loss: tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2026/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2027/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2028/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2029/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2030/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2031/4345]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2032/4345]: training_loss: tensor(0.1632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2033/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2034/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2035/4345]: training_loss: tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2036/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2037/4345]: training_loss: tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2038/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2039/4345]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2040/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2041/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2042/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2043/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2044/4345]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2045/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2046/4345]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2047/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2048/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2049/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2050/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2051/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2052/4345]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2053/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2054/4345]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2055/4345]: training_loss: tensor(0.1507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2056/4345]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2057/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2058/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2059/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2060/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2061/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2062/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2063/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2064/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2065/4345]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2066/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2067/4345]: training_loss: tensor(0.1982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2068/4345]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2069/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2070/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2071/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2072/4345]: training_loss: tensor(0.2412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2073/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2074/4345]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2075/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2076/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2077/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2078/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2079/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2080/4345]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2081/4345]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2082/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2083/4345]: training_loss: tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2084/4345]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2085/4345]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2086/4345]: training_loss: tensor(0.2664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2087/4345]: training_loss: tensor(0.2662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2088/4345]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2089/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2090/4345]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2091/4345]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2092/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2093/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2094/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2095/4345]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2096/4345]: training_loss: tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2097/4345]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2098/4345]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2099/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2100/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2101/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2102/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2103/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2104/4345]: training_loss: tensor(0.2664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2105/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2106/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2107/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2108/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2109/4345]: training_loss: tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2110/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2111/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2112/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2113/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2114/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2115/4345]: training_loss: tensor(0.1517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2116/4345]: training_loss: tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2117/4345]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2118/4345]: training_loss: tensor(0.2888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2119/4345]: training_loss: tensor(0.1431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2120/4345]: training_loss: tensor(0.1303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2121/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2122/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2123/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2124/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2125/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2126/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2127/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2128/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2129/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2130/4345]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2131/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2132/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2133/4345]: training_loss: tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2134/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2135/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2136/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2137/4345]: training_loss: tensor(0.2992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2138/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2139/4345]: training_loss: tensor(0.2032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2140/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2141/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2142/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2143/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2144/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2145/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2146/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2147/4345]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2148/4345]: training_loss: tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2149/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2150/4345]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2151/4345]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2152/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2153/4345]: training_loss: tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2154/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2155/4345]: training_loss: tensor(0.2406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2156/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2157/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2158/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2159/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2160/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2161/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2162/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2163/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2164/4345]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2165/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2166/4345]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2167/4345]: training_loss: tensor(0.0305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2168/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2169/4345]: training_loss: tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2170/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2171/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2172/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2173/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [4/5], Step [15211/21730], Train Loss: 0.0410, Valid Loss: 0.2058\n",
      "batch_no [2174/4345]: training_loss: tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2175/4345]: training_loss: tensor(0.2223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2176/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2177/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2178/4345]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2179/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2180/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2181/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2182/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2183/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2184/4345]: training_loss: tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2185/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2186/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2187/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2188/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2189/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2190/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2191/4345]: training_loss: tensor(0.1284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2192/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2193/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2194/4345]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2195/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2196/4345]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2197/4345]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2198/4345]: training_loss: tensor(0.1588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2199/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2200/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2201/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2202/4345]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2203/4345]: training_loss: tensor(0.1356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2204/4345]: training_loss: tensor(0.1506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2205/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2206/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2207/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2208/4345]: training_loss: tensor(0.3175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2209/4345]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2210/4345]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2211/4345]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2212/4345]: training_loss: tensor(0.1626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2213/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2214/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2215/4345]: training_loss: tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2216/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2217/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2218/4345]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2219/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2220/4345]: training_loss: tensor(0.1489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2221/4345]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2222/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2223/4345]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2224/4345]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2225/4345]: training_loss: tensor(0.2338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2226/4345]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2227/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2228/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2229/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2230/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2231/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2232/4345]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2233/4345]: training_loss: tensor(0.1171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2234/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2235/4345]: training_loss: tensor(0.3814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2236/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2237/4345]: training_loss: tensor(0.2665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2238/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2239/4345]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2240/4345]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2241/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2242/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2243/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2244/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2245/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2246/4345]: training_loss: tensor(0.1979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2247/4345]: training_loss: tensor(0.1187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2248/4345]: training_loss: tensor(0.1912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2249/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2250/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2251/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2252/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2253/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2254/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2255/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2256/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2257/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2258/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2259/4345]: training_loss: tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2260/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2261/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2262/4345]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2263/4345]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2264/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2265/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2266/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2267/4345]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2268/4345]: training_loss: tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2269/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2270/4345]: training_loss: tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2271/4345]: training_loss: tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2272/4345]: training_loss: tensor(0.1494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2273/4345]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2274/4345]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2275/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2276/4345]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2277/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2278/4345]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2279/4345]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2280/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2281/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2282/4345]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2283/4345]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2284/4345]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2285/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2286/4345]: training_loss: tensor(0.1731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2287/4345]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2288/4345]: training_loss: tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2289/4345]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2290/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2291/4345]: training_loss: tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2292/4345]: training_loss: tensor(0.2780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2293/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2294/4345]: training_loss: tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2295/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2296/4345]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2297/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2298/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2299/4345]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2300/4345]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2301/4345]: training_loss: tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2302/4345]: training_loss: tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2303/4345]: training_loss: tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2304/4345]: training_loss: tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2305/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2306/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2307/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2308/4345]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2309/4345]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2310/4345]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2311/4345]: training_loss: tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2312/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2313/4345]: training_loss: tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2314/4345]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2315/4345]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2316/4345]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2317/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2318/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2319/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2320/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2321/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2322/4345]: training_loss: tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2323/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2324/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2325/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2326/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2327/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2328/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2329/4345]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2330/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2331/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2332/4345]: training_loss: tensor(0.1893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2333/4345]: training_loss: tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2334/4345]: training_loss: tensor(0.1862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2335/4345]: training_loss: tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2336/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2337/4345]: training_loss: tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2338/4345]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2339/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2340/4345]: training_loss: tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2341/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2342/4345]: training_loss: tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2343/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2344/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2345/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2346/4345]: training_loss: tensor(0.1528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2347/4345]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2348/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2349/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2350/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2351/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2352/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2353/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2354/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2355/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2356/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2357/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2358/4345]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2359/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2360/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2361/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2362/4345]: training_loss: tensor(0.1476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2363/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2364/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2365/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2366/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2367/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2368/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2369/4345]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2370/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2371/4345]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2372/4345]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2373/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2374/4345]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2375/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2376/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2377/4345]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2378/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2379/4345]: training_loss: tensor(0.1270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2380/4345]: training_loss: tensor(0.1239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2381/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2382/4345]: training_loss: tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2383/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2384/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2385/4345]: training_loss: tensor(0.0573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2386/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2387/4345]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2388/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2389/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2390/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2391/4345]: training_loss: tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2392/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2393/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2394/4345]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2395/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2396/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2397/4345]: training_loss: tensor(0.2458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2398/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2399/4345]: training_loss: tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2400/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2401/4345]: training_loss: tensor(0.1378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2402/4345]: training_loss: tensor(0.1365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2403/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2404/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2405/4345]: training_loss: tensor(0.0459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2406/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2407/4345]: training_loss: tensor(0.2506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2408/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2409/4345]: training_loss: tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2410/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2411/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2412/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2413/4345]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2414/4345]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2415/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2416/4345]: training_loss: tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2417/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2418/4345]: training_loss: tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2419/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2420/4345]: training_loss: tensor(0.2483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2421/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2422/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2423/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2424/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2425/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2426/4345]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2427/4345]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2428/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2429/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2430/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2431/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2432/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2433/4345]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2434/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2435/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2436/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2437/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2438/4345]: training_loss: tensor(0.3920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2439/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2440/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2441/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2442/4345]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2443/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2444/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2445/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2446/4345]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2447/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2448/4345]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2449/4345]: training_loss: tensor(0.1254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2450/4345]: training_loss: tensor(0.1285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2451/4345]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2452/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2453/4345]: training_loss: tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2454/4345]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2455/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2456/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2457/4345]: training_loss: tensor(0.1414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2458/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2459/4345]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2460/4345]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2461/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2462/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2463/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2464/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2465/4345]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2466/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2467/4345]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2468/4345]: training_loss: tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2469/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2470/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2471/4345]: training_loss: tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2472/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2473/4345]: training_loss: tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2474/4345]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2475/4345]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2476/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2477/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2478/4345]: training_loss: tensor(0.2273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2479/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2480/4345]: training_loss: tensor(0.2752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2481/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2482/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2483/4345]: training_loss: tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2484/4345]: training_loss: tensor(0.2762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2485/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2486/4345]: training_loss: tensor(0.1066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2487/4345]: training_loss: tensor(0.2038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2488/4345]: training_loss: tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2489/4345]: training_loss: tensor(0.2631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2490/4345]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2491/4345]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2492/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2493/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2494/4345]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2495/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2496/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2497/4345]: training_loss: tensor(0.1410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2498/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2499/4345]: training_loss: tensor(0.2476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2500/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2501/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2502/4345]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2503/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2504/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2505/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2506/4345]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2507/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2508/4345]: training_loss: tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2509/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2510/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2511/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2512/4345]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2513/4345]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2514/4345]: training_loss: tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2515/4345]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2516/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2517/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2518/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2519/4345]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2520/4345]: training_loss: tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2521/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2522/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2523/4345]: training_loss: tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2524/4345]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2525/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2526/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2527/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2528/4345]: training_loss: tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2529/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2530/4345]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2531/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2532/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2533/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2534/4345]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2535/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2536/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2537/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2538/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2539/4345]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2540/4345]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2541/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2542/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2543/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2544/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2545/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2546/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2547/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2548/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2549/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2550/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2551/4345]: training_loss: tensor(0.5457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2552/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2553/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2554/4345]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2555/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2556/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2557/4345]: training_loss: tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2558/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2559/4345]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2560/4345]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2561/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2562/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2563/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2564/4345]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2565/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2566/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2567/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2568/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2569/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2570/4345]: training_loss: tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2571/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2572/4345]: training_loss: tensor(0.1810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2573/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2574/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2575/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2576/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2577/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2578/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2579/4345]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2580/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2581/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2582/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2583/4345]: training_loss: tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2584/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2585/4345]: training_loss: tensor(0.1862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2586/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2587/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2588/4345]: training_loss: tensor(0.0587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2589/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2590/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2591/4345]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2592/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2593/4345]: training_loss: tensor(0.2440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2594/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2595/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2596/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2597/4345]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2598/4345]: training_loss: tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2599/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2600/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2601/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2602/4345]: training_loss: tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2603/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2604/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2605/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2606/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2607/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2608/4345]: training_loss: tensor(0.1804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2609/4345]: training_loss: tensor(0.1900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2610/4345]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2611/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2612/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2613/4345]: training_loss: tensor(0.1529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2614/4345]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2615/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2616/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2617/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2618/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2619/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2620/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2621/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2622/4345]: training_loss: tensor(0.1913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2623/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2624/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2625/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2626/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2627/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2628/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2629/4345]: training_loss: tensor(0.2382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2630/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2631/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2632/4345]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2633/4345]: training_loss: tensor(0.0502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2634/4345]: training_loss: tensor(0.0492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2635/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2636/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2637/4345]: training_loss: tensor(0.0445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2638/4345]: training_loss: tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2639/4345]: training_loss: tensor(0.0534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2640/4345]: training_loss: tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2641/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2642/4345]: training_loss: tensor(0.0313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2643/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2644/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2645/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2646/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2647/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2648/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2649/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2650/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2651/4345]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2652/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2653/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2654/4345]: training_loss: tensor(0.1233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2655/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2656/4345]: training_loss: tensor(0.2069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2657/4345]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2658/4345]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2659/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2660/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2661/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2662/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2663/4345]: training_loss: tensor(0.1705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2664/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2665/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2666/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2667/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2668/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2669/4345]: training_loss: tensor(0.1670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2670/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2671/4345]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2672/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2673/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2674/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2675/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2676/4345]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2677/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2678/4345]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2679/4345]: training_loss: tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2680/4345]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2681/4345]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2682/4345]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2683/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2684/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2685/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2686/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2687/4345]: training_loss: tensor(0.1355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2688/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2689/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2690/4345]: training_loss: tensor(0.2351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2691/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2692/4345]: training_loss: tensor(0.0313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2693/4345]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2694/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2695/4345]: training_loss: tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2696/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2697/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2698/4345]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2699/4345]: training_loss: tensor(0.1508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2700/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2701/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2702/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2703/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2704/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2705/4345]: training_loss: tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2706/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2707/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2708/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2709/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2710/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2711/4345]: training_loss: tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2712/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2713/4345]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2714/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2715/4345]: training_loss: tensor(0.1616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2716/4345]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2717/4345]: training_loss: tensor(0.2001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2718/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2719/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2720/4345]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2721/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2722/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2723/4345]: training_loss: tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2724/4345]: training_loss: tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2725/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2726/4345]: training_loss: tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2727/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2728/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2729/4345]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2730/4345]: training_loss: tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2731/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2732/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2733/4345]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2734/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2735/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2736/4345]: training_loss: tensor(0.1614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2737/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2738/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2739/4345]: training_loss: tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2740/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2741/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2742/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2743/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2744/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2745/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2746/4345]: training_loss: tensor(0.7335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2747/4345]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2748/4345]: training_loss: tensor(0.3593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2749/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2750/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2751/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2752/4345]: training_loss: tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2753/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2754/4345]: training_loss: tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2755/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2756/4345]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2757/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2758/4345]: training_loss: tensor(0.1821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2759/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2760/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2761/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2762/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2763/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2764/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2765/4345]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2766/4345]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2767/4345]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2768/4345]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2769/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2770/4345]: training_loss: tensor(0.1317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2771/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2772/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2773/4345]: training_loss: tensor(0.1621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2774/4345]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2775/4345]: training_loss: tensor(0.2832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2776/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2777/4345]: training_loss: tensor(0.1424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2778/4345]: training_loss: tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2779/4345]: training_loss: tensor(0.1375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2780/4345]: training_loss: tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2781/4345]: training_loss: tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2782/4345]: training_loss: tensor(0.1261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2783/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2784/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2785/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2786/4345]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2787/4345]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2788/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2789/4345]: training_loss: tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2790/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2791/4345]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2792/4345]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2793/4345]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2794/4345]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2795/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2796/4345]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2797/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2798/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2799/4345]: training_loss: tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2800/4345]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2801/4345]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2802/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2803/4345]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2804/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2805/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2806/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2807/4345]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2808/4345]: training_loss: tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2809/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2810/4345]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2811/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2812/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2813/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2814/4345]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2815/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2816/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2817/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2818/4345]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2819/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2820/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2821/4345]: training_loss: tensor(0.2050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2822/4345]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2823/4345]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2824/4345]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2825/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2826/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2827/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2828/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2829/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2830/4345]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2831/4345]: training_loss: tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2832/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2833/4345]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2834/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2835/4345]: training_loss: tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2836/4345]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2837/4345]: training_loss: tensor(0.0611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2838/4345]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2839/4345]: training_loss: tensor(0.0572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2840/4345]: training_loss: tensor(0.1381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2841/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2842/4345]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2843/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2844/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2845/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2846/4345]: training_loss: tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2847/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2848/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2849/4345]: training_loss: tensor(0.0686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2850/4345]: training_loss: tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2851/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2852/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2853/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2854/4345]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2855/4345]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2856/4345]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2857/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2858/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2859/4345]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2860/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2861/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2862/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2863/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2864/4345]: training_loss: tensor(0.3791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2865/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2866/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2867/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2868/4345]: training_loss: tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2869/4345]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2870/4345]: training_loss: tensor(0.1128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2871/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2872/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2873/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2874/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2875/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2876/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2877/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2878/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2879/4345]: training_loss: tensor(0.1231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2880/4345]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2881/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2882/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2883/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2884/4345]: training_loss: tensor(0.2079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2885/4345]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2886/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2887/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2888/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2889/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2890/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2891/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2892/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2893/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2894/4345]: training_loss: tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2895/4345]: training_loss: tensor(0.1784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2896/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2897/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2898/4345]: training_loss: tensor(0.1577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2899/4345]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2900/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2901/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2902/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2903/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2904/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2905/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2906/4345]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2907/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2908/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2909/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2910/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2911/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2912/4345]: training_loss: tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2913/4345]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2914/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2915/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2916/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2917/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2918/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2919/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2920/4345]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2921/4345]: training_loss: tensor(0.0480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2922/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2923/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2924/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2925/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2926/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2927/4345]: training_loss: tensor(0.2123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2928/4345]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2929/4345]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2930/4345]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2931/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2932/4345]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2933/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2934/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2935/4345]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2936/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2937/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2938/4345]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2939/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2940/4345]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2941/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2942/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2943/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2944/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2945/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2946/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2947/4345]: training_loss: tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2948/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2949/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2950/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2951/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2952/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2953/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2954/4345]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2955/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2956/4345]: training_loss: tensor(0.1585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2957/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2958/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2959/4345]: training_loss: tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2960/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2961/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2962/4345]: training_loss: tensor(0.1322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2963/4345]: training_loss: tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2964/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2965/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2966/4345]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2967/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2968/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2969/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2970/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2971/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2972/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2973/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2974/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2975/4345]: training_loss: tensor(0.5385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2976/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2977/4345]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2978/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2979/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2980/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2981/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2982/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2983/4345]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2984/4345]: training_loss: tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2985/4345]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2986/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2987/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2988/4345]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2989/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2990/4345]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2991/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2992/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2993/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2994/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2995/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2996/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2997/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2998/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2999/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3000/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3001/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3002/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3003/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3004/4345]: training_loss: tensor(0.1400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3005/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3006/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3007/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3008/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3009/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3010/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3011/4345]: training_loss: tensor(0.2050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3012/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3013/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3014/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3015/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3016/4345]: training_loss: tensor(0.1405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3017/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3018/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3019/4345]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3020/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3021/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3022/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3023/4345]: training_loss: tensor(0.1603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3024/4345]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3025/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3026/4345]: training_loss: tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3027/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3028/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3029/4345]: training_loss: tensor(0.1658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3030/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3031/4345]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3032/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3033/4345]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3034/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3035/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3036/4345]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3037/4345]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3038/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3039/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3040/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3041/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3042/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3043/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3044/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3045/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3046/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3047/4345]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3048/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3049/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3050/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3051/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3052/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3053/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3054/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3055/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3056/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3057/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3058/4345]: training_loss: tensor(0.3016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3059/4345]: training_loss: tensor(0.1090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3060/4345]: training_loss: tensor(0.2496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3061/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3062/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3063/4345]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3064/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3065/4345]: training_loss: tensor(0.1955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3066/4345]: training_loss: tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3067/4345]: training_loss: tensor(0.1873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3068/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3069/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3070/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3071/4345]: training_loss: tensor(0.2479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3072/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3073/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3074/4345]: training_loss: tensor(0.3201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3075/4345]: training_loss: tensor(0.1413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3076/4345]: training_loss: tensor(0.1532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3077/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3078/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3079/4345]: training_loss: tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3080/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3081/4345]: training_loss: tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3082/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3083/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3084/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3085/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3086/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3087/4345]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3088/4345]: training_loss: tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3089/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3090/4345]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3091/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3092/4345]: training_loss: tensor(0.0410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3093/4345]: training_loss: tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3094/4345]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3095/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3096/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3097/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3098/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3099/4345]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3100/4345]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3101/4345]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3102/4345]: training_loss: tensor(0.1450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3103/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3104/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3105/4345]: training_loss: tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3106/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3107/4345]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3108/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3109/4345]: training_loss: tensor(0.0666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3110/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3111/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3112/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3113/4345]: training_loss: tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3114/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3115/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3116/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3117/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3118/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3119/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3120/4345]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3121/4345]: training_loss: tensor(0.0439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3122/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3123/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3124/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3125/4345]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3126/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3127/4345]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3128/4345]: training_loss: tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3129/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3130/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3131/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3132/4345]: training_loss: tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3133/4345]: training_loss: tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3134/4345]: training_loss: tensor(0.1606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3135/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3136/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3137/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3138/4345]: training_loss: tensor(0.1272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3139/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3140/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3141/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3142/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3143/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3144/4345]: training_loss: tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3145/4345]: training_loss: tensor(0.1932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3146/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3147/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3148/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3149/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3150/4345]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3151/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3152/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3153/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3154/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3155/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3156/4345]: training_loss: tensor(0.3149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3157/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3158/4345]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3159/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3160/4345]: training_loss: tensor(0.2124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3161/4345]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3162/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3163/4345]: training_loss: tensor(0.1792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3164/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3165/4345]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3166/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3167/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3168/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3169/4345]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3170/4345]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3171/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3172/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3173/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3174/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3175/4345]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3176/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3177/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3178/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3179/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3180/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3181/4345]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3182/4345]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3183/4345]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3184/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3185/4345]: training_loss: tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3186/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3187/4345]: training_loss: tensor(0.1910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3188/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3189/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3190/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3191/4345]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3192/4345]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3193/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3194/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3195/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3196/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3197/4345]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3198/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3199/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3200/4345]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3201/4345]: training_loss: tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3202/4345]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3203/4345]: training_loss: tensor(0.1153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3204/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3205/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3206/4345]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3207/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3208/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3209/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3210/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3211/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3212/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3213/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3214/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3215/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3216/4345]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3217/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3218/4345]: training_loss: tensor(0.2519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3219/4345]: training_loss: tensor(0.3129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3220/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3221/4345]: training_loss: tensor(0.2152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3222/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3223/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3224/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3225/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3226/4345]: training_loss: tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3227/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3228/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3229/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3230/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3231/4345]: training_loss: tensor(0.1604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3232/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3233/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3234/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3235/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3236/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3237/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3238/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3239/4345]: training_loss: tensor(0.2734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3240/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3241/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3242/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3243/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3244/4345]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3245/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3246/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3247/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3248/4345]: training_loss: tensor(0.2196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3249/4345]: training_loss: tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3250/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3251/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3252/4345]: training_loss: tensor(0.2082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3253/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3254/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3255/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3256/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3257/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3258/4345]: training_loss: tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3259/4345]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3260/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3261/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3262/4345]: training_loss: tensor(0.2193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3263/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3264/4345]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3265/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3266/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3267/4345]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3268/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3269/4345]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3270/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3271/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3272/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3273/4345]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3274/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3275/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3276/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3277/4345]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3278/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3279/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3280/4345]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3281/4345]: training_loss: tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3282/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3283/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3284/4345]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3285/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3286/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3287/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3288/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3289/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3290/4345]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3291/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3292/4345]: training_loss: tensor(0.2553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3293/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3294/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3295/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3296/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3297/4345]: training_loss: tensor(0.1281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3298/4345]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3299/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3300/4345]: training_loss: tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3301/4345]: training_loss: tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3302/4345]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3303/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3304/4345]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3305/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3306/4345]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3307/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3308/4345]: training_loss: tensor(0.1452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3309/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3310/4345]: training_loss: tensor(0.1996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3311/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3312/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3313/4345]: training_loss: tensor(0.1308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3314/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3315/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3316/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3317/4345]: training_loss: tensor(0.2310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3318/4345]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3319/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3320/4345]: training_loss: tensor(0.0555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3321/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3322/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3323/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3324/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3325/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3326/4345]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3327/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3328/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3329/4345]: training_loss: tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3330/4345]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3331/4345]: training_loss: tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3332/4345]: training_loss: tensor(0.4879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3333/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3334/4345]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3335/4345]: training_loss: tensor(0.1458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3336/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3337/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3338/4345]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3339/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3340/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3341/4345]: training_loss: tensor(0.1436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3342/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3343/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3344/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3345/4345]: training_loss: tensor(0.1261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3346/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3347/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3348/4345]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3349/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3350/4345]: training_loss: tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3351/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3352/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3353/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3354/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3355/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3356/4345]: training_loss: tensor(0.1441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3357/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3358/4345]: training_loss: tensor(0.1103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3359/4345]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3360/4345]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3361/4345]: training_loss: tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3362/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3363/4345]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3364/4345]: training_loss: tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3365/4345]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3366/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3367/4345]: training_loss: tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3368/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3369/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3370/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3371/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3372/4345]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3373/4345]: training_loss: tensor(0.1463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3374/4345]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3375/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3376/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3377/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3378/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3379/4345]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3380/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3381/4345]: training_loss: tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3382/4345]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3383/4345]: training_loss: tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3384/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3385/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3386/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3387/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3388/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3389/4345]: training_loss: tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3390/4345]: training_loss: tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3391/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3392/4345]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3393/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3394/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3395/4345]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3396/4345]: training_loss: tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3397/4345]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3398/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3399/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3400/4345]: training_loss: tensor(0.0462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3401/4345]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3402/4345]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3403/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3404/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3405/4345]: training_loss: tensor(0.0541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3406/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3407/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3408/4345]: training_loss: tensor(0.1737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3409/4345]: training_loss: tensor(0.3602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3410/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3411/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3412/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3413/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3414/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3415/4345]: training_loss: tensor(0.1696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3416/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3417/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3418/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3419/4345]: training_loss: tensor(0.2494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3420/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3421/4345]: training_loss: tensor(0.1550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3422/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3423/4345]: training_loss: tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3424/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3425/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3426/4345]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3427/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3428/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3429/4345]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3430/4345]: training_loss: tensor(0.2558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3431/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3432/4345]: training_loss: tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3433/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3434/4345]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3435/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3436/4345]: training_loss: tensor(0.1582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3437/4345]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3438/4345]: training_loss: tensor(0.1987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3439/4345]: training_loss: tensor(0.1195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3440/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3441/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3442/4345]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3443/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3444/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3445/4345]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3446/4345]: training_loss: tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3447/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3448/4345]: training_loss: tensor(0.1595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3449/4345]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3450/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3451/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3452/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3453/4345]: training_loss: tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3454/4345]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3455/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3456/4345]: training_loss: tensor(0.1765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3457/4345]: training_loss: tensor(0.0640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3458/4345]: training_loss: tensor(0.1260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3459/4345]: training_loss: tensor(0.3330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3460/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3461/4345]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3462/4345]: training_loss: tensor(0.1486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3463/4345]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3464/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3465/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3466/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3467/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3468/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3469/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3470/4345]: training_loss: tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3471/4345]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3472/4345]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3473/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3474/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3475/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3476/4345]: training_loss: tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3477/4345]: training_loss: tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3478/4345]: training_loss: tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3479/4345]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3480/4345]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3481/4345]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3482/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3483/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3484/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3485/4345]: training_loss: tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3486/4345]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3487/4345]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3488/4345]: training_loss: tensor(0.1677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3489/4345]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3490/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3491/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3492/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3493/4345]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3494/4345]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3495/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3496/4345]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3497/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3498/4345]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3499/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3500/4345]: training_loss: tensor(0.1125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3501/4345]: training_loss: tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3502/4345]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3503/4345]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3504/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3505/4345]: training_loss: tensor(0.1429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3506/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3507/4345]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3508/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3509/4345]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3510/4345]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3511/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3512/4345]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3513/4345]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3514/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3515/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3516/4345]: training_loss: tensor(0.4121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3517/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3518/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3519/4345]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3520/4345]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3521/4345]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3522/4345]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3523/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3524/4345]: training_loss: tensor(0.2688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3525/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3526/4345]: training_loss: tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3527/4345]: training_loss: tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3528/4345]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3529/4345]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3530/4345]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3531/4345]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3532/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3533/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3534/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3535/4345]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3536/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3537/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3538/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3539/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3540/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3541/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3542/4345]: training_loss: tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3543/4345]: training_loss: tensor(0.2529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3544/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3545/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3546/4345]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3547/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3548/4345]: training_loss: tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3549/4345]: training_loss: tensor(0.1853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3550/4345]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3551/4345]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3552/4345]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3553/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3554/4345]: training_loss: tensor(0.1623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3555/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3556/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3557/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3558/4345]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3559/4345]: training_loss: tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3560/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3561/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3562/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3563/4345]: training_loss: tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3564/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3565/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3566/4345]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3567/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3568/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3569/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3570/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3571/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3572/4345]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3573/4345]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3574/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3575/4345]: training_loss: tensor(0.1744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3576/4345]: training_loss: tensor(0.2311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3577/4345]: training_loss: tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3578/4345]: training_loss: tensor(0.1355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3579/4345]: training_loss: tensor(0.2512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3580/4345]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3581/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3582/4345]: training_loss: tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3583/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3584/4345]: training_loss: tensor(0.1725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3585/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3586/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3587/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3588/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3589/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3590/4345]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3591/4345]: training_loss: tensor(0.2047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3592/4345]: training_loss: tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3593/4345]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3594/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3595/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3596/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3597/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3598/4345]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3599/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3600/4345]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3601/4345]: training_loss: tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3602/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3603/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3604/4345]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3605/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3606/4345]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3607/4345]: training_loss: tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3608/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3609/4345]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3610/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3611/4345]: training_loss: tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3612/4345]: training_loss: tensor(0.1923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3613/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3614/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3615/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3616/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3617/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3618/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3619/4345]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3620/4345]: training_loss: tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3621/4345]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3622/4345]: training_loss: tensor(0.1549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3623/4345]: training_loss: tensor(0.2139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3624/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3625/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3626/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3627/4345]: training_loss: tensor(0.2367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3628/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3629/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3630/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3631/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3632/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3633/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3634/4345]: training_loss: tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3635/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3636/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3637/4345]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3638/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3639/4345]: training_loss: tensor(0.1301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3640/4345]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3641/4345]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3642/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3643/4345]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3644/4345]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3645/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3646/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3647/4345]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3648/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3649/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3650/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3651/4345]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3652/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3653/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3654/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3655/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3656/4345]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3657/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3658/4345]: training_loss: tensor(0.1889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3659/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3660/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3661/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3662/4345]: training_loss: tensor(0.0541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3663/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3664/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3665/4345]: training_loss: tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3666/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3667/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3668/4345]: training_loss: tensor(0.2248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3669/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3670/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3671/4345]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3672/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3673/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3674/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3675/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3676/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3677/4345]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3678/4345]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3679/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3680/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3681/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3682/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3683/4345]: training_loss: tensor(0.1315, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3684/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3685/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3686/4345]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3687/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3688/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3689/4345]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3690/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3691/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3692/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3693/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3694/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3695/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3696/4345]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3697/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3698/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3699/4345]: training_loss: tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3700/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3701/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3702/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3703/4345]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3704/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3705/4345]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3706/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3707/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3708/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3709/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3710/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3711/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3712/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3713/4345]: training_loss: tensor(0.1578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3714/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3715/4345]: training_loss: tensor(0.1492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3716/4345]: training_loss: tensor(0.1550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3717/4345]: training_loss: tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3718/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3719/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3720/4345]: training_loss: tensor(0.2466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3721/4345]: training_loss: tensor(0.3591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3722/4345]: training_loss: tensor(0.1071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3723/4345]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3724/4345]: training_loss: tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3725/4345]: training_loss: tensor(0.2628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3726/4345]: training_loss: tensor(0.2690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3727/4345]: training_loss: tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3728/4345]: training_loss: tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3729/4345]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3730/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3731/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3732/4345]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3733/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3734/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3735/4345]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3736/4345]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3737/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3738/4345]: training_loss: tensor(0.2554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3739/4345]: training_loss: tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3740/4345]: training_loss: tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3741/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3742/4345]: training_loss: tensor(0.1421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3743/4345]: training_loss: tensor(0.1377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3744/4345]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3745/4345]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3746/4345]: training_loss: tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3747/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3748/4345]: training_loss: tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3749/4345]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3750/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3751/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3752/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3753/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3754/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3755/4345]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3756/4345]: training_loss: tensor(0.1753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3757/4345]: training_loss: tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3758/4345]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3759/4345]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3760/4345]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3761/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3762/4345]: training_loss: tensor(0.1329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3763/4345]: training_loss: tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3764/4345]: training_loss: tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3765/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3766/4345]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3767/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3768/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3769/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3770/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3771/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3772/4345]: training_loss: tensor(0.1917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3773/4345]: training_loss: tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3774/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3775/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3776/4345]: training_loss: tensor(0.1685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3777/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3778/4345]: training_loss: tensor(0.1664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3779/4345]: training_loss: tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3780/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3781/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3782/4345]: training_loss: tensor(0.1278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3783/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3784/4345]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3785/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3786/4345]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3787/4345]: training_loss: tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3788/4345]: training_loss: tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3789/4345]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3790/4345]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3791/4345]: training_loss: tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3792/4345]: training_loss: tensor(0.2350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3793/4345]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3794/4345]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3795/4345]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3796/4345]: training_loss: tensor(0.5176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3797/4345]: training_loss: tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3798/4345]: training_loss: tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3799/4345]: training_loss: tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3800/4345]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3801/4345]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3802/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3803/4345]: training_loss: tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3804/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3805/4345]: training_loss: tensor(0.3693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3806/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3807/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3808/4345]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3809/4345]: training_loss: tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3810/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3811/4345]: training_loss: tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3812/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3813/4345]: training_loss: tensor(0.3113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3814/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3815/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3816/4345]: training_loss: tensor(0.1793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3817/4345]: training_loss: tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3818/4345]: training_loss: tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3819/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3820/4345]: training_loss: tensor(0.1333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3821/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3822/4345]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3823/4345]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3824/4345]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3825/4345]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3826/4345]: training_loss: tensor(0.0543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3827/4345]: training_loss: tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3828/4345]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3829/4345]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3830/4345]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3831/4345]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3832/4345]: training_loss: tensor(0.0596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3833/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3834/4345]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3835/4345]: training_loss: tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3836/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3837/4345]: training_loss: tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3838/4345]: training_loss: tensor(0.0629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3839/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3840/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3841/4345]: training_loss: tensor(0.0342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3842/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3843/4345]: training_loss: tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3844/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3845/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3846/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3847/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3848/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3849/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3850/4345]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3851/4345]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3852/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3853/4345]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3854/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3855/4345]: training_loss: tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3856/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3857/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3858/4345]: training_loss: tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3859/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3860/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3861/4345]: training_loss: tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3862/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3863/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3864/4345]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3865/4345]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3866/4345]: training_loss: tensor(0.2315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3867/4345]: training_loss: tensor(0.1489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3868/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3869/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3870/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3871/4345]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3872/4345]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3873/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3874/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3875/4345]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3876/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3877/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3878/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3879/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3880/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3881/4345]: training_loss: tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3882/4345]: training_loss: tensor(0.2132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3883/4345]: training_loss: tensor(0.2850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3884/4345]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3885/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3886/4345]: training_loss: tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3887/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3888/4345]: training_loss: tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3889/4345]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3890/4345]: training_loss: tensor(0.0290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3891/4345]: training_loss: tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3892/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3893/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3894/4345]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3895/4345]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3896/4345]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3897/4345]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3898/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3899/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3900/4345]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3901/4345]: training_loss: tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3902/4345]: training_loss: tensor(0.0686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3903/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3904/4345]: training_loss: tensor(0.1520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3905/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3906/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3907/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3908/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3909/4345]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3910/4345]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3911/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3912/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3913/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3914/4345]: training_loss: tensor(0.3836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3915/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3916/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3917/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3918/4345]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3919/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3920/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3921/4345]: training_loss: tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3922/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3923/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3924/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3925/4345]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3926/4345]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3927/4345]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3928/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3929/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3930/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3931/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3932/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3933/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3934/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3935/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3936/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3937/4345]: training_loss: tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3938/4345]: training_loss: tensor(0.1549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3939/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3940/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3941/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3942/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3943/4345]: training_loss: tensor(0.1756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3944/4345]: training_loss: tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3945/4345]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3946/4345]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3947/4345]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3948/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3949/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3950/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3951/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3952/4345]: training_loss: tensor(0.5886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3953/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3954/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3955/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3956/4345]: training_loss: tensor(0.1154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3957/4345]: training_loss: tensor(0.0534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3958/4345]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3959/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3960/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3961/4345]: training_loss: tensor(0.1793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3962/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3963/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3964/4345]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3965/4345]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3966/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3967/4345]: training_loss: tensor(0.1095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3968/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3969/4345]: training_loss: tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3970/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3971/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3972/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3973/4345]: training_loss: tensor(0.2300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3974/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3975/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3976/4345]: training_loss: tensor(0.1445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3977/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3978/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3979/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3980/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3981/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3982/4345]: training_loss: tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3983/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3984/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3985/4345]: training_loss: tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3986/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3987/4345]: training_loss: tensor(0.1507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3988/4345]: training_loss: tensor(0.4049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3989/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3990/4345]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3991/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3992/4345]: training_loss: tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3993/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3994/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3995/4345]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3996/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3997/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3998/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3999/4345]: training_loss: tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4000/4345]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4001/4345]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4002/4345]: training_loss: tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4003/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4004/4345]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4005/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4006/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4007/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4008/4345]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4009/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4010/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4011/4345]: training_loss: tensor(0.1190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4012/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4013/4345]: training_loss: tensor(0.2190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4014/4345]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4015/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4016/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4017/4345]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4018/4345]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4019/4345]: training_loss: tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4020/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4021/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4022/4345]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4023/4345]: training_loss: tensor(0.1302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4024/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4025/4345]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4026/4345]: training_loss: tensor(0.1360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4027/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4028/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4029/4345]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4030/4345]: training_loss: tensor(0.2174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4031/4345]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4032/4345]: training_loss: tensor(0.0365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4033/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4034/4345]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4035/4345]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4036/4345]: training_loss: tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4037/4345]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4038/4345]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4039/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4040/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4041/4345]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4042/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4043/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4044/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4045/4345]: training_loss: tensor(0.3194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4046/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4047/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4048/4345]: training_loss: tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4049/4345]: training_loss: tensor(0.1292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4050/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4051/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4052/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4053/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4054/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4055/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4056/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4057/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4058/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4059/4345]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4060/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4061/4345]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4062/4345]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4063/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4064/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4065/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4066/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4067/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4068/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4069/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4070/4345]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4071/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4072/4345]: training_loss: tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4073/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4074/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4075/4345]: training_loss: tensor(0.1625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4076/4345]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4077/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4078/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4079/4345]: training_loss: tensor(0.1964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4080/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4081/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4082/4345]: training_loss: tensor(0.1637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4083/4345]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4084/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4085/4345]: training_loss: tensor(0.0570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4086/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4087/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4088/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4089/4345]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4090/4345]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4091/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4092/4345]: training_loss: tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4093/4345]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4094/4345]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4095/4345]: training_loss: tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4096/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4097/4345]: training_loss: tensor(0.1137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4098/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4099/4345]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4100/4345]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4101/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4102/4345]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4103/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4104/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4105/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4106/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4107/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4108/4345]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4109/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4110/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4111/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4112/4345]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4113/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4114/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4115/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4116/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4117/4345]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4118/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4119/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4120/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4121/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4122/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4123/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4124/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4125/4345]: training_loss: tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4126/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4127/4345]: training_loss: tensor(0.3471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4128/4345]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4129/4345]: training_loss: tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4130/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4131/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4132/4345]: training_loss: tensor(0.1523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4133/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4134/4345]: training_loss: tensor(0.1473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4135/4345]: training_loss: tensor(0.0365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4136/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4137/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4138/4345]: training_loss: tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4139/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4140/4345]: training_loss: tensor(0.1896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4141/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4142/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4143/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4144/4345]: training_loss: tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4145/4345]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4146/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4147/4345]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4148/4345]: training_loss: tensor(0.2874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4149/4345]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4150/4345]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4151/4345]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4152/4345]: training_loss: tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4153/4345]: training_loss: tensor(0.1257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4154/4345]: training_loss: tensor(0.1106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4155/4345]: training_loss: tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4156/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4157/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4158/4345]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4159/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4160/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4161/4345]: training_loss: tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4162/4345]: training_loss: tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4163/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4164/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4165/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4166/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4167/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4168/4345]: training_loss: tensor(0.1263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4169/4345]: training_loss: tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4170/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4171/4345]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4172/4345]: training_loss: tensor(0.4958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4173/4345]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4174/4345]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4175/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4176/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4177/4345]: training_loss: tensor(0.1244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4178/4345]: training_loss: tensor(0.0635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4179/4345]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4180/4345]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4181/4345]: training_loss: tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4182/4345]: training_loss: tensor(0.2366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4183/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4184/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4185/4345]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4186/4345]: training_loss: tensor(0.2696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4187/4345]: training_loss: tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4188/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4189/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4190/4345]: training_loss: tensor(0.1113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4191/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4192/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4193/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4194/4345]: training_loss: tensor(0.3021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4195/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4196/4345]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4197/4345]: training_loss: tensor(0.1267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4198/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4199/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4200/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4201/4345]: training_loss: tensor(0.1403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4202/4345]: training_loss: tensor(0.1622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4203/4345]: training_loss: tensor(0.2847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4204/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4205/4345]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4206/4345]: training_loss: tensor(0.1180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4207/4345]: training_loss: tensor(0.1827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4208/4345]: training_loss: tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4209/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4210/4345]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4211/4345]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4212/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4213/4345]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4214/4345]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4215/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4216/4345]: training_loss: tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4217/4345]: training_loss: tensor(0.0486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4218/4345]: training_loss: tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4219/4345]: training_loss: tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4220/4345]: training_loss: tensor(0.1114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4221/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4222/4345]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4223/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4224/4345]: training_loss: tensor(0.1459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4225/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4226/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4227/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4228/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4229/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4230/4345]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4231/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4232/4345]: training_loss: tensor(0.2516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4233/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4234/4345]: training_loss: tensor(0.0389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4235/4345]: training_loss: tensor(0.2753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4236/4345]: training_loss: tensor(0.0596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4237/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4238/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4239/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4240/4345]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4241/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4242/4345]: training_loss: tensor(0.2265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4243/4345]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4244/4345]: training_loss: tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4245/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4246/4345]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4247/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4248/4345]: training_loss: tensor(0.1559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4249/4345]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4250/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4251/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4252/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4253/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4254/4345]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4255/4345]: training_loss: tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4256/4345]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4257/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4258/4345]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4259/4345]: training_loss: tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4260/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4261/4345]: training_loss: tensor(0.1838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4262/4345]: training_loss: tensor(0.1825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4263/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4264/4345]: training_loss: tensor(0.2717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4265/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4266/4345]: training_loss: tensor(0.1628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4267/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4268/4345]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4269/4345]: training_loss: tensor(0.3869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4270/4345]: training_loss: tensor(0.1623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4271/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4272/4345]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4273/4345]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4274/4345]: training_loss: tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4275/4345]: training_loss: tensor(0.0445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4276/4345]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4277/4345]: training_loss: tensor(0.0502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4278/4345]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4279/4345]: training_loss: tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4280/4345]: training_loss: tensor(0.0505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4281/4345]: training_loss: tensor(0.1570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4282/4345]: training_loss: tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4283/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4284/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4285/4345]: training_loss: tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4286/4345]: training_loss: tensor(0.1499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4287/4345]: training_loss: tensor(0.1472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4288/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4289/4345]: training_loss: tensor(0.1325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4290/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4291/4345]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4292/4345]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4293/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4294/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4295/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4296/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4297/4345]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4298/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4299/4345]: training_loss: tensor(0.1123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4300/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4301/4345]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4302/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4303/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4304/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4305/4345]: training_loss: tensor(0.2929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4306/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4307/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4308/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4309/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4310/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4311/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4312/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4313/4345]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4314/4345]: training_loss: tensor(0.3716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4315/4345]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4316/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4317/4345]: training_loss: tensor(0.3715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4318/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4319/4345]: training_loss: tensor(0.2159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4320/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4321/4345]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4322/4345]: training_loss: tensor(0.2152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4323/4345]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4324/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4325/4345]: training_loss: tensor(0.1118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4326/4345]: training_loss: tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4327/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4328/4345]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4329/4345]: training_loss: tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4330/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4331/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4332/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4333/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4334/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4335/4345]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4336/4345]: training_loss: tensor(0.0650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4337/4345]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4338/4345]: training_loss: tensor(0.0475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4339/4345]: training_loss: tensor(0.1660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4340/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4341/4345]: training_loss: tensor(0.1792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4342/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4343/4345]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4344/4345]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4345/4345]: training_loss: tensor(0.2030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4346/4345]: training_loss: tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [4/5], Step [17384/21730], Train Loss: 0.0388, Valid Loss: 0.1786\n",
      "batch_no [1/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4/4345]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [5/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [6/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [7/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [8/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [9/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [10/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [11/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [12/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [13/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [14/4345]: training_loss: tensor(0.1683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [15/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [16/4345]: training_loss: tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [17/4345]: training_loss: tensor(0.1957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [18/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [19/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [20/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [21/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [22/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [23/4345]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [24/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [25/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [26/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [27/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [28/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [29/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [30/4345]: training_loss: tensor(0.2056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [31/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [32/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [33/4345]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [34/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [35/4345]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [36/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [37/4345]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [38/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [39/4345]: training_loss: tensor(0.1862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [40/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [41/4345]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [42/4345]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [43/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [44/4345]: training_loss: tensor(0.0650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [45/4345]: training_loss: tensor(0.1397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [46/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [47/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [48/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [49/4345]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [50/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [51/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [52/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [53/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [54/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [55/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [56/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [57/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [58/4345]: training_loss: tensor(0.2170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [59/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [60/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [61/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [62/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [63/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [64/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [65/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [66/4345]: training_loss: tensor(0.3146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [67/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [68/4345]: training_loss: tensor(0.5107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [69/4345]: training_loss: tensor(0.1704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [70/4345]: training_loss: tensor(0.2209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [71/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [72/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [73/4345]: training_loss: tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [74/4345]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [75/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [76/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [77/4345]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [78/4345]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [79/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [80/4345]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [81/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [82/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [83/4345]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [84/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [85/4345]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [86/4345]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [87/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [88/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [89/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [90/4345]: training_loss: tensor(0.1924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [91/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [92/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [93/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [94/4345]: training_loss: tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [95/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [96/4345]: training_loss: tensor(0.1443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [97/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [98/4345]: training_loss: tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [99/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [100/4345]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [101/4345]: training_loss: tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [102/4345]: training_loss: tensor(0.3623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [103/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [104/4345]: training_loss: tensor(0.1280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [105/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [106/4345]: training_loss: tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [107/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [108/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [109/4345]: training_loss: tensor(0.0362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [110/4345]: training_loss: tensor(0.0459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [111/4345]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [112/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [113/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [114/4345]: training_loss: tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [115/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [116/4345]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [117/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [118/4345]: training_loss: tensor(0.2420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [119/4345]: training_loss: tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [120/4345]: training_loss: tensor(0.0413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [121/4345]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [122/4345]: training_loss: tensor(0.0572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [123/4345]: training_loss: tensor(0.2763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [124/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [125/4345]: training_loss: tensor(0.1378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [126/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [127/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [128/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [129/4345]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [130/4345]: training_loss: tensor(0.0468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [131/4345]: training_loss: tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [132/4345]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [133/4345]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [134/4345]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [135/4345]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [136/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [137/4345]: training_loss: tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [138/4345]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [139/4345]: training_loss: tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [140/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [141/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [142/4345]: training_loss: tensor(0.2289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [143/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [144/4345]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [145/4345]: training_loss: tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [146/4345]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [147/4345]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [148/4345]: training_loss: tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [149/4345]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [150/4345]: training_loss: tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [151/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [152/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [153/4345]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [154/4345]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [155/4345]: training_loss: tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [156/4345]: training_loss: tensor(0.1629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [157/4345]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [158/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [159/4345]: training_loss: tensor(0.2163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [160/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [161/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [162/4345]: training_loss: tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [163/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [164/4345]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [165/4345]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [166/4345]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [167/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [168/4345]: training_loss: tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [169/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [170/4345]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [171/4345]: training_loss: tensor(0.2837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [172/4345]: training_loss: tensor(0.1529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [173/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [174/4345]: training_loss: tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [175/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [176/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [177/4345]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [178/4345]: training_loss: tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [179/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [180/4345]: training_loss: tensor(0.1789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [181/4345]: training_loss: tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [182/4345]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [183/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [184/4345]: training_loss: tensor(0.2296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [185/4345]: training_loss: tensor(0.0606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [186/4345]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [187/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [188/4345]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [189/4345]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [190/4345]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [191/4345]: training_loss: tensor(0.1242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [192/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [193/4345]: training_loss: tensor(0.1955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [194/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [195/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [196/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [197/4345]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [198/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [199/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [200/4345]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [201/4345]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [202/4345]: training_loss: tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [203/4345]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [204/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [205/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [206/4345]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [207/4345]: training_loss: tensor(0.1414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [208/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [209/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [210/4345]: training_loss: tensor(0.1449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [211/4345]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [212/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [213/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [214/4345]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [215/4345]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [216/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [217/4345]: training_loss: tensor(0.1493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [218/4345]: training_loss: tensor(0.1391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [219/4345]: training_loss: tensor(0.1491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [220/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [221/4345]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [222/4345]: training_loss: tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [223/4345]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [224/4345]: training_loss: tensor(0.1994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [225/4345]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [226/4345]: training_loss: tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [227/4345]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [228/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [229/4345]: training_loss: tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [230/4345]: training_loss: tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [231/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [232/4345]: training_loss: tensor(0.3638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [233/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [234/4345]: training_loss: tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [235/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [236/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [237/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [238/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [239/4345]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [240/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [241/4345]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [242/4345]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [243/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [244/4345]: training_loss: tensor(0.1416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [245/4345]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [246/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [247/4345]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [248/4345]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [249/4345]: training_loss: tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [250/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [251/4345]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [252/4345]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [253/4345]: training_loss: tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [254/4345]: training_loss: tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [255/4345]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [256/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [257/4345]: training_loss: tensor(0.0487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [258/4345]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [259/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [260/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [261/4345]: training_loss: tensor(0.1460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [262/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [263/4345]: training_loss: tensor(0.2464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [264/4345]: training_loss: tensor(0.1966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [265/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [266/4345]: training_loss: tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [267/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [268/4345]: training_loss: tensor(0.0290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [269/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [270/4345]: training_loss: tensor(0.2116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [271/4345]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [272/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [273/4345]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [274/4345]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [275/4345]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [276/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [277/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [278/4345]: training_loss: tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [279/4345]: training_loss: tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [280/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [281/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [282/4345]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [283/4345]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [284/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [285/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [286/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [287/4345]: training_loss: tensor(0.2677, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [288/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [289/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [290/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [291/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [292/4345]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [293/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [294/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [295/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [296/4345]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [297/4345]: training_loss: tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [298/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [299/4345]: training_loss: tensor(0.1743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [300/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [301/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [302/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [303/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [304/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [305/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [306/4345]: training_loss: tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [307/4345]: training_loss: tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [308/4345]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [309/4345]: training_loss: tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [310/4345]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [311/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [312/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [313/4345]: training_loss: tensor(0.1762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [314/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [315/4345]: training_loss: tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [316/4345]: training_loss: tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [317/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [318/4345]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [319/4345]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [320/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [321/4345]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [322/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [323/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [324/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [325/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [326/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [327/4345]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [328/4345]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [329/4345]: training_loss: tensor(0.2528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [330/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [331/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [332/4345]: training_loss: tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [333/4345]: training_loss: tensor(0.1913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [334/4345]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [335/4345]: training_loss: tensor(0.0576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [336/4345]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [337/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [338/4345]: training_loss: tensor(0.2251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [339/4345]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [340/4345]: training_loss: tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [341/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [342/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [343/4345]: training_loss: tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [344/4345]: training_loss: tensor(0.1515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [345/4345]: training_loss: tensor(0.1943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [346/4345]: training_loss: tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [347/4345]: training_loss: tensor(0.2207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [348/4345]: training_loss: tensor(0.1526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [349/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [350/4345]: training_loss: tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [351/4345]: training_loss: tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [352/4345]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [353/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [354/4345]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [355/4345]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [356/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [357/4345]: training_loss: tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [358/4345]: training_loss: tensor(0.1251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [359/4345]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [360/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [361/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [362/4345]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [363/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [364/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [365/4345]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [366/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [367/4345]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [368/4345]: training_loss: tensor(0.0354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [369/4345]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [370/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [371/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [372/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [373/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [374/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [375/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [376/4345]: training_loss: tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [377/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [378/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [379/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [380/4345]: training_loss: tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [381/4345]: training_loss: tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [382/4345]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [383/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [384/4345]: training_loss: tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [385/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [386/4345]: training_loss: tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [387/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [388/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [389/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [390/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [391/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [392/4345]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [393/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [394/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [395/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [396/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [397/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [398/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [399/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [400/4345]: training_loss: tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [401/4345]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [402/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [403/4345]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [404/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [405/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [406/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [407/4345]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [408/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [409/4345]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [410/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [411/4345]: training_loss: tensor(0.1309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [412/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [413/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [414/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [415/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [416/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [417/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [418/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [419/4345]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [420/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [421/4345]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [422/4345]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [423/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [424/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [425/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [426/4345]: training_loss: tensor(0.1904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [427/4345]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [428/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [429/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [430/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [431/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [432/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [433/4345]: training_loss: tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [434/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [435/4345]: training_loss: tensor(0.2527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [436/4345]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [437/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [438/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [439/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [440/4345]: training_loss: tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [441/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [442/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [443/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [444/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [445/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [446/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [447/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [448/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [449/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [450/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [451/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [452/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [453/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [454/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [455/4345]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [456/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [457/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [458/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [459/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [460/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [461/4345]: training_loss: tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [462/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [463/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [464/4345]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [465/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [466/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [467/4345]: training_loss: tensor(0.0629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [468/4345]: training_loss: tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [469/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [470/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [471/4345]: training_loss: tensor(0.1225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [472/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [473/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [474/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [475/4345]: training_loss: tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [476/4345]: training_loss: tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [477/4345]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [478/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [479/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [480/4345]: training_loss: tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [481/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [482/4345]: training_loss: tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [483/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [484/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [485/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [486/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [487/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [488/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [489/4345]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [490/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [491/4345]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [492/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [493/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [494/4345]: training_loss: tensor(0.1116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [495/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [496/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [497/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [498/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [499/4345]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [500/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [501/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [502/4345]: training_loss: tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [503/4345]: training_loss: tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [504/4345]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [505/4345]: training_loss: tensor(0.2232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [506/4345]: training_loss: tensor(0.0637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [507/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [508/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [509/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [510/4345]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [511/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [512/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [513/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [514/4345]: training_loss: tensor(0.2791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [515/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [516/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [517/4345]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [518/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [519/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [520/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [521/4345]: training_loss: tensor(0.1406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [522/4345]: training_loss: tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [523/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [524/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [525/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [526/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [527/4345]: training_loss: tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [528/4345]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [529/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [530/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [531/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [532/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [533/4345]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [534/4345]: training_loss: tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [535/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [536/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [537/4345]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [538/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [539/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [540/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [541/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [542/4345]: training_loss: tensor(0.1481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [543/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [544/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [545/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [546/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [547/4345]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [548/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [549/4345]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [550/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [551/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [552/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [553/4345]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [554/4345]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [555/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [556/4345]: training_loss: tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [557/4345]: training_loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [558/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [559/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [560/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [561/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [562/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [563/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [564/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [565/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [566/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [567/4345]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [568/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [569/4345]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [570/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [571/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [572/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [573/4345]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [574/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [575/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [576/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [577/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [578/4345]: training_loss: tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [579/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [580/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [581/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [582/4345]: training_loss: tensor(0.1658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [583/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [584/4345]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [585/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [586/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [587/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [588/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [589/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [590/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [591/4345]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [592/4345]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [593/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [594/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [595/4345]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [596/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [597/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [598/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [599/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [600/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [601/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [602/4345]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [603/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [604/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [605/4345]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [606/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [607/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [608/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [609/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [610/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [611/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [612/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [613/4345]: training_loss: tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [614/4345]: training_loss: tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [615/4345]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [616/4345]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [617/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [618/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [619/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [620/4345]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [621/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [622/4345]: training_loss: tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [623/4345]: training_loss: tensor(0.5514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [624/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [625/4345]: training_loss: tensor(0.1147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [626/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [627/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [628/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [629/4345]: training_loss: tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [630/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [631/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [632/4345]: training_loss: tensor(0.3714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [633/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [634/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [635/4345]: training_loss: tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [636/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [637/4345]: training_loss: tensor(0.1607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [638/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [639/4345]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [640/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [641/4345]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [642/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [643/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [644/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [645/4345]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [646/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [647/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [648/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [649/4345]: training_loss: tensor(0.1226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [650/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [651/4345]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [652/4345]: training_loss: tensor(0.2075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [653/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [654/4345]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [655/4345]: training_loss: tensor(0.1750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [656/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [657/4345]: training_loss: tensor(0.1282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [658/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [659/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [660/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [661/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [662/4345]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [663/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [664/4345]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [665/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [666/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [667/4345]: training_loss: tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [668/4345]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [669/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [670/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [671/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [672/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [673/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [674/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [675/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [676/4345]: training_loss: tensor(0.2620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [677/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [678/4345]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [679/4345]: training_loss: tensor(0.1833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [680/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [681/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [682/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [683/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [684/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [685/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [686/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [687/4345]: training_loss: tensor(0.4037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [688/4345]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [689/4345]: training_loss: tensor(0.1687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [690/4345]: training_loss: tensor(0.1819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [691/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [692/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [693/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [694/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [695/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [696/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [697/4345]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [698/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [699/4345]: training_loss: tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [700/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [701/4345]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [702/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [703/4345]: training_loss: tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [704/4345]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [705/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [706/4345]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [707/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [708/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [709/4345]: training_loss: tensor(0.0362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [710/4345]: training_loss: tensor(0.3595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [711/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [712/4345]: training_loss: tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [713/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [714/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [715/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [716/4345]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [717/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [718/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [719/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [720/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [721/4345]: training_loss: tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [722/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [723/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [724/4345]: training_loss: tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [725/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [726/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [727/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [728/4345]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [729/4345]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [730/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [731/4345]: training_loss: tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [732/4345]: training_loss: tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [733/4345]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [734/4345]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [735/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [736/4345]: training_loss: tensor(0.1680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [737/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [738/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [739/4345]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [740/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [741/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [742/4345]: training_loss: tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [743/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [744/4345]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [745/4345]: training_loss: tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [746/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [747/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [748/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [749/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [750/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [751/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [752/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [753/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [754/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [755/4345]: training_loss: tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [756/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [757/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [758/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [759/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [760/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [761/4345]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [762/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [763/4345]: training_loss: tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [764/4345]: training_loss: tensor(0.4066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [765/4345]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [766/4345]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [767/4345]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [768/4345]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [769/4345]: training_loss: tensor(0.1194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [770/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [771/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [772/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [773/4345]: training_loss: tensor(0.5177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [774/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [775/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [776/4345]: training_loss: tensor(0.2948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [777/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [778/4345]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [779/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [780/4345]: training_loss: tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [781/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [782/4345]: training_loss: tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [783/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [784/4345]: training_loss: tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [785/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [786/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [787/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [788/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [789/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [790/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [791/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [792/4345]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [793/4345]: training_loss: tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [794/4345]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [795/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [796/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [797/4345]: training_loss: tensor(0.1855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [798/4345]: training_loss: tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [799/4345]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [800/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [801/4345]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [802/4345]: training_loss: tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [803/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [804/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [805/4345]: training_loss: tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [806/4345]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [807/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [808/4345]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [809/4345]: training_loss: tensor(0.0468, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [810/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [811/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [812/4345]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [813/4345]: training_loss: tensor(0.1919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [814/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [815/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [816/4345]: training_loss: tensor(0.1815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [817/4345]: training_loss: tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [818/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [819/4345]: training_loss: tensor(0.0621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [820/4345]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [821/4345]: training_loss: tensor(0.2173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [822/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [823/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [824/4345]: training_loss: tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [825/4345]: training_loss: tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [826/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [827/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [828/4345]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [829/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [830/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [831/4345]: training_loss: tensor(0.2882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [832/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [833/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [834/4345]: training_loss: tensor(0.2694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [835/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [836/4345]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [837/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [838/4345]: training_loss: tensor(0.0603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [839/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [840/4345]: training_loss: tensor(0.1806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [841/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [842/4345]: training_loss: tensor(0.0560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [843/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [844/4345]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [845/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [846/4345]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [847/4345]: training_loss: tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [848/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [849/4345]: training_loss: tensor(0.0427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [850/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [851/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [852/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [853/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [854/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [855/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [856/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [857/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [858/4345]: training_loss: tensor(0.2784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [859/4345]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [860/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [861/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [862/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [863/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [864/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [865/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [866/4345]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [867/4345]: training_loss: tensor(0.1644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [868/4345]: training_loss: tensor(0.1349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [869/4345]: training_loss: tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [870/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [871/4345]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [872/4345]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [873/4345]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [874/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [875/4345]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [876/4345]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [877/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [878/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [879/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [880/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [881/4345]: training_loss: tensor(0.2619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [882/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [883/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [884/4345]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [885/4345]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [886/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [887/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [888/4345]: training_loss: tensor(0.4828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [889/4345]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [890/4345]: training_loss: tensor(0.0611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [891/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [892/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [893/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [894/4345]: training_loss: tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [895/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [896/4345]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [897/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [898/4345]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [899/4345]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [900/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [901/4345]: training_loss: tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [902/4345]: training_loss: tensor(0.1189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [903/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [904/4345]: training_loss: tensor(0.2833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [905/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [906/4345]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [907/4345]: training_loss: tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [908/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [909/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [910/4345]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [911/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [912/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [913/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [914/4345]: training_loss: tensor(0.1405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [915/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [916/4345]: training_loss: tensor(0.4127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [917/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [918/4345]: training_loss: tensor(0.1630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [919/4345]: training_loss: tensor(0.0505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [920/4345]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [921/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [922/4345]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [923/4345]: training_loss: tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [924/4345]: training_loss: tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [925/4345]: training_loss: tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [926/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [927/4345]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [928/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [929/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [930/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [931/4345]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [932/4345]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [933/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [934/4345]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [935/4345]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [936/4345]: training_loss: tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [937/4345]: training_loss: tensor(0.1959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [938/4345]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [939/4345]: training_loss: tensor(0.1117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [940/4345]: training_loss: tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [941/4345]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [942/4345]: training_loss: tensor(0.2204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [943/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [944/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [945/4345]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [946/4345]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [947/4345]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [948/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [949/4345]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [950/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [951/4345]: training_loss: tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [952/4345]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [953/4345]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [954/4345]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [955/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [956/4345]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [957/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [958/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [959/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [960/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [961/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [962/4345]: training_loss: tensor(0.2596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [963/4345]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [964/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [965/4345]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [966/4345]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [967/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [968/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [969/4345]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [970/4345]: training_loss: tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [971/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [972/4345]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [973/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [974/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [975/4345]: training_loss: tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [976/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [977/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [978/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [979/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [980/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [981/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [982/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [983/4345]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [984/4345]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [985/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [986/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [987/4345]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [988/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [989/4345]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [990/4345]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [991/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [992/4345]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [993/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [994/4345]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [995/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [996/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [997/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [998/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [999/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1000/4345]: training_loss: tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1001/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1002/4345]: training_loss: tensor(0.0305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1003/4345]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1004/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1005/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/4345]: training_loss: tensor(0.3026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/4345]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/4345]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/4345]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/4345]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1019/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/4345]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1023/4345]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/4345]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/4345]: training_loss: tensor(0.1672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1040/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1043/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/4345]: training_loss: tensor(0.2408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/4345]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/4345]: training_loss: tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/4345]: training_loss: tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/4345]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/4345]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/4345]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/4345]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1070/4345]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/4345]: training_loss: tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/4345]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/4345]: training_loss: tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/4345]: training_loss: tensor(0.0594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/4345]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/4345]: training_loss: tensor(0.0492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1100/4345]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/4345]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/4345]: training_loss: tensor(0.2627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/4345]: training_loss: tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/4345]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/4345]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/4345]: training_loss: tensor(0.1703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/4345]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/4345]: training_loss: tensor(0.0436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/4345]: training_loss: tensor(0.3192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1129/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/4345]: training_loss: tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/4345]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/4345]: training_loss: tensor(0.2642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/4345]: training_loss: tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/4345]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/4345]: training_loss: tensor(0.1409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/4345]: training_loss: tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/4345]: training_loss: tensor(0.1411, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1156/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/4345]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/4345]: training_loss: tensor(0.1779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/4345]: training_loss: tensor(0.2914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/4345]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/4345]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/4345]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/4345]: training_loss: tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/4345]: training_loss: tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/4345]: training_loss: tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/4345]: training_loss: tensor(0.1705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/4345]: training_loss: tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/4345]: training_loss: tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/4345]: training_loss: tensor(0.2188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/4345]: training_loss: tensor(0.2046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/4345]: training_loss: tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/4345]: training_loss: tensor(0.1452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/4345]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/4345]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/4345]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/4345]: training_loss: tensor(0.1748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/4345]: training_loss: tensor(0.1165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/4345]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1213/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1215/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/4345]: training_loss: tensor(0.1338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/4345]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/4345]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/4345]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/4345]: training_loss: tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1242/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/4345]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/4345]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/4345]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/4345]: training_loss: tensor(0.1842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/4345]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/4345]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/4345]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/4345]: training_loss: tensor(0.3908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/4345]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/4345]: training_loss: tensor(0.2486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/4345]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/4345]: training_loss: tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/4345]: training_loss: tensor(0.1560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/4345]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/4345]: training_loss: tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/4345]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/4345]: training_loss: tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1299/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/4345]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/4345]: training_loss: tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/4345]: training_loss: tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/4345]: training_loss: tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/4345]: training_loss: tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/4345]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/4345]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/4345]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/4345]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1328/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/4345]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/4345]: training_loss: tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/4345]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/4345]: training_loss: tensor(0.2027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/4345]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/4345]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/4345]: training_loss: tensor(0.1472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/4345]: training_loss: tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1361/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1363/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/4345]: training_loss: tensor(0.1714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/4345]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/4345]: training_loss: tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/4345]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/4345]: training_loss: tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1385/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1386/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1387/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1388/4345]: training_loss: tensor(0.1380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1389/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1390/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1391/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1392/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1393/4345]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1394/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1395/4345]: training_loss: tensor(0.1308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1396/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1397/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1398/4345]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1399/4345]: training_loss: tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1400/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1401/4345]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1402/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1403/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1404/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1405/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1406/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1407/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1408/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1409/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1410/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1411/4345]: training_loss: tensor(0.1338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1412/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1413/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1414/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1415/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1416/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1417/4345]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1418/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1419/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1420/4345]: training_loss: tensor(0.4510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1421/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1422/4345]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1423/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1424/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1425/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1426/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1427/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1428/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1429/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1430/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1431/4345]: training_loss: tensor(0.1206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1432/4345]: training_loss: tensor(0.3712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1433/4345]: training_loss: tensor(0.1262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1434/4345]: training_loss: tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1435/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1436/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1437/4345]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1438/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1439/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1440/4345]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1441/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1442/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1443/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1444/4345]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1445/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1446/4345]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1447/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1448/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1449/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1450/4345]: training_loss: tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1451/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1452/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1453/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1454/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1455/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1456/4345]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1457/4345]: training_loss: tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1458/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1459/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1460/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1461/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1462/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1463/4345]: training_loss: tensor(0.1870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1464/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1465/4345]: training_loss: tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1466/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1467/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1468/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1469/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1470/4345]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1471/4345]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1472/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1473/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1474/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1475/4345]: training_loss: tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1476/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1477/4345]: training_loss: tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1478/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1479/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1480/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1481/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1482/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1483/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1484/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1485/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1486/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1487/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1488/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1489/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1490/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1491/4345]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1492/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1493/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1494/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1495/4345]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1496/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1497/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1498/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1499/4345]: training_loss: tensor(0.4019, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1500/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1501/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1502/4345]: training_loss: tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1503/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1504/4345]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1505/4345]: training_loss: tensor(0.1856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1506/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1507/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1508/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1509/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1510/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1511/4345]: training_loss: tensor(0.1616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1512/4345]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1513/4345]: training_loss: tensor(0.0635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1514/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1515/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1516/4345]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1517/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1518/4345]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1519/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1520/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1521/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1522/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1523/4345]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1524/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1525/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1526/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1527/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1528/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1529/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1530/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1531/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1532/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1533/4345]: training_loss: tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1534/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1535/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1536/4345]: training_loss: tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1537/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1538/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1539/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1540/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1541/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1542/4345]: training_loss: tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1543/4345]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1544/4345]: training_loss: tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1545/4345]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1546/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1547/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1548/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1549/4345]: training_loss: tensor(0.1210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1550/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1551/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1552/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1553/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1554/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1555/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1556/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1557/4345]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1558/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1559/4345]: training_loss: tensor(0.3010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1560/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1561/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1562/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1563/4345]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1564/4345]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1565/4345]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1566/4345]: training_loss: tensor(0.2047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1567/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1568/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1569/4345]: training_loss: tensor(0.5950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1570/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1571/4345]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1572/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1573/4345]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1574/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1575/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1576/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1577/4345]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1578/4345]: training_loss: tensor(0.0429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1579/4345]: training_loss: tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1580/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1581/4345]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1582/4345]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1583/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1584/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1585/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1586/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1587/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1588/4345]: training_loss: tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1589/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1590/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1591/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1592/4345]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1593/4345]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1594/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1595/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1596/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1597/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1598/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1599/4345]: training_loss: tensor(0.4217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1600/4345]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1601/4345]: training_loss: tensor(0.1548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1602/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1603/4345]: training_loss: tensor(0.0637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1604/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1605/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1606/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1607/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1608/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1609/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1610/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1611/4345]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1612/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1613/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1614/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1615/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1616/4345]: training_loss: tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1617/4345]: training_loss: tensor(0.2636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1618/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1619/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1620/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1621/4345]: training_loss: tensor(0.0576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1622/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1623/4345]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1624/4345]: training_loss: tensor(0.1258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1625/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1626/4345]: training_loss: tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1627/4345]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1628/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1629/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1630/4345]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1631/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1632/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1633/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1634/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1635/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1636/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1637/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1638/4345]: training_loss: tensor(0.2841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1639/4345]: training_loss: tensor(0.2481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1640/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1641/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1642/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1643/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1644/4345]: training_loss: tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1645/4345]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1646/4345]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1647/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1648/4345]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1649/4345]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1650/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1651/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1652/4345]: training_loss: tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1653/4345]: training_loss: tensor(0.1295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1654/4345]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1655/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1656/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1657/4345]: training_loss: tensor(0.0429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1658/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1659/4345]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1660/4345]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1661/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1662/4345]: training_loss: tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1663/4345]: training_loss: tensor(0.1843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1664/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1665/4345]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1666/4345]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1667/4345]: training_loss: tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1668/4345]: training_loss: tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1669/4345]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1670/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1671/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1672/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1673/4345]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1674/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1675/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1676/4345]: training_loss: tensor(0.4012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1677/4345]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1678/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1679/4345]: training_loss: tensor(0.1510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1680/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1681/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1682/4345]: training_loss: tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1683/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1684/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1685/4345]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1686/4345]: training_loss: tensor(0.2205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1687/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1688/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1689/4345]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1690/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1691/4345]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1692/4345]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1693/4345]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1694/4345]: training_loss: tensor(0.1692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1695/4345]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1696/4345]: training_loss: tensor(0.1298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1697/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1698/4345]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1699/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1700/4345]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1701/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1702/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1703/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1704/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1705/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1706/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1707/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1708/4345]: training_loss: tensor(0.2535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1709/4345]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1710/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1711/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1712/4345]: training_loss: tensor(0.1205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1713/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1714/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1715/4345]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1716/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1717/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1718/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1719/4345]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1720/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1721/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1722/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1723/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1724/4345]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1725/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1726/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1727/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1728/4345]: training_loss: tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1729/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1730/4345]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1731/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1732/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1733/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1734/4345]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1735/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1736/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1737/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1738/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1739/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1740/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1741/4345]: training_loss: tensor(0.0495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1742/4345]: training_loss: tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1743/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1744/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1745/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1746/4345]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1747/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1748/4345]: training_loss: tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1749/4345]: training_loss: tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1750/4345]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1751/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1752/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1753/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1754/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1755/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1756/4345]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1757/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1758/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1759/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1760/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1761/4345]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1762/4345]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1763/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1764/4345]: training_loss: tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1765/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1766/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1767/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1768/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1769/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1770/4345]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1771/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1772/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1773/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1774/4345]: training_loss: tensor(0.2161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1775/4345]: training_loss: tensor(0.2170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1776/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1777/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1778/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1779/4345]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1780/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1781/4345]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1782/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1783/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1784/4345]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1785/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1786/4345]: training_loss: tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1787/4345]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1788/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1789/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1790/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1791/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1792/4345]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1793/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1794/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1795/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1796/4345]: training_loss: tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1797/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1798/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1799/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1800/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1801/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1802/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1803/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1804/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1805/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1806/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1807/4345]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1808/4345]: training_loss: tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1809/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1810/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1811/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1812/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1813/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1814/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1815/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1816/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1817/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1818/4345]: training_loss: tensor(0.1544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1819/4345]: training_loss: tensor(0.2069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1820/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1821/4345]: training_loss: tensor(0.1775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1822/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1823/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1824/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1825/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1826/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1827/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1828/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1829/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1830/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1831/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1832/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1833/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1834/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1835/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1836/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1837/4345]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1838/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1839/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1840/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1841/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1842/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1843/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1844/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1845/4345]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1846/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1847/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1848/4345]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1849/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1850/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1851/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1852/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1853/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1854/4345]: training_loss: tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1855/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1856/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1857/4345]: training_loss: tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1858/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1859/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1860/4345]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1861/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1862/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1863/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1864/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1865/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1866/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1867/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1868/4345]: training_loss: tensor(0.0290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1869/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1870/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1871/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1872/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1873/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1874/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1875/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1876/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1877/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1878/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1879/4345]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1880/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1881/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1882/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1883/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1884/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1885/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1886/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1887/4345]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1888/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1889/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1890/4345]: training_loss: tensor(0.1938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1891/4345]: training_loss: tensor(0.1556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1892/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1893/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1894/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1895/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1896/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1897/4345]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1898/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1899/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1900/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1901/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1902/4345]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1903/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1904/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1905/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1906/4345]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1907/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1908/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1909/4345]: training_loss: tensor(0.1682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1910/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1911/4345]: training_loss: tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1912/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1913/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1914/4345]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1915/4345]: training_loss: tensor(0.2283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1916/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1917/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1918/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1919/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1920/4345]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1921/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1922/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1923/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1924/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1925/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1926/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1927/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1928/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1929/4345]: training_loss: tensor(0.2487, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1930/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1931/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1932/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1933/4345]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1934/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1935/4345]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1936/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1937/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1938/4345]: training_loss: tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1939/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1940/4345]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1941/4345]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1942/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1943/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1944/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1945/4345]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1946/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1947/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1948/4345]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1949/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1950/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1951/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1952/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1953/4345]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1954/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1955/4345]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1956/4345]: training_loss: tensor(0.0410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1957/4345]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1958/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1959/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1960/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1961/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1962/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1963/4345]: training_loss: tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1964/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1965/4345]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1966/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1967/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1968/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1969/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1970/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1971/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1972/4345]: training_loss: tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1973/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1974/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1975/4345]: training_loss: tensor(0.2159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1976/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1977/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1978/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1979/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1980/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1981/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1982/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1983/4345]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1984/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1985/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1986/4345]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1987/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1988/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1989/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1990/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1991/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1992/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1993/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1994/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1995/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1996/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1997/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1998/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1999/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2000/4345]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2001/4345]: training_loss: tensor(0.2630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2002/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2003/4345]: training_loss: tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2004/4345]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2005/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2006/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2007/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2008/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2009/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2010/4345]: training_loss: tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2011/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2012/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2013/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2014/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2015/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2016/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2017/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2018/4345]: training_loss: tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2019/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2020/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2021/4345]: training_loss: tensor(0.3703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2022/4345]: training_loss: tensor(0.3389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2023/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2024/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2025/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2026/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2027/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2028/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2029/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2030/4345]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2031/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2032/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2033/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2034/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2035/4345]: training_loss: tensor(0.1303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2036/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2037/4345]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2038/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2039/4345]: training_loss: tensor(0.0404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2040/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2041/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2042/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2043/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2044/4345]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2045/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2046/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2047/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2048/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2049/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2050/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2051/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2052/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2053/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2054/4345]: training_loss: tensor(0.1384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2055/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2056/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2057/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2058/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2059/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2060/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2061/4345]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2062/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2063/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2064/4345]: training_loss: tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2065/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2066/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2067/4345]: training_loss: tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2068/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2069/4345]: training_loss: tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2070/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2071/4345]: training_loss: tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2072/4345]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2073/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2074/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2075/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2076/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2077/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2078/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2079/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2080/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2081/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2082/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2083/4345]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2084/4345]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2085/4345]: training_loss: tensor(0.1292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2086/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2087/4345]: training_loss: tensor(0.2781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2088/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2089/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2090/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2091/4345]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2092/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2093/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2094/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2095/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2096/4345]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2097/4345]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2098/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2099/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2100/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2101/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2102/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2103/4345]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2104/4345]: training_loss: tensor(0.2297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2105/4345]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2106/4345]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2107/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2108/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2109/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2110/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2111/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2112/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2113/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2114/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2115/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2116/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2117/4345]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2118/4345]: training_loss: tensor(0.3062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2119/4345]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2120/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2121/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2122/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2123/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2124/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2125/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2126/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2127/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2128/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2129/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2130/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2131/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2132/4345]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2133/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2134/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2135/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2136/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2137/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2138/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2139/4345]: training_loss: tensor(0.1154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2140/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2141/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2142/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2143/4345]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2144/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2145/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2146/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2147/4345]: training_loss: tensor(0.2058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2148/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2149/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2150/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2151/4345]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2152/4345]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2153/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2154/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2155/4345]: training_loss: tensor(0.1959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2156/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2157/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2158/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2159/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2160/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2161/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2162/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2163/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2164/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2165/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2166/4345]: training_loss: tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2167/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2168/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2169/4345]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2170/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2171/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2172/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2173/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [5/5], Step [19557/21730], Train Loss: 0.0306, Valid Loss: 0.2659\n",
      "batch_no [2174/4345]: training_loss: tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2175/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2176/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2177/4345]: training_loss: tensor(0.3162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2178/4345]: training_loss: tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2179/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2180/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2181/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2182/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2183/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2184/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2185/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2186/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2187/4345]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2188/4345]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2189/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2190/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2191/4345]: training_loss: tensor(0.1441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2192/4345]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2193/4345]: training_loss: tensor(0.1766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2194/4345]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2195/4345]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2196/4345]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2197/4345]: training_loss: tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2198/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2199/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2200/4345]: training_loss: tensor(0.1450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2201/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2202/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2203/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2204/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2205/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2206/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2207/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2208/4345]: training_loss: tensor(0.2917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2209/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2210/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2211/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2212/4345]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2213/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2214/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2215/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2216/4345]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2217/4345]: training_loss: tensor(0.2860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2218/4345]: training_loss: tensor(0.1869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2219/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2220/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2221/4345]: training_loss: tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2222/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2223/4345]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2224/4345]: training_loss: tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2225/4345]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2226/4345]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2227/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2228/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2229/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2230/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2231/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2232/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2233/4345]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2234/4345]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2235/4345]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2236/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2237/4345]: training_loss: tensor(0.2713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2238/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2239/4345]: training_loss: tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2240/4345]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2241/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2242/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2243/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2244/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2245/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2246/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2247/4345]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2248/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2249/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2250/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2251/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2252/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2253/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2254/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2255/4345]: training_loss: tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2256/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2257/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2258/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2259/4345]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2260/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2261/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2262/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2263/4345]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2264/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2265/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2266/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2267/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2268/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2269/4345]: training_loss: tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2270/4345]: training_loss: tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2271/4345]: training_loss: tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2272/4345]: training_loss: tensor(0.0442, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2273/4345]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2274/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2275/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2276/4345]: training_loss: tensor(0.0462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2277/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2278/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2279/4345]: training_loss: tensor(0.0439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2280/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2281/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2282/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2283/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2284/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2285/4345]: training_loss: tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2286/4345]: training_loss: tensor(0.1346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2287/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2288/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2289/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2290/4345]: training_loss: tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2291/4345]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2292/4345]: training_loss: tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2293/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2294/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2295/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2296/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2297/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2298/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2299/4345]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2300/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2301/4345]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2302/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2303/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2304/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2305/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2306/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2307/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2308/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2309/4345]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2310/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2311/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2312/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2313/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2314/4345]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2315/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2316/4345]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2317/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2318/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2319/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2320/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2321/4345]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2322/4345]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2323/4345]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2324/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2325/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2326/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2327/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2328/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2329/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2330/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2331/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2332/4345]: training_loss: tensor(0.0290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2333/4345]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2334/4345]: training_loss: tensor(0.1168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2335/4345]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2336/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2337/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2338/4345]: training_loss: tensor(0.1211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2339/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2340/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2341/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2342/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2343/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2344/4345]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2345/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2346/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2347/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2348/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2349/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2350/4345]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2351/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2352/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2353/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2354/4345]: training_loss: tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2355/4345]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2356/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2357/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2358/4345]: training_loss: tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2359/4345]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2360/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2361/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2362/4345]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2363/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2364/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2365/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2366/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2367/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2368/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2369/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2370/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2371/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2372/4345]: training_loss: tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2373/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2374/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2375/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2376/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2377/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2378/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2379/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2380/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2381/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2382/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2383/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2384/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2385/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2386/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2387/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2388/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2389/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2390/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2391/4345]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2392/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2393/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2394/4345]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2395/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2396/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2397/4345]: training_loss: tensor(0.4011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2398/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2399/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2400/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2401/4345]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2402/4345]: training_loss: tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2403/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2404/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2405/4345]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2406/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2407/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2408/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2409/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2410/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2411/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2412/4345]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2413/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2414/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2415/4345]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2416/4345]: training_loss: tensor(0.2405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2417/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2418/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2419/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2420/4345]: training_loss: tensor(0.3417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2421/4345]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2422/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2423/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2424/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2425/4345]: training_loss: tensor(0.1887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2426/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2427/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2428/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2429/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2430/4345]: training_loss: tensor(0.2282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2431/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2432/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2433/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2434/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2435/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2436/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2437/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2438/4345]: training_loss: tensor(0.1156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2439/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2440/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2441/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2442/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2443/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2444/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2445/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2446/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2447/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2448/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2449/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2450/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2451/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2452/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2453/4345]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2454/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2455/4345]: training_loss: tensor(0.0427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2456/4345]: training_loss: tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2457/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2458/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2459/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2460/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2461/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2462/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2463/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2464/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2465/4345]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2466/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2467/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2468/4345]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2469/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2470/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2471/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2472/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2473/4345]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2474/4345]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2475/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2476/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2477/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2478/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2479/4345]: training_loss: tensor(0.1670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2480/4345]: training_loss: tensor(0.3022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2481/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2482/4345]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2483/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2484/4345]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2485/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2486/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2487/4345]: training_loss: tensor(0.1833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2488/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2489/4345]: training_loss: tensor(0.3259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2490/4345]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2491/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2492/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2493/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2494/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2495/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2496/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2497/4345]: training_loss: tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2498/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2499/4345]: training_loss: tensor(0.2612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2500/4345]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2501/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2502/4345]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2503/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2504/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2505/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2506/4345]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2507/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2508/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2509/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2510/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2511/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2512/4345]: training_loss: tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2513/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2514/4345]: training_loss: tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2515/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2516/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2517/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2518/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2519/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2520/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2521/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2522/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2523/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2524/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2525/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2526/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2527/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2528/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2529/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2530/4345]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2531/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2532/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2533/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2534/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2535/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2536/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2537/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2538/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2539/4345]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2540/4345]: training_loss: tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2541/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2542/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2543/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2544/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2545/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2546/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2547/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2548/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2549/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2550/4345]: training_loss: tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2551/4345]: training_loss: tensor(0.5828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2552/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2553/4345]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2554/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2555/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2556/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2557/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2558/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2559/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2560/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2561/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2562/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2563/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2564/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2565/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2566/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2567/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2568/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2569/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2570/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2571/4345]: training_loss: tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2572/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2573/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2574/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2575/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2576/4345]: training_loss: tensor(0.2826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2577/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2578/4345]: training_loss: tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2579/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2580/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2581/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2582/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2583/4345]: training_loss: tensor(0.1575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2584/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2585/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2586/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2587/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2588/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2589/4345]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2590/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2591/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2592/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2593/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2594/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2595/4345]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2596/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2597/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2598/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2599/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2600/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2601/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2602/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2603/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2604/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2605/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2606/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2607/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2608/4345]: training_loss: tensor(0.1271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2609/4345]: training_loss: tensor(0.2434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2610/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2611/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2612/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2613/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2614/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2615/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2616/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2617/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2618/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2619/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2620/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2621/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2622/4345]: training_loss: tensor(0.2022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2623/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2624/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2625/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2626/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2627/4345]: training_loss: tensor(0.0606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2628/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2629/4345]: training_loss: tensor(0.1332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2630/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2631/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2632/4345]: training_loss: tensor(0.1839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2633/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2634/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2635/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2636/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2637/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2638/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2639/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2640/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2641/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2642/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2643/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2644/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2645/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2646/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2647/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2648/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2649/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2650/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2651/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2652/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2653/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2654/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2655/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2656/4345]: training_loss: tensor(0.2343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2657/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2658/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2659/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2660/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2661/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2662/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2663/4345]: training_loss: tensor(0.2602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2664/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2665/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2666/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2667/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2668/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2669/4345]: training_loss: tensor(0.1870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2670/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2671/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2672/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2673/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2674/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2675/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2676/4345]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2677/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2678/4345]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2679/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2680/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2681/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2682/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2683/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2684/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2685/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2686/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2687/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2688/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2689/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2690/4345]: training_loss: tensor(0.2361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2691/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2692/4345]: training_loss: tensor(0.1431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2693/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2694/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2695/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2696/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2697/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2698/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2699/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2700/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2701/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2702/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2703/4345]: training_loss: tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2704/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2705/4345]: training_loss: tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2706/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2707/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2708/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2709/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2710/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2711/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2712/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2713/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2714/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2715/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2716/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2717/4345]: training_loss: tensor(0.1871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2718/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2719/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2720/4345]: training_loss: tensor(0.1193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2721/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2722/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2723/4345]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2724/4345]: training_loss: tensor(0.1187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2725/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2726/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2727/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2728/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2729/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2730/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2731/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2732/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2733/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2734/4345]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2735/4345]: training_loss: tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2736/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2737/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2738/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2739/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2740/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2741/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2742/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2743/4345]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2744/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2745/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2746/4345]: training_loss: tensor(0.4700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2747/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2748/4345]: training_loss: tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2749/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2750/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2751/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2752/4345]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2753/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2754/4345]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2755/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2756/4345]: training_loss: tensor(0.3513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2757/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2758/4345]: training_loss: tensor(0.2536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2759/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2760/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2761/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2762/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2763/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2764/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2765/4345]: training_loss: tensor(0.0637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2766/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2767/4345]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2768/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2769/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2770/4345]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2771/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2772/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2773/4345]: training_loss: tensor(0.2095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2774/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2775/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2776/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2777/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2778/4345]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2779/4345]: training_loss: tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2780/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2781/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2782/4345]: training_loss: tensor(0.0572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2783/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2784/4345]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2785/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2786/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2787/4345]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2788/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2789/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2790/4345]: training_loss: tensor(0.0305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2791/4345]: training_loss: tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2792/4345]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2793/4345]: training_loss: tensor(0.2269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2794/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2795/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2796/4345]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2797/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2798/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2799/4345]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2800/4345]: training_loss: tensor(0.0361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2801/4345]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2802/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2803/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2804/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2805/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2806/4345]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2807/4345]: training_loss: tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2808/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2809/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2810/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2811/4345]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2812/4345]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2813/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2814/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2815/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2816/4345]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2817/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2818/4345]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2819/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2820/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2821/4345]: training_loss: tensor(0.2020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2822/4345]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2823/4345]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2824/4345]: training_loss: tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2825/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2826/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2827/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2828/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2829/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2830/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2831/4345]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2832/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2833/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2834/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2835/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2836/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2837/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2838/4345]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2839/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2840/4345]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2841/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2842/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2843/4345]: training_loss: tensor(0.0486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2844/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2845/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2846/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2847/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2848/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2849/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2850/4345]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2851/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2852/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2853/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2854/4345]: training_loss: tensor(0.1530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2855/4345]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2856/4345]: training_loss: tensor(0.2441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2857/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2858/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2859/4345]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2860/4345]: training_loss: tensor(0.1217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2861/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2862/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2863/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2864/4345]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2865/4345]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2866/4345]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2867/4345]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2868/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2869/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2870/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2871/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2872/4345]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2873/4345]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2874/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2875/4345]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2876/4345]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2877/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2878/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2879/4345]: training_loss: tensor(0.1874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2880/4345]: training_loss: tensor(0.1616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2881/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2882/4345]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2883/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2884/4345]: training_loss: tensor(0.2203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2885/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2886/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2887/4345]: training_loss: tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2888/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2889/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2890/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2891/4345]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2892/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2893/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2894/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2895/4345]: training_loss: tensor(0.1370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2896/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2897/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2898/4345]: training_loss: tensor(0.1916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2899/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2900/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2901/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2902/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2903/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2904/4345]: training_loss: tensor(0.1610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2905/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2906/4345]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2907/4345]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2908/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2909/4345]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2910/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2911/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2912/4345]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2913/4345]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2914/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2915/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2916/4345]: training_loss: tensor(0.1123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2917/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2918/4345]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2919/4345]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2920/4345]: training_loss: tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2921/4345]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2922/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2923/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2924/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2925/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2926/4345]: training_loss: tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2927/4345]: training_loss: tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2928/4345]: training_loss: tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2929/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2930/4345]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2931/4345]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2932/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2933/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2934/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2935/4345]: training_loss: tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2936/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2937/4345]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2938/4345]: training_loss: tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2939/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2940/4345]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2941/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2942/4345]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2943/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2944/4345]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2945/4345]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2946/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2947/4345]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2948/4345]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2949/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2950/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2951/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2952/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2953/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2954/4345]: training_loss: tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2955/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2956/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2957/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2958/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2959/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2960/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2961/4345]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2962/4345]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2963/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2964/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2965/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2966/4345]: training_loss: tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2967/4345]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2968/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2969/4345]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2970/4345]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2971/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2972/4345]: training_loss: tensor(0.1344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2973/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2974/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2975/4345]: training_loss: tensor(0.5705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2976/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2977/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2978/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2979/4345]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2980/4345]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2981/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2982/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2983/4345]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2984/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2985/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2986/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2987/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2988/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2989/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2990/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2991/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2992/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2993/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2994/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2995/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2996/4345]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2997/4345]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2998/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2999/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3000/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3001/4345]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3002/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3003/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3004/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3005/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3006/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3007/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3008/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3009/4345]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3010/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3011/4345]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3012/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3013/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3014/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3015/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3016/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3017/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3018/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3019/4345]: training_loss: tensor(0.7877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3020/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3021/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3022/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3023/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3024/4345]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3025/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3026/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3027/4345]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3028/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3029/4345]: training_loss: tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3030/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3031/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3032/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3033/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3034/4345]: training_loss: tensor(0.2342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3035/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3036/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3037/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3038/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3039/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3040/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3041/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3042/4345]: training_loss: tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3043/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3044/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3045/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3046/4345]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3047/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3048/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3049/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3050/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3051/4345]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3052/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3053/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3054/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3055/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3056/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3057/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3058/4345]: training_loss: tensor(0.2228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3059/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3060/4345]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3061/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3062/4345]: training_loss: tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3063/4345]: training_loss: tensor(0.0423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3064/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3065/4345]: training_loss: tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3066/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3067/4345]: training_loss: tensor(0.1498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3068/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3069/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3070/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3071/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3072/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3073/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3074/4345]: training_loss: tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3075/4345]: training_loss: tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3076/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3077/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3078/4345]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3079/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3080/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3081/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3082/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3083/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3084/4345]: training_loss: tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3085/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3086/4345]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3087/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3088/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3089/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3090/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3091/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3092/4345]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3093/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3094/4345]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3095/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3096/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3097/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3098/4345]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3099/4345]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3100/4345]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3101/4345]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3102/4345]: training_loss: tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3103/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3104/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3105/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3106/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3107/4345]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3108/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3109/4345]: training_loss: tensor(0.0445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3110/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3111/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3112/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3113/4345]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3114/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3115/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3116/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3117/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3118/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3119/4345]: training_loss: tensor(0.3482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3120/4345]: training_loss: tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3121/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3122/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3123/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3124/4345]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3125/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3126/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3127/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3128/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3129/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3130/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3131/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3132/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3133/4345]: training_loss: tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3134/4345]: training_loss: tensor(0.2467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3135/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3136/4345]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3137/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3138/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3139/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3140/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3141/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3142/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3143/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3144/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3145/4345]: training_loss: tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3146/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3147/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3148/4345]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3149/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3150/4345]: training_loss: tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3151/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3152/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3153/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3154/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3155/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3156/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3157/4345]: training_loss: tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3158/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3159/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3160/4345]: training_loss: tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3161/4345]: training_loss: tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3162/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3163/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3164/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3165/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3166/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3167/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3168/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3169/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3170/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3171/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3172/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3173/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3174/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3175/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3176/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3177/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3178/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3179/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3180/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3181/4345]: training_loss: tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3182/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3183/4345]: training_loss: tensor(0.1397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3184/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3185/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3186/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3187/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3188/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3189/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3190/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3191/4345]: training_loss: tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3192/4345]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3193/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3194/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3195/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3196/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3197/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3198/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3199/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3200/4345]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3201/4345]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3202/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3203/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3204/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3205/4345]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3206/4345]: training_loss: tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3207/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3208/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3209/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3210/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3211/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3212/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3213/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3214/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3215/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3216/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3217/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3218/4345]: training_loss: tensor(0.3139, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3219/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3220/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3221/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3222/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3223/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3224/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3225/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3226/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3227/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3228/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3229/4345]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3230/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3231/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3232/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3233/4345]: training_loss: tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3234/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3235/4345]: training_loss: tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3236/4345]: training_loss: tensor(0.2531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3237/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3238/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3239/4345]: training_loss: tensor(0.2859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3240/4345]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3241/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3242/4345]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3243/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3244/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3245/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3246/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3247/4345]: training_loss: tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3248/4345]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3249/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3250/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3251/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3252/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3253/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3254/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3255/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3256/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3257/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3258/4345]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3259/4345]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3260/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3261/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3262/4345]: training_loss: tensor(0.1412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3263/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3264/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3265/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3266/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3267/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3268/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3269/4345]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3270/4345]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3271/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3272/4345]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3273/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3274/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3275/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3276/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3277/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3278/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3279/4345]: training_loss: tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3280/4345]: training_loss: tensor(0.0364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3281/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3282/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3283/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3284/4345]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3285/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3286/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3287/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3288/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3289/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3290/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3291/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3292/4345]: training_loss: tensor(0.2421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3293/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3294/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3295/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3296/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3297/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3298/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3299/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3300/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3301/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3302/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3303/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3304/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3305/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3306/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3307/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3308/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3309/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3310/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3311/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3312/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3313/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3314/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3315/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3316/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3317/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3318/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3319/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3320/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3321/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3322/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3323/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3324/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3325/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3326/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3327/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3328/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3329/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3330/4345]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3331/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3332/4345]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3333/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3334/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3335/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3336/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3337/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3338/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3339/4345]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3340/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3341/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3342/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3343/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3344/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3345/4345]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3346/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3347/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3348/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3349/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3350/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3351/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3352/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3353/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3354/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3355/4345]: training_loss: tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3356/4345]: training_loss: tensor(0.1407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3357/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3358/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3359/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3360/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3361/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3362/4345]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3363/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3364/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3365/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3366/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3367/4345]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3368/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3369/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3370/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3371/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3372/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3373/4345]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3374/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3375/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3376/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3377/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3378/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3379/4345]: training_loss: tensor(0.1410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3380/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3381/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3382/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3383/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3384/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3385/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3386/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3387/4345]: training_loss: tensor(0.2952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3388/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3389/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3390/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3391/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3392/4345]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3393/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3394/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3395/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3396/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3397/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3398/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3399/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3400/4345]: training_loss: tensor(0.1730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3401/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3402/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3403/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3404/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3405/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3406/4345]: training_loss: tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3407/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3408/4345]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3409/4345]: training_loss: tensor(0.4047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3410/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3411/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3412/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3413/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3414/4345]: training_loss: tensor(0.3422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3415/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3416/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3417/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3418/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3419/4345]: training_loss: tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3420/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3421/4345]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3422/4345]: training_loss: tensor(0.1341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3423/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3424/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3425/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3426/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3427/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3428/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3429/4345]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3430/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3431/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3432/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3433/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3434/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3435/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3436/4345]: training_loss: tensor(0.2612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3437/4345]: training_loss: tensor(0.2450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3438/4345]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3439/4345]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3440/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3441/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3442/4345]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3443/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3444/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3445/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3446/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3447/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3448/4345]: training_loss: tensor(0.2491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3449/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3450/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3451/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3452/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3453/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3454/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3455/4345]: training_loss: tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3456/4345]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3457/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3458/4345]: training_loss: tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3459/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3460/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3461/4345]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3462/4345]: training_loss: tensor(0.2874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3463/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3464/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3465/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3466/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3467/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3468/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3469/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3470/4345]: training_loss: tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3471/4345]: training_loss: tensor(0.0586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3472/4345]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3473/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3474/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3475/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3476/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3477/4345]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3478/4345]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3479/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3480/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3481/4345]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3482/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3483/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3484/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3485/4345]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3486/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3487/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3488/4345]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3489/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3490/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3491/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3492/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3493/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3494/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3495/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3496/4345]: training_loss: tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3497/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3498/4345]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3499/4345]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3500/4345]: training_loss: tensor(0.1454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3501/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3502/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3503/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3504/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3505/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3506/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3507/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3508/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3509/4345]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3510/4345]: training_loss: tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3511/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3512/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3513/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3514/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3515/4345]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3516/4345]: training_loss: tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3517/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3518/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3519/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3520/4345]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3521/4345]: training_loss: tensor(0.1634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3522/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3523/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3524/4345]: training_loss: tensor(0.1231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3525/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3526/4345]: training_loss: tensor(0.0686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3527/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3528/4345]: training_loss: tensor(0.1310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3529/4345]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3530/4345]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3531/4345]: training_loss: tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3532/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3533/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3534/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3535/4345]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3536/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3537/4345]: training_loss: tensor(0.1099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3538/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3539/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3540/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3541/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3542/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3543/4345]: training_loss: tensor(0.2982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3544/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3545/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3546/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3547/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3548/4345]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3549/4345]: training_loss: tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3550/4345]: training_loss: tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3551/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3552/4345]: training_loss: tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3553/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3554/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3555/4345]: training_loss: tensor(0.0462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3556/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3557/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3558/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3559/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3560/4345]: training_loss: tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3561/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3562/4345]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3563/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3564/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3565/4345]: training_loss: tensor(0.1352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3566/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3567/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3568/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3569/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3570/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3571/4345]: training_loss: tensor(9.5176e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3572/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3573/4345]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3574/4345]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3575/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3576/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3577/4345]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3578/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3579/4345]: training_loss: tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3580/4345]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3581/4345]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3582/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3583/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3584/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3585/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3586/4345]: training_loss: tensor(0.3963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3587/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3588/4345]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3589/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3590/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3591/4345]: training_loss: tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3592/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3593/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3594/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3595/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3596/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3597/4345]: training_loss: tensor(0.1992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3598/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3599/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3600/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3601/4345]: training_loss: tensor(0.1987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3602/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3603/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3604/4345]: training_loss: tensor(0.1963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3605/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3606/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3607/4345]: training_loss: tensor(0.2035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3608/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3609/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3610/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3611/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3612/4345]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3613/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3614/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3615/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3616/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3617/4345]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3618/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3619/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3620/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3621/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3622/4345]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3623/4345]: training_loss: tensor(0.2901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3624/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3625/4345]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3626/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3627/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3628/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3629/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3630/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3631/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3632/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3633/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3634/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3635/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3636/4345]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3637/4345]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3638/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3639/4345]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3640/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3641/4345]: training_loss: tensor(0.1587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3642/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3643/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3644/4345]: training_loss: tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3645/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3646/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3647/4345]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3648/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3649/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3650/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3651/4345]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3652/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3653/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3654/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3655/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3656/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3657/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3658/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3659/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3660/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3661/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3662/4345]: training_loss: tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3663/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3664/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3665/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3666/4345]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3667/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3668/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3669/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3670/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3671/4345]: training_loss: tensor(0.0290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3672/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3673/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3674/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3675/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3676/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3677/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3678/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3679/4345]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3680/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3681/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3682/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3683/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3684/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3685/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3686/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3687/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3688/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3689/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3690/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3691/4345]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3692/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3693/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3694/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3695/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3696/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3697/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3698/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3699/4345]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3700/4345]: training_loss: tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3701/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3702/4345]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3703/4345]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3704/4345]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3705/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3706/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3707/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3708/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3709/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3710/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3711/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3712/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3713/4345]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3714/4345]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3715/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3716/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3717/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3718/4345]: training_loss: tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3719/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3720/4345]: training_loss: tensor(0.4288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3721/4345]: training_loss: tensor(0.2273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3722/4345]: training_loss: tensor(0.4737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3723/4345]: training_loss: tensor(0.2715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3724/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3725/4345]: training_loss: tensor(0.2110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3726/4345]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3727/4345]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3728/4345]: training_loss: tensor(0.1755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3729/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3730/4345]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3731/4345]: training_loss: tensor(0.0575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3732/4345]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3733/4345]: training_loss: tensor(0.3168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3734/4345]: training_loss: tensor(0.1661, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3735/4345]: training_loss: tensor(0.0582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3736/4345]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3737/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3738/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3739/4345]: training_loss: tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3740/4345]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3741/4345]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3742/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3743/4345]: training_loss: tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3744/4345]: training_loss: tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3745/4345]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3746/4345]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3747/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3748/4345]: training_loss: tensor(0.1575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3749/4345]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3750/4345]: training_loss: tensor(0.1125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3751/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3752/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3753/4345]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3754/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3755/4345]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3756/4345]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3757/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3758/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3759/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3760/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3761/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3762/4345]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3763/4345]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3764/4345]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3765/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3766/4345]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3767/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3768/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3769/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3770/4345]: training_loss: tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3771/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3772/4345]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3773/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3774/4345]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3775/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3776/4345]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3777/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3778/4345]: training_loss: tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3779/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3780/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3781/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3782/4345]: training_loss: tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3783/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3784/4345]: training_loss: tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3785/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3786/4345]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3787/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3788/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3789/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3790/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3791/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3792/4345]: training_loss: tensor(0.2321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3793/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3794/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3795/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3796/4345]: training_loss: tensor(0.5088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3797/4345]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3798/4345]: training_loss: tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3799/4345]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3800/4345]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3801/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3802/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3803/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3804/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3805/4345]: training_loss: tensor(0.2468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3806/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3807/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3808/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3809/4345]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3810/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3811/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3812/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3813/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3814/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3815/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3816/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3817/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3818/4345]: training_loss: tensor(0.1753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3819/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3820/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3821/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3822/4345]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3823/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3824/4345]: training_loss: tensor(0.2192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3825/4345]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3826/4345]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3827/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3828/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3829/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3830/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3831/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3832/4345]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3833/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3834/4345]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3835/4345]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3836/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3837/4345]: training_loss: tensor(0.1709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3838/4345]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3839/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3840/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3841/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3842/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3843/4345]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3844/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3845/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3846/4345]: training_loss: tensor(0.1505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3847/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3848/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3849/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3850/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3851/4345]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3852/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3853/4345]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3854/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3855/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3856/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3857/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3858/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3859/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3860/4345]: training_loss: tensor(0.1175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3861/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3862/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3863/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3864/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3865/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3866/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3867/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3868/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3869/4345]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3870/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3871/4345]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3872/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3873/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3874/4345]: training_loss: tensor(0.0468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3875/4345]: training_loss: tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3876/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3877/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3878/4345]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3879/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3880/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3881/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3882/4345]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3883/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3884/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3885/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3886/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3887/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3888/4345]: training_loss: tensor(0.1511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3889/4345]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3890/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3891/4345]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3892/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3893/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3894/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3895/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3896/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3897/4345]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3898/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3899/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3900/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3901/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3902/4345]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3903/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3904/4345]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3905/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3906/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3907/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3908/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3909/4345]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3910/4345]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3911/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3912/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3913/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3914/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3915/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3916/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3917/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3918/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3919/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3920/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3921/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3922/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3923/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3924/4345]: training_loss: tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3925/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3926/4345]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3927/4345]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3928/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3929/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3930/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3931/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3932/4345]: training_loss: tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3933/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3934/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3935/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3936/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3937/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3938/4345]: training_loss: tensor(0.2181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3939/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3940/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3941/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3942/4345]: training_loss: tensor(0.1938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3943/4345]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3944/4345]: training_loss: tensor(0.1815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3945/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3946/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3947/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3948/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3949/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3950/4345]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3951/4345]: training_loss: tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3952/4345]: training_loss: tensor(0.6444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3953/4345]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3954/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3955/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3956/4345]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3957/4345]: training_loss: tensor(0.1952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3958/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3959/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3960/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3961/4345]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3962/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3963/4345]: training_loss: tensor(0.0342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3964/4345]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3965/4345]: training_loss: tensor(0.1923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3966/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3967/4345]: training_loss: tensor(0.1520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3968/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3969/4345]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3970/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3971/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3972/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3973/4345]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3974/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3975/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3976/4345]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3977/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3978/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3979/4345]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3980/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3981/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3982/4345]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3983/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3984/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3985/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3986/4345]: training_loss: tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3987/4345]: training_loss: tensor(0.2906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3988/4345]: training_loss: tensor(0.2477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3989/4345]: training_loss: tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3990/4345]: training_loss: tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3991/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3992/4345]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3993/4345]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3994/4345]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3995/4345]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3996/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3997/4345]: training_loss: tensor(0.1388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3998/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3999/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4000/4345]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4001/4345]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4002/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4003/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4004/4345]: training_loss: tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4005/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4006/4345]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4007/4345]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4008/4345]: training_loss: tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4009/4345]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4010/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4011/4345]: training_loss: tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4012/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4013/4345]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4014/4345]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4015/4345]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4016/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4017/4345]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4018/4345]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4019/4345]: training_loss: tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4020/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4021/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4022/4345]: training_loss: tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4023/4345]: training_loss: tensor(0.2456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4024/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4025/4345]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4026/4345]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4027/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4028/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4029/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4030/4345]: training_loss: tensor(0.1765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4031/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4032/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4033/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4034/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4035/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4036/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4037/4345]: training_loss: tensor(0.1167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4038/4345]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4039/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4040/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4041/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4042/4345]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4043/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4044/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4045/4345]: training_loss: tensor(0.3338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4046/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4047/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4048/4345]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4049/4345]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4050/4345]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4051/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4052/4345]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4053/4345]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4054/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4055/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4056/4345]: training_loss: tensor(0.1751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4057/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4058/4345]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4059/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4060/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4061/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4062/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4063/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4064/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4065/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4066/4345]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4067/4345]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4068/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4069/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4070/4345]: training_loss: tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4071/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4072/4345]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4073/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4074/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4075/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4076/4345]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4077/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4078/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4079/4345]: training_loss: tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4080/4345]: training_loss: tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4081/4345]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4082/4345]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4083/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4084/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4085/4345]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4086/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4087/4345]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4088/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4089/4345]: training_loss: tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4090/4345]: training_loss: tensor(0.1795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4091/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4092/4345]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4093/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4094/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4095/4345]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4096/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4097/4345]: training_loss: tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4098/4345]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4099/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4100/4345]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4101/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4102/4345]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4103/4345]: training_loss: tensor(0.1422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4104/4345]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4105/4345]: training_loss: tensor(0.1819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4106/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4107/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4108/4345]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4109/4345]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4110/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4111/4345]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4112/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4113/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4114/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4115/4345]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4116/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4117/4345]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4118/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4119/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4120/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4121/4345]: training_loss: tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4122/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4123/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4124/4345]: training_loss: tensor(0.0305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4125/4345]: training_loss: tensor(0.2057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4126/4345]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4127/4345]: training_loss: tensor(0.1294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4128/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4129/4345]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4130/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4131/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4132/4345]: training_loss: tensor(0.2030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4133/4345]: training_loss: tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4134/4345]: training_loss: tensor(0.2789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4135/4345]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4136/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4137/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4138/4345]: training_loss: tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4139/4345]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4140/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4141/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4142/4345]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4143/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4144/4345]: training_loss: tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4145/4345]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4146/4345]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4147/4345]: training_loss: tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4148/4345]: training_loss: tensor(0.2620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4149/4345]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4150/4345]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4151/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4152/4345]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4153/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4154/4345]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4155/4345]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4156/4345]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4157/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4158/4345]: training_loss: tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4159/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4160/4345]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4161/4345]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4162/4345]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4163/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4164/4345]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4165/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4166/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4167/4345]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4168/4345]: training_loss: tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4169/4345]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4170/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4171/4345]: training_loss: tensor(0.1680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4172/4345]: training_loss: tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4173/4345]: training_loss: tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4174/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4175/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4176/4345]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4177/4345]: training_loss: tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4178/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4179/4345]: training_loss: tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4180/4345]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4181/4345]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4182/4345]: training_loss: tensor(0.1882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4183/4345]: training_loss: tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4184/4345]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4185/4345]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4186/4345]: training_loss: tensor(0.1613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4187/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4188/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4189/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4190/4345]: training_loss: tensor(0.1314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4191/4345]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4192/4345]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4193/4345]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4194/4345]: training_loss: tensor(0.1983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4195/4345]: training_loss: tensor(0.0505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4196/4345]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4197/4345]: training_loss: tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4198/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4199/4345]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4200/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4201/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4202/4345]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4203/4345]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4204/4345]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4205/4345]: training_loss: tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4206/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4207/4345]: training_loss: tensor(0.1564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4208/4345]: training_loss: tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4209/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4210/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4211/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4212/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4213/4345]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4214/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4215/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4216/4345]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4217/4345]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4218/4345]: training_loss: tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4219/4345]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4220/4345]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4221/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4222/4345]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4223/4345]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4224/4345]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4225/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4226/4345]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4227/4345]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4228/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4229/4345]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4230/4345]: training_loss: tensor(0.1160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4231/4345]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4232/4345]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4233/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4234/4345]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4235/4345]: training_loss: tensor(0.1795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4236/4345]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4237/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4238/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4239/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4240/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4241/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4242/4345]: training_loss: tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4243/4345]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4244/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4245/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4246/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4247/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4248/4345]: training_loss: tensor(0.1540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4249/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4250/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4251/4345]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4252/4345]: training_loss: tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4253/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4254/4345]: training_loss: tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4255/4345]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4256/4345]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4257/4345]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4258/4345]: training_loss: tensor(0.2152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4259/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4260/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4261/4345]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4262/4345]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4263/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4264/4345]: training_loss: tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4265/4345]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4266/4345]: training_loss: tensor(0.3323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4267/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4268/4345]: training_loss: tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4269/4345]: training_loss: tensor(0.5455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4270/4345]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4271/4345]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4272/4345]: training_loss: tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4273/4345]: training_loss: tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4274/4345]: training_loss: tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4275/4345]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4276/4345]: training_loss: tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4277/4345]: training_loss: tensor(0.1568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4278/4345]: training_loss: tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4279/4345]: training_loss: tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4280/4345]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4281/4345]: training_loss: tensor(0.1625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4282/4345]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4283/4345]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4284/4345]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4285/4345]: training_loss: tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4286/4345]: training_loss: tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4287/4345]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4288/4345]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4289/4345]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4290/4345]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4291/4345]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4292/4345]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4293/4345]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4294/4345]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4295/4345]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4296/4345]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4297/4345]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4298/4345]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4299/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4300/4345]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4301/4345]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4302/4345]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4303/4345]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4304/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4305/4345]: training_loss: tensor(0.2264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4306/4345]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4307/4345]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4308/4345]: training_loss: tensor(0.1430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4309/4345]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4310/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4311/4345]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4312/4345]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4313/4345]: training_loss: tensor(0.1619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4314/4345]: training_loss: tensor(0.3486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4315/4345]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4316/4345]: training_loss: tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4317/4345]: training_loss: tensor(0.4494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4318/4345]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4319/4345]: training_loss: tensor(0.1506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4320/4345]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4321/4345]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4322/4345]: training_loss: tensor(0.1298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4323/4345]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4324/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4325/4345]: training_loss: tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4326/4345]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4327/4345]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4328/4345]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4329/4345]: training_loss: tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4330/4345]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4331/4345]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4332/4345]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4333/4345]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4334/4345]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4335/4345]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4336/4345]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4337/4345]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4338/4345]: training_loss: tensor(0.1369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4339/4345]: training_loss: tensor(0.1919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4340/4345]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4341/4345]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4342/4345]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4343/4345]: training_loss: tensor(0.0476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4344/4345]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4345/4345]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4346/4345]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [5/5], Step [21730/21730], Train Loss: 0.0269, Valid Loss: 0.2117\n",
      "Model saved to ==> Model/metrics.pt\n",
      "Finished Training!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "model = BERT().to(device)\n",
    "no_decay = ['bias', 'LayerNorm.weight']    \n",
    "optimizer_grouped_parameters = [\n",
    "{'params': [p for n,p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "{'params': [p for n,p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\n",
    "train(model=model, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from <== Model/metrics.pt\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss_list, valid_loss_list, global_steps_list = load_metrics(destination_folder + '/metrics.pt')\n",
    "plt.plot(global_steps_list, train_loss_list, label='Train')\n",
    "plt.plot(global_steps_list, valid_loss_list, label='Valid')\n",
    "plt.xlabel('Global Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "2JaIsEttvMeO",
    "outputId": "abaab65e-2aeb-47a3-fd2e-68afb4817bed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from <== Model/metrics.pt\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss_list, valid_loss_list, global_steps_list = load_metrics(destination_folder + '/metrics.pt')\n",
    "plt.plot(global_steps_list, train_loss_list, label='Train')\n",
    "plt.plot(global_steps_list, valid_loss_list, label='Valid')\n",
    "plt.xlabel('Global Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3qLyO8EWwJEo"
   },
   "source": [
    "# Evaluation (Version3 modifications start here!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_new_data = pd.read_csv('annotations.csv',sep=',',encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2319 entries, 0 to 2318\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Text    2319 non-null   object\n",
      " 1   Label   2319 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 36.4+ KB\n"
     ]
    }
   ],
   "source": [
    "test_new_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@China_Amb_India Fucking piece of shit your whole community is solely responsible for the Chinese virus. Chinese dipshits should apologise to the world for their existence on this planet.\n",
      "I’m not being funny but #Coronavirus in....   China  Iran  Italy   That’s some pretty random shit.\n",
      "Got on the Victoria Line today to seven sisters and some chinese cunt coughed on me. Decked the cunt in his throat but now im worried that i’m gonna get the bat virus ffs\n",
      "It's appalling that the media &amp; libtards bitch about the virus being referred to as the \"Chinese Virus\" but no one seems to give a shit about the poor lives of the dogs &amp; cats that are being eaten alive &amp; tortured for food. Why isn't PETA raising hell about this?  Just EVIL.\n",
      "Dude fuck the Chinese man fuck em and if you think what I’m saying is wrong or rude you’re the problem\n",
      "Dear @Twitter Could you please kindly TURN ON your Moderation so we the ppl stop seeing the \"Korean / Vietnamese Bitching\" going on under  #COVID19 ? Thank you. Some of us r interested in the #Coronavirus information not in Asian bitching... Thank you in advance...\n",
      "@realDonaldTrump so much for it being the Chinese virus..yo ass sponsored the fucking country’s quarantine https://t.co/oxTRUbhJ6m\n",
      "@ToolFree2point0 @koreanjewcrypto ☝️☝️☝️☝️Just like the media blowing up the terms “Chinese Virus” and “Wuhan Virus” and then callin Trump racist as soon as he starts saying it.  They are hypocritical pieces of shit.\n",
      "@BarrazaSdfjbjr @UofAlec You cant blame donald Trump for a virus spreading across every country on the planet. He was being impeached in america when the outbreak was in Wuhan China. Blame the people who worked in the labs or ate a bat in Wuhan. Those dumb fucks are responsible way more than trump. 😂\n",
      "Fuck you China and your Chinese virus 😡 Rest in peace Pickup man 🙏🙏\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(test_new_data['Text'][i]) for i in range(len(test_new_data['Text'])) if i <10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new_data = test_new_data.astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@China_Amb_India Fucking piece of shit your whole community is solely responsible for the Chinese virus. Chinese dipshits should apologise to the world for their existence on this planet.\n",
      "Im not being funny but #Coronavirus in....   China  Iran  Italy   Thats some pretty random shit.\n",
      "Got on the Victoria Line today to seven sisters and some chinese cunt coughed on me. Decked the cunt in his throat but now im worried that im gonna get the bat virus ffs\n",
      "It's appalling that the media &amp; libtards bitch about the virus being referred to as the \"Chinese Virus\" but no one seems to give a shit about the poor lives of the dogs &amp; cats that are being eaten alive &amp; tortured for food. Why isn't PETA raising hell about this?  Just EVIL.\n",
      "Dude fuck the Chinese man fuck em and if you think what Im saying is wrong or rude youre the problem\n",
      "Dear @Twitter Could you please kindly TURN ON your Moderation so we the ppl stop seeing the \"Korean / Vietnamese Bitching\" going on under  #COVID19 ? Thank you. Some of us r interested in the #Coronavirus information not in Asian bitching... Thank you in advance...\n",
      "@realDonaldTrump so much for it being the Chinese virus..yo ass sponsored the fucking countrys quarantine https://t.co/oxTRUbhJ6m\n",
      "@ToolFree2point0 @koreanjewcrypto Just like the media blowing up the terms Chinese Virus and Wuhan Virus and then callin Trump racist as soon as he starts saying it.  They are hypocritical pieces of shit.\n",
      "@BarrazaSdfjbjr @UofAlec You cant blame donald Trump for a virus spreading across every country on the planet. He was being impeached in america when the outbreak was in Wuhan China. Blame the people who worked in the labs or ate a bat in Wuhan. Those dumb fucks are responsible way more than trump. \n",
      "Fuck you China and your Chinese virus  Rest in peace Pickup man \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(test_new_data['Text'][i]) for i in range(len(test_new_data['Text'])) if i <10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import emoji\n",
    "# text = []\n",
    "# for i in tqdm(test_new_data['Text']):\n",
    "#     text.append(emoji.demojize(i, delimiters=(\"\", \"\")))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [print(text[i]) for i in range(len(text)) if i <9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_new_data['Text'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 2319/2319 [00:00<00:00, 96746.38it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "for i in tqdm(test_new_data['Text']):\n",
    "    user_name = re.findall(r'@\\w*', i)\n",
    "    for i in user_name:\n",
    "        test_new_data['Text'] = test_new_data['Text'].apply(lambda x: x.replace(i, \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fucking piece of shit your whole community is solely responsible for the Chinese virus. Chinese dipshits should apologise to the world for their existence on this planet.\n",
      "Im not being funny but #Coronavirus in....   China  Iran  Italy   Thats some pretty random shit.\n",
      "Got on the Victoria Line today to seven sisters and some chinese cunt coughed on me. Decked the cunt in his throat but now im worried that im gonna get the bat virus ffs\n",
      "It's appalling that the media &amp; libtards bitch about the virus being referred to as the \"Chinese Virus\" but no one seems to give a shit about the poor lives of the dogs &amp; cats that are being eaten alive &amp; tortured for food. Why isn't PETA raising hell about this?  Just EVIL.\n",
      "Dude fuck the Chinese man fuck em and if you think what Im saying is wrong or rude youre the problem\n",
      "Dear  Could you please kindly TURN ON your Moderation so we the ppl stop seeing the \"Korean / Vietnamese Bitching\" going on under  #COVID19 ? Thank you. Some of us r interested in the #Coronavirus information not in Asian bitching... Thank you in advance...\n",
      " so much for it being the Chinese virus..yo ass sponsored the fucking countrys quarantine https://t.co/oxTRUbhJ6m\n",
      "  Just like the media blowing up the terms Chinese Virus and Wuhan Virus and then callin Trump racist as soon as he starts saying it.  They are hypocritical pieces of shit.\n",
      "  You cant blame donald Trump for a virus spreading across every country on the planet. He was being impeached in america when the outbreak was in Wuhan China. Blame the people who worked in the labs or ate a bat in Wuhan. Those dumb fucks are responsible way more than trump. \n",
      "Fuck you China and your Chinese virus  Rest in peace Pickup man \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(test_new_data['Text'][i]) for i in range(len(test_new_data['Text'])) if i <10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2319/2319 [00:00<00:00, 3399.41it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(test_new_data['Text']):\n",
    "    user_name = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', i)\n",
    "    for i in user_name:\n",
    "        test_new_data['Text'] = test_new_data['Text'].apply(lambda x: x.replace(i, \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fucking piece of shit your whole community is solely responsible for the Chinese virus. Chinese dipshits should apologise to the world for their existence on this planet.\n",
      "Im not being funny but #Coronavirus in....   China  Iran  Italy   Thats some pretty random shit.\n",
      "Got on the Victoria Line today to seven sisters and some chinese cunt coughed on me. Decked the cunt in his throat but now im worried that im gonna get the bat virus ffs\n",
      "It's appalling that the media &amp; libtards bitch about the virus being referred to as the \"Chinese Virus\" but no one seems to give a shit about the poor lives of the dogs &amp; cats that are being eaten alive &amp; tortured for food. Why isn't PETA raising hell about this?  Just EVIL.\n",
      "Dude fuck the Chinese man fuck em and if you think what Im saying is wrong or rude youre the problem\n",
      "Dear  Could you please kindly TURN ON your Moderation so we the ppl stop seeing the \"Korean / Vietnamese Bitching\" going on under  #COVID19 ? Thank you. Some of us r interested in the #Coronavirus information not in Asian bitching... Thank you in advance...\n",
      " so much for it being the Chinese virus..yo ass sponsored the fucking countrys quarantine \n",
      "  Just like the media blowing up the terms Chinese Virus and Wuhan Virus and then callin Trump racist as soon as he starts saying it.  They are hypocritical pieces of shit.\n",
      "  You cant blame donald Trump for a virus spreading across every country on the planet. He was being impeached in america when the outbreak was in Wuhan China. Blame the people who worked in the labs or ate a bat in Wuhan. Those dumb fucks are responsible way more than trump. \n",
      "Fuck you China and your Chinese virus  Rest in peace Pickup man \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(test_new_data['Text'][i]) for i in range(len(test_new_data['Text'])) if i <10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new_data['Text'] = test_new_data['Text'].apply(lambda x:re.sub(r'[^A-Za-z0-9 ]+', ' ', x).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 26/26 [00:00<00:00, 1133.47it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "symbol_list = ['!','@','#','$','%','^','&','*','(',')','-','+','?','>','<','=','/','.',':',';','  ','   ','    ','      ','      ','  ']\n",
    "for i in tqdm(symbol_list):\n",
    "    test_new_data['Text'] = test_new_data['Text'].apply(lambda x: x.replace(i, ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fucking piece of shit your whole community is solely responsible for the chinese virus chinese dipshits should apologise to the world for their existence on this planet                        \n",
      "im not being funny but coronavirus in china iran italy thats some pretty random shit                        \n",
      "got on the victoria line today to seven sisters and some chinese cunt coughed on me decked the cunt in his throat but now im worried that im gonna get the bat virus ffs                       \n",
      "it s appalling that the media amp libtards bitch about the virus being referred to as the chinese virus but no one seems to give a shit about the poor lives of the dogs amp cats that are being eaten alive amp tortured for food why isn t peta raising hell about this just evil                        \n",
      "dude fuck the chinese man fuck em and if you think what im saying is wrong or rude youre the problem                       \n",
      "dear could you please kindly turn on your moderation so we the ppl stop seeing the korean vietnamese bitching going on under covid19 thank you some of us r interested in the coronavirus information not in asian bitching thank you in advance                        \n",
      " so much for it being the chinese virus yo ass sponsored the fucking countrys quarantine                        \n",
      " just like the media blowing up the terms chinese virus and wuhan virus and then callin trump racist as soon as he starts saying it they are hypocritical pieces of shit                        \n",
      " you cant blame donald trump for a virus spreading across every country on the planet he was being impeached in america when the outbreak was in wuhan china blame the people who worked in the labs or ate a bat in wuhan those dumb fucks are responsible way more than trump                        \n",
      "fuck you china and your chinese virus rest in peace pickup man                        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(test_new_data['Text'][i] + '                       ') for i in range(len(test_new_data['Text'])) if i <10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fucking piece of shit your whole community is...</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im not being funny but coronavirus in china ir...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>got on the victoria line today to seven sister...</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it s appalling that the media amp libtards bit...</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dude fuck the chinese man fuck em and if you t...</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text    Label\n",
       "0   fucking piece of shit your whole community is...     Hate\n",
       "1  im not being funny but coronavirus in china ir...  Neutral\n",
       "2  got on the victoria line today to seven sister...     Hate\n",
       "3  it s appalling that the media amp libtards bit...     Hate\n",
       "4  dude fuck the chinese man fuck em and if you t...     Hate"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "Counterhate             359\n",
       "Hate                    678\n",
       "Neutral                 961\n",
       "Non-Asian Aggression    321\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap=test_new_data.groupby(by=['Label'])\n",
    "ap.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2923673997412678"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "678/(678+359+961+321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer the multi-label data into binary\n",
    "label1 = []\n",
    "for i in test_new_data['Label']:\n",
    "    if i == 'Hate':\n",
    "        label1.append(1.0)\n",
    "    elif i == 'Non-Asian Aggression':\n",
    "        label1.append(1.0)\n",
    "    else:\n",
    "        label1.append(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new_data['Label']  = label1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fucking piece of shit your whole community is...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im not being funny but coronavirus in china ir...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>got on the victoria line today to seven sister...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it s appalling that the media amp libtards bit...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dude fuck the chinese man fuck em and if you t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Label\n",
       "0   fucking piece of shit your whole community is...    1.0\n",
       "1  im not being funny but coronavirus in china ir...    0.0\n",
       "2  got on the victoria line today to seven sister...    1.0\n",
       "3  it s appalling that the media amp libtards bit...    1.0\n",
       "4  dude fuck the chinese man fuck em and if you t...    1.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0.0    1320\n",
       "1.0     999\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap=test_new_data.groupby(by=['Label'])\n",
    "ap.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new_data['comment'] = test_new_data['Text']\n",
    "test_new_data['attack'] = test_new_data['Label']\n",
    "test_new_data = test_new_data.drop(columns = ['Text','Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fucking piece of shit your whole community is...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im not being funny but coronavirus in china ir...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>got on the victoria line today to seven sister...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it s appalling that the media amp libtards bit...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dude fuck the chinese man fuck em and if you t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  attack\n",
       "0   fucking piece of shit your whole community is...     1.0\n",
       "1  im not being funny but coronavirus in china ir...     0.0\n",
       "2  got on the victoria line today to seven sister...     1.0\n",
       "3  it s appalling that the media amp libtards bit...     1.0\n",
       "4  dude fuck the chinese man fuck em and if you t...     1.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double_comment = []\n",
    "# for i in test_new_data['comment']:\n",
    "#     double_comment.append(i+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_new_data['comment'] = double_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new_data.to_csv('Data/test6.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2319 entries, 0 to 2318\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   comment  2319 non-null   object \n",
      " 1   attack   2319 non-null   float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 36.4+ KB\n"
     ]
    }
   ],
   "source": [
    "test_new_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1efbe0285e0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the distrubution of the anti-asian test data\n",
    "\n",
    "seq_len = [len(i.split()) for i in test_new_data['comment']]\n",
    "pd.Series(seq_len).hist(bins = 50,range=[0,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1efbe028610>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_len = [len(i.split()) for i in test_new_data['comment']]\n",
    "pd.Series(seq_len).hist(bins = 50,range=[0,300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aFBhAW6rwKly"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "def evaluate(model, test_loader):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    y_prob = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (comment,attack ), _ in test_loader:\n",
    "\n",
    "                comment = comment.type(torch.LongTensor)           \n",
    "                comment = comment.to(device)\n",
    "                attack = attack.type(torch.LongTensor)  \n",
    "                attack = attack.to(device)\n",
    "                output = model(comment, attack)\n",
    "\n",
    "                _, output = output\n",
    "                y_prob.extend(output.tolist())\n",
    "                y_pred.extend(torch.argmax(output, 1).tolist())\n",
    "                y_true.extend(attack.tolist())\n",
    "    return y_true, y_pred,y_prob    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "colab_type": "code",
    "id": "RLlijSLcxv2j",
    "outputId": "e301fd54-912a-4b75-fd8f-c79379d2a008"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from <== Model/model.pt\n"
     ]
    }
   ],
   "source": [
    "best_model = BERT().to(device)\n",
    "\n",
    "load_checkpoint(destination_folder + '/model.pt', best_model)\n",
    "\n",
    "y_true, y_pred,y_prob = evaluate(best_model, test_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_true = []\n",
    "for i in y_true:\n",
    "    if i == 1:\n",
    "        label_true.append([1,0])\n",
    "    else:\n",
    "        label_true.append([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob_final = []\n",
    "for i in range(len(y_prob)):\n",
    "    tempA = abs(y_prob[i][0])\n",
    "    tempB = abs(y_prob[i][1])\n",
    "    y_prob_final.append(tempB/(tempA+tempB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob_final2 = []\n",
    "for i in range(len(y_prob)):\n",
    "    tempA = abs(y_prob[i][0])\n",
    "    tempB = abs(y_prob[i][1])\n",
    "    y_prob_final2.append(tempA/(tempA+tempB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.6339    0.6587    0.6460       999\n",
      "           0     0.7338    0.7121    0.7228      1320\n",
      "\n",
      "    accuracy                         0.6891      2319\n",
      "   macro avg     0.6839    0.6854    0.6844      2319\n",
      "weighted avg     0.6908    0.6891    0.6897      2319\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#version7: convert non-asian aggression to hate(0.0->1.0)  \n",
    "from sklearn.metrics import roc_auc_score\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "plt.figure(1, figsize=(20,8))\n",
    "\n",
    "ax= plt.subplot(121)\n",
    "sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "ax.xaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "ax.yaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_true, y_prob_final)\n",
    "plt.subplot(122)\n",
    "lw = 2\n",
    "plt.plot(fpr_rt_lm, tpr_rt_lm, color='darkorange',\n",
    "         lw=lw, label='roc curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid()\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.4326    0.6622    0.5233       678\n",
      "           0     0.8212    0.6411    0.7201      1641\n",
      "\n",
      "    accuracy                         0.6473      2319\n",
      "   macro avg     0.6269    0.6517    0.6217      2319\n",
      "weighted avg     0.7076    0.6473    0.6625      2319\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJEAAAHwCAYAAAAW3v7qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd5xU1f3/8dfZpfcivSki2HvFRqLGAoqoILZgCzEWNCZ2Y4smmu8vJsbYsDeaCgJq7KJi7yJgARFBEBCQXrac3x8zuAsuRZjZO7v7evrYx9x7bnsvs7jD5557TogxIkmSJEmSJK1LXtIBJEmSJEmSlPssIkmSJEmSJGm9LCJJkiRJkiRpvSwiSZIkSZIkab0sIkmSJEmSJGm9LCJJkiRJkiRpvSwiSVkQQqgdQhgdQlgQQnhsE85zUgjh+UxmS0II4X8hhH5J55AkSZIkbTyLSKrSQggnhhDeDyEsDiHMTBc79svAqY8DWgBNY4y9N/YkMcZHY4y/yUCe1YQQuoUQYghh+BrtO6Xbx2zgea4JITyyvv1ijIfHGB/cyLiSJEmVXgjhmxDCsvTn0u9DCA+EEOqtsU/XEMLLIYRF6ZuVo0MI266xT4MQwr9DCN+mzzUpvb5Z+X5Hkioji0iqskIIFwL/Bv5GquDTHrgd6JmB03cAvowxFmbgXNkyB+gaQmhaqq0f8GWmLhBS/P+MJEnShjkyxlgP2BnYBbhs1YYQwj7A88BIoDWwBfAJ8EYIoWN6nxrAS8B2wGFAA6ArMBfYM1uhQwjVsnVuSbnFf9ypSgohNASuA86JMQ6PMS6JMRbEGEfHGC9K71MzfddmRvrr3yGEmult3UII00MIfwohzE73Yjotve1a4Crg+PTdnzPW7LETQtg83eOnWnr91BDC1+m7SlNCCCeVah9b6riuIYT30nee3gshdC21bUwI4a8hhDfS53l+PXecVgJPAn3Tx+cDfYBH1/izuiWEMC2EsDCE8EEIYf90+2HA5aW+z09K5bghhPAGsBTomG47M739jhDC46XOf1MI4aUQQtjgN1CSJKkSizF+DzxHqpi0yj+Ah2KMt8QYF8UY58UYrwTeBq5J7/NbUjdGe8UYJ8QYi2OMs2OMf40xPlPWtUII24UQXgghzAshzAohXJ5ufyCEcH2p/bqFEKaXWv8mhHBJCOFTYEkI4crSn/HS+9wSQvhPerlhCOHe9Ofm70II16c/f0qqQCwiqaraB6gFjFjHPlcAe5P65b0Tqbs3V5ba3hJoCLQBzgBuCyE0jjFeTap309AYY70Y473rChJCqAv8Bzg8xlif1N2ij8vYrwnwdHrfpsDNwNNr9CQ6ETgNaA7UAP68rmsDD5H6sAFwKDAemLHGPu+R+jNoAgwCHgsh1IoxPrvG97lTqWNOAfoD9YGpa5zvT8CO6QLZ/qT+7PrFGON6skqSJFUJIYS2wOHApPR6HVKfEcsaa3MYcEh6+WDg2Rjj4g28Tn3gReBZUr2bOpHqybShTgC6A42Ah4EjQggN0udedYNyUHrfB4HC9DV2AX4DnPkLriUpB1hEUlXVFPhhPY+bnQRcl757Mwe4llRxZJWC9PaC9J2dxUCXjcxTDGwfQqgdY5wZYxxfxj7dga9ijA/HGAtjjIOBz4EjS+1zf4zxyxjjMlIfKHYu4zw/iTG+CTQJIXQhVUx6qIx9Hokxzk1f859ATdb/fT4QYxyfPqZgjfMtBU4mVQR7BDgvxji9rJNIkiRVMU+GEBYB04DZwNXp9iak/u02s4xjZgKrep83Xcs+a9MD+D7G+M8Y4/J0D6d3fsHx/4kxTosxLosxTgU+BI5Ob/s1sDTG+HYIoQWpotgF6ScAZgP/It0jXlLFYRFJVdVcYLP1PL/dmtV70UxNt/10jjWKUEuB1QY/3BAxxiXA8cBZwMwQwtMhhK03IM+qTG1KrX+/EXkeBs4FfkUZPbPSj+xNTD9C9yOp3lfrG5hx2ro2xhjfBb4GAqlilyRJkuDodM/0bsDWlHzmmk/qpmOrMo5pBfyQXp67ln3Wph0weaOSpqz5mW8Qqd5JkOohv6oXUgegOqnPuj+mP1PeRar3vKQKxCKSqqq3gOWU3CkpywxSv/BWac/PH/XaUEuAOqXWW5beGGN8LsZ4CKlf+p8Dd29AnlWZvtvITKs8DJwNPJPuJfST9ONml5Dqitw4xtgIWECq+AOwtkfQ1vloWgjhHFI9mmYAF298dEmSpMonxvgq8ADw/9LrS0h9fi1r1t8+lDyC9iJwaHq4hA0xDdhyLdvW+fl1VdQ11h8DuqUfx+tFSRFpGrAC2CzG2Cj91SDGuN0G5pSUIywiqUqKMS4gNfj1bSGEo0MIdUII1UMIh4cQ/pHebTBwZQihWXqA6qtIPX61MT4GDgghtE8P6l16po0WIYSj0r/sV5B6LK6ojHM8A3QOIZwYQqgWQjge2BZ4aiMzARBjnAIcSGoMqDXVJ/Xs+hygWgjhKlKzfKwyC9g8/IIZ2EIInYHrST3SdgpwcQhhnY/dSZIkVUH/Bg4p9TnpUqBfCGFACKF+CKFxeuDrfUgNuwCpm4PTgCdCCFuHEPJCCE1DCJeHEI4o4xpPAS1DCBeE1KQy9UMIe6W3fUxqjKMmIYSWwAXrC5weAmIMcD8wJcY4Md0+k9TMcv8MITRI59oyhHDgRvy5SEqQRSRVWTHGm4ELSQ2WPYfUL9xzSc1YBqlCx/vAp8A4Us94X//zM23QtV4AhqbP9QGrF37ySA02PQOYR6qgc3YZ55hL6rn1P5Hqqnwx0CPG+MOa+25EvrExxrJ6WT0H/A/4ktSjc8tZvdvyqsEd54YQPlzfddKPDz4C3BRj/CTG+BWpGd4eDumZ7yRJkvRTQeYh4C/p9bGkJkI5htS4R1NJDVC9X/ozFTHGFaQG1/4ceAFYCLxL6rG4n411FGNcRGpQ7iNJDYvwFakhDiBVkPoE+IZUAWjoBkYflM4waI3235Ka+GUCqcfzHueXPXonKQcEJ0SSJEmSJEnS+tgTSZIkSZIkSetlEUmSJEmSJEnrZRFJkiRJkiRJ62URSZIkSZIkSetlEUmSJEmSJEnrVS3pAGvz2pfznDZO2gB7dmySdASpQqhVjZDta9Te5dys/O5a9tF/s55dWqVRo0axU6dOScdQKUuWLKFu3bpJx9AafF9yj+9JbvJ9yT0ffPDBDzHGZhtzbM4WkSRJklT+WrRowfvvv590DJUyZswYunXrlnQMrcH3Jff4nuQm35fcE0KYurHHWkSSJClTgk+JS5IkqfLy064kSZIkSZLWy55IkiRlSnDoIkmSJFVe9kSSJEmSJEnSetkTSZKkTHFMJEmSJFViFpEkScoUH2eTJElSJeYtU0mSJEmSJK2XPZEkScoUH2eTJElSJeanXUmSJEmSJK2XPZEkScoUx0SSJElSJWYRSZKkTPFxNkmSJFViftqVJEmSJEnSetkTSZKkTPFxNkmSJFVi9kSSJEmSJEnSetkTSZKkTHFMJEmSJFViftqVJClTQsjOl1SGEMJ9IYTZIYTP1rI9hBD+E0KYFEL4NISwa3lnlCRJlYtFJEmSpIrpAeCwdWw/HNgq/dUfuKMcMkmSpErMx9kkScoUH2dTOYoxvhZC2Hwdu/QEHooxRuDtEEKjEEKrGOPMcgkoSZJyz+TRm3S4RSRJkqTKqQ0wrdT69HSbRSRJkqqaZfN4/ob+7Fzt2U06jUUkSZIyxfGLlFvK+oGMZe4YQn9Sj7zRrFkzxowZk8VY+qUWL17se5KDfF9yj+9JbvJ9SV71FbN549b/cOkzB7Pf5g2A+zf6XBaRJEmSKqfpQLtS622BGWXtGGMcCAwE6NKlS+zWrVvWw2nDjRkzBt+T3OP7knt8T3KT70uyli0r4Mzf/JFBYw8B4JBdl/H6lI0/n4M3SJKUKSEvO1/SxhkF/DY9S9vewALHQ5IkqeqYPn0hB+w7kEFjm1Gv5gpG/PEj/jLs4U06pz2RJEnKlIQKPiGE+4AewOwY4/bptibAUGBz4BugT4xxfnrbZcAZQBEwIMb4XLp9N1IzftUGngHOTw/KrBwUQhgMdAM2CyFMB64GqgPEGO8k9R4eAUwClgKnJZNUkiSVq+Ii3ho0kF7nz2bWPNiiyXxGnTaY7a8bB3mbVgayiCRJUsX3APBf4KFSbZcCL8UYbwwhXJpevySEsC3QF9gOaA28GELoHGMsIjUFfH/gbVIFiMOA/5Xbd6FfJMZ4wnq2R+CccoojSZLKy/iH4LWLoFrtsrcvnMqzz3Vj1rxu/LrT1ww75TGadtkD6jTf5EtbRJIkKVPykhlYey1Tvfck1UsF4EFgDHBJun1IjHEFMCWEMAnYM4TwDdAgxvgWQAjhIeBoLCJJkiTljqKV8Gy/9e529SGv0q7RQvqdfTjVq/8ZOvfOyOUtIkmSlONKz5yVNjA9EPK6tFg1/k2McWYIYdWtpzakehqtsmra94L08prtkiRJygWf3AUvnlWy3vtlaLgFAPPmr+D8S97lpmt3o3WrOuTlVePMi9pmPIJFJEmSMiVLYyKVnjkrA9Y27fsGTwcvSZKkcjD/K/j6afjkdsivBT+MK9m2+aHQrhuEwPjxs+nZcxSTJ89n4fLqjBzZN2uRLCJJkpQpIZnH2dZiVgihVboXUitgdrp9bdO+T08vr9kuSZKk8rDoO/jgn1CwBCaPhiVrmVS13zjYbHsARo36gpNOGs7ixSvZdddW/Pe/h2c1okUkSZIqp1FAP+DG9OvIUu2DQgg3kxpYeyvg3RhjUQhhUXoq+HeA3wK3ln9sSZKkKqJgKXz9FDx1fKpHeywue79mO8G+f4UGHaBua6izGTFG/va31/nLX14hRujbd3vuvfco6tSpntXIFpEkScqULD3Ott7Llj3V+43AsBDCGcC3QG+AGOP4EMIwYAJQCJyTnpkN4A+kZnqrTWpAbQfVliRJypa3r4d3/55aLl1A2vpEaHsAVK8DnXpBjXqrHRZj5MQThzNkyGeEAH//+0Fccsm+hHLoFW8RSZKkCm4dU70ftJb9bwBuKKP9fWD7DEaTJEmqer58HGa+s+59VvwI4+4pWT/sAdjm5NTwCOu5MRlCYPvtm1G/fg0GDTqWHj06b3rmDWQRSZKkTMmtMZEkSZJU3gqWwtMnQHHhhh9z6gRous16d1u2rIDatVOPq11++f6ccspOtG/fcGOTbhSLSJIkZUpCj7NJkiQpB6xcDCOPThWQ8mumxjFapwAde0DTrdd76rvuep/rr3+dN944nfbtGxJCKPcCElhEkiRJkiRJ+uUWz4QJD0PRitT6m1eVbGu4Bexx0SZfoqCgiPPPf5Y77ngfgBEjJnL++Xtv8nk3lkUkSZIyxcfZJEmSKr6FU2H+V+veZ+ViGNWr7G0du8Oh921yjDlzltC792O8+upUatTIZ+DAHvTrt/Mmn3dTWESSJEmSJElVW8ESePXPsOAb+ObZX3Zsh0Og1V6p5ea7wVZHb3KcTz+dxVFHDWbq1AW0bFmPESOOZ++9227yeTeVRSRJkjLFMZEkSZIqns+HpAbDXlP7Mie6LRGLYIczYZuTMhpn7tyl7L///SxcuII99mjNiBHH06ZNg4xeY2NZRJIkKVN8nE2SJKliiMUwZxws/Gb1AlL7X8MOv4O2B0K9VolEa9q0DldddQAffzyLgQN7/DQjWy6wiCRJkiRJkqqOqS/C44f8vP3k96HFbuWfB1i8eCVffTWXXXZJFa4uvHAfAEKO3aS0iCRJUqb4OJskSVLuKi6EQXvDrA9Wb2+5Z6r3UUIFpClT5tOz5xC++24R7733Ozp2bJxzxaNVLCJJkiRJkqTK76sRqxeQug+GzsdBXnKlkVdemULv3o8xd+4yunRpSlFRcWJZNoRFJEmSMiVH7xhJkiQJeOeGkuULixLtRR5j5Pbb3+P885+lqChy+OGdGDz4WBo2rJVYpg1hv3tJkiRJklS5LZwG8z5PLe95WaIFpJUri/j975/i3HP/R1FR5OKLuzJ69Ak5X0ACeyJJkpQ5jokkSZKUe1YsgLvbl6xv+9vksgAff/w99933EbVqVeOee47kpJN2TDTPL2ERSZKkTLGIJEmSlFuW/wi3NS5Z73YzNN06uTzAnnu24d57j2K77Zqz++6tE83yS1lEkiRJkiRJlU/B0tULSPtcA7v9MZEoQ4d+RsOGtTjssE4A9Ou3cyI5NpVFJEmSMsWBtSVJknLHf+qWLG93KnS9utwjFBdH/vKXl/nb38bSsGFNJk48h1at6pd7jkyxiCRJkiRJkiqPGGHwPiXrXY6HQ+8r9xgLF67g5JOHM3r0l+TnB6677le0bFmv3HNkkkUkSZIyxTGRJEmSkjftFZj5Tsl6jyHlHmHSpHkcddRgJk78gcaNazFsWG8OPrhjuefINItIkiRlio+zSZIkJWfZXHj3Rnj//5W0nb+s3GO8/PIUjjtuGPPnL2fbbZsxcmRfOnVqUu45ssEikiRJkiRJqliWzYWxV8Cnd619nyMGQbVa5ZcprWbNfBYvXsmRR3bmkUeOoUGDmuWeIVssIkmSlCk+ziZJkpR9y+bC7ZutfXvHHrDXFdB673KLVFwcyctL9Urfd9/2vPnmGey6a6uf2ioLi0iSJEmSJKliWPgt3NupZL3tAXD4Q1C/fUlbOQ8x8P33iznuuGFcdFFXevbcGoDdd29drhnKi0UkSZIyxTGRJEmSsufrZ2BE95L1rU+A7oOSywO8//4Mjj56CN99t4gff3yZHj06k59feXunW0SSJClDgkUkSZKkzIkRFn4DYy6EwmXwzXMl2/a6HPa7IbFoAI8++ilnnjma5csL2X//9jz+eJ9KXUACi0iSJEmSJCkXPX8mfHbfz9t7PQ0djyj/PGlFRcVcfvlL/OMfbwLQv/+u3HrrEdSokZ9YpvJiEUmSpAyxJ5IkSVKGLJy2egGpc2/Y/nRosDk03TqxWAC///1T3HvvR1Srlsd//nMYf/jDHonmKU8WkSRJkiRJUm5YsSD1+FrpAtI586BW4+QyreH3v9+N556bzMMP96Jbt82TjlOuLCJJkpQpdkSSJEn65Wa+CzPegMmjYNqY1bcdfGdOFJAmTZpHp05NANhjjzZMmnQeNWtWvZJK1fuOJUmSJElSMlYugh8+K1mf9gqMveLn+7X/NRx6HzToUH7ZyhBj5J//fItLLnmRwYOPpU+f7QCqZAEJLCJJkpQxjokkSZKU9uGtMOcTusycCc89UtL+2b1rP2bX86FGQ9h1ANRumv2M67FsWQH9+z/FI498CsCUKfMTTpQ8i0iSJGWIRSRJklSlxQjfPAvPnAzL5wHQCmBuGfs22xmq1UofVwTdh0CjjuWVdL2++24hvXoN5b33ZlC3bnUefrgXvXptk3SsxFlEkiRJkiRJG6ZgKSyYUrL+zg2w4GsgwPfvpQpCpXzR4c906dxl9XO02AVa7Jb9rBvp7ben06vXUL7/fjFbbNGIkSP7ssMOLZKOlRMsIkmSlCH2RJIkSZXagilwzwb2Ftrnatj7Sma+NpYuO3bLaqxMKiwspl+/J/n++8X86lebM2xYbzbbrE7SsXKGRSRJkiRJkgQrF8PUF6C44Ofb3vgLzP+yZL3ptiXLtZvB/n9PLedVg+a7pF4roGrV8hg27DgefPATbrrpYKpXz086Uk6pmO+qJEk5yJ5IkiSpQpj1Ebx+CbDGZ5epz2/Y8YfcDTuemfFYSZk/fxnDh0/kjDN2BWCnnVpy880tE06VmywiSZKUKdaQJElSLitYBh/+C8Zese796rWB1l1/3t6wI3S9FqrVzE6+BEyYMIeePYcwadI8ateuzokn7pB0pJxmEUmSJEmSpMpu4bdwd4fV2w74BzTbcfW2Oi2g+c7llytBo0d/wUknDWfRopXsvHNL9tuvfdKRcp5FJEmSMsTH2SRJUs568Q8ly3VbwjH/qzLFojXFGPn738dy5ZUvEyP06bMd9913FHXr1kg6Ws6ziCRJkiRJUmVUuAI+HwTL58GS71NtWx0DRz2RbK4ELV1awOmnj2To0PEA3HDDr7nssv28GbiBLCJJkpQhfviQJEnlau7nMPpYiLHs7fMm/rxt6xOymynHLV9eyPvvz6BevRo8+ugxHHVUl6QjVSgWkSRJyhCLSJIkKetiMbx9PYx/ABZM2bBj8mvCzudA7WawRfesxst1TZrUZtSoE4gxst12zZOOU+FYRJIkSZIkqaJ48Q/w6cDV23a7EHY4o+z9q9WGhltkP1cOu+eeD5kwYQ4333woANtu2yzhRBWXRSRJkjLEnkiSJClrYoQne8LXo0vaDn8YtjwKajZILlcOKygo4o9/fI7bbnsPgN69t2WffdolnKpis4gkSZIkSVKuG9gWFs8oWT9jEjTaMrk8Oe6HH5bSp89jvPLKN9Sokc+dd3a3gJQBFpEkScqUBDsihRDOB36XTnF3jPHfIYQmwFBgc+AboE+McX56/8uAM4AiYECM8bkkckuSpDIs+AZmvgPv3ZQeHDtA4bLUtrqt4JQPoW7LJBPmtHHjZtGz5xCmTPmRli3rMXx4HwtIGWIRSZKkCi6EsD2pAtKewErg2RDC0+m2l2KMN4YQLgUuBS4JIWwL9AW2A1oDL4YQOscYi5L5DiRJquKWzoZnT4XiwtT61BfK3i/kwe+np15VprFjv+Wwwx5hyZICdt+9NSNGHE/btj7ulykWkSRJypAEx0TaBng7xrg0neNVoBfQE+iW3udBYAxwSbp9SIxxBTAlhDCJVAHqrfKNLUlSFbVgCkx4uKRo9PZfy95vq2Ogfjvoem2qcFS9HjgG4zrtsENz2rVryG67teLuu4+kdu3qSUeqVCwiSZKUIdkqIoUQ+gP9SzUNjDGWnpblM+CGEEJTYBlwBPA+0CLGOBMgxjgzhLBqHts2wNuljp+ebpMkSZkWI8z+CN68Cr5/D/KqrT62UWk7nQ2deqaWN9se6rUuv5wV2JIlK6lWLY+aNavRsGEtxo49jSZNajvpSRZYRJIkKcelC0YD17F9YgjhJuAFYDHwCVC4jlOW9YkqblJISZJUtufPhM/uK3vbNidB486p5dZdocPB5Zerkvjmmx/p2XMIe+zRmrvvPpIQAk2b1kk6VqVlEUmSpAxJ8m5XjPFe4N50jr+R6l00K4TQKt0LqRUwO737dKD06JJtgbXcEpUkSRtt6kurF5A694ED/w9CPtRsCDXqJZetEhgz5huOO24Yc+cuY/nyQn78cTmNG9dOOlalZhFJkqRKIITQPMY4O4TQHjgG2AfYAugH3Jh+HZnefRQwKIRwM6mBtbcC3i3/1JIkVVLzvoDhR8CCr0vazltk0SiD7rjjPQYMeJbCwmIOO6wTgwcfS6NGtZKOVelZRJIkKVOSfez+ifSYSAXAOTHG+SGEG4FhIYQzgG+B3gAxxvEhhGHABFKPvZ3jzGySJG2kGPnpqfAfJ8MH/4JP7lh9nxPftoCUIStXFjFgwP+4664PALjooq78/e8HkZ/vjHXlwSKSJEkZkvDjbPuX0TYXOGgt+98A3JDtXJIkVThLZsHXT8GG3F9ZNH3tM6sB7PZH2OcaqOkU85nyt7+9zl13fUDNmvncc89RnHzyjklHqlIsIkmSJEmStMqdLTfywFU3kyLsfA5seRR0OAScISyj/vznrrzzzndcd1039tjDyWXLm0UkSZIyxGlkJUmq4AqWlSy3PbBk5rR1yasGO/4emu+UvVxV3LPPTuLAAztQu3Z16tWrwf/+d1LSkaosi0iSJEmSpKpl1kcw/4uftxetKFk+fky5xVHZiosj11wzhr/+9TVOOmkHHn64lzftEmYRSZKkDPFDjSRJOWzpbChcBi8PgMmj1r1vzYblk0lrtWjRCk45ZQQjR35BXl5g991bJx1JWESSJCljLCJJkpRjCpbCp3fBh7fAwqk/397l+LKP69g9u7m0TpMnz6NnzyGMHz+Hxo1rMXTocRxyyJZJxxIWkSRJkiRJlUXBUvjudXjzGlgwBZbO+vk+9dtBvbZw7LPOmpaDXnrpa/r0eZx585axzTabMWrUCXTq1CTpWEqziCRJUqbYEUmSpPIXi+G1S9lu8tvwwetl71OvLexxMWx/GtSoV7759Is88MAnzJu3jCOP7MwjjxxDgwY1k46kUiwiSZIkSZIqnhlvwXdjYc4nMPFRmpXeVq8tbH4odL0WqtWC2k2TSqlf6K67erDXXm04++w9yMvzDl2usYgkSVKGOCaSJElZtuR7GHogEMqeXe3Ix6F+W2i1V7lH08aZNWsxV175Mv/+92HUrVuDOnWqc+65eyYdS2thEUmSJEmSlLxYDDGufXvRCriz1c/bd/sT5OXz/uJO7N752OzlU8Z98MEMjj56KNOnL6RGjXxuu80BzXOdRSRJkjLEnkiSJP0CM9+F2R+llud/AR/8a8OP3eMS2O5UaNABqtcGYPGYMRmPqOwZPHgcp58+iuXLC9l333ZcddWBSUfSBrCIJElShlhEkiRpHYoL4fFDoGAJFC6HH8aVvV/IW/d5tv0tHHBj5vOpXBQVFXPFFS9z001vAHDmmbtw223dqVEjP+Fk2hAWkSRJkiRJ2fXlEzD6uLK37dg/9ZpXA3Y+B5puXX65VK5WrCjk2GOH8fTTX5GfH7jllsM4++w9vBFXgVhEkiQpU/z8I0kSTB8Li6bBezfBvImpnkWFy0u2t9wTfn1ranmz7aB63WRyqtzVqJFPs2Z1adq0No891ptf/WqLpCPpF7KIJEmSVEGFEA4DbgHygXtijDeusb0h8AjQntTnvv8XY7y/3INKqvxihEXTYcwF8NXwte/XcyR0Oqr8ciknrFxZRI0a+YQQuPPO7lxzzYF06NAo6VjaCBaRJEnKELtiqzyFEPKB24BDgOnAeyGEUTHGCaV2OweYEGM8MoTQDPgihPBojHFlApElVWYP7wJzPlm9rUtfqNcaul6T6o1UrQ74u7JKiTEybNg0BgwYyNixp9OgQU1q1qxmAakCs4gkSVKGWERSOdsTmBRj/BoghDAE6AmULiJFoH5I/XDWA+YBheUdVFIlVFQA920FRStSX8vnl2xrsDmcOuGnWdNUNS1fXkj//ktXTUsAACAASURBVKN5+OGvAXjmma/o23f7hFNpU1lEkiRJqpjaANNKrU8H9lpjn/8Co4AZQH3g+Bhj8ZonCiH0B/oDNGvWjDFOk51TFi9e7HuSg6ri+1J36WQ2n/kQUEyzH8eWuc+YXV9O9TZ6453yDUfVfE9y1Q8/rOAvfxnP558vombNPC67bGtatvzB96cSsIiknykuKuL6C0+jUZNmDLj6nz+1Pzf8UR6//7/c/Mj/qN+wEYUFBTx8201MnTSREPLo2/+PdNlh1wSTS+Xj+5kzueKyi5k79wdCyOO43n046ZR+3Pz/buLVMa9QvXp12rZrz3XX/50GDRpQsHIl1117NRPGf0ZeCFx82RXsseea/85TZWBPJJWzsn7g4hrrhwIfA78GtgReCCG8HmNcuNpBMQ4EBgJ06dIlduvWLfNptdHGjBmD70nuqXLvy6cD4YPf/7y9YUfomy4o1W1Bt5BXvrlKqXLvSY56553pDBgwlJkzF9OhQ0OuvLITZ57ZI+lYypDk/oYrZ704ehit2m6+Wtu8ObOY8PF7NGnW8qe2158fCcA1/32UP/71Fobd+x+Ki392c1OqdPKr5fPniy/lydH/45HBQxkyeBCTJ01i73325Yknn+LxEaPp0GFz7r37LgCeePyx1OuTo7nznvv55//d5N8VSZkwHWhXar0tqR5HpZ0GDI8pk4ApgHNnS/plRvaCF0oVkPa6Ao4aDsc+C6dNhHqtUl8JFpCUG776ai4HHvgAM2cu5sADO/Dee7+jU6d6ScdSBvm3XKuZ98Nsxr33Bvv9ZvUZE4becwvHnXbOauPgzfh2CtvstDsADRo1oU7dekydNLE840qJaNasOdtsux0AdevWo2PHjsyePYuu++5HtWqpDp477rQzs2d9D8DXkyex1957A9C0aVPq16/P+M8+Sya8siqEkJUvaS3eA7YKIWwRQqgB9CX16Fpp3wIHAYQQWgBdgK/LNaWkim3muzDpyZL1Uz6G/a6HrXrB5odCfo3ksinnbLVVU047bWf+8IfdeeGFU2jWrG7SkZRhWSsihRAOL6PtrGxdT5kx9O5/c9xp55KXV/Kj8fE7r9O4aTPabbHVavu222IrPn7nNYqKCpnz/QymTv6CeXNml3dkKVHffTedzydOZIcdd1qt/cnhT7Dv/gcA0LnL1ox5+SUKCwuZPn0aEyeMZ9b3M5OIq2wLWfqSyhBjLATOBZ4DJgLDYozjQwhnlfrM9VegawhhHPAScEmM8YdkEkuqEJbNg3H3wSd3whOHwaBSj+CfMx+a77T2Y1UlzZ+/jMmT5/20fttt3bn99u5Ur56fYCplSzbHRPpLCGFFjPFlgBDCJUA34M4sXlOb4JN3x9KgYWM6dNqaL8Z9CMCK5ct5ZtgDXHDdLT/bf99DejBz2jdc/8fTadq8JVtuvQP5+f6PQlXH0iVL+NMFA7jo0supV6+km+7dd91BfrV8uvdI9eg7+phjmfL1ZE7scyytWrdmp513Ib+af1ckbboY4zPAM2u03VlqeQbwm/LOJamCihFub1r2toPvhFpOy67VTZw4h549hxAjvPvumTRuXJu8PO+AVWbZLCIdBTwVQrgIOIzU8/dHreuA0jOD/Om6mznq+H5ZjKc1TZ74KR+/+zrjPniTgpUrWb50CffdfC0/zJrJdQNOAWD+D3O4/oJTufzme2nYuCnH/+6Cn46/8aLf0bx1u7WdXqpUCgoKuPCCARzR/UgOPqTk32ejnhzBa6+OYeC9D/z0GFK1atW46NLLf9rntyf1pX37zcs7ssqBj55JkiqspbPhvs4l6407Q7tfQc1GsMfFULtJctmUk55++ktOOOEJFi1ayU47tWDx4pU0blw76VjKsqwVkWKMP4QQjgJeBD4AjosxrjljyJrH/DQzyGtfzlvnvsq8Y/qdzTH9zgbgi3Ef8tzwR/nD5X9fbZ9Lz+jFFTffT/2GjVixfDkQqVmrNhM+epe8/Gq0br9FAsml8hVj5JqrrqBjx4789tTTfmp/4/XXuP/eu7n3wUeoXbvkF+iyZcuIMVKnTh3eevMN8vPz2bJTpySiS5Ik/VxxIdzRolRDgNO/SCyOcluMkZtueoPLL3+JGKF37225//6e1K3r+FhVQcaLSCGERaSmlw3p1xpAR+C4EEKMMTbI9DWVjEUL5vPvqy8ghEDjps0448Krko4klYuPPvyAp0aNZKvOnelzTE8AzrvgQm762/WsLFjJWWemCks77LQTf7n6OubNm8sf+p9BXl4ezZu34IYb/5FkfGWRPZEkSRXSC6WGrm21N/QZk1gU5balSws488xRDB6cmiTmr3/9FVdcsb+fgaqQjBeRYoz1M31Olb8uO+xKlx12/Vn7jfeO+Gl5sxatuP7OoeUZS8oJu+62O5+M//nduf0POLDM/du0acuop5/LdixJkqSNM//LkuUT3gQLAlqL556bxODBn1GvXg0eeaQXPXtunXQklbOsPc4WQugFvBxjXJBebwR0izE+ue4jJUmqmPzMLUnKeTHCgzvAslITNS5Pz6x17HP+MtM69eq1DTfeeBDdu3dm++2bJx1HCchb/y4b7epVBSSAGOOPwNVZvJ4kSYkKIWTlS5KkjChcAS/0h7njYemskq/iAqheD5psk3RC5aD77/+ITz+d9dP6JZfsZwGpCsvm7GxlFaiyeT1JkiRJ0prmT4Jxd8N7a4zLeNbMkuUaDaB6nfLNpZxWUFDEn/70PLfe+i6bb96Izz77g4NnK6tFnfdDCDcDt5EaYPs8UrO0SZJUKdlpSJKUU5bOgVHHwHdjV2+vVgtOnQB1WyaTSzlv7tyl9OnzOC+/PIXq1fO48sr9LSAJyG4R6TzgL8BQUjO1PQ+ck8XrSZIkSZIAigpg2pjVC0jbnQq7XQjNdkgqlSqAzz6bTc+eQ/j66/m0aFGX4cOPp2vXdknHUo7IWhEpxrgEuDRb55ckKdc4fpEkKXFFK+HBHWF+qZlkOxwCRz4GNRsml0sVwsiRn3PyySNYvHglu+3WihEjjqddO39uVCKbs7M1Ay4GtgNqrWqPMf46W9eUJClJ1pAkSYmb8PDqBaRqtaFzbwtI2iBLlxawePFKTjhhe+655yjq1KmedCTlmGw+zvYoqUfZegBnAf2AOVm8niRJkiRVPTPegomDIBbBJ3eUtF9Y7B0OrVeM8afe1CecsAOtW9fngAM62MNaZcpmEalpjPHeEML5McZXgVdDCK9m8XqSJCUqL88PW5KkcvLViNSg2dVqQeHyn2/vO9YCktZr6tQf6dv3CW699XB23701AAceuHmyoZTTsllEKki/zgwhdAdmAG2zeD1JkiRJqvzGXgnv3JBaLl1A2u9vUKM+tNkPmu+cTDZVGK+9NpVjjx3GDz8s5eKLX+Dll/slHUkVQDaLSNeHEBoCfwJuBRoAF2TxepIkJcobvpKkrFvyfUkBCaDPGGi5J+TXgLz8xGKpYrnzzvc577z/UVhYzG9+syVDhhybdCRVENksIs2PMS4AFgC/Aggh7JvF60mSlCjHDpAkZdTUl2D44VCzUUnbslLDzJ63MNXzSNpABQVFnH/+s9xxx/sA/OlP+3DjjQdTrVpewslUUWSziHQrsOsGtEmSJEmSVvnsfvhqOHz9VGp9WRnzE+15qQUk/SIxRnr1GsrTT39FzZr5DBx4JL/97U5Jx1IFk/EiUghhH6Ar0CyEcGGpTQ0A+1dKkiotOyJJkjbZywPgo1tXbzv2eWhe6h/7+TWhZsPyzaUKL4TA6afvwscff8/w4cez555tko6kCigbPZFqAPXS5y5dGl8IHJeF60mSJElSxVZcCM+dDhMeLmk76glosQc0aJdcLlV406YtoF27VNHxmGO24bDDOlGnTvWEU6miyngRKcb4KvBqCGFZjPEfpbeFEHoDX2X6mpIk5QLHRJIk/SIz3oJP7oRYDBMfWX3bH+ZAnc2SyaVKobg4cu21Y7jppjd45ZV+7LNPqhhpAUmbIpujZ/Uto+2yLF5PkqQqK4TwxxDC+BDCZyGEwSGEWiGEJiGEF0IIX6VfG5fa/7IQwqQQwhchhEOTzC5JVc7cz+Gj22BwV5jw0M8LSOcttICkTbJo0QqOO24Y1133GgUFxYwbNzvpSKoksjEm0uHAEUCbEMJ/Sm2qDxRk+nqSJOWKpHoihRDaAAOAbWOMy0IIw0jdzNkWeCnGeGMI4VLgUuCSEMK26e3bAa2BF0MInWOMRYl8A5JU1Qw9YPXBsrvdDLU3gzrNocMhEJwpSxvv66/n07PnED77bDaNGtViyJBjOfTQTknHUiWRjTGRZgAfAEelX1fpACzNwvUkScoJCT/NVg2oHUIoAOqQ+n18GdAtvf1BYAxwCdATGBJjXAFMCSFMAvYE3irnzJJUtYx/CCY9WVJA2uoY2O5U2PLIRGOp8nj55Sn07v0Y8+YtY+utN2PkyL507tw06ViqRLIxJtInwCchhEdJ3eE8EegDTAGeyPT1JEmq6mKM34UQ/h/wLbAMeD7G+HwIoUWMcWZ6n5khhObpQ9oAb5c6xfR0myQpWz74F4y5cPW2Ix9P/A6EKo9Fi1b8VEDq3n0rHn30GBo2rJV0LFUy2XicrTOpLvInAHOBoUCIMf4q09eSJCmXZOtxthBCf6B/qaaBMcaBpbY3JtW7aAvgR+CxEMLJ6zplGW0xE1klSWuIxTBn3OoFpINuh47dLSApo+rXr8lDDx3N2LHfcv31vyY/38cilXnZeJztc+B14MgY4yRIDfaZhetIklQlpAtGA9exy8HAlBjjHIAQwnCgKzArhNAq3QupFbBqVM3pQOn5otuSevxNkpQJC7+FcfdC8Up498bVt538PrTYLZlcqnRmzVrMW29N5+ijtwage/fOdO/eOeFUqsyyUUQ6llRPpFdCCM8CQyj7jqckSZVKgjeUvwX2DiHUIfU420HA+8ASoB9wY/p1ZHr/UcCgEMLNpAbW3gp4t7xDS1KldXeHn7eFPOh6rQUkZcyHH86kZ88hfP/9Yl566bcccEAZP3dShmVjTKQRwIgQQl3gaOCPQIsQwh3AiBjj85m+piRJuSCp2dlijO+EEB4HPgQKgY9I9VyqBwwLIZxBqtDUO73/+PQMbhPS+5/jzGySlCFFpSak3vwwaHsANN0WOvVMLpMqnSFDPuP000eybFkhXbu2c/BslZts9EQCIMa4BHgUeDSE0ITUB9dLAYtIkiRlWIzxauDqNZpXkOqVVNb+NwA3ZDuXJFU5b1xZsnz0SMivkVwWVTpFRcVceeXL3HjjGwCcfvrO3H57d2rWzNo/7aXVlMtPWoxxHnBX+kuSpErJ8VElqYqb/TG894/Ucu3NLCApoxYuXMGJJz7B009/RX5+4F//OpRzz90zsZ7QqposV0qSJElSJgw9sGT5iEeTy6FKac6cJbz55jQaN67FY4/15qCDOiYdSVWQRSRJkjLEO4GSVIW9ehGsXJha3vtK2Pw3yeZRpbPllk148sm+tGlTny23bJJ0HFVRFpEkScoQa0iSVAUVLodnToKvhpe07XXl2veXNlCMkX//+23y8/MYMGAvAGdgU+IsIkmSJEnSL1GwlM1n3A8vDoNP7lh922lfQLWayeRSpbF8eSFnnfUUDz74Cfn5gR49OtOxY+OkY0kWkSRJyhQfZ5OkSq5wOUx5BsY/yOYzR8HMUtta7QVHjYB6rRKLp8ph5sxF9Oo1lHfe+Y46darzwAM9LSApZ1hEkiRJkqQN8fFt8OqfV2876DbYbHtoe0AymVSpvPvud/TqNZQZMxbRvn1DRo7sy847t0w6lvQTi0iSJGWIHZEkqZIrVUCa2fQIWvW4KVVAkjJg9Ogv6N37MVasKGL//dvz+ON9aN68btKxpNVYRJIkSZKktYkRHt0TZr1f0tbrab74tg6tLCApg3bcsQX169fk1FO34T//OZwaNfKTjiT9jEUkSZIyxDGRJKkSGnfP6gWkuq2g4xHw7ZjEIqnyWLx4JXXrVieEQIcOjfj007No1ap+0rGktcpLOoAkSZVFCNn5kiQlZPl8eKF/yfrZP8Dvv0sujyqVzz//gd12G8hNN73xU5sFJOU6i0iSJEmStKaCZXBbk5L1Q++D2k2t7isjnnnmK/ba6x6+/HIuQ4eOZ+XKoqQjSRvEIpIkSRkSQsjKlyQpAU+fULLcZBvo0je5LKo0YozcdNNYevQYxMKFKzj22G14/fXTHP9IFYZjIkmSJElSaU/1hckjU8uNt4JTx9sDSZts2bICzjxzNIMGjQPg2mu7ceWVB5CX58+WKg6LSJIkZYj/vpCkSmD8Q/DF0JL1k973f/DKiAED/segQeOoW7c6Dz/ci169tkk6kvSLWUSSJClDfPRMkiqwT++Gr56Ab54rabtgJeRXTy6TKpVrrunGxIk/cMcd3dlhhxZJx5E2ikUkSZIkSVXbty+vPgsbwBmTLSBpk73wwmQOOqgjeXmBNm0a8Prrp3nTSRWaA2tLkpQhDqwtSRXU8CNKlg+5C37/HTTqmFweVXiFhcVccMGz/OY3j3Ddda/+1O7vdVV09kSSJEmSVHW9fQMUrUgt738j7Nh/3ftL6zFv3jKOP/5xXnzxa6pXz6Nt2wZJR5IyxiKSJEkZ4s1FSaqA3riyZHmPi5LLoUph/PjZ9Ow5hMmT59O8eV2eeKIP++3XPulYUsZYRJIkKUPsoi5JFUzhipLlMyZBcLQPbbxRo77gpJOGs3jxSnbdtRUjRhxP+/YNk44lZZT/l5QkSZJUNc14s2S5QYfkcqjCKy6O/N//vcnixSvp23d7Xn/9NAtIqpTsiSRJUobYEUmSKphvX0q9Nt4K8vynkTZeXl7g8cd7M3ToeM47b097J6vSsieSJEmSpKpp4qOp17wayeZQhfTttwv485+fp6ioGIAWLeoxYMBeFpBUqVlulyQpQ/zQKEkVSMESWPhNanmX8xKNoopn7NhvOeaYocyZs5Tmzety8cX7Jh1JKhcWkSRJyhBrSJJUgTy0c8lyp6OTy6EK5+67P+Ccc56hoKCYQw7pyO9+t2vSkaRy4+NskiRJkqqWj++AHyellht3hrotks2jCqGgoIhzz32G/v2foqCgmD/+cW+eeeYkGjeunXQ0qdzYE0mSpAzJsyuSJOW+giXw0tkl66dOSC6LKowFC5bTq9dQXnnlG2rUyGfgwB7067fz+g+UKhmLSJIkSZKqjs+HlCyf/hXk5SeXRRVGnTrVAWjZsh4jRhzP3nu3TTiRlAyLSJIkZYgdkSQpx316D7zwu9RyrcbQuFOyeZTzioqKyc/Po3r1fIYN682KFYW0adMg6VhSYhwTSZIkSVLV8Mr5JctHDEouh3JecXHkmmvGcPjhj1JYWAzAZpvVsYCkKs+eSJIkZUiwK5Ik5a7iIihcmlo+6V1ouUeyeZSzFi9eSb9+TzJ8+ETy8gKvvTaVX/96i6RjSTnBIpIkSRmSZw1JknLXtDEly023SyyGctuUKfPp2XMI48bNpmHDmgwZcpwFJKkUi0iSJEmSKr/PB6deQz5Ur5NsFuWkMWO+4bjjhjF37jK6dGnKyJF96dJls6RjSTnFIpIkSRni42ySlKNihM/uTS1vdWyyWZST3nxzGocc8jCFhcUcfngnBg06lkaNaiUdS8o5FpEkSZIkVW6Lvi1Z3v3C5HIoZ+21VxsOOmgLdtqpBX/720Hk5zsHlVQWi0iSJGWIHZG0KUIIdWOMS5LOIVVKRStLllvtlVwO5ZTZs5cQAjRrVpf8/DxGjz6B6tXzk44l5TTLq5IkZUjI0n+q3EIIXUMIE4CJ6fWdQgi3JxxLqpwadUo6gXLERx/NZPfdB3LsscNYubIIwAKStAEsIkmSJCXrX8ChwFyAGOMnwAGJJpKkSmzo0M/Yd9/7mDZtIQUFxSxatCLpSFKF4eNskiRlSJ6dhrSRYozT1hiYvSipLJJUWRUXR6666hVuuOF1AE49dWfuvLM7NWv6z2JpQ/m3RZIkKVnTQghdgRhCqAEMIP1omyQpMxYuXMHJJw9n9OgvycsL/POfv+H88/dyZlXpF7KIJElShvhBVBvpLOAWoA0wHXgeODvRRFJlM/ujpBMoYfff/xGjR39J48a1GDasNwcf3DHpSFKFZBFJkqQMsYakjdQlxnhS6YYQwr7AGwnlkSqfhVNTrwu+TjaHEnPeeXsxdeoCzj57Dzp1apJ0HKnCcmBtSZKkZN26gW2SNlq6yr/rBcnGULmJMXLnne8zY8YiAPLyAjfffKgFJGkT2RNJkqQMyUuoK1IIoQswtFRTR+Aq4KF0++bAN0CfGOP89DGXAWeQGsB5QIzxuXKMLCCEsA/QFWgWQriw1KYGgPNMS5mwbB7M+gBeuyjdYJfRqmDFikLOOutpHnjgYx588BPGjj2N/Hz7T0iZ4N8kSZIquBjjFzHGnWOMOwO7AUuBEcClwEsxxq2Al9LrhBC2BfoC2wGHAbeHECxalL8aQD1SN/Xql/paCBy3IScIIRwWQvgihDAphHDpWvbpFkL4OIQwPoTwaoayS7nvx6/h9qbwxG9K2prvlFwelYuZMxfRrduDPPDAx9SuXY0LLtjLApKUQfZEkiQpQ3JkTKSDgMkxxqkhhJ5At3T7g8AY4BKgJzAkxrgCmBJCmATsCbxV/nGrrhjjq8CrIYQHYoxTf+nx6cLfbcAhpAbkfi+EMCrGOKHUPo2A24HDYozfhhCaZyi+lPs+u7dkufW+sMVhsO0pyeVR1n3++UJOPvluvvtuEe3aNWDkyL7sskurpGNJlYpFJEmSKpe+wOD0cosY40yAGOPMUgWENsDbpY6Znm5TMpaGEP6PVM+wWqsaY4y/Xs9xewKTYoxfA4QQhpAqEE4otc+JwPAY47fpc87OZHAppxWtTL1u1w8OeyDRKMq+QYPGcf75n7ByZTH77deexx/vTYsW9ZKOJVU6FpEkScqQkKWuSCGE/kD/Uk0DY4wDy9ivBnAUcNn6TllGW9z4hNpEj5Iau6oHcBbQD5izAce1AaaVWp8O7LXGPp2B6iGEMaQelbslxvjQmicq/TPWrFkzxowZ88u+A2XV4sWLfU82Qsfp02gPTP6xFtOy8Ofn+5JbXn55KitXFtOjRysGDOjAxInvM3Fi0qkE/l2pbCwiSZKUIdl6nC1dMPpZ0agMhwMfxhhnpddnhRBapXshtQJW9UKZDrQrdVxbYEbGAuuXahpjvDeEcH6pR9w2ZOyiDSkGViM1TtZBQG3grRDC2zHGL1c7qNTPWJcuXWK3bt1+6fegLBozZgy+JxvhliMA2HLLLdlyj24ZP73vS2458MDIVlsN5+KLj8naTR1tHP+uVC6OMCZJUuVxAiWPsgGMItWrhfTryFLtfUMINUMIWwBbAe+WW0qtqSD9OjOE0D2EsAupwt76bEgxcDrwbIxxSYzxB+A1wJGFVfnFYihcllr+/+zdd3xUZdrG8d89SSiB0DuIgFKEVQSRIhZARUUERJQguoCs6IqKiwXdVbHuq2tbsSOC6K70EhREkRWxAQIiglSR3juEkva8f8xIIjUJc3Iyk+vrZz7nOWdOuTAQyD1Pia/obxbxxPLlO7jssvdZu3YPEOwN3KxZWRWQRDymIpKIiEiYBMw8eWWHmcUTnGB5fJbDzwFXmtmK0HvPATjnFgOjCc6dMxXo65xLD+P/CsmZZ8ysJHA/8AAwBLgvG9f9ANQ2s5qhoYyJBAuEWSUBl5hZbOj3SDNAAzwk+u3M0tnurA7+5RBPTJ26kqZN32XmzDX84x//8zuOSIGi4WwiIiJRwDl3ACh71LEdBIcxHe/8Z4Fn8yCanIJz7pNQcw/QGsDMWmbjujQzuxv4DIgBhjrnFpvZnaH333bOLTGzqcBCIAMY4pxb5MWvQyRfmftCcFu4FBQp5W8WCRvnHC+99D0DBnxBRobj+uvr8dZb1/odS6RAOWURycz6AcOAfQQ/GWsEPOyc+9zjbCIiIhFFHeglJ8wsBriJ4ATZU51zi8ysPfB3gvMXNTrVPZxzU4ApRx17+6j9F4AXwpVbJCJsD9VKS9fxN4eEzaFDadx++8f85z8LAXjiict47LHLCAT0t69IXspOT6TbnHOvmtlVQHmgF8GikopIIiIiWWgeBsmh9wjOaTQHGGRma4AWBD+sm+hrMpFI9/v349av+ptDwiItLYM2bYbz/ffrKVYsjg8+uJ7Onc/xO5ZIgZSdItLv/yJuBwxzzv1k+leyiIiIyOlqApznnMswsyLAduBs59xmn3OJRLZdK2HT7GA7prC/WSQsYmMDdO58Dps27ScpKZHzztNk6SJ+yU4RaZ6ZfQ7UBB4xswSCY+pFREQkC/WolxxKcc5lADjnDpnZchWQRMJgXZaJlsuf518OOW1btyZToUIxAO6/vwV9+lxAiRIqDIr4KTurs/UGHgYuDE3aWYjgkDYRERERyb16ZrYw9Po5y/7PZrbQ73AiEWvx8OC2QS8IxPibRXIlLS2D/v0/o379N1i1ahcQHDKuApKI/07YE8nMGh91qJZGsYmIiJyY/p6UHNKEHiJe2PhdcBtXzN8ckiu7dh2ka9exTJu2itjYAPPmbaRWrdJ+xxKRkJMNZ3vpJO85oE2Ys4iIiEQ01ZAkJ5xza/zOIBJ1UvZnts/v618OyZUlS7bRocNIVq7cSfny8YwbdxOXXHKm37FEJIsTFpGcc63zMoiIiIiIiMhpWZtlPqSy9fzLITn2ySfLufnmcezbl0KjRpWYODGR6tVL+h1LRI5yyom1zSwe6A9Ud871MbPaQF3n3CeepxMREYkgGs4mIuKjtEPw8Q3BdhkVkCLJ+vV7ueGG0aSkpHPTTQ0YNqwj8fFxfscSkePIzupsw4B5wEWh/fXAGEBFJBEREZEwMLOiBD+wW+Z3FpGItWk2ZKQF2zWv9TeL5Ei1aiV4+eW27NlzmEceuVgfyojkY9kpcbgGCAAAIABJREFUIp3lnOtqZt0AnHMHTX+qRUREjhHQ346SC2Z2HfAiwRVwa5rZ+cBTzrkO/iYTiTDf/D24jSsGlz7vbxY5pXXr9vDbb7u59NLgnEd9+zb1OZGIZEcgG+ekhD4dcwBmdhZw2NNUIiIiIgXHE0BTYDeAc24BUMPHPCKRxznY/EOwXacLBGL8zSMn9e23a2nS5F06dBjB8uU7/I4jIjmQnZ5IA4GpwBlm9l+gJdDTy1AiIiKRSB11JZfSnHN79PtHJJdSD8Cg4oQ+84YWA32NIyc3ZMh87rprMqmpGVxxRS3KlYv3O5KI5MApi0jOuWlmNh9oDhjQzzm33fNkIiIiEUYlAMmlRWZ2MxATWsDkXuA7nzOJRI7lYzlSQCpdGxKq+xpHji81NZ3+/T/j9deDPcbuu68ZL7zQltjY7AyOEZH8Ijs9kQAuAy4m+N05DpjgWSIRERGRguUe4B8Epwv4CPgMeMbXRCKRJGVfcFu4JNy23N8sclw7dhzgxhvH8OWXqylUKIa3376WXr0a+R1LRHLhlEUkM3sTOBsYETp0h5ld4Zzr62kyERGRCBPQcCTJnbrOuX8QLCSJSE65jOC2Xnd/c8gJLV++g2++WUvFisWYMKErLVqc4XckEcml7PREugz4k3Pu94m1hwM/e5pKREREpOB42cwqA2OAkc65xX4HEoko818JbuM0t05+1aLFGYwa1YULL6xKtWol/I4jIqchOwNQlwFZBxafASz0Jo6IiEjkMvPmJdHNOdcaaAVsAwab2c9m9qi/qUQixPqZsOe3YLvhX/3NIkdkZDieeuorkpKWHjl2/fXnqIAkEgVO2BPJzD4mOAdSSWCJmc0J7TdDkz2KiIgcQ6trSW455zYDg8zsS+Ah4HE0L5LIqW3P0nGvVC3/csgR+/en0LPnRMaNW0LJkoX57bd+lC5d1O9YIhImJxvO9mKepRAREREpoMzsHKAr0AXYAYwE7vc1lEikmPdScFvrWn9zCACrV++mY8eRLFy4hRIlCvPRRzeogCQSZU5YRHLOfZWXQURERCKdOiJJLg0juIBJW+fcRr/DiESMj2+C3b8G2xUa+5tF+Oqr1XTpMobt2w9Qp05ZkpISqVevnN+xRCTMsrM6W3PgNeAcoBAQAyQ75zSgVUREROQ0Oeea+51BJOKkHoDlYzL3z7/LvyzCBx/8RO/ek0hLy+Dqq89mxIgbKFWqiN+xRMQD2Vmd7XUgkeCKIU2APwO1vQwlIiISiQLqiiQ5YGajnXM3mdnPBOedPPIW4Jxz5/kUTST/27kks33PPihU3L8sQr165YiJMe67rwXPPXcFMTHZWb9JRCJRdopIOOdWmlmMcy4dGGZmmlhbRETkKKohSQ71C23b+5pCJJKVb6gCkk8OHkylaNE4AJo2rcrSpXdTo0Ypn1OJiNeyUyI+YGaFgAVm9i8z+xtQzONcIiIiIlHNObcp1LzLObcm6wvQ2ByR7LAYvxMUSAsWbOacc95gzJjM1fFUQBIpGLJTRLo1dN7dQDJwBtDZy1AiIiKRyMw8eUnUu/I4x67J8xQikeTXj/1OUGCNGbOYli2HsmbNHt5+ex7OuVNfJCJR45TD2UKfhgEcAp4EMLNRBJei9cw363Z5eXuRqHFV18f9jiASEQ7++LrfEUT+wMz+SrDHUS0zW5jlrQTgW39SiUSAncvh+yeDbZfub5YCJCPDMXDglzzzzNcA9OjRkLffbq8PO0QKmGzNiXQcLcKaQkREJApoGlHJoY+AT4H/Ax7Ocnyfc26nP5FE8rmMdPhP48z9zlP8y1KA7Nt3mFtvnUBS0jICAePFF6/kvvuaq4AkUgDltogkIiIiIqfHOedWm1nfo98wszIqJIkcx8wBkJocbF/8LBSv4m+eAqJbt3FMnryCUqWKMGpUF9q2PcvvSCLikxMWkcys8YneAuK8iSMiIhK59Ims5NBHBFdmmwc4gv/G+p0DavkRSiTfcg5+ejPYDsTCBf39zVOAPPNMG7ZsSeajjzpTu3ZZv+OIiI9O1hPppZO8tzTcQURERCJdQDUkyQHnXPvQtqbfWUQiwvhrIO1gsN1hAsQW8TdPFHPO8e2367j44uoAnH9+JebM+Ys+LBGRExeRnHOt8zKIiIiISEFkZi2BBc65ZDO7BWgM/Ns5t9bnaCL5x4rxsPqzzP3ql/uXJcodPpzGXXdNZujQBXzwQSduvbUhoN62IhKkOUBFRETCJGDevCTqvQUcMLOGwEPAGuBDfyOJ5BMuA36bCpNuyDz2tzSIK+pfpii2efN+2rT5gKFDF1C0aCyFCsX4HUlE8hlNrC0iIiLirzTnnDOzjsCrzrn3zKyH36FEfLf5B/ioebCQ9LseiyCgwoYX5s3bSKdOo1i/fi/VqpUgKSmRxo0r+x1LRPIZFZFERETCRF39JZf2mdkjwK3AJWYWgxYxEYGpvf5YQOowAco18C9PFBsx4mduu20Shw6l0bLlGYwbdxMVKxb3O5aI5EOnHM5mQbeY2eOh/epm1tT7aCIiIpFFw9kkl7oCh4HbnHObgarAC/5GEvHZrGdgx+Jg+9zb4W+pULuTv5mi1KFDaTz++AwOHUqjd+9GTJ/+ZxWQROSEsjMn0ptAC6BbaH8f8IZniUREREQKkFDh6L9ASTNrDxxyzn3gcywRfy0entlu/QoENIDCK0WKxDJxYldef/0a3n33OgoX1v9rETmx7BSRmjnn+gKHAJxzu4BCnqYSERGJQGbevCS6mdlNwBzgRuAmYLaZdfE3lYjPdq8Mbnssgrhi/maJQitW7OD55785st+gQQX69m2qYdkickrZKTOnhsbmOwAzKw9knPwSEREREcmmfwAXOue2wpF/a30BjPU1lYhf9q3PbBcu5V+OKPX557/StetYdu8+RI0apeja9U9+RxKRCJKdItIgYAJQwcyeBboAj3qaSkREJAIF9Amu5E7g9wJSyA6y11tcJDrtWpHZTqjqX44o45zjlVdm8eCD08jIcHTqVI927Wr7HUtEIswpi0jOuf+a2TzgcsCATs65JZ4nExERiTD6qV9yaaqZfQaMCO13Bab4mEfEX8tGBreFEvzNEUUOHUrjjjs+4YMPfgLg8ccvZeDAVgS0eoOI5NApi0hmVh04AHyc9Zhzbq2XwUREREQKAufcg2bWGbiY4Ad2g51zE3yOJeKfZaOD2xpX+ZsjSmzevJ9OnUYye/YG4uPjGD68E1261Pc7lohEqOwMZ5tMcD4kA4oANYFlQAMPc4mIiEQcjWaTnDCz2sCLwFnAz8ADzrkN/qYS8dmW+XB4d7Bd72Z/s0SJokVj2b37EGeeWZKkpEQaNqzkdyQRiWDZGc52btZ9M2sM3OFZIhEREZGCYSjwATATuA54DejsayIRv33SNbNds51/OaJARoYjEDBKlizClCndSUgoRPnyWulORE5PjqdvcM7NBy70IIuIiEhEC5h58soOMytlZmPNbKmZLTGzFmZWxsymmdmK0LZ0lvMfMbOVZrbMzDRmxB8Jzrl3nXPLnHMvAjX8DiTiq/Vfw+6VwXbrf0NsYX/zRKi0tAweeOBz7rzzE5xzANSqVVoFJBEJi+zMidQ/y24AaAxs8yyRiIiI5MarwFTnXBczKwTEA38HpjvnnjOzh4GHgQFmVh9IJDg0vQrwhZnVcc6l+xW+gCpiZo0IThkAUDTrfuiDO5HolpoMP70Dh3fBrGcyj593p3+ZItiuXQfp1m0cn332K7GxAfr1a0aDBhX8jiUiUSQ7cyJlXRYhjeAcSeO8iSMiIhK5/JoTycxKAJcCPQGccylAipl1BFqFThsOzAAGAB2Bkc65w8BvZrYSaAp8n6fBZRPwcpb9zVn2HdAmzxOJ5LVPusKqyX881u079ULKhaVLt9OhwwhWrNhJuXLxjBt3kwpIIhJ2Jy0imVkMUNw592Ae5REREYlYPq6UXItgL+FhZtYQmAf0Ayo65zYBOOc2mdnvP01UBWZluX596JjkIedca78ziPju8N7M9kVPQeWmUKWFf3ki1JQpK+jWbRx79x6mYcOKJCUlcuaZpfyOJSJR6IRFJDOLdc6lhSbSFhEREZ+YWR+gT5ZDg51zg7PsxxIcbn6Pc262mb1KcOjaCW95nGPu9JOKiOTA9sWw4etg++bZwQKS5NjEiUvp3HkUzsGNN9Zn2LCOFCtWyO9YIhKlTtYTaQ7Bf5AuMLNJwBgg+fc3nXPjPc4mIiISUbI7CXZOhQpGg09yynpgvXNudmh/LMEi0hYzqxzqhVQZ2Jrl/DOyXF8N2Bjm2CIiJ7d+Zma7ktbtya0rrqjFeedVpEuX+vzjH5dgfo2tFpECITtzIpUBdhAcl+8IfnrpABWRRERE8gHn3GYzW2dmdZ1zy4DLgV9Crx7Ac6FtUuiSScBHZvYywYm1axP88EhEJG/8+glMvyvYrpvo36RyEWrDhr2ULRtPkSKxFC9eiNmz/0Lhwtn50U5E5PSc7DtNhdDKbIvILB79Tl3eRUREjuLzz0D3AP8Nrcy2CuhFcFXV0WbWG1gL3AjgnFtsZqMJFpnSgL5amc0/Fuw20B2o5Zx7ysyqA5WccyrsSfT6sl9m+7zb/csRgb77bh2dO4+ibduzGD68E2amApKI5JmTfbeJAYqjeRNERESyxceJtXHOLQCaHOety09w/rPAs56Gkux6E8gg2Ov7KWAfwZVwNb5Holf64eD2+slQXQsRZtfQoT9y552fkJqawYYN+zh4MI34+Di/Y4lIAXKyItIm59xTeZZEREREpGBq5pxrbGY/AjjndoV6lIlEp43fw/4NwXb58/zNEiHS0jK4//7PGDQo2EHxnnua8tJLbYmLi/E5mYgUNCcrImlgsoiISA6Y/uqU3Ek1sxhCPb3NrDzBnkki0enL+zLbccX9yxEhduw4QNeuY5k+/Tfi4gK89da19O6tBbRFxB8nKyIdt/u7iIiIiITVIGACwfkonwW6AI/6G0nEQ2kHg9vW/4YipfzNEgH++c+vmT79NypUKMb48TfRsmV1vyOJSAF2wiKSc25nXgYRERGJdH7OiSSRyzn3XzObR/ADPAM6OeeW+BxLxDvbfw5uz2jtb44I8fTTbdi16xBPPtmKM84o6XccESngAn4HEBERiRYB8+Yl0S20GtsB4GNgEpAcOiYSfZK3ZLYLJfiXIx9zzvHOO3NJTk4BID4+jqFDO6qAJCL5gtaCFBEREfHXZILzIRlQBKgJLAMa+BlKxBNpBzLbJWv6lyOfSk5OoVevJMaM+YUZM9YwYsQNfkcSEfkDFZFERETCxEzdhiTnnHPnZt03s8bAHT7FEfHWnt+C2xI1fI2RH61Zs5uOHUfy009bSEgoRPfu5576IhGRPKYikoiIiEg+4pybb2YX+p1DxBNjQmv3pCb7myOfmTlzDV26jGbbtgOcfXYZJk1K5JxzyvsdS0TkGCoiiYiIhInmL5LcMLP+WXYDQGNgm09xRLyTsi+z3eY1/3LkM++8M5e77/6UtLQM2rY9i5Ejb6B06aJ+xxIROS5NrC0iIiLir4Qsr8IE50jq6GsiES84l9mu19W/HPmIc45ZszaQlpZB//7NmTz5ZhWQRCRfU08kERGRMNGUSJJTZhYDFHfOPeh3FpE8o1XZjjAz3nrrWq6/vh4dOtT1O46IyCmpJ5KIiEiYBMw8eUl0MrNY51w6weFrIlJA/PTTZq699iP27TsMQJEisSogiUjEUBFJRERExB9zQtsFZjbJzG41s86/v3xNJuKFLfP8TuC7sWN/4aKLhjJlygr++c+v/Y4jIpJjGs4mIiISJppYW3KpDLADaAM4wELb8X6GEgm7bx4JbrNOsF1AZGQ4nnxyBk89NROAW289j4EDW/kbSkQkF1REEhEREfFHhdDKbIvILB79zh3/EpEItX8jbJodbLd4wtcoeW3fvsP06DGRCROWEggY//rXFfTv3wLTcGURiUAqIomIiISJfh6QHIoBivPH4tHvVESS6DL3xcx2wzv8y5HH9u49TMuWQ1m0aCslSxZm1KguXHXV2X7HEhHJNRWRREREwiRw3FqAyAltcs495XcIkTzx+xC2uolQrJK/WfJQiRKFadnyDFJT05k0qRt16pT1O5KIyGlREUlERETEH6o6SsHx85Dg9oxWvsbIC845du8+ROnSRQEYNOgaDh5MpWTJIj4nExE5fVqdTUREJEzMvHlJ1Lrc7wAieSa+QnBb9hx/c3gsJSWdPn0+pnnz99i9+xAAhQrFqIAkIlFDRSQRERERHzjndvqdQSTPpKcEt6Xr+pvDQ1u27KdNm+EMGfIja9fuYd68jX5HEhEJOw1nExERCZOAeg2JiBxr+j1weLffKTw1f/4mOnUaybp1e6laNYGJExNp0qSK37FERMJOPZFERETCJGDmyUvkRMzsajNbZmYrzezhk5x3oZmlm1mXvMwnAsDPg4PbouUhvry/WTwwatQiLr54KOvW7aVFi2rMndtHBSQRiVoqIomIiIhEIDOLAd4ArgHqA93MrP4Jznse+CxvE4oA3z6WOZQt8Wuw6PrxY/78TSQmjuPgwTRuu+18vvyyB5UqFfc7loiIZzScTUREJEzUaUjyWFNgpXNuFYCZjQQ6Ar8cdd49wDjgwryNJwKsTMpsl67tXw6PNG5cmfvvb0H16iW5556mmP4iEJEopyKSiIiISGSqCqzLsr8eaJb1BDOrClwPtEFFJMlrB7bC9p+D7c5ToqYX0ooVO0hJST+y/+KLbX1MIyKSt1REEhERCRPNXyR57Hi/4dxR+/8GBjjn0k/WQ8LM+gB9AMqXL8+MGTPClVHCYP/+/RH5NamxcSg1Qu1Zy3dzaM0MH9OEx9y5O3nyySUkJMTywgt1I/LrEs0i9c9KtNPXJbqoiCQiIiISmdYDZ2TZrwYcvaZ4E2BkqIBUDmhnZmnOuYlZT3LODQYGA9StW9e1atXKq8ySCzNmzCDivibOwaBrgu3yDWl+VTd/85wm5xz//vcsBgxYREaG4/LLz6J06eKR93WJchH5Z6UA0NclukRHn1IREZF8wMybl8gJ/ADUNrOaZlYISAQmZT3BOVfTOVfDOVcDGAvcdXQBScQTn94KaYeC7cb9/M1ymg4fTuO22ybRv//nZGQ4Hn30EsaP70p8vD6PF5GCR9/5REREwkSfzEhecs6lmdndBFddiwGGOucWm9mdofff9jWgFFzfPApL/pu5X/cm/7Kcpk2b9tG582hmzVpPfHwc77/fkRtvbOB3LBER36iIJCIiIhKhnHNTgClHHTtu8cg51zMvMkkB992TMPvZzP1790NcMf/ynKZvvlnLrFnrqV69JElJiZx/fiW/I4mI+EpFJBERkTDR0s4iUqDt3wjfP5G5f8eGiC4gAdx4YwPeffcwHTrUpUKFyP61iIiEg3rei4iIiIjI6ftuYGb77t1QvIp/WXIpPT2Dv/99OvPnbzpy7C9/aawCkohIiHoiiYiIhIn6IYlIgfTtQFg5Abb/HNw/qyMULulvplzYvfsQ3bqNY+rUlYwcuYilS++mUKEYv2OJiOQrKiKJiIiESUDD2USkIMhIgx/+Bfs2wMHtsHz0H99v/Yo/uU7DsmXb6dBhJMuX76Bs2aIMHdpRBSQRkeNQEUlERERERLIn7TC8WuT4790yF8rUi7h5kD79dAWJiePYu/cw555bgaSkRGrWLO13LBGRfElFJBERkTBRPyQRiVrOwb61MPLSzGOFEuDi/wMzqNkOStbwLV5uDRo0m/vum4pz0LnzOQwf3onixQv5HUtEJN9SEUlERERERE7u+yeDr98VLgV37/IvT5hUq1YCgCeeuIzHHruMQEAfB4iInIyKSCIiImGiKZFEJCqlHYa5Lwbb8RWhaDm49Ud/M52GlJT0I/Mdde58Dr/80pd69cr5nEpEJDIE/A4gIiISLczMk5eIiK++exxSk4PtKwdDz0UQE+dvplz6/vt11KnzGrNmrT9yTAUkEZHsUxFJRERERESO78C24EpsABaA6m38zXMahg37kVathrNmzR4GDZrtdxwRkYik4WwiIiJhok9mRCSqOAfLRmfu91oKhYr7lyeX0tIyeOCBz3n11WDh6O67L+Tll6/yOZWISGRSEUlERERERI619CP4393BdpWWULq2v3lyYefOg3TtOpYvvlhFXFyAN95ox+23X+B3LBGRiKUikoiISJho/iIRiRpbF8CUWzL3mzzgX5ZcyshwXHnlh8yfv4ny5eMZP74rF19c3e9YIiIRTT3vRURERETkj75+JLN9w1So3cm/LLkUCBhPPdWKCy6ozNy5fVRAEhEJAxWRREREwsQ8emXr2WarzexnM1tgZnNDx8qY2TQzWxHals5y/iNmttLMlpmZJgcRkUzOweqpwXadm6BG5HyLcM4xf/6mI/vXXluH2bP/QvXqJX1MJSISPVREEhERCRMz8+SVA62dc+c755qE9h8GpjvnagPTQ/uYWX0gEWgAXA28aWYx4fs/ISJR48p3/E6QbcnJKSQmjqNZsyHMnLnmyPGYGP3IIyISLvqOKiIiEr06AsND7eFApyzHRzrnDjvnfgNWAk19yCci+c2i92F0q8z9IqX8SpIja9fu4eKLhzF69GKKFo1l//4UvyOJiEQlTawtIiISJj5/MuOAz83MAe845wYDFZ1zmwCcc5vMrELo3KrArCzXrg8dE5GCbs7/wa7lwXZCZMwh9PXXa7jhhtFs23aAs84qzaRJ3ahfv7zfsUREopKKSCIiIvmcmfUB+mQ5NDhUJMqqpXNuY6hQNM3Mlp7slsc55k43p4hEuLkvZxaQ2v0Halztb55sGDx4HnffPYXU1AyuuKIWo0Z1oUyZon7HEhGJWioiiYiIhEkO5y/KtlDB6Oii0dHnbAxtt5rZBILD07aYWeVQL6TKwNbQ6euBM7JcXg3YGP7kIhJRlvwns312Z4jL38WYbduSefjhL0hNzeC++5rxwgttiY3VbB0iIl7Sd1kREZEw8Wt1NjMrZmYJv7eBtsAiYBLQI3RaDyAp1J4EJJpZYTOrCdQG5uT21y0iES4jDRa8BVt/DO53/TrfF5AAypcvxqhRXRg2rCOvvHK1CkgiInlAPZFEREQiX0VgQqgnVCzwkXNuqpn9AIw2s97AWuBGAOfcYjMbDfwCpAF9nXPp/kQXEV9lpMOoy2Djd5nHKjb2L88p/PzzFubN20TPnucDcOWVZ/mcSESkYFERSUREJEw8Gs12Ss65VUDD4xzfAVx+gmueBZ71OJqI5HfzXvljASnxG4iL9y/PSUyYsIRbb53AoUNp1K5dhpYtI2PibxGRaKIikoiIiIhIQfXzu5ntu7ZD0bL+ZTmBjAzH009/xRNPfAXALbecR+PGlX1OJSJSMKmIJCIiEiaBbM1gJCKST6yYkLka25Xv5MsC0v79KfTsOZFx45YQCBjPP38F99/fwrOFDERE5ORURBIREQkT/UwjIhHl0z9ntuvc5F+OE1i9ejcdO45k4cItlCxZmJEju3D11Wf7HUtEpEDTEgYiIiIiIgVR6drB7VVDoUgpf7OcwMaN+6hbtyyzZ/9FBSQRkXxAPZFERETCxDScTUQixb71sPXHYLv8MfPy+8Y5B4CZUaNGKT7//BZq1ixNqVJFfE4mIiKgnkgiIiIiIgXPnOcz28Wr+Jcji5SUdO688xNeeCFztbhGjSqrgCQiko+oJ5KIiEiYaE4kEYkIqQdgwevB9hmtoFglX+MAbN2azA03jOabb9YSHx9Hjx4NqVixuN+xRETkKCoiiYiIhIlWZxORiDCkVma70T3+5Qj58cdNdOo0irVr91C1agITJyaqgCQikk+piCQiIiIiUpAc2BLclq0Ptdr7GmX06MX07DmRgwfTaN68GuPH30Tlygm+ZhIRkRNTEUlERCRMNJxNRCJK97kQU8i3xw8ePI877vgEgF69zuett66lcGH9eCIikp/pu7SIiIiISEGxfXFm2/xdY6ddu9pUq1aCBx5owb33NsNUiRcRyfdURBIREQkT/fwjIvne/7LMgRRbOM8fv2HDXipXTiAQMKpVK8HSpX0pVsy/3lAiIpIz/n78ICIiIiIieWPrAlj3ZbBd9eI8f/wXX6zi3HPf4plnZh45pgKSiEhkUU8kERGRMDGtziYi+dmMv2W2O4zLs8c65xg0aDb9+39ORoZj3rxNZGQ4AgF9zxQRiTQqIomIiISJfh4SkXxr3wZYNyPYbtwP4ivkyWMPH07jr3+dzLBhCwD4+98v5umn26iAJCISoVREEhERERGJdp/ektlu/liePHLz5v107jyK779fT9GisQwb1pGuXf+UJ88WERFvqIgkIiISJhrOJiL5lnPB7YUPQdGyefLIe+75lO+/X88ZZ5Rg4sREGjeunCfPFRER76iIJCIiIiISzZyD9V8F2zWvybPHvvbaNQQCxqBBV1OxYvE8e66IiHhHq7OJiIiEiZk3LxGR05K8KbNdpp5nj0lPz2DYsB9JT88AoFKl4owa1UUFJBGRKKKeSCIiImGi4Wwiku9s+A5Gtgy2Y4tAsUqePGb37kPcfPM4Pv10JStX7uTZZy/35DkiIuIvFZFERERERKLNb1NhSnc4tDPz2Hl3evKo5ct30KHDCJYt20HZskW54opanjxHRET8pyKSiIhImGjFahHJF9IOwfij5j665gOof2vYHzV16koSE8eyZ89hzj23AklJidSsWTrszxERkfxBRSQRERERkWiSsj+zff1kqNwcipYJ6yOcc7z00vcMGPAFGRmO66+vxwcfXE/x4oXC+hwREclfVEQSEREJE82JJCL5SpGyUKudJ7dOT3dMnryCjAzHwIGX8fjjlxFQd0wRkainIpL8wdhHexJXpCgWiCEQCND+4UHsXPcr3494nfS0VAKBAM0S+1K+Rl3S01L5/qPX2LF2BWYBmt54B5XqnOf3L0Hz6gr+AAAgAElEQVTEE28P7M41l/6JbTv30eTGfwJQukQ8Hz5/G2dWKcOajTu55aH32L3vINUrl2HB+EdZvmYrAHN+Xs29z46kaJE4/vuv3tSqVo70DMeUmT/z2KBJfv6yJMy0kpqIFBSxsQHGjLmR775bR4cOdf2OIyIieURFJDnGVfc9R5HiJY/sz50wlIbX3ky1BheyftEPzJswlKv/9jwrvp0KQMdH3+Lgvt188frjtB/wbywQ8Cu6iGc+/HgWb4/6iiFP//nIsQd6XcmMOct4cdg0Huh1JQ/0asujg5IAWLV+O80TnzvmPv/+YDoz564gLjaGT9+5h7Yt6/P5t7/k2a9DREQKAJfuyW1nz17PG2/8wNChHYmNDVCuXLwKSCIiBYxnP+2b2QXHOXadV88T75gZqQcPAJByMJn4ksEx9bs3raVy3fMBKJpQikLxxdi+doVvOUW89O38X9m558AfjrVvdR7/+Xg2AP/5eDbXtT55T7yDh1KZOTf4ZyQ1LZ0FS9dRtUIpbwKLL8yjl4hIth3cCQteD7bTksN22+HDF3Dppe/z4YcLeeeduWG7r4iIRBYvu4y8a2bn/r5jZt2ARz18noSBmTHttUf5+P/uZfk3nwJwYZc+zJswlDF//zNzx79H4449AShTrRbrFs4iIz2dfds3s2PtSg7s2uZjepG8VaFsApu37wVg8/a9lC+TcOS9GlXL8v2IAXw+pB8tG511zLUlixel3aXn8uWcZXmWV0REoty+9fBOFZj1THC/SNnTvmVaWgb9+39Gz55JpKSkc9ddTejT55jPikVEpIDwcjhbF2CsmXUHLgb+DLT18HkSBtfc/yLxpcpycN9upg36ByUqVmPNj99yYZfbObPRxayeN5Pv/vMqbfv9k7NbtGX35nV88nw/ipepQIVa52CBGL9/CSK+27x9L3WueZyde5JpdM4ZjH65D427PMu+5EMAxMQEGP5cT94cMYPVG3b4nFbCKaBJkUTET3Oeg/TDwXb5hnDZS6d1u127DtK161imTVtFbGyAN95opwKSiEgB51lPJOfcKiARGEewoNTWObfnZNeYWR8zm2tmc+d8MtKraHIS8aWCn1gVTShF9YYt2L56Ob/O+oLq57cE4MzGl7B9TbDnRCAmhqZd+tDh76/T5s7HSTmQTIkKVX3LLpLXtu7YR6VyJQCoVK4E23buAyAlNY2de4JDCH5cso5V67dT+8wKR65749Fu/Lp2G69/NCOvI4uISDT64QUYdw0seCO4X+os+PMCOPPyXN9yw4a9NG06hGnTVlG+fDz/+9+fVUASEZHwF5HM7GczW2hmC4GxQBmgBjA7dOyEnHODnXNNnHNNmrZPDHc0OYXUw4dIPXTgSHvjkh8pXeVM4kuWZcuKnwHYvOwnEsoHC0VpKYdIPRzsWbFxyXwsJkCpytX9CS/ig8lf/cwt1zUD4JbrmvHJjOC3uHKlix9Z5rhG1bKcXb08v63fDsDAu9pTMqEoD7wwzp/Q4inNiSQieW7dDJj5EKyemnms08enfduKFYtTq1Zpzj+/Ej/8cDuXXHLmad9TREQinxfD2dp7cE/JA4f27eLLd4Jj6DMy0qnVpBVVGzQhtnBR5ox5B5eRTkxcHBd1vyd0/h6mvfYoZgHiS5Xlkh4P+BlfxFPD/68nl1xQm3KlirNy6tM8/fYUXhw2jf88fxs9OrVg3aZddH/oPQAubnw2j/31WtLS00lPd9zz7Eh27T1A1QqlePj2q1m6ajPfjxgAwNujvuL9Cd/7+UuTcFLFR0Ty2ujWme3OU6BUbSh9dq5u5ZwjOTmV4sULERsbYNSoLsTFBShWrFCYwoqISKQLexHJObcGwMyaA4udc/tC+wlAfWBNuJ8p4ZFQrjId/vHGMccrnt2A6x4ZdMzx4mUrcv0T7+ZFNBHf9Xjk/eMeb3fna8ccmzh9AROnLzjm+Iatuyna6O5wRxMRkYJq++LM9vWToeY1ub7VgQOp9O49iY0b9zFt2q0UKhRDqVJFwhBSRESiiZcTa78FNM6yn3ycYyIiIlHD1BVJRPLChm9h7XT4bmDmsVrtcn27dev20KnTKObP30Tx4oVYvHgrjRpVDkNQERGJNl4Wkcw5537fcc5lmJmXzxMRERERiV7OwZZ5MPLiPx5vfWyP8ez69tu1dO48mq1bkznrrNIkJSXSoEGFU18oIiIFkpdFnVVmdi/B3kcAdwGrPHyeiIiIr0wdkUTEK3vXwrtHTW7d+D6o1w0qN83VLYcMmc9dd00mNTWDK66oxahRXShTpmgYwoqISLQK++psWdwJXARsANYDzYA+Hj5PRETEV1qdTUS8YBmp8N5Rk2Vf/H/Q+pVcF5A++WQ5t9/+MampGfTr14xPP+2uApKIiJySZz2RnHNbgUSv7i8iIiIiUhDU/+1pyEgN7lzxFvypN8TEndY927WrTZcu9WnX7mx69WoUhpQiIlIQeFZEMrMiQG+gAXBkaQfn3G1ePVNERMRX6jYkIuG2ZATld38dbJ/RGhremetbLVq0lbJli1K5cgKBgDF6dBdM43BFRCQHvBzO9iFQCbgK+AqoBuzz8HkiIiIiItFh7f/g7Sow5ebMY9dPzvXtkpKW0qLFe3TuPJrDh9MAVEASEZEc87KIdLZz7jEg2Tk3HLgWONfD54mIiPjKPPpPRAqgz3pD8qbM/VvmQVzO5yxyzvH001/RqdMo9u9PoVat0mRkuFNfKCIichxers4WGrjNbjP7E7AZqOHh80RERHylD/VFJCxS9sPe1cF2s38wI7UNrSo2zvFtkpNT6NkzibFjf8EMnnvuCh588CL1QBIRkVzzsog02MxKA48Ck4DiwGMePk9EREREJLKlHYIV4zP3WwyEr7/N8W1Wr95Nx44jWbhwCyVKFGbEiBto1652GIOKiEhB5GURabpzbhcwE6gFYGY1PXyeiIiIr/TZvoictneqwqGdwXbZ+rlehW3MmMUsXLiFOnXKkpSUSL165cIYUkRECiovi0jjgKP73Y4FLvDwmSIiIgWWmcUAc4ENzrn2ZlYGGEVwOPlq4KbQBzyY2SMEV1FNB+51zn3mS2gRyeQyMgtIhUtB89x34n/ggYtwDvr0uYBSpYqc+gIREZFsCPvE2mZWz8xuAEqaWecsr56A/gYTEZHoZR69sq8fsCTL/sMEewbXBqaH9jGz+kAi0AC4GngzVIASET+5jMx2351QLzHbl6akpDNgwDTWrdsDBFdee+ihliogiYhIWHmxOltdoD1QCrguy6sxcLsHzxMRESnwzKwawZVQh2Q53BEYHmoPBzplOT7SOXfYOfcbsBJomldZReQoyVtgRn94JcvQtRxMfr1tWzJXXvkh//rXd3TrNg7ntPqaiIh4I+zD2ZxzSUCSmV3qnJuZ9T0zaxnu54mIiOQX5tGsSGbWB+iT5dBg59zgo077N/AQkJDlWEXn3CYA59wmM6sQOl4VmJXlvPWhYyKSl3YshQ0zYdodfzxeuXm2b/HTT5vp2HEka9bsoUqVBF5++SqtviYiIp7xck6kf3PsnEivHeeYiIhIVPDq57ZQwejoolGW51p7YKtzbp6ZtcrGLY+XVF0XRPJKRjosHQGf3vrH43W6QL3ucNZ12brN2LG/0KPHRA4cSKVZs6qMH9+VKlUSTn2hiIhILoW9iGRmLYCLgPJm1j/LWyUAzbcgIiISfi2BDmbWjuD8gyXM7D/AFjOrHOqFVBnYGjp/PXBGluurARvzNLFIQfbprcEi0u8a9IA6N0Kta7N9iyefnMETT3wFQI8eDXn77fYUKeLl58MiIiLezIlUCChOsECVkOW1F+jiwfNERETyBb/m1XbOPeKcq+acq0Fwwuz/OeduASYBPUKn9QCSQu1JQKKZFTazmkBtYE6uf+EikjM7ssx/f/NsuPr9HBWQAOLj4wgEjJdfbsuwYR1VQBIRkTzhxZxIXwFfmdn7zrk14b6/iIiIZNtzwGgz6w2sBW4EcM4tNrPRwC9AGtDXOZfuX0zJLTO7GniVYG/vIc655456vzswILS7H/irc+6nvE0pQHDltf0bYc002LYgeOyW+VCxUbZvkZ6eQUxM8DPgBx64iLZtz6Jhw0pepBURETkuLz+yOGBmLxBcPvjI2qLOuTYePlNERMQ/+WAuW+fcDGBGqL0DuPwE5z0LPJtnwSTszCwGeAO4kuAQxR/MbJJz7pcsp/0GXOac22Vm1xCcW6tZ3qcVJnaAVZP/eKxY9gtA//vfb/z1r5OZOrU7NWuWxsxUQBIRkTznxXC23/0XWArUBJ4EVgM/ePg8ERERX5lH/4mcQFNgpXNulXMuBRgJdMx6gnPuO+fcrtDuLILzX0leW/15lgKSQaEE6D4Hilc+5aXOOcaP30Dbth+yfPkOXntNI09FRMQ/XvZEKuuce8/M+mUZ4vaVh88TERERKUiqAuuy7K/n5L2MegOfeppIjrVuBoy7KnP/vkMQUyhblx4+nEbfvlN4772VADzyyMU8/XRrD0KKiIhkj5dFpNTQdpOZXUtw1Rd9+iUiIlHL1GlI8tbxfse5455o1ppgEeniE7zfB+gDUL58eWbMmBGmiAVbQvIvXLC075H9H+u8wp6vv8vWtTt3pvD444tZvHgvhQoZDz1Uj8svj+Hrr2d6FVdyaP/+/fqzks/oa5I/6esSXbwsIj1jZiWB+4HXgBLA3zx8noiIiEhBsh44I8t+NYIf2v2BmZ0HDAGuCc2TdQzn3GCC8yVRt25d16pVq7CHLTAObIMv+8GW+bBrWebxKwfT6Lzbs3WLgwdTqV//TVav3ku1aiV49NGzueOO6zwKLLk1Y8YM9Gclf9HXJH/S1yW6eFZEcs59EmruAdTvVkREop46Ikke+wGobWY1gQ1AInBz1hPMrDowHrjVObc87yMWIM7BoqHw+V+Ofa/VK3Bu72zfqmjROPr3b87IkYsZP/4mliyZG8agIiIiuedlTyQREZGCRVUkyUPOuTQzuxv4DIgBhjrnFpvZnaH33wYeB8oCb1pwvGWac66JX5mjTnoqrPoYFg2DVZ/88b3qbaD1q1DyLIgreupbpWewYsVO6tUrB8DddzflzjubEBcXw5IlXoQXERHJORWRRERERCKUc24KMOWoY29naf8FOE7XGDltB3fAm+WO/95ty6F07Wzfas+eQ3TvPp5vv13HnDl/oXbtspgZcXExYQorIiISHgGvbhzqWn3KYyIiItHCPPpPRPKhdV/+cf+C++Gu7XC/y1EBacWKHTRv/h6TJ68gEDA2b94f5qAiIiLh42VPpHFA46OOjQUu8PCZIiIiIiLe2/BNcFu6DvRamqvlGT/7bCWJiePYvfsQDRqUZ9KkbtSqVTrMQUVERMIn7EUkM6sHNABKmlnnLG+VAIqE+3kiIiL5RS5+hhSRSDR/EMx/NdiudGGO//A753jllVk8+OA0MjIcnTrV44MPOpGQUNiDsCIiIuHjRU+kukB7oBSQdS3SfUD21jUVEREREcmPVoyHL/tl7l/QP8e3WL58Bw8//AUZGY7HH7+UgQNbEQioCi0iIvlf2ItIzrkkIMnMWjjnvg/3/UVERPIr/QgoUgB89WBm+9YfocL5Ob5F3brleOed9iQkFKZLl/phDCciIuItL+dEWmdmE4CWgAO+Afo559Z7+EwRERH/qIokEr0O74GvH4Y9q4L7lzyXowLSnDkb2LHjANdcE5x0u1evRl6kFBER8ZRnq7MBw4BJQBWgKvBx6JiIiIiISOQ4uAMmdoSf3s48dk73bF/+4Yc/cemlw+jadSwrVuzwIKCIiEje8LInUgXnXNai0ftmdp+HzxMREfGVqSuSSHR6s1xmu3ApuHE6JFQ75WXp6RkMGPAFL70UnOHhttsaUaNGKa9SioiIeM7LItI2M7sFGBHa7wbooxcRERERyf+cA9wf50AqXRc6T4FStU55+a5dB+nWbRyfffYrsbEBXnvtGu68s4l3eUVERPKAl0Wk24DXgVcIzon0XeiYiIhIVMrhKt8ikl+tmQ5jrzj2+G1Ls3X50qXb6dBhBCtW7KRcuXjGjbuJSy89M8whRURE8p5nRSTn3Fqgg1f3FxERyW9UQxKJAmmHjy0gxVeAWxdk+xZ79hxi7do9NGxYkYkTEzWETUREokbYi0hm9vhJ3nbOuafD/UwRERERkbD44s7Mdvc5UOnCHN+iWbNqfPppd5o2rUqxYoXCGE5ERMRfXqzOlnycF0BvYIAHzxMREckfzKOXiHhv4WB4yWDx+8H9ys2zXUA6eDCVW24Zz5gxi48ca926pgpIIiISdcLeE8k599LvbTNLAPoBvYCRwEsnuk5ERERExDfT7shsWwy0H5mty9av30unTiOZN28TX3yximuvrUN8fJxHIUVERPzlyZxIZlYG6A90B4YDjZ1zu7x4loiISH5h6jYkEnk2z4UxbTL3E7+Bqi2zdel3362jc+dRbNmSTK1apUlKSlQBSUREopoXcyK9AHQGBgPnOuf2h/sZIiIi+ZFWZxOJMDuWwn+PGrKWzQLS0KE/8te/TiYlJZ02bWoyenQXypaN9yCkiIhI/uHFnEj3A1WAR4GNZrY39NpnZns9eJ6IiIiISM6kJsP752TuX/YS9DuUrUuffXYmvXtPIiUlnXvvbcpnn92iApKIiBQIYS8iOecCzrmizrkE51yJLK8E51yJcD9PREQkv9C82iIRZPuizPa1I6BJf4gtnK1L27WrTalSRRgy5DpeffUaYmO9+FxWREQk//FkTiQRERERkXxl5zI4sA1S98G0O2Hf2uDxQglQL/GUl2/dmkyFCsUAaNSoMqtX96NkySJeJhYREcl3VEQSEREJF3UbEsl/dq2ApOthx+Ljv9/61VPeIilpKbfeOoE337yWW245D0AFJBERKZBURBIRERGR6HR4Dwyt88djVVqCS4PaXeBPvaBo2RNe7pzjn//8mkcf/RKA6dN/O1JEEhERKYhURBIREQkTU1ckkfxl2ZjM9oUPQbO/Q+GS2bo0OTmFXr2SGDPmF8zgn/+8nAEDsrdym4iISLRSEUlERCRMTDUkkfzl6wHBbdFycOnz2b5szZrddOo0igULNpOQUIiPPrqB9u3rnPpCERGRKKcikoiIiIhEjw3fwd41MPNBOLQzeKz5Y9m+3DlHYuI4FizYzNlnl2HSpETOOae8R2FFREQii4pIIiIiYaKOSCI+2rMaJl4H2xcd+965f8n2bcyMd9+9joEDZzBkyHWULl00fBlFREQiXMDvACIiIiIip2X3rzD83D8WkOomQuP74G9pEBd/0stTU9MZO/aXI/t/+lMFxo27SQUkERGRo6gnkoiISLioK5JI3ks7DO+dnblf/XLo9DHEZa8AtG1bMjfeOIavvlrDkCHX0bt3Y4+CioiIRD4VkURERMJEq7OJ+GB4g8z2+X2h5dPZLiAtXLiFDh1GsGbNHipVKk6DBhU8CikiIhIdVEQSERERkci1b11wW6UlXP56ti8bN+4X/vzniRw4kMqFF1ZhwoSuVK1awqOQIiIi0UFzIomIiISJmTcvETmJmCLB7fWfZOv0jAzHwIFf0qXLGA4cSOWWW87jq696qoAkIiKSDSoiiYiIiEhkykiHlL3BdjYrrsnJKYwatZhAwHjhhSv54INOFC0a52FIERGR6KHhbCIiImGiTkMieeyVLP+U/b1H0ikkJBQmKSmR337bzdVXn33qC0REROQI9UQSEREJEw1nE8lDX9yV2a5+BcQWPuGpX375Gw89NA3nHAB165ZTAUlERCQX1BNJRERERCLL6mnw01uZ+zdOO+5pzjneeOMH7rtvKunpjpYtz6Bjx3p5FFJERCT6qIgkIiISNuo2JOKp1IOw8G2Y0T/z2L37j3tqSko6fftOZsiQHwF46KGLaN++Tl6kFBERiVoqIomIiIhIZBhSEw5sydxv91+IK3bMaVu27OeGG0bz7bfrKFIkliFDrqN79/PyMKiIiEh0UhFJREQkTDR/kYiHlo/9YwHp5tlQuekxpy1btp0rr/yQdev2UrVqAhMnJtKkSZU8DCoiIhK9NLG2iIhIhDOzImY2x8x+MrPFZvZk6HgZM5tmZitC29JZrnnEzFaa2TIzu8q/9CLZsH0RfHxj5n7/9OMWkACqVEmgRInCtGhRjblz+6iAJCIiEkbqiSQiIhImPnZEOgy0cc7tN7M44Bsz+xToDEx3zj1nZg8DDwMDzKw+kAg0AKoAX5hZHef+v707j5OiOvc//vkKI4sgCCoaQYebEI2CEBAkMawaRUNAovyQaIwkN6io+DPRaIzXGL2axOUmGrcocV/ADQFXvCCgQRFFQDBqMKKOGtlcwirMPPePqsFmmKVn6J4ZmO+bV7+o5dSpp7qmps88dep0FNfVAZhV6vmLvpweOgm05X3QkpJg48ZimjRpTMuWTZg69Ue0bduMJk3c1DUzM8sl90QyMzPLESk/r6pEonR04YL0FcBQ4M50+Z3Asen0UGB8RGyIiHeAJUD53TrM6toHf4O3JyXTXz8evjZki9Wff76BoUPH87OfTSEigKQ3khNIZmZmueckkpmZWT0nabSklzNeo8sp00jSfGAZ8ExEzAHaRcRHAOn/e6bF9wHez9i8KF1mVr8smw/jv/Pl/OE3bLF6yZJV9O49jscee4vHHnuLd9/9rJYDNDMza1h8i8bMzCxHlKcH2iLiFuCWKsoUA90ktQYmSupcSfHyAo1tCNEsdz5+FV79M/z7XXhv+pfLj38Gmu+5efaZZ95mxIiH+OST9Rx44B5MnnwChYWt6yBgMzOzhsNJJDMzsx1IRHwqaQYwCPhY0t4R8ZGkvUl6KUHS86hDxmbtgQ9rN1KzCtzTfetlQx+F/Y4AICK49to5/OIXUykpCYYM2Z+77x7Grrs2qeVAzczMGh4/zmZmZpYrytOrqt1Ke6Q9kJDUDDgCeAOYDPw4LfZjIB1YhsnACZKaSOoIdAJeqvFxm+VKZHSI+/alcPRdcNq/4GtDNy++9dZ5nHPO05SUBBdd1IeJE0c4gWRmZlZL3BPJzMwsR+rw29n2Bu6U1IjkBtEDEfGYpBeAByT9FHgPGA4QEYslPQC8DmwCzvA3s1m98NIfvpzufVG5I8ufdNLB3H33QsaO7cXw4QfVYnBmZmbmJJKZmdl2LiIWAt8sZ/lK4PAKtrkcuDzPoZlVz6f/SP7fZe8tEkjz5/+LTp3asMsuO9O8eQGzZp2CsvnqQjMzM8spP85mZmaWI1J+XmYNworFsOi2ZPqwyzYvvvfehfTuPY5RoyYR6eNuTiCZmZnVDSeRzMzMzKzuPXv2l9MtO1BcXMIvf/kMJ500kQ0bitltt6YUF/tLBM3MzOqSH2czMzPLEdXlqEhm27P3Z8B705LpA0byaeu+/PD79/Pkk0to3HgnrrtuEKef3rNOQzQzMzMnkczMzHLHOSSzmnlgwObJN/f5DUMOHcdbb62kbdtmPPTQ/6N//8K6i83MzMw2cxLJzMzMzOrOsgVfTh87hev+ZwlvvbWSgw9ux6RJJ1BY2LruYjMzM7MtOIlkZmaWI+6IZFZN6z+FB/p9Of/VwVxzzSbatGnG+ed/hxYtdq672MzMzGwrHljbzMzMzGpf8Ua471DWrV7Dr58cyL+7XAhA06aNueyygU4gmZmZ1UPuiWRmZpYj/tZxsyqsWwn/mAjPnQ/rV/HBZy059vZRvFy0D+/sfgD3HVnXAZqZmVllnEQyMzMzs9pxS3vYtB6AF99tz7A7RvCvf7eksLA1v/p1/7qNzczMzKrkJJKZmVmOyKMimVUuSgC44+/HcOpdvfhiI/TvX8iDDw5n992b13FwZmZWn2zcuJGioiLWr19f16Fst5o2bUr79u0pKCjIWZ1OIpmZmeWIH2czq8T0sZRs3MgvphzFn57rBcAZZ/Tkj388ioKCRnUcnJmZ1TdFRUW0bNmSwsJC5EZWtUUEK1eupKioiI4dO+asXg+sbWZmZmb5VVIMr/4ZKVgTbSgo2IlbbhnM9dcf4wSSmZmVa/369bRt29YJpBqSRNu2bXPek8s9kczMzMwsr0oW3clOJL31rn/yJk59cw09enylrsMyM7N6zgmkbZOP9889kczMzMwsPyKYctEoDv3ei3y6rik03Y2dW7RyAsnMzGw75SSSmZlZjkj5eZltj2Ljeq445waGXrEfLxftw60vdofjnq7rsMzMzKotIigpKamTfW/atKlO9lsRJ5HMzMxyRHn6Z7ZdWTqVtU+dy8heP+LX164E4PKjp3HuxGmwV886Ds7MzCw7S5cu5Rvf+AZjxoyhe/fuvP/++5x33nl07tyZLl26MGHChM1lr7zySrp06ULXrl254IILtqrr448/ZtiwYXTt2pWuXbsye/Zsli5dSufOnTeXufrqq7nkkksA6N+/PxdeeCH9+vXj8ssvp7CwcHMSa+3atXTo0IGNGzfy9ttvM2jQIHr06EGfPn1444038vum4DGRzMzMzCwXlk6FZ07lvXc/4dg7TuDVDzrToskG7jtxIt+/4h4oaFrXEZqZ2fbqmjzdVPtFVLr6zTff5Pbbb+fGG2/k4YcfZv78+SxYsIAVK1bQs2dP+vbty/z583n00UeZM2cOzZs3Z9WqVVvVM3bsWPr168fEiRMpLi5m9erVfPLJJ5Xu+9NPP2XmzJkAzJs3j5kzZzJgwACmTJnCUUcdRUFBAaNHj+bmm2+mU6dOzJkzhzFjxjB9+vSavx9ZcBLJzMwsR/zomTVob4xn2QfL6Xnt6Sxb3YKv7hNMuv8YDupzRV1HZmZmViP77bcfvXv3BuD5559n5MiRNGrUiHbt2tGvXz/mzp3LzJkzGTVqFM2bNwegTZs2W9Uzffp07rrrLgAaNWpEq1atqkwijRgxYovpCRMmMGDAAMaPH8+YMWNYvXo1s2fPZvjw4ZvLbdiwYZuPuSpOIpmZmSKXLiAAABMQSURBVJnZtlt8O3u2hBFHt+Lvq/ZlwgPDadOmWV1HZWZmO4Iqegzlyy677LJ5OqL8GCKiRt+C1rhx4y3GWVq/fn2F+x4yZAi/+tWvWLVqFa+88goDBw5kzZo1tG7dmvnz51d739vCYyKZmZnliPL0MqvPNm4s5r1Xnt88/z9X9uXJp05yAsnMzHYoffv2ZcKECRQXF7N8+XJmzZpFr169OPLII7nttttYu3YtQLmPsx1++OHcdNNNABQXF/P555/Trl07li1bxsqVK9mwYQOPPfZYhftu0aIFvXr14uyzz2bw4ME0atSIXXfdlY4dO/Lggw8CSTJrwYIFeTjyLTmJZGZmlivOIlkDs2LFWo464nb6H/kQK9Yk3fgbd+hN48ZuYpqZ2Y5l2LBhHHzwwXTt2pWBAwdy5ZVXstdeezFo0CCGDBnCIYccQrdu3bj66qu32vbaa6/l2WefpUuXLvTo0YPFixdTUFDAxRdfzKGHHsrgwYM54IADKt3/iBEjuOeee7Z4zO3ee+/lr3/9K127duWggw5i0qRJOT/usvw4m5mZmZlV22uvfcyQwXez9L01tGtZQNGnu7L7MZdBo53rOjQzM7NtVlhYyKJFizbPS+Kqq67iqquu2qrsBRdcUO63spVq165duQmesWPHMnbs2K2Wz5gxY6tlxx9//FaP1HXs2JGnnnqqssPIOSeRzMzMckTuNmQNwSf/4JHfX8rJ1xayZkNjDmn/ARNPmUD7wj2h25i6js7MzMzyyH2NzczMzCwrJSXBb385nuOu/BprNjTmxO4LmXXG7bTv90MYPg0aN63rEM3MzCyP3BPJzMwsR2rwxRxm25VZM9/hknElSMEfRq3i3N+chfa4FZpt/XXGZmZmtuNxEsnMzMzMKhcB65bTf82v+e2RH9Gzwwccfepo2Ld/XUdmZmY7sIhAvktXY2XHUMoFJ5HMzMxyxE0c2xHNmLGUXZ84gu7t3gbg4iPTFYf8ou6CMjOzHV7Tpk1ZuXIlbdu2dSKpBiKClStX0rRpbh81dxLJzMwsV9y+sR1IRHDT9S9w9jlPs1eLIbz687+we4v1UNACfvIWyENrmplZ/rRv356ioiKWL19e16Fst5o2bUr79u1zWqeTSGZmZmbbKUmDgGuBRsC4iPh9mfVK1x8DrAVOiYh5VdX7xRfFnHXWE9xyyzxgJ0Z+cxG7NVsHPy/J/UGYmZmVo6CggI4dO9Z1GFaGk0hmZmY5IndFslokqRFwA/BdoAiYK2lyRLyeUexooFP6OhS4Kf2/QsXFwREDb+O5v31Ik8bFjBs+iZO+9Q6csTE/B2JmZmbbDSeRzMzMzLZPvYAlEfFPAEnjgaFAZhJpKHBXJCNrviiptaS9I+Kjiip9b+nnvP32h3xl18959JTx9Nz3QzjuBdipUT6PxczMzLYDTiKZmZnliMd8tFq2D/B+xnwRW/cyKq/MPkCFSaSNxTvRe7/3eeTHE9i7sAMcNBb27pWrmM3MzGw7pnx85ZvtuCSNjohb6joOs/rO14qZ5Zuk4cBREfGf6fyPgF4RcVZGmceB30XE8+n8NOCXEfFKmbpGA6PT2c7Aolo4BMve7sCKug7CtuLzUv/4nNRPPi/1z/4R0bImG7onklXXaMB/GJtVzdeKmeVbEdAhY7498GENypAmvW8BkPRyRByS21BtW/ic1E8+L/WPz0n95PNS/0h6uabb+rtZzczMzLZPc4FOkjpK2hk4AZhcpsxk4GQlegOfVTYekpmZmVll3BPJzMzMbDsUEZsknQk8DTQCbouIxZJOS9ffDDwBHAMsAdYCo+oqXjMzM9v+OYlk1eXHc8yy42vFzPIuIp4gSRRlLrs5YzqAM6pZrX9/1T8+J/WTz0v943NSP/m81D81PiceWNvMzMzMzMzMzKrkMZHMzMzMzMzMzKxKTiI1UJKGSQpJB6Tz3SQdk7G+v6Rvb0P9q3MRp1k+pD/712TMnyvpkiq2OVbSgdXczxbXUU3qyNi2UJK/ctvMckbSIElvSloi6YJy1kvSden6hZK610WcDUkW5+TE9FwslDRbUte6iLMhqeqcZJTrKalY0vG1GV9Dlc15Sdth8yUtljSztmNsaLL4/dVK0hRJC9Jz4jH68kzSbZKWVfQ3RE0/551EarhGAs+TfJMLQDeSgTdL9QdqnEQyq+c2AD+QtHs1tjkWqG4CqD9bXkc1qcPMLOckNQJuAI4m+b00spwk99FAp/Q1GripVoNsYLI8J+8A/SLiYOAyPM5IXmV5TkrL/YFkkHvLs2zOi6TWwI3AkIg4CBhe64E2IFleK2cAr0dEV5I28jXpN4ta/twBDKpkfY0+551EaoAktQAOA34KnJBevJcCI9Js/fnAacA56XwfSd+XNEfSq5L+V1K70rok3S7ptTR7eVyZfe0u6QVJ36vlwzSrzCaShvc5ZVdI2k/StPTneZqkfdPeREOAq9Jr4qtlttnq+pBUyJbXUb+ydUj6maS56R2ZhyU1T+trJ2liunyByvQKlPQf6b565uPNMbMGoRewJCL+GRFfAOOBoWXKDAXuisSLQGtJe9d2oA1IleckImZHxCfp7ItA+1qOsaHJ5joBOAt4GFhWm8E1YNmclx8Cj0TEewAR4XOTX9mckwBaShLQAlhF0ia3PImIWSTvc0Vq9DnvJFLDdCzwVES8RfJD1Rm4GJgQEd0i4g/AzcAf0/nnSHot9Y6Ib5L8UvhlWtd/AZ9FRJf0rtj00p2kiabHgYsj4vHaOjizLN0AnCipVZnl15P8Mj0YuBe4LiJmA5OB89Jr4u0y22x1fUTEUra8jmaWU8cjEdEzvSPzd5LELsB1wMx0eXdgcemOJO1P0lAdFRFzc/RemFnDsw/wfsZ8UbqsumUsd6r7fv8UeDKvEVmV50TSPsAwks98qx3ZXCtfB3aTNEPSK5JOrrXoGqZszsn1wDeAD4HXgLMjoqR2wrMK1OhzvnHewrH6bCTwp3R6fDq/uOLiQHKna0KamdyZpDs1wBF8+UgcGXfHCoBpwBnpH89m9UpEfC7pLmAssC5j1beAH6TTdwNXZlFdRddHVTpL+m+gNckdmdJu8AOBk9M4i4HPJO0G7AFMAo6LiKquWTOzyqicZWW/sjebMpY7Wb/fkgaQJJG+k9eILJtz8ifg/IgoTjpYWC3I5rw0BnoAhwPNgBckvZjeRLfcy+acHAXMJ2nnfhV4RtJzEfF5voOzCtXoc949kRoYSW1JLtxxkpYC5wEjKP8HKNOfgesjogtwKtC0tErK/0HbBLxC8svCrL76E0kjfJdKymTzB1NF10dV7gDOTLf7bRbbfUZyt+CwLOs3M6tIEdAhY749yd3h6pax3Mnq/ZZ0MDAOGBoRK2sptoYqm3NyCDA+bVcfD9wo6djaCa/Byvb311MRsSYiVgCzAA9Enz/ZnJNRJL3wIyKWkNx0PaCW4rPy1ehz3kmkhud4kkd19ouIwojoQHIB7wu0zCj37zLzrYAP0ukfZyyfCpxZOpP2loDkD++fAAdU9k0WZnUpIlYBD/DlY2QAs/myd92JJI+qwdbXRKaKro+y25Sdbwl8JKkg3VepacDpkAxUKGnXdPkXJI+jnizph5UenJlZ5eYCnSR1TMdGPIHkkdtMk0l+30hSb5LH1z+q7UAbkCrPiaR9gUeAH7lHRa2o8pxERMe0TV0IPASMiYhHaz/UBiWb31+TgD6SGqdjTh5KMnSA5Uc25+Q9kp5hpcOe7A/8s1ajtLJq9DnvJFLDMxKYWGbZw8BewIHpgL8jgCnAsNKBtYFLgAclPQesyNj2v0meN14kaQEwoHRF+hjOCcAASWPydkRm2+YaIPNb2sYCoyQtBH4EnJ0uHw+clw5o/dUydVxC+ddH2euobB3/BcwBngHeyNjubJLr5jWSHn0Hla6IiDXAYJIBu8sb3NPMrEoRsYnkJtDTJH9YPRARiyWdJum0tNgTJA38JcCtgD/L8yjLc3Ix0Jakt8t8SS/XUbgNQpbnxGpZNuclIv4OPAUsBF4CxkVEuV9zbtsuy2vlMuDbaft2GsljoCvKr9FyQdL9wAvA/pKKJP00F5/zivCj7WZmZmZmZmZmVjn3RDIzMzMzMzMzsyo5iWRmZmZmZmZmZlVyEsnMzMzMzMzMzKrkJJKZmZmZmZmZmVXJSSQzMzMzMzMzM6uSk0hmVZBUnH6N7iJJD0pqvg113SHp+HR6nKQDKynbX9K3a7CPpZJ2z3Z5BXWcIun6XOzXzMzMrKHIaDeWvgorKbs6B/u7Q9I76b7mSfpWDerY3CaVdGGZdbO3Nca0nsz29BRJraso303SMbnYt5nllpNIZlVbFxHdIqIz8AVwWuZKSY1qUmlE/GdEvF5Jkf5AtZNIZmZmZlZnStuNpa+ltbDP8yKiG3AB8JfqblymTXphmXW5aotmtqdXAWdUUb4b4CSSWT3kJJJZ9TwHfC3tJfSspPuA1yQ1knSVpLmSFko6FUCJ6yW9LulxYM/SiiTNkHRIOj0ovXu0QNK09K7VacA56V2bPpL2kPRwuo+5kg5Lt20raaqkVyX9BVC2ByOpl6TZ6bazJe2fsbqDpKckvSnpNxnbnCTppTSuv5RNoknaRdLj6bEskjSimu+xmZmZ2Q5BUou0bTdP0muShpZTZm9JszJ66vRJlx8p6YV02wcltahid7OAr6Xb/jyta5Gk/58uK7eNVtomlfR7oFkax73putXp/xMyewalPaCOq6gNXIUXgH3SerZqi0raGbgUGJHGMiKN/bZ0P6+W9z6aWe1oXNcBmG0vJDUGjgaeShf1AjpHxDuSRgOfRURPSU2Av0maCnwT2B/oArQDXgduK1PvHsCtQN+0rjYRsUrSzcDqiLg6LXcf8MeIeF7SvsDTwDeA3wDPR8Slkr4HjK7GYb2R7neTpCOAK4DjMo8PWAvMTZNga4ARwGERsVHSjcCJwF0ZdQ4CPoyI76Vxt6pGPGZmZmbbs2aS5qfT7wDDgWER8bmSx/5flDQ5IiJjmx8CT0fE5enNueZp2YuAIyJijaTzgZ+TJFcq8n2Sm5s9gFHAoSQ3F+dImgn8B5W00SLiAklnpr2ayhpP0gZ8Ik3yHA6cDvyUctrAEfFOeQGmx3c48Nd00VZt0Yg4TtLFwCERcWa63RXA9Ij4iZJH4V6S9L8RsaaS98PM8sBJJLOqZTYGniP50Ps28FLGB+SRwMFKxzsCWgGdgL7A/RFRDHwoaXo59fcGZpXWFRGrKojjCOBAaXNHo10ltUz38YN028clfVKNY2sF3CmpExBAQca6ZyJiJYCkR4DvAJuAHiRJJYBmwLIydb4GXC3pD8BjEfFcNeIxMzMz256ty0zCSCoArpDUFygh6YHTDvhXxjZzgdvSso9GxHxJ/YADSZIyADuT9OApz1WSLgKWkyR1DgcmliZY0nZcH5IboTVtoz0JXJcmigaRtF3XSaqoDVw2iVTani4EXgGeyShfUVs005HAEEnnpvNNgX2Bv1fjGMwsB5xEMqvaurJ3ZNIP88w7HwLOioiny5Q7huQDsTLKogwkj59+KyLWlRNLNtuX5zLg2YgYpuQRuhkZ68rWGWmsd0bEryqqMCLeSu+AHQP8Lr0bVdldMzMzM7Md1YnAHkCPtBf3UpIEyGYRMStNMn0PuFvSVcAnJDf0Rmaxj/Mi4qHSmbRHz1a2pY0WEeslzQCOIumRdH/p7iinDVyOdRHRLe399BjJmEjXUXlbNJOA4yLizWziNbP88ZhIZrnxNHB6egcJSV+XtAvJs+knpM+L7w0MKGfbF4B+kjqm27ZJl/8baJlRbipwZumMpNLE1iySBgqSjgZ2q0bcrYAP0ulTyqz7rqQ2kpoBxwJ/A6YBx0vaszRWSftlbiTpK8DaiLgHuBroXo14zMzMzHYkrYBlaQJpALBf2QJpW2pZRNxK0uO9O/AicJik0jGOmkv6epb7nAUcm26zCzAMeC7LNtrG0vZsOcaTPCbXh6TtCxW3gcsVEZ8BY4Fz020qaouWbQc/DZyl9O6ppG9WtA8zyy/3RDLLjXEk3XPnpR9uy0kSLxOBgSSPeL0FzCy7YUQsT8dUekTSTiSPh30XmAI8lA4ceBbJB+4NkhaSXLuzSAbf/i1wv6R5af3vVRLnQkkl6fQDwJUkXYh/DpR91O554G6SARrvi4iXAdLu0lPTWDeS3El6N2O7LiTdqkvS9adXEo+ZmZnZjuxeYIqkl4H5JGMAldUfOE/SRmA1cHLaPjyFpI3XJC13EUl7slIRMU/SHcBL6aJxEfGqpKOouo12C0l7cV5EnFhm3VSScTAnR8QXpXVTfhu4svhelbQAOIGK26LPAhekj8D9jqTH0p/S2AQsBQZX/k6YWT5oyzHdzMzMzMzMzMzMtubH2czMzMzMzMzMrEpOIpmZmZmZmZmZWZWcRDIzMzMzMzMzsyo5iWRmZmZmZmZmZlVyEsnMzMzMzMzMzKrkJJKZmZmZmZmZmVXJSSQzMzMzMzMzM6uSk0hmZmZmZmZmZlal/wMDdAyDEFtlGQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#version6: test anti-asian data by removing emoji\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "plt.figure(1, figsize=(20,8))\n",
    "\n",
    "ax= plt.subplot(121)\n",
    "sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "ax.xaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "ax.yaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_true, y_prob_final)\n",
    "plt.subplot(122)\n",
    "lw = 2\n",
    "plt.plot(fpr_rt_lm, tpr_rt_lm, color='darkorange',\n",
    "         lw=lw, label='roc curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid()\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.4269    0.6504    0.5155       678\n",
      "           0     0.8157    0.6392    0.7168      1641\n",
      "\n",
      "    accuracy                         0.6425      2319\n",
      "   macro avg     0.6213    0.6448    0.6161      2319\n",
      "weighted avg     0.7020    0.6425    0.6579      2319\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#version5: test anti_asian data with double text\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "plt.figure(1, figsize=(20,8))\n",
    "\n",
    "ax= plt.subplot(121)\n",
    "sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "ax.xaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "ax.yaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_true, y_prob_final)\n",
    "plt.subplot(122)\n",
    "lw = 2\n",
    "plt.plot(fpr_rt_lm, tpr_rt_lm, color='darkorange',\n",
    "         lw=lw, label='roc curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid()\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.4438    0.6003    0.5103       678\n",
      "           0     0.8067    0.6892    0.7433      1641\n",
      "\n",
      "    accuracy                         0.6632      2319\n",
      "   macro avg     0.6253    0.6448    0.6268      2319\n",
      "weighted avg     0.7006    0.6632    0.6752      2319\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#version4: test anti-asian data with max_length = 60\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "plt.figure(1, figsize=(20,8))\n",
    "\n",
    "ax= plt.subplot(121)\n",
    "sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "ax.xaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "ax.yaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_true, y_prob_final)\n",
    "plt.subplot(122)\n",
    "lw = 2\n",
    "plt.plot(fpr_rt_lm, tpr_rt_lm, color='darkorange',\n",
    "         lw=lw, label='roc curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid()\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.4330    0.6578    0.5222       678\n",
      "           0     0.8200    0.6441    0.7215      1641\n",
      "\n",
      "    accuracy                         0.6481      2319\n",
      "   macro avg     0.6265    0.6510    0.6219      2319\n",
      "weighted avg     0.7069    0.6481    0.6632      2319\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#version 3: test the anti-asian data with max_length = 128\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "plt.figure(1, figsize=(20,8))\n",
    "\n",
    "ax= plt.subplot(121)\n",
    "sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "ax.xaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "ax.yaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_true, y_prob_final)\n",
    "plt.subplot(122)\n",
    "lw = 2\n",
    "plt.plot(fpr_rt_lm, tpr_rt_lm, color='darkorange',\n",
    "         lw=lw, label='roc curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid()\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.8453    0.7790    0.8108      2756\n",
      "           0     0.9705    0.9808    0.9756     20422\n",
      "\n",
      "    accuracy                         0.9568     23178\n",
      "   macro avg     0.9079    0.8799    0.8932     23178\n",
      "weighted avg     0.9556    0.9568    0.9560     23178\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# version 2 after modify the roc curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "plt.figure(1, figsize=(20,8))\n",
    "\n",
    "ax= plt.subplot(121)\n",
    "sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "ax.xaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "ax.yaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_true, y_prob_final)\n",
    "plt.subplot(122)\n",
    "lw = 2\n",
    "plt.plot(fpr_rt_lm, tpr_rt_lm, color='darkorange',\n",
    "         lw=lw, label='roc curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid()\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.8150    0.4179    0.5525      3869\n",
      "           0     0.8936    0.9810    0.9352     19275\n",
      "\n",
      "    accuracy                         0.8868     23144\n",
      "   macro avg     0.8543    0.6994    0.7439     23144\n",
      "weighted avg     0.8804    0.8868    0.8713     23144\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc score: 0.6994486220075837\n"
     ]
    }
   ],
   "source": [
    "# Version one\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "plt.figure(1, figsize=(20,8))\n",
    "\n",
    "ax= plt.subplot(121)\n",
    "sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "ax.xaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "ax.yaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_true, y_pred)\n",
    "plt.subplot(122)\n",
    "plt.plot(fpr_rt_lm, tpr_rt_lm)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "#     plt.legend(loc='best')\n",
    "plt.show()\n",
    "print('auc score:', roc_auc_score(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT fine-tune on fake news detection.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03029ccf12164dafbe4fd9bb6e3722cc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "07a9cf8752b743679a39fd412702301d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03029ccf12164dafbe4fd9bb6e3722cc",
      "placeholder": "​",
      "style": "IPY_MODEL_23c685a4d97447ca95727f733d2c2c9d",
      "value": " 433/433 [00:06&lt;00:00, 67.8B/s]"
     }
    },
    "0ff4905c17974a6092918a8d77b9d100": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7cddb6e6e8c948d981adb8697a03ad9a",
      "max": 433,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8f673376fffb43a59f88fda6509e002a",
      "value": 433
     }
    },
    "1ffc393f193a4c3f8390dc8307df77fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a01574d4d574cb9ac60e3d688125fae",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e4a0002383884d32a62cb26f0db5276d",
      "value": 440473133
     }
    },
    "23c685a4d97447ca95727f733d2c2c9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4a83eca6a3e84bb980085bd7ced5ced5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f5be3cee49247c8973561ced8e5181d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7128ff90beac44faa7f71b36b1e56099": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1ffc393f193a4c3f8390dc8307df77fb",
       "IPY_MODEL_af544453c65043b2a7f6fdb21d06a940"
      ],
      "layout": "IPY_MODEL_8672b461409d421facbada337e8f273d"
     }
    },
    "78f713ef2a9b4d46a6f359dd151c8904": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0ff4905c17974a6092918a8d77b9d100",
       "IPY_MODEL_07a9cf8752b743679a39fd412702301d"
      ],
      "layout": "IPY_MODEL_db05a76085614d789d15b245fa83e73b"
     }
    },
    "7a01574d4d574cb9ac60e3d688125fae": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7cddb6e6e8c948d981adb8697a03ad9a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8672b461409d421facbada337e8f273d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f673376fffb43a59f88fda6509e002a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "af544453c65043b2a7f6fdb21d06a940": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a83eca6a3e84bb980085bd7ced5ced5",
      "placeholder": "​",
      "style": "IPY_MODEL_4f5be3cee49247c8973561ced8e5181d",
      "value": " 440M/440M [00:06&lt;00:00, 70.7MB/s]"
     }
    },
    "db05a76085614d789d15b245fa83e73b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e4a0002383884d32a62cb26f0db5276d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
