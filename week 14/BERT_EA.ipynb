{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iln7yp-SGoVa"
   },
   "source": [
    "# BERT model constructed with EA data\n",
    "\n",
    "2020/11/27\n",
    "\n",
    "Student: Xuanyu Su                                                                 \n",
    "Supervisor: Isar Nejadgholi\n",
    "\n",
    "\n",
    "in this module, we use EA data to train a bert model and test 2400 covid data on it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSk3itMJP-eF"
   },
   "outputs": [],
   "source": [
    "source_folder = 'Data'\n",
    "destination_folder = 'Model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ouQ6KBTpsgji",
    "outputId": "87e5c56c-f4a3-4e39-ed3e-667cb24da4cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9u_UOXXHGCZ",
    "outputId": "c11ce909-357e-4bd8-c9f6-0843753da2a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RwTbVN6LsiMq",
    "outputId": "0250df2e-2cce-4468-a647-439f17cbf862"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.5.1)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.3)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMdEsRnhGulL"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "EA_data = pd.read_csv('/content/drive/My Drive/hs_AsianPrejudice_20kdataset_cleaned_anonymized.tsv',sep = '\\t', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "id": "9EWDsyKqHx9-",
    "outputId": "ad1e0888-f31d-4d47-b40b-4f42873d188d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>annot1</th>\n",
       "      <th>annot2</th>\n",
       "      <th>expert</th>\n",
       "      <th>text</th>\n",
       "      <th>text.clean</th>\n",
       "      <th>annot1.name</th>\n",
       "      <th>annot2.name</th>\n",
       "      <th>expert.name</th>\n",
       "      <th>target.annot1.clean</th>\n",
       "      <th>target.annot2.clean</th>\n",
       "      <th>hostile.threatening</th>\n",
       "      <th>hostile.dehumanization</th>\n",
       "      <th>hostile.interpersonal</th>\n",
       "      <th>COVID relevant</th>\n",
       "      <th>EA relevant</th>\n",
       "      <th>hashtags.annotator1</th>\n",
       "      <th>hashtags.annotator2</th>\n",
       "      <th>hashtags.decision</th>\n",
       "      <th>East Asia</th>\n",
       "      <th>China</th>\n",
       "      <th>Hong Kong</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Korea</th>\n",
       "      <th>Singapore</th>\n",
       "      <th>Taiwan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>idstr_1212372663416639488</td>\n",
       "      <td>none_of_the_above</td>\n",
       "      <td>none_of_the_above</td>\n",
       "      <td>none_of_the_above</td>\n",
       "      <td>@CNN No doubt a ChiNa female. #shameonchina #B...</td>\n",
       "      <td>@cnn no doubt a china female. HASHTAG_EASTASIA...</td>\n",
       "      <td>annotator_GNZuCtwed3</td>\n",
       "      <td>annotator_gbEGjSAk6r</td>\n",
       "      <td>expert_GNZuCtwed3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no_no</td>\n",
       "      <td>yes_yes</td>\n",
       "      <td>hashtags_not_used_at_all_to_identify_themes</td>\n",
       "      <td>hashtags_not_used_at_all_to_identify_themes</td>\n",
       "      <td>agree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>idstr_1212374922993053696</td>\n",
       "      <td>none_of_the_above</td>\n",
       "      <td>none_of_the_above</td>\n",
       "      <td>none_of_the_above</td>\n",
       "      <td>@KongTsungGan The #HongKongPoliceBrutality is ...</td>\n",
       "      <td>@kongtsunggan the HASHTAG_EASTASIA is happenin...</td>\n",
       "      <td>annotator_CAgNlUizNm</td>\n",
       "      <td>annotator_cYKSVBW4HD</td>\n",
       "      <td>expert_GNZuCtwed3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no_yes</td>\n",
       "      <td>yes_yes</td>\n",
       "      <td>hashtags_not_used_at_all_to_identify_themes</td>\n",
       "      <td>hashtags_only_used_to_identify_covid_relevance</td>\n",
       "      <td>disagree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>idstr_1213452156251987973</td>\n",
       "      <td>none_of_the_above</td>\n",
       "      <td>none_of_the_above</td>\n",
       "      <td>none_of_the_above</td>\n",
       "      <td>Afraid.  #ChinaPneumonia</td>\n",
       "      <td>afraid.  #HASHTAG</td>\n",
       "      <td>annotator_vDe7GN0NrL</td>\n",
       "      <td>annotator_HtRmsP3KiK</td>\n",
       "      <td>expert_CAgNlUizNm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no_no</td>\n",
       "      <td>no_no</td>\n",
       "      <td>hashtags_not_used_at_all_to_identify_themes</td>\n",
       "      <td>hashtags_not_used_at_all_to_identify_themes</td>\n",
       "      <td>agree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>idstr_1213471445294075909</td>\n",
       "      <td>none_of_the_above</td>\n",
       "      <td>none_of_the_above</td>\n",
       "      <td>none_of_the_above</td>\n",
       "      <td>RT @shapponeko @Mugisalty @CatBus2D everybody ...</td>\n",
       "      <td>rt @shapponeko @mugisalty @catbus2d everybody ...</td>\n",
       "      <td>annotator_TbUBpfn6iP</td>\n",
       "      <td>annotator_dqrONtdjbt</td>\n",
       "      <td>expert_GNZuCtwed3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no_yes</td>\n",
       "      <td>no_no</td>\n",
       "      <td>hashtags_not_used_at_all_to_identify_themes</td>\n",
       "      <td>hashtags_not_used_at_all_to_identify_themes</td>\n",
       "      <td>agree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>idstr_1213494381073711105</td>\n",
       "      <td>entity_directed_hostility</td>\n",
       "      <td>entity_directed_hostility</td>\n",
       "      <td>entity_directed_hostility</td>\n",
       "      <td>RT @JAbbott45624607 @annie_sparrow This makes ...</td>\n",
       "      <td>rt @jabbott45624607 @annie_sparrow this makes ...</td>\n",
       "      <td>annotator_oemYWm1Tjg</td>\n",
       "      <td>annotator_IBsVsBliwX</td>\n",
       "      <td>expert_GNZuCtwed3</td>\n",
       "      <td>China</td>\n",
       "      <td>China</td>\n",
       "      <td>twoAnnotations_NoNo</td>\n",
       "      <td>twoAnnotations_NoNo</td>\n",
       "      <td>twoAnnotations_NoNo</td>\n",
       "      <td>yes_yes</td>\n",
       "      <td>yes_yes</td>\n",
       "      <td>hashtags_not_used_at_all_to_identify_themes</td>\n",
       "      <td>hashtags_only_used_to_identify_covid_relevance</td>\n",
       "      <td>disagree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id                     annot1  ... Singapore Taiwan\n",
       "0  idstr_1212372663416639488          none_of_the_above  ...       NaN    NaN\n",
       "1  idstr_1212374922993053696          none_of_the_above  ...       NaN    NaN\n",
       "2  idstr_1213452156251987973          none_of_the_above  ...       NaN    NaN\n",
       "3  idstr_1213471445294075909          none_of_the_above  ...       NaN    NaN\n",
       "4  idstr_1213494381073711105  entity_directed_hostility  ...       NaN    NaN\n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 172,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EA_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDrHsOaiPIQf",
    "outputId": "166a252c-ea69-4285-e903-43bc6d1de236"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expert\n",
       "counter_speech                         116\n",
       "discussion_of_eastasian_prejudice     1029\n",
       "entity_directed_criticism             1433\n",
       "entity_directed_hostility             3898\n",
       "none_of_the_above                    13524\n",
       "dtype: int64"
      ]
     },
     "execution_count": 173,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# types of label\n",
    "ap = EA_data.groupby(by=['expert'])\n",
    "ap.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XS61s_i9PIk-",
    "outputId": "39d8a8b5-7017-404d-cd25-441130a4badb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:00<00:00, 1176539.36it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1915907.18it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "relevant1 = []\n",
    "relevant2 = []\n",
    "for i in tqdm(EA_data['hashtags.annotator1']):\n",
    "    if i == 'hashtags_not_used_at_all_to_identify_themes':\n",
    "        relevant1.append(0)\n",
    "    else:\n",
    "        relevant1.append(1)\n",
    "for i in tqdm(EA_data['hashtags.annotator2']):\n",
    "    if i == 'hashtags_not_used_at_all_to_identify_themes':\n",
    "        relevant2.append(0)\n",
    "    else:\n",
    "        relevant2.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrMxfGSxPY0b"
   },
   "outputs": [],
   "source": [
    "final_decision = []\n",
    "for i in range(len(relevant1)):\n",
    "    sum_temp = relevant1[i] + relevant2[i]\n",
    "    if sum_temp > 1:\n",
    "        final_decision.append(1)\n",
    "    else:\n",
    "        final_decision.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JYG4Cpn9PZQ6",
    "outputId": "a6317bb4-5623-4d23-c0cd-8917c286a560"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag keep ratio is: 0.3807\n"
     ]
    }
   ],
   "source": [
    "print(\"Tag keep ratio is:\",final_decision.count(1)/len(final_decision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Js90C4fSXY_"
   },
   "outputs": [],
   "source": [
    "# keep labels and text.clean\n",
    "new_EA = EA_data[['text.clean', 'expert']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "YawYTS2pShJv",
    "outputId": "84f878e1-d3b7-4454-9159-2cef56a48edc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text.clean</th>\n",
       "      <th>expert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@cnn no doubt a china female. HASHTAG_EASTASIA...</td>\n",
       "      <td>none_of_the_above</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@kongtsunggan the HASHTAG_EASTASIA is happenin...</td>\n",
       "      <td>none_of_the_above</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>afraid.  #HASHTAG</td>\n",
       "      <td>none_of_the_above</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt @shapponeko @mugisalty @catbus2d everybody ...</td>\n",
       "      <td>none_of_the_above</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rt @jabbott45624607 @annie_sparrow this makes ...</td>\n",
       "      <td>entity_directed_hostility</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text.clean                     expert\n",
       "0  @cnn no doubt a china female. HASHTAG_EASTASIA...          none_of_the_above\n",
       "1  @kongtsunggan the HASHTAG_EASTASIA is happenin...          none_of_the_above\n",
       "2                                  afraid.  #HASHTAG          none_of_the_above\n",
       "3  rt @shapponeko @mugisalty @catbus2d everybody ...          none_of_the_above\n",
       "4  rt @jabbott45624607 @annie_sparrow this makes ...  entity_directed_hostility"
      ]
     },
     "execution_count": 178,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_EA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aBo7T0CJDxp"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import emoji\n",
    "\n",
    "class Word_Preprocessing():\n",
    "    def eliminate_url(self,df,target):\n",
    "        print('Start eliminate url: : )')\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        text = df_temp[target_column_name]\n",
    "        for i in tqdm(text):\n",
    "            urls = re.findall(r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})', i)\n",
    "            for i in urls:\n",
    "                df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(i, \"\"))\n",
    "        return df_temp\n",
    "    \n",
    "    def eliminate_username(self,df,target):\n",
    "        print('Start eliminate username: : )')\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        for i in tqdm(df_temp[target_column_name]):\n",
    "            user_name = re.findall(r'@\\w*', i)\n",
    "            for i in user_name:\n",
    "                df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(i, \"\"))\n",
    "        return df_temp\n",
    "     \n",
    "    \n",
    "    def convert_abbreviation(self, df, target):\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        am = \"'m\"\n",
    "        are = \"'re\"\n",
    "        have = \"'ve\"\n",
    "        not_ = \"n't\"\n",
    "        df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(am, \" am\"))\n",
    "        df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(are, \"  are\"))\n",
    "        df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(have, \" have\"))\n",
    "        df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(not_, \" not\"))\n",
    "        return df_temp\n",
    "    \n",
    "    \n",
    "    def final_check(self,df,target):\n",
    "        print('Start Final check: ')\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x:re.sub(r'[^A-Za-z0-9 ]+', ' ', x).lower())\n",
    "        return df_temp\n",
    "            \n",
    "    def eliminate_symbol(self,df,target):\n",
    "        print('Start eliminate symbol: : )')\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        symbol_list = [',',\"'\",'!','@','$','%','^','&','*','(',')','-','+','?','>','<','=','.',':',';','  ','  ','   ','    ','      ','      ','  ']\n",
    "        for i in tqdm(symbol_list):\n",
    "            df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(i, ' '))\n",
    "        return df_temp\n",
    "    \n",
    "    def process_all(self, df,target):\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        df_fresh = self.convert_abbreviation(df_temp,target_column_name)\n",
    "        df_remove_url = self.eliminate_url(df_fresh,target_column_name)\n",
    "        df_remove_username = self.eliminate_username(df_remove_url, target_column_name)\n",
    "        df_remove_symbol = self.eliminate_symbol(df_remove_username, target_column_name)\n",
    "        df_final_check = self.final_check(df_remove_symbol, target_column_name)\n",
    "        print(\"finished!!\")\n",
    "        return df_final_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0EyFR3z3JdY4"
   },
   "outputs": [],
   "source": [
    "processor = Word_Preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hVnOVxsQJgkE",
    "outputId": "16138f29-4470-47bc-d705-d39232d5b715"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  0%|          | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start eliminate url: : )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "100%|██████████| 20000/20000 [01:34<00:00, 212.01it/s]\n",
      "  0%|          | 0/20000 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  0%|          | 15/20000 [00:00<02:28, 134.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start eliminate username: : )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:04<00:00, 4219.96it/s]\n",
      "  0%|          | 0/27 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      " 48%|████▊     | 13/27 [00:00<00:00, 123.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start eliminate symbol: : )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:00<00:00, 101.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Final check: \n",
      "finished!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "new_EA = processor.process_all(new_EA, 'text.clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xZl3L6CbPZKu",
    "outputId": "a3d0042f-3bec-48da-9e7e-5efea411b61c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " no doubt a china female hashtag eastasia  hashtag ------\n",
      " the hashtag eastasia is happening behind the live stream they are cooperating with the ccp hashtag eastasia  hashtag ------\n",
      "afraid  hashtag ------\n",
      "rt everybody should wear masks  hashtag ------\n",
      "rt this makes me remember the sad days in 2003 china covered up the sars situation and allowed it to spread to hk causing 299 dead and many suffered stop the hashtag eastasia virus from spreading  ------\n",
      " thats how sars started in hong kong in 2003 mainland chinese government tried to cover things up until its broke out in hong kong hundreds of hong kong citizens and medics were dead due to no cure at early stage of eruption  hashtag ------\n",
      " before you wear n95 masks you should look into getting a fit test because unlike surgical masks one size does not fit all for n95 masks having best fit n95 for your face will ensure a good face seal for protection  hashtag hashtag eastasia virus  ------\n",
      "please wear a mask when you come to hong kong   hashtag  hashtag  hashtag  ------\n",
      "hongkongers pls protect ourselves  hashtag ------\n",
      " we call it  chinese pneumonia  before we have more information hashtag eastasia virus  ------\n"
     ]
    }
   ],
   "source": [
    "[print(new_EA['text.clean'][i],'------') for i in range(10)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cqYE2LN3PZEp",
    "outputId": "d0581e96-27b2-4e50-e6da-3eb998d17ae4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "new_EA['text.clean'] = new_EA['text.clean'].apply(lambda x: x.replace('hashtag', \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZaluuygePY-q",
    "outputId": "6df51df3-8bd0-43a4-ed1a-2f1a2ed03e55"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "new_EA['text.clean'] = new_EA['text.clean'].apply(lambda x: x.replace('rt', \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H7owoWUOPY6_",
    "outputId": "0361e946-1e29-4f7e-fefe-9d844876c904"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " no doubt a china female  eastasia   ------\n",
      " the  eastasia is happening behind the live stream they are cooperating with the ccp  eastasia   ------\n",
      "afraid   ------\n",
      " everybody should wear masks   ------\n",
      " this makes me remember the sad days in 2003 china covered up the sars situation and allowed it to spread to hk causing 299 dead and many suffered stop the  eastasia virus from spreading  ------\n",
      " thats how sars staed in hong kong in 2003 mainland chinese government tried to cover things up until its broke out in hong kong hundreds of hong kong citizens and medics were dead due to no cure at early stage of eruption   ------\n",
      " before you wear n95 masks you should look into getting a fit test because unlike surgical masks one size does not fit all for n95 masks having best fit n95 for your face will ensure a good face seal for protection    eastasia virus  ------\n",
      "please wear a mask when you come to hong kong         ------\n",
      "hongkongers pls protect ourselves   ------\n",
      " we call it  chinese pneumonia  before we have more information  eastasia virus  ------\n"
     ]
    }
   ],
   "source": [
    "[print(new_EA['text.clean'][i],'------') for i in range(10)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 696
    },
    "id": "lg69m_z7RXrR",
    "outputId": "f2f50e4b-84c5-45f5-8621-54bc07d303ba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'length')"
      ]
     },
     "execution_count": 186,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get data distributions\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Plot histogram with the length. Truncate max length to 5000 tokens.\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "new_EA['length'] = new_EA['text.clean'].apply(lambda x: len(x.split()))\n",
    "sns.distplot(new_EA[new_EA['length'] < 150]['length'])\n",
    "plt.title('Frequence of documents of a given length', fontsize=14)\n",
    "plt.xlabel('length', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWIdhdAmTeoU"
   },
   "outputs": [],
   "source": [
    "new_EA = new_EA.drop(columns=['length'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uph-G0xLUFET"
   },
   "source": [
    "## Merge labels ( hostility and criticism VS others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QhJUT10yTej6",
    "outputId": "c3c3a21b-2df4-41a3-d248-b3d97be7e6e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:00<00:00, 1600329.66it/s]\n"
     ]
    }
   ],
   "source": [
    "mixed_label = []\n",
    "for i in tqdm(new_EA['expert']):\n",
    "    if i == 'counter_speech':\n",
    "        mixed_label.append('0')\n",
    "    elif i == 'discussion_of_eastasian_prejudice':\n",
    "        mixed_label.append('0')\n",
    "    elif i == 'none_of_the_above':\n",
    "        mixed_label.append('0')\n",
    "    else:\n",
    "        mixed_label.append('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Syrkhyy9Teai"
   },
   "outputs": [],
   "source": [
    "# assign new labels columns into original table\n",
    "new_EA['expert'] = mixed_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "YKU8ICKGVCQ_",
    "outputId": "d7feedc9-c574-4e7d-f70d-2d724eaf7114"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text.clean</th>\n",
       "      <th>expert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no doubt a china female  eastasia</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the  eastasia is happening behind the live st...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>afraid</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>everybody should wear masks</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this makes me remember the sad days in 2003 c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text.clean expert\n",
       "0                no doubt a china female  eastasia        0\n",
       "1   the  eastasia is happening behind the live st...      0\n",
       "2                                           afraid        0\n",
       "3                      everybody should wear masks        0\n",
       "4   this makes me remember the sad days in 2003 c...      1"
      ]
     },
     "execution_count": 190,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_EA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RvEl8lhPAS9"
   },
   "source": [
    "# model build "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Efi5828O3AL"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "# Preliminaries\n",
    "\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n",
    "\n",
    "# Models\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Training\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VYuopTJNPEab",
    "outputId": "1e11baf9-c934-49b8-a5ce-0c3aabb39e75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMlHGoFaPYpn"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYfsGDSpYjYI"
   },
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_b1h3-f8cR_T"
   },
   "outputs": [],
   "source": [
    "new_EA.rename(columns={ new_EA.columns[0]: \"text\" }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "8sDhmfwVdNrW",
    "outputId": "e68863b0-f77b-4f31-84cf-569a9a6358ca"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>expert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no doubt a china female  eastasia</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the  eastasia is happening behind the live st...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>afraid</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>everybody should wear masks</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this makes me remember the sad days in 2003 c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text expert\n",
       "0                no doubt a china female  eastasia        0\n",
       "1   the  eastasia is happening behind the live st...      0\n",
       "2                                           afraid        0\n",
       "3                      everybody should wear masks        0\n",
       "4   this makes me remember the sad days in 2003 c...      1"
      ]
     },
     "execution_count": 195,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_EA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KWofz0bHYi6N"
   },
   "outputs": [],
   "source": [
    "text = new_EA['text']\n",
    "label = new_EA['expert']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CtGBDQoYizL"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(text, label, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kXzJoUTIYiu1"
   },
   "outputs": [],
   "source": [
    "x_test, x_valid, y_text, y_valid = train_test_split(x_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nX_mfQgDYin4"
   },
   "outputs": [],
   "source": [
    "def pack_data(text, label, name):\n",
    "    EA = {'text':text,'hate':label}\n",
    "    EA_final = pd.DataFrame(EA)\n",
    "    file_name = name +'.csv'\n",
    "    EA_final.to_csv(file_name,index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5IMgQjwZY0vN"
   },
   "outputs": [],
   "source": [
    "pack_data(x_train, y_train, 'Data/EAtrain')\n",
    "pack_data(x_test, y_text, 'Data/EAtest')\n",
    "pack_data(x_valid, y_valid, 'Data/EAvalid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ovE7s0A0Xf3D"
   },
   "outputs": [],
   "source": [
    "# Sections of config\n",
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 80\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nF-z3gC4PxCz"
   },
   "outputs": [],
   "source": [
    "# Model parameter\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
    "\n",
    "# Fields\n",
    "\n",
    "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
    "text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n",
    "                   fix_length=MAX_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\n",
    "fields = [('text', text_field),('expert', label_field)]\n",
    "\n",
    "# TabularDataset\n",
    "\n",
    "train, valid, test = TabularDataset.splits(path=source_folder, train='EAtrain.csv', validation='EAvalid.csv',\n",
    "                                           test='EAtest.csv', format='CSV', fields=fields, skip_header=True)\n",
    "\n",
    "\n",
    "train_iter = BucketIterator(train, batch_size=8, sort_key=lambda x: len(x.text),\n",
    "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
    "valid_iter = BucketIterator(valid, batch_size=8, sort_key=lambda x: len(x.text),\n",
    "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
    "test_iter = Iterator(test, batch_size=8, device=device, train=False, shuffle=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_dnPGICVeYa"
   },
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "\n",
    "        options_name = \"bert-base-uncased\"\n",
    "        self.encoder = BertForSequenceClassification.from_pretrained(options_name)\n",
    "\n",
    "    def forward(self, text, label):\n",
    "        loss, text_fea = self.encoder(text, labels=label)[:2]\n",
    "        \n",
    "\n",
    "        return loss, text_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lITJWZ1PS1lU"
   },
   "outputs": [],
   "source": [
    "# Save and Load Functions\n",
    "\n",
    "def save_checkpoint(save_path, model, valid_loss):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'valid_loss': valid_loss}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "def load_checkpoint(load_path, model):\n",
    "    \n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    return state_dict['valid_loss']\n",
    "\n",
    "\n",
    "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'train_loss_list': train_loss_list,\n",
    "                  'valid_loss_list': valid_loss_list,\n",
    "                  'global_steps_list': global_steps_list}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_metrics(load_path):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z9zLrkFwS5ZY"
   },
   "outputs": [],
   "source": [
    "# Training Function\n",
    "\n",
    "def train(model,\n",
    "          optimizer,\n",
    "          criterion = nn.BCELoss(),\n",
    "          train_loader = train_iter,\n",
    "          valid_loader = valid_iter,\n",
    "          num_epochs = 5,\n",
    "          eval_every = len(train_iter) // 2,\n",
    "          file_path = destination_folder,\n",
    "          best_valid_loss = float(\"Inf\")):\n",
    "    \n",
    "    # initialize running values\n",
    "    running_loss = 0.0\n",
    "    valid_running_loss = 0.0 \n",
    "    global_step = 0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    global_steps_list = []\n",
    "\n",
    "    # training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        batch_no = 1\n",
    "        for (text, expert), _ in train_loader:\n",
    "            expert = expert.type(torch.LongTensor)           \n",
    "            expert = expert.to(device)\n",
    "            text = text.type(torch.LongTensor)  \n",
    "            text = text.to(device)\n",
    "            output = model(text, expert)\n",
    "            loss, _ = output\n",
    "            print('batch_no [{}/{}]:'.format(batch_no, int(len(x_train)/16)),'training_loss:',loss)\n",
    "            batch_no+=1\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update running values\n",
    "            running_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            # evaluation step\n",
    "            if global_step % eval_every == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():                    \n",
    "\n",
    "                    # validation loop\n",
    "                    for (text, expert), _ in valid_loader:\n",
    "                        expert = expert.type(torch.LongTensor)           \n",
    "                        expert = expert.to(device)\n",
    "                        text = text.type(torch.LongTensor)  \n",
    "                        text = text.to(device)\n",
    "                        output = model(text, expert)\n",
    "                        loss, _ = output\n",
    "                        \n",
    "                        valid_running_loss += loss.item()\n",
    "\n",
    "                # evaluation\n",
    "                average_train_loss = running_loss / eval_every\n",
    "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
    "                train_loss_list.append(average_train_loss)\n",
    "                valid_loss_list.append(average_valid_loss)\n",
    "                global_steps_list.append(global_step)\n",
    "\n",
    "                # resetting running values\n",
    "                running_loss = 0.0                \n",
    "                valid_running_loss = 0.0\n",
    "                model.train()\n",
    "\n",
    "                # print progress\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
    "                              average_train_loss, average_valid_loss))\n",
    "                \n",
    "                # checkpoint\n",
    "                if best_valid_loss > average_valid_loss:\n",
    "                    best_valid_loss = average_valid_loss\n",
    "                    save_checkpoint(file_path + '/' + 'model.pt', model, best_valid_loss)\n",
    "                    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    \n",
    "    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    print('Finished Training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "060IhzVuegXQ",
    "outputId": "2bb209c6-f6d7-4073-e58e-363d30a05f16"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "batch_no [1008/1000]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/1000]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/1000]: training_loss: tensor(0.1362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/1000]: training_loss: tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/1000]: training_loss: tensor(0.1996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/1000]: training_loss: tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/1000]: training_loss: tensor(0.1355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/1000]: training_loss: tensor(0.3927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/1000]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/1000]: training_loss: tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/1000]: training_loss: tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1019/1000]: training_loss: tensor(0.4478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/1000]: training_loss: tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/1000]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/1000]: training_loss: tensor(0.0572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1023/1000]: training_loss: tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/1000]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/1000]: training_loss: tensor(0.0492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/1000]: training_loss: tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/1000]: training_loss: tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/1000]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/1000]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/1000]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/1000]: training_loss: tensor(0.1268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/1000]: training_loss: tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/1000]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/1000]: training_loss: tensor(0.8121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/1000]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/1000]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/1000]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/1000]: training_loss: tensor(0.2327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/1000]: training_loss: tensor(0.1181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1040/1000]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/1000]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/1000]: training_loss: tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1043/1000]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/1000]: training_loss: tensor(0.5270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/1000]: training_loss: tensor(0.1175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/1000]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/1000]: training_loss: tensor(0.4709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/1000]: training_loss: tensor(0.0486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/1000]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/1000]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/1000]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/1000]: training_loss: tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/1000]: training_loss: tensor(1.0286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/1000]: training_loss: tensor(0.3771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/1000]: training_loss: tensor(0.2720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/1000]: training_loss: tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/1000]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/1000]: training_loss: tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/1000]: training_loss: tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/1000]: training_loss: tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/1000]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/1000]: training_loss: tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/1000]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/1000]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/1000]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/1000]: training_loss: tensor(0.3929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/1000]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/1000]: training_loss: tensor(0.3894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/1000]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/1000]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/1000]: training_loss: tensor(0.3108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/1000]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/1000]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/1000]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/1000]: training_loss: tensor(0.1273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/1000]: training_loss: tensor(0.2253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/1000]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/1000]: training_loss: tensor(0.1975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/1000]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/1000]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/1000]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/1000]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/1000]: training_loss: tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/1000]: training_loss: tensor(0.1186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/1000]: training_loss: tensor(0.2498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/1000]: training_loss: tensor(0.1489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/1000]: training_loss: tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/1000]: training_loss: tensor(0.2160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/1000]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/1000]: training_loss: tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/1000]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/1000]: training_loss: tensor(0.1518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/1000]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/1000]: training_loss: tensor(0.1836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/1000]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/1000]: training_loss: tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/1000]: training_loss: tensor(0.3130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/1000]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/1000]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1100/1000]: training_loss: tensor(0.3083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/1000]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/1000]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/1000]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/1000]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/1000]: training_loss: tensor(0.2101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/1000]: training_loss: tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/1000]: training_loss: tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/1000]: training_loss: tensor(0.6899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/1000]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/1000]: training_loss: tensor(0.1452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/1000]: training_loss: tensor(0.1616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/1000]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/1000]: training_loss: tensor(0.1704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/1000]: training_loss: tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/1000]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/1000]: training_loss: tensor(0.1481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/1000]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/1000]: training_loss: tensor(0.3708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/1000]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/1000]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/1000]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/1000]: training_loss: tensor(0.2353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/1000]: training_loss: tensor(0.1146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/1000]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/1000]: training_loss: tensor(0.2177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/1000]: training_loss: tensor(0.3182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/1000]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/1000]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1129/1000]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/1000]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/1000]: training_loss: tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/1000]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/1000]: training_loss: tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/1000]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/1000]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/1000]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/1000]: training_loss: tensor(0.2275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/1000]: training_loss: tensor(0.2438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/1000]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/1000]: training_loss: tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/1000]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/1000]: training_loss: tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/1000]: training_loss: tensor(0.2930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/1000]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/1000]: training_loss: tensor(0.2025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/1000]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/1000]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/1000]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/1000]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/1000]: training_loss: tensor(0.1366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/1000]: training_loss: tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/1000]: training_loss: tensor(0.3684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/1000]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/1000]: training_loss: tensor(0.1284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/1000]: training_loss: tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/1000]: training_loss: tensor(0.3831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/1000]: training_loss: tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/1000]: training_loss: tensor(0.1922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/1000]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/1000]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/1000]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/1000]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/1000]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/1000]: training_loss: tensor(0.0436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/1000]: training_loss: tensor(0.5416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/1000]: training_loss: tensor(0.1863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/1000]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/1000]: training_loss: tensor(0.2095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/1000]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/1000]: training_loss: tensor(0.1849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/1000]: training_loss: tensor(0.3637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/1000]: training_loss: tensor(0.4250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/1000]: training_loss: tensor(0.3307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/1000]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/1000]: training_loss: tensor(0.1184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/1000]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/1000]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/1000]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/1000]: training_loss: tensor(0.2810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/1000]: training_loss: tensor(0.0361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/1000]: training_loss: tensor(0.1192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/1000]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/1000]: training_loss: tensor(0.1873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/1000]: training_loss: tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/1000]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/1000]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/1000]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/1000]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/1000]: training_loss: tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/1000]: training_loss: tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/1000]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/1000]: training_loss: tensor(0.1344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/1000]: training_loss: tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/1000]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/1000]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/1000]: training_loss: tensor(0.0313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/1000]: training_loss: tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/1000]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/1000]: training_loss: tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/1000]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/1000]: training_loss: tensor(0.2684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/1000]: training_loss: tensor(0.1881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/1000]: training_loss: tensor(1.0520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/1000]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/1000]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/1000]: training_loss: tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/1000]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/1000]: training_loss: tensor(0.2401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/1000]: training_loss: tensor(0.1579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1213/1000]: training_loss: tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/1000]: training_loss: tensor(0.2238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1215/1000]: training_loss: tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/1000]: training_loss: tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/1000]: training_loss: tensor(0.2390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/1000]: training_loss: tensor(0.3984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/1000]: training_loss: tensor(0.1704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/1000]: training_loss: tensor(0.6584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/1000]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/1000]: training_loss: tensor(0.1567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/1000]: training_loss: tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/1000]: training_loss: tensor(0.1333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/1000]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/1000]: training_loss: tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/1000]: training_loss: tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/1000]: training_loss: tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/1000]: training_loss: tensor(0.5924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/1000]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/1000]: training_loss: tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/1000]: training_loss: tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/1000]: training_loss: tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/1000]: training_loss: tensor(0.1483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/1000]: training_loss: tensor(0.5797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/1000]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/1000]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/1000]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/1000]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/1000]: training_loss: tensor(0.1236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/1000]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/1000]: training_loss: tensor(0.2135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/1000]: training_loss: tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/1000]: training_loss: tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/1000]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/1000]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/1000]: training_loss: tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/1000]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/1000]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/1000]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/1000]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/1000]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/1000]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/1000]: training_loss: tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/1000]: training_loss: tensor(0.1668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/1000]: training_loss: tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/1000]: training_loss: tensor(0.6024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/1000]: training_loss: tensor(0.1522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/1000]: training_loss: tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/1000]: training_loss: tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/1000]: training_loss: tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/1000]: training_loss: tensor(0.3098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/1000]: training_loss: tensor(0.4050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/1000]: training_loss: tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/1000]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/1000]: training_loss: tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/1000]: training_loss: tensor(0.1460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/1000]: training_loss: tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/1000]: training_loss: tensor(0.0534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/1000]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/1000]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/1000]: training_loss: tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/1000]: training_loss: tensor(0.1116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/1000]: training_loss: tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/1000]: training_loss: tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/1000]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/1000]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/1000]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/1000]: training_loss: tensor(0.2263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/1000]: training_loss: tensor(0.1111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/1000]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/1000]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/1000]: training_loss: tensor(0.4409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/1000]: training_loss: tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/1000]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/1000]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/1000]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/1000]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/1000]: training_loss: tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/1000]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/1000]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/1000]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/1000]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/1000]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/1000]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/1000]: training_loss: tensor(0.1483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1299/1000]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/1000]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/1000]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/1000]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/1000]: training_loss: tensor(0.0447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/1000]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/1000]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/1000]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/1000]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/1000]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/1000]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/1000]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/1000]: training_loss: tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/1000]: training_loss: tensor(0.4250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/1000]: training_loss: tensor(0.1540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/1000]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/1000]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/1000]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/1000]: training_loss: tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/1000]: training_loss: tensor(0.3911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/1000]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/1000]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/1000]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/1000]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/1000]: training_loss: tensor(0.1204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/1000]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/1000]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/1000]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/1000]: training_loss: tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/1000]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/1000]: training_loss: tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/1000]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/1000]: training_loss: tensor(0.1187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/1000]: training_loss: tensor(0.2207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/1000]: training_loss: tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/1000]: training_loss: tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/1000]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/1000]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/1000]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/1000]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/1000]: training_loss: tensor(0.0683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/1000]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/1000]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/1000]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/1000]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/1000]: training_loss: tensor(0.4323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/1000]: training_loss: tensor(0.5865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/1000]: training_loss: tensor(0.4247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/1000]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/1000]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/1000]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/1000]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/1000]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/1000]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/1000]: training_loss: tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/1000]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1361/1000]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/1000]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1363/1000]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/1000]: training_loss: tensor(0.1241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/1000]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/1000]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/1000]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/1000]: training_loss: tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/1000]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/1000]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/1000]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/1000]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/1000]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/1000]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/1000]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/1000]: training_loss: tensor(0.5409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/1000]: training_loss: tensor(0.6797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/1000]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1385/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1386/1000]: training_loss: tensor(0.5545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1387/1000]: training_loss: tensor(0.1728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1388/1000]: training_loss: tensor(0.2696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1389/1000]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1390/1000]: training_loss: tensor(0.1083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1391/1000]: training_loss: tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1392/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1393/1000]: training_loss: tensor(0.4313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1394/1000]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1395/1000]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1396/1000]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1397/1000]: training_loss: tensor(0.2098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1398/1000]: training_loss: tensor(0.1558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1399/1000]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1400/1000]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1401/1000]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1402/1000]: training_loss: tensor(0.4979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1403/1000]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1404/1000]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1405/1000]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1406/1000]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1407/1000]: training_loss: tensor(0.7145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1408/1000]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1409/1000]: training_loss: tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1410/1000]: training_loss: tensor(0.1237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1411/1000]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1412/1000]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1413/1000]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1414/1000]: training_loss: tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1415/1000]: training_loss: tensor(0.3175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1416/1000]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1417/1000]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1418/1000]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1419/1000]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1420/1000]: training_loss: tensor(0.0383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1421/1000]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1422/1000]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1423/1000]: training_loss: tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1424/1000]: training_loss: tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1425/1000]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1426/1000]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1427/1000]: training_loss: tensor(0.3120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1428/1000]: training_loss: tensor(0.2347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1429/1000]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1430/1000]: training_loss: tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1431/1000]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1432/1000]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1433/1000]: training_loss: tensor(0.0427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1434/1000]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1435/1000]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1436/1000]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1437/1000]: training_loss: tensor(0.1466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1438/1000]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1439/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1440/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1441/1000]: training_loss: tensor(0.1339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1442/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1443/1000]: training_loss: tensor(0.1167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1444/1000]: training_loss: tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1445/1000]: training_loss: tensor(0.2728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1446/1000]: training_loss: tensor(0.8854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1447/1000]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1448/1000]: training_loss: tensor(0.2331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1449/1000]: training_loss: tensor(0.0570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1450/1000]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1451/1000]: training_loss: tensor(0.1374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1452/1000]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1453/1000]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1454/1000]: training_loss: tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1455/1000]: training_loss: tensor(0.3404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1456/1000]: training_loss: tensor(0.5009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1457/1000]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1458/1000]: training_loss: tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1459/1000]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1460/1000]: training_loss: tensor(0.8492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1461/1000]: training_loss: tensor(0.4051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1462/1000]: training_loss: tensor(0.2373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1463/1000]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1464/1000]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1465/1000]: training_loss: tensor(0.3712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1466/1000]: training_loss: tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1467/1000]: training_loss: tensor(0.2215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1468/1000]: training_loss: tensor(0.2613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1469/1000]: training_loss: tensor(0.1698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1470/1000]: training_loss: tensor(0.1165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1471/1000]: training_loss: tensor(0.1657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1472/1000]: training_loss: tensor(0.0362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1473/1000]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1474/1000]: training_loss: tensor(0.3447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1475/1000]: training_loss: tensor(0.3706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1476/1000]: training_loss: tensor(0.2899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1477/1000]: training_loss: tensor(0.2029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1478/1000]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1479/1000]: training_loss: tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1480/1000]: training_loss: tensor(0.1150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1481/1000]: training_loss: tensor(0.1331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1482/1000]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1483/1000]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1484/1000]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1485/1000]: training_loss: tensor(0.2863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1486/1000]: training_loss: tensor(0.0501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1487/1000]: training_loss: tensor(0.6099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1488/1000]: training_loss: tensor(0.3193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1489/1000]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1490/1000]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1491/1000]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1492/1000]: training_loss: tensor(0.2620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1493/1000]: training_loss: tensor(0.1216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1494/1000]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1495/1000]: training_loss: tensor(0.7618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1496/1000]: training_loss: tensor(0.1993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1497/1000]: training_loss: tensor(0.3099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1498/1000]: training_loss: tensor(0.2012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1499/1000]: training_loss: tensor(0.2353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1500/1000]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1501/1000]: training_loss: tensor(0.3681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1502/1000]: training_loss: tensor(0.1613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1503/1000]: training_loss: tensor(0.4645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1504/1000]: training_loss: tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1505/1000]: training_loss: tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1506/1000]: training_loss: tensor(0.1450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1507/1000]: training_loss: tensor(0.2085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1508/1000]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1509/1000]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1510/1000]: training_loss: tensor(0.0555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1511/1000]: training_loss: tensor(0.3196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1512/1000]: training_loss: tensor(0.4669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1513/1000]: training_loss: tensor(0.0543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1514/1000]: training_loss: tensor(0.0545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1515/1000]: training_loss: tensor(0.1444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1516/1000]: training_loss: tensor(0.3618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1517/1000]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1518/1000]: training_loss: tensor(0.1402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1519/1000]: training_loss: tensor(0.1628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1520/1000]: training_loss: tensor(0.1222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1521/1000]: training_loss: tensor(0.1541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1522/1000]: training_loss: tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1523/1000]: training_loss: tensor(0.6558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1524/1000]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1525/1000]: training_loss: tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1526/1000]: training_loss: tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1527/1000]: training_loss: tensor(0.1258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1528/1000]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1529/1000]: training_loss: tensor(0.2606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1530/1000]: training_loss: tensor(0.0594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1531/1000]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1532/1000]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1533/1000]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1534/1000]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1535/1000]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1536/1000]: training_loss: tensor(0.1693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1537/1000]: training_loss: tensor(0.1182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1538/1000]: training_loss: tensor(0.3231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1539/1000]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1540/1000]: training_loss: tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1541/1000]: training_loss: tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1542/1000]: training_loss: tensor(0.1449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1543/1000]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1544/1000]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1545/1000]: training_loss: tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1546/1000]: training_loss: tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1547/1000]: training_loss: tensor(0.2016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1548/1000]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1549/1000]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1550/1000]: training_loss: tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1551/1000]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1552/1000]: training_loss: tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1553/1000]: training_loss: tensor(0.1843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1554/1000]: training_loss: tensor(0.1196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1555/1000]: training_loss: tensor(0.0575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1556/1000]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1557/1000]: training_loss: tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1558/1000]: training_loss: tensor(0.1431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1559/1000]: training_loss: tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1560/1000]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1561/1000]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1562/1000]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1563/1000]: training_loss: tensor(0.2955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1564/1000]: training_loss: tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1565/1000]: training_loss: tensor(0.0429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1566/1000]: training_loss: tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1567/1000]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1568/1000]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1569/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1570/1000]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1571/1000]: training_loss: tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1572/1000]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1573/1000]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1574/1000]: training_loss: tensor(0.4969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1575/1000]: training_loss: tensor(0.1727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1576/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1577/1000]: training_loss: tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1578/1000]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1579/1000]: training_loss: tensor(0.1337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1580/1000]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1581/1000]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1582/1000]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1583/1000]: training_loss: tensor(0.1648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1584/1000]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1585/1000]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1586/1000]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1587/1000]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1588/1000]: training_loss: tensor(0.5141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1589/1000]: training_loss: tensor(0.8284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1590/1000]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1591/1000]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1592/1000]: training_loss: tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1593/1000]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1594/1000]: training_loss: tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1595/1000]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1596/1000]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1597/1000]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1598/1000]: training_loss: tensor(0.3438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1599/1000]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1600/1000]: training_loss: tensor(0.2985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1601/1000]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1602/1000]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1603/1000]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1604/1000]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1605/1000]: training_loss: tensor(0.0573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1606/1000]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1607/1000]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1608/1000]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1609/1000]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1610/1000]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1611/1000]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1612/1000]: training_loss: tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1613/1000]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1614/1000]: training_loss: tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1615/1000]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1616/1000]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1617/1000]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1618/1000]: training_loss: tensor(0.2628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1619/1000]: training_loss: tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1620/1000]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1621/1000]: training_loss: tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1622/1000]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1623/1000]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1624/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1625/1000]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1626/1000]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1627/1000]: training_loss: tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1628/1000]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1629/1000]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1630/1000]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1631/1000]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1632/1000]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1633/1000]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1634/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1635/1000]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1636/1000]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1637/1000]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1638/1000]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1639/1000]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1640/1000]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1641/1000]: training_loss: tensor(0.1214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1642/1000]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1643/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1644/1000]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1645/1000]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1646/1000]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1647/1000]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1648/1000]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1649/1000]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1650/1000]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1651/1000]: training_loss: tensor(0.2500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1652/1000]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1653/1000]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1654/1000]: training_loss: tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1655/1000]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1656/1000]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1657/1000]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1658/1000]: training_loss: tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1659/1000]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1660/1000]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1661/1000]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1662/1000]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1663/1000]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1664/1000]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1665/1000]: training_loss: tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1666/1000]: training_loss: tensor(0.6199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1667/1000]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1668/1000]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1669/1000]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1670/1000]: training_loss: tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1671/1000]: training_loss: tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1672/1000]: training_loss: tensor(0.2929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1673/1000]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1674/1000]: training_loss: tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1675/1000]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1676/1000]: training_loss: tensor(0.8145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1677/1000]: training_loss: tensor(0.4414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1678/1000]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1679/1000]: training_loss: tensor(0.2417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1680/1000]: training_loss: tensor(0.3448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1681/1000]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1682/1000]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1683/1000]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1684/1000]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1685/1000]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1686/1000]: training_loss: tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1687/1000]: training_loss: tensor(0.1659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1688/1000]: training_loss: tensor(0.3707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1689/1000]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1690/1000]: training_loss: tensor(0.2798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1691/1000]: training_loss: tensor(0.0492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1692/1000]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1693/1000]: training_loss: tensor(0.1825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1694/1000]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1695/1000]: training_loss: tensor(0.1243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1696/1000]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1697/1000]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1698/1000]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1699/1000]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1700/1000]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1701/1000]: training_loss: tensor(0.0423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1702/1000]: training_loss: tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1703/1000]: training_loss: tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1704/1000]: training_loss: tensor(0.1384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1705/1000]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1706/1000]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1707/1000]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1708/1000]: training_loss: tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1709/1000]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1710/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1711/1000]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1712/1000]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1713/1000]: training_loss: tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1714/1000]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1715/1000]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1716/1000]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1717/1000]: training_loss: tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1718/1000]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1719/1000]: training_loss: tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1720/1000]: training_loss: tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1721/1000]: training_loss: tensor(0.7483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1722/1000]: training_loss: tensor(0.2447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1723/1000]: training_loss: tensor(0.2219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1724/1000]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1725/1000]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1726/1000]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1727/1000]: training_loss: tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1728/1000]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1729/1000]: training_loss: tensor(0.2081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1730/1000]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1731/1000]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1732/1000]: training_loss: tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1733/1000]: training_loss: tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1734/1000]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1735/1000]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1736/1000]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1737/1000]: training_loss: tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1738/1000]: training_loss: tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1739/1000]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1740/1000]: training_loss: tensor(0.2781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1741/1000]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1742/1000]: training_loss: tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1743/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1744/1000]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1745/1000]: training_loss: tensor(0.3812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1746/1000]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1747/1000]: training_loss: tensor(0.3227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1748/1000]: training_loss: tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1749/1000]: training_loss: tensor(0.3293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1750/1000]: training_loss: tensor(0.0459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1751/1000]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1752/1000]: training_loss: tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1753/1000]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1754/1000]: training_loss: tensor(0.1355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1755/1000]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1756/1000]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1757/1000]: training_loss: tensor(0.1101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1758/1000]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1759/1000]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1760/1000]: training_loss: tensor(0.4167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1761/1000]: training_loss: tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1762/1000]: training_loss: tensor(0.1044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1763/1000]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1764/1000]: training_loss: tensor(0.0362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1765/1000]: training_loss: tensor(0.2404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1766/1000]: training_loss: tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1767/1000]: training_loss: tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1768/1000]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1769/1000]: training_loss: tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1770/1000]: training_loss: tensor(0.1146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1771/1000]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1772/1000]: training_loss: tensor(0.1339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1773/1000]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1774/1000]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1775/1000]: training_loss: tensor(0.5969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1776/1000]: training_loss: tensor(0.1552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1777/1000]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1778/1000]: training_loss: tensor(0.5645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1779/1000]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1780/1000]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1781/1000]: training_loss: tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1782/1000]: training_loss: tensor(0.2458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1783/1000]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1784/1000]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1785/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1786/1000]: training_loss: tensor(0.3166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1787/1000]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1788/1000]: training_loss: tensor(0.2686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1789/1000]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1790/1000]: training_loss: tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1791/1000]: training_loss: tensor(0.8126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1792/1000]: training_loss: tensor(0.9406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1793/1000]: training_loss: tensor(0.1939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1794/1000]: training_loss: tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1795/1000]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1796/1000]: training_loss: tensor(0.2876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1797/1000]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1798/1000]: training_loss: tensor(0.1243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1799/1000]: training_loss: tensor(0.1202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1800/1000]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1801/1000]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1802/1000]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1803/1000]: training_loss: tensor(0.1059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1804/1000]: training_loss: tensor(0.6380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1805/1000]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1806/1000]: training_loss: tensor(0.3530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1807/1000]: training_loss: tensor(0.1817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1808/1000]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1809/1000]: training_loss: tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1810/1000]: training_loss: tensor(0.1623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1811/1000]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1812/1000]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1813/1000]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1814/1000]: training_loss: tensor(0.2458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1815/1000]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1816/1000]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1817/1000]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1818/1000]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1819/1000]: training_loss: tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1820/1000]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1821/1000]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1822/1000]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1823/1000]: training_loss: tensor(0.2672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1824/1000]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1825/1000]: training_loss: tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1826/1000]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1827/1000]: training_loss: tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1828/1000]: training_loss: tensor(0.3447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1829/1000]: training_loss: tensor(0.4170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1830/1000]: training_loss: tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1831/1000]: training_loss: tensor(0.1976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1832/1000]: training_loss: tensor(0.0586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1833/1000]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1834/1000]: training_loss: tensor(0.1429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1835/1000]: training_loss: tensor(0.2606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1836/1000]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1837/1000]: training_loss: tensor(0.1176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1838/1000]: training_loss: tensor(0.4249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1839/1000]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1840/1000]: training_loss: tensor(0.0462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1841/1000]: training_loss: tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1842/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1843/1000]: training_loss: tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1844/1000]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1845/1000]: training_loss: tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1846/1000]: training_loss: tensor(0.3128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1847/1000]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1848/1000]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1849/1000]: training_loss: tensor(0.1840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1850/1000]: training_loss: tensor(0.4639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1851/1000]: training_loss: tensor(0.0640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1852/1000]: training_loss: tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1853/1000]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1854/1000]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1855/1000]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1856/1000]: training_loss: tensor(0.4595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1857/1000]: training_loss: tensor(0.2192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1858/1000]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1859/1000]: training_loss: tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1860/1000]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1861/1000]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1862/1000]: training_loss: tensor(0.1868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1863/1000]: training_loss: tensor(0.1764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1864/1000]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1865/1000]: training_loss: tensor(0.2072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1866/1000]: training_loss: tensor(0.3405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1867/1000]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1868/1000]: training_loss: tensor(0.1117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1869/1000]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1870/1000]: training_loss: tensor(0.1777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1871/1000]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1872/1000]: training_loss: tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1873/1000]: training_loss: tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1874/1000]: training_loss: tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1875/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1876/1000]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1877/1000]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1878/1000]: training_loss: tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1879/1000]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1880/1000]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1881/1000]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1882/1000]: training_loss: tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1883/1000]: training_loss: tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1884/1000]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1885/1000]: training_loss: tensor(0.3334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1886/1000]: training_loss: tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1887/1000]: training_loss: tensor(0.3307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1888/1000]: training_loss: tensor(0.0342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1889/1000]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1890/1000]: training_loss: tensor(0.2623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1891/1000]: training_loss: tensor(0.0534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1892/1000]: training_loss: tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1893/1000]: training_loss: tensor(0.6790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1894/1000]: training_loss: tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1895/1000]: training_loss: tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1896/1000]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1897/1000]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1898/1000]: training_loss: tensor(0.0383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1899/1000]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1900/1000]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1901/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1902/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1903/1000]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1904/1000]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1905/1000]: training_loss: tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1906/1000]: training_loss: tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1907/1000]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1908/1000]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1909/1000]: training_loss: tensor(0.0606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1910/1000]: training_loss: tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1911/1000]: training_loss: tensor(0.1564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1912/1000]: training_loss: tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1913/1000]: training_loss: tensor(0.1709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1914/1000]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1915/1000]: training_loss: tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1916/1000]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1917/1000]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1918/1000]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1919/1000]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1920/1000]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1921/1000]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1922/1000]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1923/1000]: training_loss: tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1924/1000]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1925/1000]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1926/1000]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1927/1000]: training_loss: tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1928/1000]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1929/1000]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1930/1000]: training_loss: tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1931/1000]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1932/1000]: training_loss: tensor(0.1348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1933/1000]: training_loss: tensor(0.3059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1934/1000]: training_loss: tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1935/1000]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1936/1000]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1937/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1938/1000]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1939/1000]: training_loss: tensor(0.3568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1940/1000]: training_loss: tensor(0.2456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1941/1000]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1942/1000]: training_loss: tensor(0.3374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1943/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1944/1000]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1945/1000]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1946/1000]: training_loss: tensor(0.1577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1947/1000]: training_loss: tensor(0.2109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1948/1000]: training_loss: tensor(0.1719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1949/1000]: training_loss: tensor(0.3350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1950/1000]: training_loss: tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1951/1000]: training_loss: tensor(0.2062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1952/1000]: training_loss: tensor(0.5456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1953/1000]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1954/1000]: training_loss: tensor(0.3038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1955/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1956/1000]: training_loss: tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1957/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1958/1000]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1959/1000]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1960/1000]: training_loss: tensor(0.1128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1961/1000]: training_loss: tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1962/1000]: training_loss: tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1963/1000]: training_loss: tensor(0.6682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1964/1000]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1965/1000]: training_loss: tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1966/1000]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1967/1000]: training_loss: tensor(0.4497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1968/1000]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1969/1000]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1970/1000]: training_loss: tensor(0.3567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1971/1000]: training_loss: tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1972/1000]: training_loss: tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1973/1000]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1974/1000]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1975/1000]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1976/1000]: training_loss: tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1977/1000]: training_loss: tensor(0.3122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1978/1000]: training_loss: tensor(0.1404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1979/1000]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1980/1000]: training_loss: tensor(0.1116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1981/1000]: training_loss: tensor(0.6641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1982/1000]: training_loss: tensor(0.5315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1983/1000]: training_loss: tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1984/1000]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1985/1000]: training_loss: tensor(0.0620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1986/1000]: training_loss: tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1987/1000]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1988/1000]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1989/1000]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1990/1000]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1991/1000]: training_loss: tensor(0.2924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1992/1000]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1993/1000]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1994/1000]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1995/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1996/1000]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1997/1000]: training_loss: tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1998/1000]: training_loss: tensor(0.1434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1999/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2000/1000]: training_loss: tensor(0.8239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [3/5], Step [6000/10000], Train Loss: 0.1078, Valid Loss: 0.3665\n",
      "batch_no [1/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2/1000]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3/1000]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4/1000]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [5/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [6/1000]: training_loss: tensor(0.1296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [7/1000]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [8/1000]: training_loss: tensor(0.0570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [9/1000]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [10/1000]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [11/1000]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [12/1000]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [13/1000]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [14/1000]: training_loss: tensor(0.4349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [15/1000]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [16/1000]: training_loss: tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [17/1000]: training_loss: tensor(0.1167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [18/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [19/1000]: training_loss: tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [20/1000]: training_loss: tensor(0.0475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [21/1000]: training_loss: tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [22/1000]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [23/1000]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [24/1000]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [25/1000]: training_loss: tensor(0.1376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [26/1000]: training_loss: tensor(0.3010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [27/1000]: training_loss: tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [28/1000]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [29/1000]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [30/1000]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [31/1000]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [32/1000]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [33/1000]: training_loss: tensor(0.4253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [34/1000]: training_loss: tensor(0.3518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [35/1000]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [36/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [37/1000]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [38/1000]: training_loss: tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [39/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [40/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [41/1000]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [42/1000]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [43/1000]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [44/1000]: training_loss: tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [45/1000]: training_loss: tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [46/1000]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [47/1000]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [48/1000]: training_loss: tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [49/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [50/1000]: training_loss: tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [51/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [52/1000]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [53/1000]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [54/1000]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [55/1000]: training_loss: tensor(0.1615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [56/1000]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [57/1000]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [58/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [59/1000]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [60/1000]: training_loss: tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [61/1000]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [62/1000]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [63/1000]: training_loss: tensor(0.1233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [64/1000]: training_loss: tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [65/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [66/1000]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [67/1000]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [68/1000]: training_loss: tensor(0.6328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [69/1000]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [70/1000]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [71/1000]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [72/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [73/1000]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [74/1000]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [75/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [76/1000]: training_loss: tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [77/1000]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [78/1000]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [79/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [80/1000]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [81/1000]: training_loss: tensor(0.1304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [82/1000]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [83/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [84/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [85/1000]: training_loss: tensor(0.1296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [86/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [87/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [88/1000]: training_loss: tensor(0.2196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [89/1000]: training_loss: tensor(0.3190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [90/1000]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [91/1000]: training_loss: tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [92/1000]: training_loss: tensor(0.5177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [93/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [94/1000]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [95/1000]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [96/1000]: training_loss: tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [97/1000]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [98/1000]: training_loss: tensor(0.2342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [99/1000]: training_loss: tensor(0.2190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [100/1000]: training_loss: tensor(0.4150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [101/1000]: training_loss: tensor(0.1457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [102/1000]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [103/1000]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [104/1000]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [105/1000]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [106/1000]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [107/1000]: training_loss: tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [108/1000]: training_loss: tensor(0.1377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [109/1000]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [110/1000]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [111/1000]: training_loss: tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [112/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [113/1000]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [114/1000]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [115/1000]: training_loss: tensor(0.3015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [116/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [117/1000]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [118/1000]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [119/1000]: training_loss: tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [120/1000]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [121/1000]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [122/1000]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [123/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [124/1000]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [125/1000]: training_loss: tensor(0.2798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [126/1000]: training_loss: tensor(0.0576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [127/1000]: training_loss: tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [128/1000]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [129/1000]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [130/1000]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [131/1000]: training_loss: tensor(0.0640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [132/1000]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [133/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [134/1000]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [135/1000]: training_loss: tensor(0.2559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [136/1000]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [137/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [138/1000]: training_loss: tensor(0.1191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [139/1000]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [140/1000]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [141/1000]: training_loss: tensor(0.2079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [142/1000]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [143/1000]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [144/1000]: training_loss: tensor(0.3532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [145/1000]: training_loss: tensor(0.1622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [146/1000]: training_loss: tensor(0.2708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [147/1000]: training_loss: tensor(0.4395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [148/1000]: training_loss: tensor(0.7591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [149/1000]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [150/1000]: training_loss: tensor(0.0459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [151/1000]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [152/1000]: training_loss: tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [153/1000]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [154/1000]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [155/1000]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [156/1000]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [157/1000]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [158/1000]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [159/1000]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [160/1000]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [161/1000]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [162/1000]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [163/1000]: training_loss: tensor(0.4555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [164/1000]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [165/1000]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [166/1000]: training_loss: tensor(0.6676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [167/1000]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [168/1000]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [169/1000]: training_loss: tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [170/1000]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [171/1000]: training_loss: tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [172/1000]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [173/1000]: training_loss: tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [174/1000]: training_loss: tensor(0.3788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [175/1000]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [176/1000]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [177/1000]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [178/1000]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [179/1000]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [180/1000]: training_loss: tensor(0.1305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [181/1000]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [182/1000]: training_loss: tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [183/1000]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [184/1000]: training_loss: tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [185/1000]: training_loss: tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [186/1000]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [187/1000]: training_loss: tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [188/1000]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [189/1000]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [190/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [191/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [192/1000]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [193/1000]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [194/1000]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [195/1000]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [196/1000]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [197/1000]: training_loss: tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [198/1000]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [199/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [200/1000]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [201/1000]: training_loss: tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [202/1000]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [203/1000]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [204/1000]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [205/1000]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [206/1000]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [207/1000]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [208/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [209/1000]: training_loss: tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [210/1000]: training_loss: tensor(0.3267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [211/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [212/1000]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [213/1000]: training_loss: tensor(0.2021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [214/1000]: training_loss: tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [215/1000]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [216/1000]: training_loss: tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [217/1000]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [218/1000]: training_loss: tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [219/1000]: training_loss: tensor(0.2894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [220/1000]: training_loss: tensor(0.1213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [221/1000]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [222/1000]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [223/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [224/1000]: training_loss: tensor(0.1661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [225/1000]: training_loss: tensor(0.2252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [226/1000]: training_loss: tensor(0.2103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [227/1000]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [228/1000]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [229/1000]: training_loss: tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [230/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [231/1000]: training_loss: tensor(0.1313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [232/1000]: training_loss: tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [233/1000]: training_loss: tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [234/1000]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [235/1000]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [236/1000]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [237/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [238/1000]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [239/1000]: training_loss: tensor(0.2109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [240/1000]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [241/1000]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [242/1000]: training_loss: tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [243/1000]: training_loss: tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [244/1000]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [245/1000]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [246/1000]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [247/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [248/1000]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [249/1000]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [250/1000]: training_loss: tensor(0.2874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [251/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [252/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [253/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [254/1000]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [255/1000]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [256/1000]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [257/1000]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [258/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [259/1000]: training_loss: tensor(0.0313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [260/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [261/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [262/1000]: training_loss: tensor(0.0495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [263/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [264/1000]: training_loss: tensor(0.2818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [265/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [266/1000]: training_loss: tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [267/1000]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [268/1000]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [269/1000]: training_loss: tensor(0.1875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [270/1000]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [271/1000]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [272/1000]: training_loss: tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [273/1000]: training_loss: tensor(0.1290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [274/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [275/1000]: training_loss: tensor(0.1098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [276/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [277/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [278/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [279/1000]: training_loss: tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [280/1000]: training_loss: tensor(0.2034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [281/1000]: training_loss: tensor(0.0637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [282/1000]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [283/1000]: training_loss: tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [284/1000]: training_loss: tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [285/1000]: training_loss: tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [286/1000]: training_loss: tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [287/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [288/1000]: training_loss: tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [289/1000]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [290/1000]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [291/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [292/1000]: training_loss: tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [293/1000]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [294/1000]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [295/1000]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [296/1000]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [297/1000]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [298/1000]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [299/1000]: training_loss: tensor(0.1897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [300/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [301/1000]: training_loss: tensor(0.3202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [302/1000]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [303/1000]: training_loss: tensor(0.0541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [304/1000]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [305/1000]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [306/1000]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [307/1000]: training_loss: tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [308/1000]: training_loss: tensor(0.2783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [309/1000]: training_loss: tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [310/1000]: training_loss: tensor(0.1095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [311/1000]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [312/1000]: training_loss: tensor(0.1826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [313/1000]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [314/1000]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [315/1000]: training_loss: tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [316/1000]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [317/1000]: training_loss: tensor(0.1314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [318/1000]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [319/1000]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [320/1000]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [321/1000]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [322/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [323/1000]: training_loss: tensor(0.0290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [324/1000]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [325/1000]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [326/1000]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [327/1000]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [328/1000]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [329/1000]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [330/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [331/1000]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [332/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [333/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [334/1000]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [335/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [336/1000]: training_loss: tensor(0.0446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [337/1000]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [338/1000]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [339/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [340/1000]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [341/1000]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [342/1000]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [343/1000]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [344/1000]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [345/1000]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [346/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [347/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [348/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [349/1000]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [350/1000]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [351/1000]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [352/1000]: training_loss: tensor(0.3217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [353/1000]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [354/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [355/1000]: training_loss: tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [356/1000]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [357/1000]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [358/1000]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [359/1000]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [360/1000]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [361/1000]: training_loss: tensor(0.3694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [362/1000]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [363/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [364/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [365/1000]: training_loss: tensor(0.2191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [366/1000]: training_loss: tensor(0.3728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [367/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [368/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [369/1000]: training_loss: tensor(0.1566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [370/1000]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [371/1000]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [372/1000]: training_loss: tensor(0.2838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [373/1000]: training_loss: tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [374/1000]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [375/1000]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [376/1000]: training_loss: tensor(0.2403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [377/1000]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [378/1000]: training_loss: tensor(0.6518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [379/1000]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [380/1000]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [381/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [382/1000]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [383/1000]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [384/1000]: training_loss: tensor(0.3626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [385/1000]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [386/1000]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [387/1000]: training_loss: tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [388/1000]: training_loss: tensor(0.4032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [389/1000]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [390/1000]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [391/1000]: training_loss: tensor(0.1943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [392/1000]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [393/1000]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [394/1000]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [395/1000]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [396/1000]: training_loss: tensor(0.3093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [397/1000]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [398/1000]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [399/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [400/1000]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [401/1000]: training_loss: tensor(0.1670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [402/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [403/1000]: training_loss: tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [404/1000]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [405/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [406/1000]: training_loss: tensor(0.1180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [407/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [408/1000]: training_loss: tensor(0.0389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [409/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [410/1000]: training_loss: tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [411/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [412/1000]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [413/1000]: training_loss: tensor(0.2401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [414/1000]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [415/1000]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [416/1000]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [417/1000]: training_loss: tensor(0.1649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [418/1000]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [419/1000]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [420/1000]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [421/1000]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [422/1000]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [423/1000]: training_loss: tensor(0.3113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [424/1000]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [425/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [426/1000]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [427/1000]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [428/1000]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [429/1000]: training_loss: tensor(0.1616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [430/1000]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [431/1000]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [432/1000]: training_loss: tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [433/1000]: training_loss: tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [434/1000]: training_loss: tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [435/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [436/1000]: training_loss: tensor(0.4293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [437/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [438/1000]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [439/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [440/1000]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [441/1000]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [442/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [443/1000]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [444/1000]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [445/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [446/1000]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [447/1000]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [448/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [449/1000]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [450/1000]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [451/1000]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [452/1000]: training_loss: tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [453/1000]: training_loss: tensor(0.2681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [454/1000]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [455/1000]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [456/1000]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [457/1000]: training_loss: tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [458/1000]: training_loss: tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [459/1000]: training_loss: tensor(0.1838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [460/1000]: training_loss: tensor(0.3220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [461/1000]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [462/1000]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [463/1000]: training_loss: tensor(0.3652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [464/1000]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [465/1000]: training_loss: tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [466/1000]: training_loss: tensor(0.2179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [467/1000]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [468/1000]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [469/1000]: training_loss: tensor(0.2109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [470/1000]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [471/1000]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [472/1000]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [473/1000]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [474/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [475/1000]: training_loss: tensor(0.0413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [476/1000]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [477/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [478/1000]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [479/1000]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [480/1000]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [481/1000]: training_loss: tensor(0.9669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [482/1000]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [483/1000]: training_loss: tensor(1.6946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [484/1000]: training_loss: tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [485/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [486/1000]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [487/1000]: training_loss: tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [488/1000]: training_loss: tensor(0.6859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [489/1000]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [490/1000]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [491/1000]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [492/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [493/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [494/1000]: training_loss: tensor(0.5490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [495/1000]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [496/1000]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [497/1000]: training_loss: tensor(0.3421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [498/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [499/1000]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [500/1000]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [501/1000]: training_loss: tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [502/1000]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [503/1000]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [504/1000]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [505/1000]: training_loss: tensor(0.2065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [506/1000]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [507/1000]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [508/1000]: training_loss: tensor(0.2755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [509/1000]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [510/1000]: training_loss: tensor(0.1779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [511/1000]: training_loss: tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [512/1000]: training_loss: tensor(0.3644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [513/1000]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [514/1000]: training_loss: tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [515/1000]: training_loss: tensor(0.4674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [516/1000]: training_loss: tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [517/1000]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [518/1000]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [519/1000]: training_loss: tensor(0.2427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [520/1000]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [521/1000]: training_loss: tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [522/1000]: training_loss: tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [523/1000]: training_loss: tensor(0.0427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [524/1000]: training_loss: tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [525/1000]: training_loss: tensor(0.0573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [526/1000]: training_loss: tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [527/1000]: training_loss: tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [528/1000]: training_loss: tensor(0.3052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [529/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [530/1000]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [531/1000]: training_loss: tensor(0.4697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [532/1000]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [533/1000]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [534/1000]: training_loss: tensor(0.3144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [535/1000]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [536/1000]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [537/1000]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [538/1000]: training_loss: tensor(0.1233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [539/1000]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [540/1000]: training_loss: tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [541/1000]: training_loss: tensor(0.2395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [542/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [543/1000]: training_loss: tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [544/1000]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [545/1000]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [546/1000]: training_loss: tensor(0.3690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [547/1000]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [548/1000]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [549/1000]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [550/1000]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [551/1000]: training_loss: tensor(0.3392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [552/1000]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [553/1000]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [554/1000]: training_loss: tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [555/1000]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [556/1000]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [557/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [558/1000]: training_loss: tensor(0.1217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [559/1000]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [560/1000]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [561/1000]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [562/1000]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [563/1000]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [564/1000]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [565/1000]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [566/1000]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [567/1000]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [568/1000]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [569/1000]: training_loss: tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [570/1000]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [571/1000]: training_loss: tensor(0.1257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [572/1000]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [573/1000]: training_loss: tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [574/1000]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [575/1000]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [576/1000]: training_loss: tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [577/1000]: training_loss: tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [578/1000]: training_loss: tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [579/1000]: training_loss: tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [580/1000]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [581/1000]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [582/1000]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [583/1000]: training_loss: tensor(0.1941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [584/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [585/1000]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [586/1000]: training_loss: tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [587/1000]: training_loss: tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [588/1000]: training_loss: tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [589/1000]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [590/1000]: training_loss: tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [591/1000]: training_loss: tensor(1.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [592/1000]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [593/1000]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [594/1000]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [595/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [596/1000]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [597/1000]: training_loss: tensor(0.8906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [598/1000]: training_loss: tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [599/1000]: training_loss: tensor(0.3338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [600/1000]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [601/1000]: training_loss: tensor(0.1873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [602/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [603/1000]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [604/1000]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [605/1000]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [606/1000]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [607/1000]: training_loss: tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [608/1000]: training_loss: tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [609/1000]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [610/1000]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [611/1000]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [612/1000]: training_loss: tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [613/1000]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [614/1000]: training_loss: tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [615/1000]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [616/1000]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [617/1000]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [618/1000]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [619/1000]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [620/1000]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [621/1000]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [622/1000]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [623/1000]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [624/1000]: training_loss: tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [625/1000]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [626/1000]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [627/1000]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [628/1000]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [629/1000]: training_loss: tensor(0.2382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [630/1000]: training_loss: tensor(0.8603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [631/1000]: training_loss: tensor(0.1356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [632/1000]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [633/1000]: training_loss: tensor(0.2821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [634/1000]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [635/1000]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [636/1000]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [637/1000]: training_loss: tensor(0.1917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [638/1000]: training_loss: tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [639/1000]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [640/1000]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [641/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [642/1000]: training_loss: tensor(0.1706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [643/1000]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [644/1000]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [645/1000]: training_loss: tensor(0.3899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [646/1000]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [647/1000]: training_loss: tensor(0.2449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [648/1000]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [649/1000]: training_loss: tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [650/1000]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [651/1000]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [652/1000]: training_loss: tensor(0.3250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [653/1000]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [654/1000]: training_loss: tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [655/1000]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [656/1000]: training_loss: tensor(0.2093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [657/1000]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [658/1000]: training_loss: tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [659/1000]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [660/1000]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [661/1000]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [662/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [663/1000]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [664/1000]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [665/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [666/1000]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [667/1000]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [668/1000]: training_loss: tensor(0.1470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [669/1000]: training_loss: tensor(0.1349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [670/1000]: training_loss: tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [671/1000]: training_loss: tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [672/1000]: training_loss: tensor(0.1072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [673/1000]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [674/1000]: training_loss: tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [675/1000]: training_loss: tensor(0.1285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [676/1000]: training_loss: tensor(0.2607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [677/1000]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [678/1000]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [679/1000]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [680/1000]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [681/1000]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [682/1000]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [683/1000]: training_loss: tensor(0.4740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [684/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [685/1000]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [686/1000]: training_loss: tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [687/1000]: training_loss: tensor(0.1495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [688/1000]: training_loss: tensor(0.1110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [689/1000]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [690/1000]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [691/1000]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [692/1000]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [693/1000]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [694/1000]: training_loss: tensor(0.0480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [695/1000]: training_loss: tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [696/1000]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [697/1000]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [698/1000]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [699/1000]: training_loss: tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [700/1000]: training_loss: tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [701/1000]: training_loss: tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [702/1000]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [703/1000]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [704/1000]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [705/1000]: training_loss: tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [706/1000]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [707/1000]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [708/1000]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [709/1000]: training_loss: tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [710/1000]: training_loss: tensor(0.5328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [711/1000]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [712/1000]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [713/1000]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [714/1000]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [715/1000]: training_loss: tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [716/1000]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [717/1000]: training_loss: tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [718/1000]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [719/1000]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [720/1000]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [721/1000]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [722/1000]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [723/1000]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [724/1000]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [725/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [726/1000]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [727/1000]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [728/1000]: training_loss: tensor(0.1097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [729/1000]: training_loss: tensor(0.1729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [730/1000]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [731/1000]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [732/1000]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [733/1000]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [734/1000]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [735/1000]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [736/1000]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [737/1000]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [738/1000]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [739/1000]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [740/1000]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [741/1000]: training_loss: tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [742/1000]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [743/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [744/1000]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [745/1000]: training_loss: tensor(0.0429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [746/1000]: training_loss: tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [747/1000]: training_loss: tensor(0.2583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [748/1000]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [749/1000]: training_loss: tensor(0.1376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [750/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [751/1000]: training_loss: tensor(0.4621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [752/1000]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [753/1000]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [754/1000]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [755/1000]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [756/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [757/1000]: training_loss: tensor(0.4603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [758/1000]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [759/1000]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [760/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [761/1000]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [762/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [763/1000]: training_loss: tensor(0.1494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [764/1000]: training_loss: tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [765/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [766/1000]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [767/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [768/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [769/1000]: training_loss: tensor(0.1821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [770/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [771/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [772/1000]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [773/1000]: training_loss: tensor(0.1066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [774/1000]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [775/1000]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [776/1000]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [777/1000]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [778/1000]: training_loss: tensor(0.1958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [779/1000]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [780/1000]: training_loss: tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [781/1000]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [782/1000]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [783/1000]: training_loss: tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [784/1000]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [785/1000]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [786/1000]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [787/1000]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [788/1000]: training_loss: tensor(0.7923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [789/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [790/1000]: training_loss: tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [791/1000]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [792/1000]: training_loss: tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [793/1000]: training_loss: tensor(0.1535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [794/1000]: training_loss: tensor(0.5521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [795/1000]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [796/1000]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [797/1000]: training_loss: tensor(0.8681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [798/1000]: training_loss: tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [799/1000]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [800/1000]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [801/1000]: training_loss: tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [802/1000]: training_loss: tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [803/1000]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [804/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [805/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [806/1000]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [807/1000]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [808/1000]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [809/1000]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [810/1000]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [811/1000]: training_loss: tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [812/1000]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [813/1000]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [814/1000]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [815/1000]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [816/1000]: training_loss: tensor(0.1903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [817/1000]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [818/1000]: training_loss: tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [819/1000]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [820/1000]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [821/1000]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [822/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [823/1000]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [824/1000]: training_loss: tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [825/1000]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [826/1000]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [827/1000]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [828/1000]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [829/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [830/1000]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [831/1000]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [832/1000]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [833/1000]: training_loss: tensor(0.2943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [834/1000]: training_loss: tensor(0.0429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [835/1000]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [836/1000]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [837/1000]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [838/1000]: training_loss: tensor(0.1321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [839/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [840/1000]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [841/1000]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [842/1000]: training_loss: tensor(0.1120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [843/1000]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [844/1000]: training_loss: tensor(0.2742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [845/1000]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [846/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [847/1000]: training_loss: tensor(0.1318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [848/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [849/1000]: training_loss: tensor(0.4268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [850/1000]: training_loss: tensor(0.1995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [851/1000]: training_loss: tensor(0.6426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [852/1000]: training_loss: tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [853/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [854/1000]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [855/1000]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [856/1000]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [857/1000]: training_loss: tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [858/1000]: training_loss: tensor(0.2941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [859/1000]: training_loss: tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [860/1000]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [861/1000]: training_loss: tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [862/1000]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [863/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [864/1000]: training_loss: tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [865/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [866/1000]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [867/1000]: training_loss: tensor(0.1664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [868/1000]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [869/1000]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [870/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [871/1000]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [872/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [873/1000]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [874/1000]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [875/1000]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [876/1000]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [877/1000]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [878/1000]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [879/1000]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [880/1000]: training_loss: tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [881/1000]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [882/1000]: training_loss: tensor(0.1370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [883/1000]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [884/1000]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [885/1000]: training_loss: tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [886/1000]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [887/1000]: training_loss: tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [888/1000]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [889/1000]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [890/1000]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [891/1000]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [892/1000]: training_loss: tensor(0.3113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [893/1000]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [894/1000]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [895/1000]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [896/1000]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [897/1000]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [898/1000]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [899/1000]: training_loss: tensor(0.3520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [900/1000]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [901/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [902/1000]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [903/1000]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [904/1000]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [905/1000]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [906/1000]: training_loss: tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [907/1000]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [908/1000]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [909/1000]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [910/1000]: training_loss: tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [911/1000]: training_loss: tensor(0.1585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [912/1000]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [913/1000]: training_loss: tensor(0.1449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [914/1000]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [915/1000]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [916/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [917/1000]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [918/1000]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [919/1000]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [920/1000]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [921/1000]: training_loss: tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [922/1000]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [923/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [924/1000]: training_loss: tensor(0.1780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [925/1000]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [926/1000]: training_loss: tensor(0.4493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [927/1000]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [928/1000]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [929/1000]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [930/1000]: training_loss: tensor(0.2176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [931/1000]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [932/1000]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [933/1000]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [934/1000]: training_loss: tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [935/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [936/1000]: training_loss: tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [937/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [938/1000]: training_loss: tensor(0.1361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [939/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [940/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [941/1000]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [942/1000]: training_loss: tensor(0.4377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [943/1000]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [944/1000]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [945/1000]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [946/1000]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [947/1000]: training_loss: tensor(0.7255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [948/1000]: training_loss: tensor(0.0502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [949/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [950/1000]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [951/1000]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [952/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [953/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [954/1000]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [955/1000]: training_loss: tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [956/1000]: training_loss: tensor(0.0545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [957/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [958/1000]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [959/1000]: training_loss: tensor(0.0290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [960/1000]: training_loss: tensor(0.5635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [961/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [962/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [963/1000]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [964/1000]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [965/1000]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [966/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [967/1000]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [968/1000]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [969/1000]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [970/1000]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [971/1000]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [972/1000]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [973/1000]: training_loss: tensor(0.1849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [974/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [975/1000]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [976/1000]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [977/1000]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [978/1000]: training_loss: tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [979/1000]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [980/1000]: training_loss: tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [981/1000]: training_loss: tensor(0.2286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [982/1000]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [983/1000]: training_loss: tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [984/1000]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [985/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [986/1000]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [987/1000]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [988/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [989/1000]: training_loss: tensor(0.0573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [990/1000]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [991/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [992/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [993/1000]: training_loss: tensor(0.1574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [994/1000]: training_loss: tensor(0.1445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [995/1000]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [996/1000]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [997/1000]: training_loss: tensor(0.2326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [998/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [999/1000]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1000/1000]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [4/5], Step [7000/10000], Train Loss: 0.0695, Valid Loss: 0.3896\n",
      "batch_no [1001/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1002/1000]: training_loss: tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1003/1000]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1004/1000]: training_loss: tensor(0.1579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1005/1000]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/1000]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/1000]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/1000]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/1000]: training_loss: tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/1000]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/1000]: training_loss: tensor(0.0572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/1000]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/1000]: training_loss: tensor(0.1252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/1000]: training_loss: tensor(0.1019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/1000]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/1000]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/1000]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1019/1000]: training_loss: tensor(0.4813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/1000]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/1000]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/1000]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1023/1000]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/1000]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/1000]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/1000]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/1000]: training_loss: tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/1000]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/1000]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/1000]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/1000]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/1000]: training_loss: tensor(0.8474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/1000]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/1000]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/1000]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/1000]: training_loss: tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1040/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/1000]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/1000]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1043/1000]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/1000]: training_loss: tensor(0.1927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/1000]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/1000]: training_loss: tensor(0.3847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/1000]: training_loss: tensor(0.1271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/1000]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/1000]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/1000]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/1000]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/1000]: training_loss: tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/1000]: training_loss: tensor(0.1651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/1000]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/1000]: training_loss: tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/1000]: training_loss: tensor(0.1160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/1000]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/1000]: training_loss: tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/1000]: training_loss: tensor(0.1123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/1000]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/1000]: training_loss: tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/1000]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/1000]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/1000]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/1000]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/1000]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/1000]: training_loss: tensor(0.4205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/1000]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/1000]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/1000]: training_loss: tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/1000]: training_loss: tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/1000]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/1000]: training_loss: tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/1000]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/1000]: training_loss: tensor(0.1586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/1000]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/1000]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/1000]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/1000]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/1000]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/1000]: training_loss: tensor(0.1922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/1000]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/1000]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/1000]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/1000]: training_loss: tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/1000]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/1000]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/1000]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/1000]: training_loss: tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/1000]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/1000]: training_loss: tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/1000]: training_loss: tensor(0.3393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/1000]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/1000]: training_loss: tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/1000]: training_loss: tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/1000]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1100/1000]: training_loss: tensor(0.3875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/1000]: training_loss: tensor(0.0507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/1000]: training_loss: tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/1000]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/1000]: training_loss: tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/1000]: training_loss: tensor(0.0502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/1000]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/1000]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/1000]: training_loss: tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/1000]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/1000]: training_loss: tensor(0.3854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/1000]: training_loss: tensor(0.2831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/1000]: training_loss: tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/1000]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/1000]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/1000]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/1000]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/1000]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/1000]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/1000]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/1000]: training_loss: tensor(0.0361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/1000]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/1000]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/1000]: training_loss: tensor(0.1183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/1000]: training_loss: tensor(0.2516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/1000]: training_loss: tensor(0.3495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1129/1000]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/1000]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/1000]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/1000]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/1000]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/1000]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/1000]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/1000]: training_loss: tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/1000]: training_loss: tensor(0.3537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/1000]: training_loss: tensor(0.3988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/1000]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/1000]: training_loss: tensor(0.5201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/1000]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/1000]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/1000]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/1000]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/1000]: training_loss: tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/1000]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/1000]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/1000]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/1000]: training_loss: tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/1000]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/1000]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/1000]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/1000]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/1000]: training_loss: tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/1000]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/1000]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/1000]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/1000]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/1000]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/1000]: training_loss: tensor(0.1105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/1000]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/1000]: training_loss: tensor(0.5297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/1000]: training_loss: tensor(0.2911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/1000]: training_loss: tensor(0.4043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/1000]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/1000]: training_loss: tensor(0.1270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/1000]: training_loss: tensor(0.3211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/1000]: training_loss: tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/1000]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/1000]: training_loss: tensor(0.0459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/1000]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/1000]: training_loss: tensor(0.0579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/1000]: training_loss: tensor(0.1385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/1000]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/1000]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/1000]: training_loss: tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/1000]: training_loss: tensor(0.6805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/1000]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/1000]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/1000]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/1000]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/1000]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/1000]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/1000]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/1000]: training_loss: tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/1000]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/1000]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/1000]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/1000]: training_loss: tensor(0.1790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/1000]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/1000]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/1000]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/1000]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/1000]: training_loss: tensor(0.1671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/1000]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/1000]: training_loss: tensor(0.8873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/1000]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/1000]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/1000]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/1000]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/1000]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/1000]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1213/1000]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/1000]: training_loss: tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1215/1000]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/1000]: training_loss: tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/1000]: training_loss: tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/1000]: training_loss: tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/1000]: training_loss: tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/1000]: training_loss: tensor(0.3202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/1000]: training_loss: tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/1000]: training_loss: tensor(0.1287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/1000]: training_loss: tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/1000]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/1000]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/1000]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/1000]: training_loss: tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/1000]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/1000]: training_loss: tensor(0.5477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/1000]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/1000]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/1000]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/1000]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/1000]: training_loss: tensor(0.3981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/1000]: training_loss: tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/1000]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/1000]: training_loss: tensor(0.1925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/1000]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/1000]: training_loss: tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/1000]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/1000]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/1000]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/1000]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/1000]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/1000]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/1000]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/1000]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/1000]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/1000]: training_loss: tensor(0.1653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/1000]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/1000]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/1000]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/1000]: training_loss: tensor(0.3899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/1000]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/1000]: training_loss: tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/1000]: training_loss: tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/1000]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/1000]: training_loss: tensor(0.0517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/1000]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/1000]: training_loss: tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/1000]: training_loss: tensor(0.5287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/1000]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/1000]: training_loss: tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/1000]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/1000]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/1000]: training_loss: tensor(0.1659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/1000]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/1000]: training_loss: tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/1000]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/1000]: training_loss: tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/1000]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/1000]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/1000]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/1000]: training_loss: tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/1000]: training_loss: tensor(0.6218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/1000]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/1000]: training_loss: tensor(0.1294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/1000]: training_loss: tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/1000]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/1000]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/1000]: training_loss: tensor(0.1500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/1000]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/1000]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/1000]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/1000]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/1000]: training_loss: tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/1000]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1299/1000]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/1000]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/1000]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/1000]: training_loss: tensor(0.1242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/1000]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/1000]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/1000]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/1000]: training_loss: tensor(0.2928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/1000]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/1000]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/1000]: training_loss: tensor(0.2526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/1000]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/1000]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/1000]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/1000]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/1000]: training_loss: tensor(0.4005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/1000]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/1000]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/1000]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/1000]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/1000]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/1000]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/1000]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/1000]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/1000]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/1000]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/1000]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/1000]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/1000]: training_loss: tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/1000]: training_loss: tensor(0.0423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/1000]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/1000]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/1000]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/1000]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/1000]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/1000]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/1000]: training_loss: tensor(0.1219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/1000]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/1000]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/1000]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/1000]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/1000]: training_loss: tensor(0.2801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/1000]: training_loss: tensor(0.8656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/1000]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/1000]: training_loss: tensor(0.3506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/1000]: training_loss: tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/1000]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/1000]: training_loss: tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/1000]: training_loss: tensor(0.2054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/1000]: training_loss: tensor(0.2237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/1000]: training_loss: tensor(0.4572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1361/1000]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/1000]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1363/1000]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/1000]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/1000]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/1000]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/1000]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/1000]: training_loss: tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/1000]: training_loss: tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/1000]: training_loss: tensor(0.1683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/1000]: training_loss: tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/1000]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/1000]: training_loss: tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/1000]: training_loss: tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/1000]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/1000]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/1000]: training_loss: tensor(0.3559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/1000]: training_loss: tensor(0.1153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/1000]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1385/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1386/1000]: training_loss: tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1387/1000]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1388/1000]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1389/1000]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1390/1000]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1391/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1392/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1393/1000]: training_loss: tensor(0.0445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1394/1000]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1395/1000]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1396/1000]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1397/1000]: training_loss: tensor(0.4513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1398/1000]: training_loss: tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1399/1000]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1400/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1401/1000]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1402/1000]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1403/1000]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1404/1000]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1405/1000]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1406/1000]: training_loss: tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1407/1000]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1408/1000]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1409/1000]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1410/1000]: training_loss: tensor(0.2427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1411/1000]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1412/1000]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1413/1000]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1414/1000]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1415/1000]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1416/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1417/1000]: training_loss: tensor(0.0442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1418/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1419/1000]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1420/1000]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1421/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1422/1000]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1423/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1424/1000]: training_loss: tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1425/1000]: training_loss: tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1426/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1427/1000]: training_loss: tensor(0.1369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1428/1000]: training_loss: tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1429/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1430/1000]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1431/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1432/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1433/1000]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1434/1000]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1435/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1436/1000]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1437/1000]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1438/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1439/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1440/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1441/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1442/1000]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1443/1000]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1444/1000]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1445/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1446/1000]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1447/1000]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1448/1000]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1449/1000]: training_loss: tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1450/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1451/1000]: training_loss: tensor(0.1536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1452/1000]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1453/1000]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1454/1000]: training_loss: tensor(0.6699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1455/1000]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1456/1000]: training_loss: tensor(0.1776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1457/1000]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1458/1000]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1459/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1460/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1461/1000]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1462/1000]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1463/1000]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1464/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1465/1000]: training_loss: tensor(0.4441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1466/1000]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1467/1000]: training_loss: tensor(0.2627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1468/1000]: training_loss: tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1469/1000]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1470/1000]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1471/1000]: training_loss: tensor(0.7980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1472/1000]: training_loss: tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1473/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1474/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1475/1000]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1476/1000]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1477/1000]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1478/1000]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1479/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1480/1000]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1481/1000]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1482/1000]: training_loss: tensor(0.1301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1483/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1484/1000]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1485/1000]: training_loss: tensor(0.2921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1486/1000]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1487/1000]: training_loss: tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1488/1000]: training_loss: tensor(0.6379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1489/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1490/1000]: training_loss: tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1491/1000]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1492/1000]: training_loss: tensor(0.6809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1493/1000]: training_loss: tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1494/1000]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1495/1000]: training_loss: tensor(0.4428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1496/1000]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1497/1000]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1498/1000]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1499/1000]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1500/1000]: training_loss: tensor(0.0560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1501/1000]: training_loss: tensor(0.1519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1502/1000]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1503/1000]: training_loss: tensor(0.4392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1504/1000]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1505/1000]: training_loss: tensor(0.0517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1506/1000]: training_loss: tensor(0.1187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1507/1000]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1508/1000]: training_loss: tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1509/1000]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1510/1000]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1511/1000]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1512/1000]: training_loss: tensor(0.7950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1513/1000]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1514/1000]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1515/1000]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1516/1000]: training_loss: tensor(0.1757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1517/1000]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1518/1000]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1519/1000]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1520/1000]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1521/1000]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1522/1000]: training_loss: tensor(0.1881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1523/1000]: training_loss: tensor(0.3227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1524/1000]: training_loss: tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1525/1000]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1526/1000]: training_loss: tensor(0.2425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1527/1000]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1528/1000]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1529/1000]: training_loss: tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1530/1000]: training_loss: tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1531/1000]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1532/1000]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1533/1000]: training_loss: tensor(0.1492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1534/1000]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1535/1000]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1536/1000]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1537/1000]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1538/1000]: training_loss: tensor(0.1582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1539/1000]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1540/1000]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1541/1000]: training_loss: tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1542/1000]: training_loss: tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1543/1000]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1544/1000]: training_loss: tensor(0.3157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1545/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1546/1000]: training_loss: tensor(0.1910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1547/1000]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1548/1000]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1549/1000]: training_loss: tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1550/1000]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1551/1000]: training_loss: tensor(0.2936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1552/1000]: training_loss: tensor(0.4792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1553/1000]: training_loss: tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1554/1000]: training_loss: tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1555/1000]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1556/1000]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1557/1000]: training_loss: tensor(0.0502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1558/1000]: training_loss: tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1559/1000]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1560/1000]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1561/1000]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1562/1000]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1563/1000]: training_loss: tensor(0.3426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1564/1000]: training_loss: tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1565/1000]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1566/1000]: training_loss: tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1567/1000]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1568/1000]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1569/1000]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1570/1000]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1571/1000]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1572/1000]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1573/1000]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1574/1000]: training_loss: tensor(0.2723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1575/1000]: training_loss: tensor(0.1354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1576/1000]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1577/1000]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1578/1000]: training_loss: tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1579/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1580/1000]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1581/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1582/1000]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1583/1000]: training_loss: tensor(0.1292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1584/1000]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1585/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1586/1000]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1587/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1588/1000]: training_loss: tensor(0.1653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1589/1000]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1590/1000]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1591/1000]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1592/1000]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1593/1000]: training_loss: tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1594/1000]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1595/1000]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1596/1000]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1597/1000]: training_loss: tensor(0.1726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1598/1000]: training_loss: tensor(0.1218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1599/1000]: training_loss: tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1600/1000]: training_loss: tensor(0.8311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1601/1000]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1602/1000]: training_loss: tensor(0.2666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1603/1000]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1604/1000]: training_loss: tensor(0.1182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1605/1000]: training_loss: tensor(0.0456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1606/1000]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1607/1000]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1608/1000]: training_loss: tensor(0.1721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1609/1000]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1610/1000]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1611/1000]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1612/1000]: training_loss: tensor(0.1665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1613/1000]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1614/1000]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1615/1000]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1616/1000]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1617/1000]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1618/1000]: training_loss: tensor(0.3151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1619/1000]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1620/1000]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1621/1000]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1622/1000]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1623/1000]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1624/1000]: training_loss: tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1625/1000]: training_loss: tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1626/1000]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1627/1000]: training_loss: tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1628/1000]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1629/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1630/1000]: training_loss: tensor(0.1625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1631/1000]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1632/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1633/1000]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1634/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1635/1000]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1636/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1637/1000]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1638/1000]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1639/1000]: training_loss: tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1640/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1641/1000]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1642/1000]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1643/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1644/1000]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1645/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1646/1000]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1647/1000]: training_loss: tensor(0.0586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1648/1000]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1649/1000]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1650/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1651/1000]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1652/1000]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1653/1000]: training_loss: tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1654/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1655/1000]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1656/1000]: training_loss: tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1657/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1658/1000]: training_loss: tensor(0.1195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1659/1000]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1660/1000]: training_loss: tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1661/1000]: training_loss: tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1662/1000]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1663/1000]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1664/1000]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1665/1000]: training_loss: tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1666/1000]: training_loss: tensor(0.1367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1667/1000]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1668/1000]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1669/1000]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1670/1000]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1671/1000]: training_loss: tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1672/1000]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1673/1000]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1674/1000]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1675/1000]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1676/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1677/1000]: training_loss: tensor(0.1358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1678/1000]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1679/1000]: training_loss: tensor(0.4137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1680/1000]: training_loss: tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1681/1000]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1682/1000]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1683/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1684/1000]: training_loss: tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1685/1000]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1686/1000]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1687/1000]: training_loss: tensor(0.1530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1688/1000]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1689/1000]: training_loss: tensor(0.1514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1690/1000]: training_loss: tensor(0.2590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1691/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1692/1000]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1693/1000]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1694/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1695/1000]: training_loss: tensor(0.0436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1696/1000]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1697/1000]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1698/1000]: training_loss: tensor(0.0517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1699/1000]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1700/1000]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1701/1000]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1702/1000]: training_loss: tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1703/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1704/1000]: training_loss: tensor(0.3015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1705/1000]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1706/1000]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1707/1000]: training_loss: tensor(0.0468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1708/1000]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1709/1000]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1710/1000]: training_loss: tensor(0.2467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1711/1000]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1712/1000]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1713/1000]: training_loss: tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1714/1000]: training_loss: tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1715/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1716/1000]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1717/1000]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1718/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1719/1000]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1720/1000]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1721/1000]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1722/1000]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1723/1000]: training_loss: tensor(0.1128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1724/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1725/1000]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1726/1000]: training_loss: tensor(0.2212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1727/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1728/1000]: training_loss: tensor(0.3690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1729/1000]: training_loss: tensor(0.2753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1730/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1731/1000]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1732/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1733/1000]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1734/1000]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1735/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1736/1000]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1737/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1738/1000]: training_loss: tensor(0.5311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1739/1000]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1740/1000]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1741/1000]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1742/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1743/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1744/1000]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1745/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1746/1000]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1747/1000]: training_loss: tensor(0.3768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1748/1000]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1749/1000]: training_loss: tensor(0.5459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1750/1000]: training_loss: tensor(0.2486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1751/1000]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1752/1000]: training_loss: tensor(0.0290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1753/1000]: training_loss: tensor(0.0547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1754/1000]: training_loss: tensor(0.2865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1755/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1756/1000]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1757/1000]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1758/1000]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1759/1000]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1760/1000]: training_loss: tensor(0.3182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1761/1000]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1762/1000]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1763/1000]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1764/1000]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1765/1000]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1766/1000]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1767/1000]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1768/1000]: training_loss: tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1769/1000]: training_loss: tensor(0.0462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1770/1000]: training_loss: tensor(0.2676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1771/1000]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1772/1000]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1773/1000]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1774/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1775/1000]: training_loss: tensor(0.2128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1776/1000]: training_loss: tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1777/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1778/1000]: training_loss: tensor(0.2591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1779/1000]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1780/1000]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1781/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1782/1000]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1783/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1784/1000]: training_loss: tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1785/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1786/1000]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1787/1000]: training_loss: tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1788/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1789/1000]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1790/1000]: training_loss: tensor(0.2729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1791/1000]: training_loss: tensor(0.7511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1792/1000]: training_loss: tensor(0.5792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1793/1000]: training_loss: tensor(0.3006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1794/1000]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1795/1000]: training_loss: tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1796/1000]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1797/1000]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1798/1000]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1799/1000]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1800/1000]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1801/1000]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1802/1000]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1803/1000]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1804/1000]: training_loss: tensor(0.7196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1805/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1806/1000]: training_loss: tensor(0.1608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1807/1000]: training_loss: tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1808/1000]: training_loss: tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1809/1000]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1810/1000]: training_loss: tensor(0.6541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1811/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1812/1000]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1813/1000]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1814/1000]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1815/1000]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1816/1000]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1817/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1818/1000]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1819/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1820/1000]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1821/1000]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1822/1000]: training_loss: tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1823/1000]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1824/1000]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1825/1000]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1826/1000]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1827/1000]: training_loss: tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1828/1000]: training_loss: tensor(0.3492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1829/1000]: training_loss: tensor(0.2151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1830/1000]: training_loss: tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1831/1000]: training_loss: tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1832/1000]: training_loss: tensor(0.0468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1833/1000]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1834/1000]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1835/1000]: training_loss: tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1836/1000]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1837/1000]: training_loss: tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1838/1000]: training_loss: tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1839/1000]: training_loss: tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1840/1000]: training_loss: tensor(0.0354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1841/1000]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1842/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1843/1000]: training_loss: tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1844/1000]: training_loss: tensor(0.3135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1845/1000]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1846/1000]: training_loss: tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1847/1000]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1848/1000]: training_loss: tensor(0.2706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1849/1000]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1850/1000]: training_loss: tensor(0.2392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1851/1000]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1852/1000]: training_loss: tensor(0.3371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1853/1000]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1854/1000]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1855/1000]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1856/1000]: training_loss: tensor(0.4303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1857/1000]: training_loss: tensor(0.4377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1858/1000]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1859/1000]: training_loss: tensor(0.0429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1860/1000]: training_loss: tensor(0.1978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1861/1000]: training_loss: tensor(0.0493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1862/1000]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1863/1000]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1864/1000]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1865/1000]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1866/1000]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1867/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1868/1000]: training_loss: tensor(0.2769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1869/1000]: training_loss: tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1870/1000]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1871/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1872/1000]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1873/1000]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1874/1000]: training_loss: tensor(0.2579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1875/1000]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1876/1000]: training_loss: tensor(0.2097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1877/1000]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1878/1000]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1879/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1880/1000]: training_loss: tensor(0.4982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1881/1000]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1882/1000]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1883/1000]: training_loss: tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1884/1000]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1885/1000]: training_loss: tensor(0.2547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1886/1000]: training_loss: tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1887/1000]: training_loss: tensor(0.2373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1888/1000]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1889/1000]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1890/1000]: training_loss: tensor(0.1183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1891/1000]: training_loss: tensor(0.1828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1892/1000]: training_loss: tensor(0.1688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1893/1000]: training_loss: tensor(0.4815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1894/1000]: training_loss: tensor(0.1294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1895/1000]: training_loss: tensor(0.0442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1896/1000]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1897/1000]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1898/1000]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1899/1000]: training_loss: tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1900/1000]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1901/1000]: training_loss: tensor(0.1069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1902/1000]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1903/1000]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1904/1000]: training_loss: tensor(0.1231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1905/1000]: training_loss: tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1906/1000]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1907/1000]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1908/1000]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1909/1000]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1910/1000]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1911/1000]: training_loss: tensor(0.5426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1912/1000]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1913/1000]: training_loss: tensor(0.0354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1914/1000]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1915/1000]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1916/1000]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1917/1000]: training_loss: tensor(0.2185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1918/1000]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1919/1000]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1920/1000]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1921/1000]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1922/1000]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1923/1000]: training_loss: tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1924/1000]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1925/1000]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1926/1000]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1927/1000]: training_loss: tensor(0.0290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1928/1000]: training_loss: tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1929/1000]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1930/1000]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1931/1000]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1932/1000]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1933/1000]: training_loss: tensor(0.2398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1934/1000]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1935/1000]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1936/1000]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1937/1000]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1938/1000]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1939/1000]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1940/1000]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1941/1000]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1942/1000]: training_loss: tensor(0.1099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1943/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1944/1000]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1945/1000]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1946/1000]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1947/1000]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1948/1000]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1949/1000]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1950/1000]: training_loss: tensor(0.2341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1951/1000]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1952/1000]: training_loss: tensor(0.1545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1953/1000]: training_loss: tensor(0.1616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1954/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1955/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1956/1000]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1957/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1958/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1959/1000]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1960/1000]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1961/1000]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1962/1000]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1963/1000]: training_loss: tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1964/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1965/1000]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1966/1000]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1967/1000]: training_loss: tensor(0.2899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1968/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1969/1000]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1970/1000]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1971/1000]: training_loss: tensor(0.1412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1972/1000]: training_loss: tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1973/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1974/1000]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1975/1000]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1976/1000]: training_loss: tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1977/1000]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1978/1000]: training_loss: tensor(0.2606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1979/1000]: training_loss: tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1980/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1981/1000]: training_loss: tensor(0.4700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1982/1000]: training_loss: tensor(0.4537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1983/1000]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1984/1000]: training_loss: tensor(0.1700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1985/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1986/1000]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1987/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1988/1000]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1989/1000]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1990/1000]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1991/1000]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1992/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1993/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1994/1000]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1995/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1996/1000]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1997/1000]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1998/1000]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1999/1000]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2000/1000]: training_loss: tensor(0.6189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [4/5], Step [8000/10000], Train Loss: 0.0702, Valid Loss: 0.4510\n",
      "batch_no [1/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2/1000]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3/1000]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4/1000]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [5/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [6/1000]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [7/1000]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [8/1000]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [9/1000]: training_loss: tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [10/1000]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [11/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [12/1000]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [13/1000]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [14/1000]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [15/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [16/1000]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [17/1000]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [18/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [19/1000]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [20/1000]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [21/1000]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [22/1000]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [23/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [24/1000]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [25/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [26/1000]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [27/1000]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [28/1000]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [29/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [30/1000]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [31/1000]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [32/1000]: training_loss: tensor(0.5003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [33/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [34/1000]: training_loss: tensor(0.2604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [35/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [36/1000]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [37/1000]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [38/1000]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [39/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [40/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [41/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [42/1000]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [43/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [44/1000]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [45/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [46/1000]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [47/1000]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [48/1000]: training_loss: tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [49/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [50/1000]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [51/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [52/1000]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [53/1000]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [54/1000]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [55/1000]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [56/1000]: training_loss: tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [57/1000]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [58/1000]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [59/1000]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [60/1000]: training_loss: tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [61/1000]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [62/1000]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [63/1000]: training_loss: tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [64/1000]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [65/1000]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [66/1000]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [67/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [68/1000]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [69/1000]: training_loss: tensor(0.2064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [70/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [71/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [72/1000]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [73/1000]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [74/1000]: training_loss: tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [75/1000]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [76/1000]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [77/1000]: training_loss: tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [78/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [79/1000]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [80/1000]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [81/1000]: training_loss: tensor(0.2464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [82/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [83/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [84/1000]: training_loss: tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [85/1000]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [86/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [87/1000]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [88/1000]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [89/1000]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [90/1000]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [91/1000]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [92/1000]: training_loss: tensor(0.3881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [93/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [94/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [95/1000]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [96/1000]: training_loss: tensor(0.1731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [97/1000]: training_loss: tensor(0.3963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [98/1000]: training_loss: tensor(0.1389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [99/1000]: training_loss: tensor(0.1776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [100/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [101/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [102/1000]: training_loss: tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [103/1000]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [104/1000]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [105/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [106/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [107/1000]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [108/1000]: training_loss: tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [109/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [110/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [111/1000]: training_loss: tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [112/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [113/1000]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [114/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [115/1000]: training_loss: tensor(0.2711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [116/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [117/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [118/1000]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [119/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [120/1000]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [121/1000]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [122/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [123/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [124/1000]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [125/1000]: training_loss: tensor(0.2398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [126/1000]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [127/1000]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [128/1000]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [129/1000]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [130/1000]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [131/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [132/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [133/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [134/1000]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [135/1000]: training_loss: tensor(0.2864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [136/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [137/1000]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [138/1000]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [139/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [140/1000]: training_loss: tensor(0.2829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [141/1000]: training_loss: tensor(0.2597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [142/1000]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [143/1000]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [144/1000]: training_loss: tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [145/1000]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [146/1000]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [147/1000]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [148/1000]: training_loss: tensor(0.2654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [149/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [150/1000]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [151/1000]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [152/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [153/1000]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [154/1000]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [155/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [156/1000]: training_loss: tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [157/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [158/1000]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [159/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [160/1000]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [161/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [162/1000]: training_loss: tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [163/1000]: training_loss: tensor(0.8960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [164/1000]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [165/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [166/1000]: training_loss: tensor(0.2198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [167/1000]: training_loss: tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [168/1000]: training_loss: tensor(0.1226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [169/1000]: training_loss: tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [170/1000]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [171/1000]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [172/1000]: training_loss: tensor(0.1224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [173/1000]: training_loss: tensor(0.1157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [174/1000]: training_loss: tensor(0.4946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [175/1000]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [176/1000]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [177/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [178/1000]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [179/1000]: training_loss: tensor(0.0615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [180/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [181/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [182/1000]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [183/1000]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [184/1000]: training_loss: tensor(0.1591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [185/1000]: training_loss: tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [186/1000]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [187/1000]: training_loss: tensor(0.0666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [188/1000]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [189/1000]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [190/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [191/1000]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [192/1000]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [193/1000]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [194/1000]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [195/1000]: training_loss: tensor(0.0645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [196/1000]: training_loss: tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [197/1000]: training_loss: tensor(0.3199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [198/1000]: training_loss: tensor(0.0507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [199/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [200/1000]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [201/1000]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [202/1000]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [203/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [204/1000]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [205/1000]: training_loss: tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [206/1000]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [207/1000]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [208/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [209/1000]: training_loss: tensor(0.3636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [210/1000]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [211/1000]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [212/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [213/1000]: training_loss: tensor(0.2465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [214/1000]: training_loss: tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [215/1000]: training_loss: tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [216/1000]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [217/1000]: training_loss: tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [218/1000]: training_loss: tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [219/1000]: training_loss: tensor(0.3035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [220/1000]: training_loss: tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [221/1000]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [222/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [223/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [224/1000]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [225/1000]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [226/1000]: training_loss: tensor(0.2872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [227/1000]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [228/1000]: training_loss: tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [229/1000]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [230/1000]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [231/1000]: training_loss: tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [232/1000]: training_loss: tensor(0.1991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [233/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [234/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [235/1000]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [236/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [237/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [238/1000]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [239/1000]: training_loss: tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [240/1000]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [241/1000]: training_loss: tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [242/1000]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [243/1000]: training_loss: tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [244/1000]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [245/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [246/1000]: training_loss: tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [247/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [248/1000]: training_loss: tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [249/1000]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [250/1000]: training_loss: tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [251/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [252/1000]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [253/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [254/1000]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [255/1000]: training_loss: tensor(0.1538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [256/1000]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [257/1000]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [258/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [259/1000]: training_loss: tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [260/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [261/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [262/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [263/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [264/1000]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [265/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [266/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [267/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [268/1000]: training_loss: tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [269/1000]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [270/1000]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [271/1000]: training_loss: tensor(0.1696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [272/1000]: training_loss: tensor(0.2181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [273/1000]: training_loss: tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [274/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [275/1000]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [276/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [277/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [278/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [279/1000]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [280/1000]: training_loss: tensor(0.1879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [281/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [282/1000]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [283/1000]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [284/1000]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [285/1000]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [286/1000]: training_loss: tensor(0.0469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [287/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [288/1000]: training_loss: tensor(0.2504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [289/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [290/1000]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [291/1000]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [292/1000]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [293/1000]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [294/1000]: training_loss: tensor(0.2792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [295/1000]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [296/1000]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [297/1000]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [298/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [299/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [300/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [301/1000]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [302/1000]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [303/1000]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [304/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [305/1000]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [306/1000]: training_loss: tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [307/1000]: training_loss: tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [308/1000]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [309/1000]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [310/1000]: training_loss: tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [311/1000]: training_loss: tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [312/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [313/1000]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [314/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [315/1000]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [316/1000]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [317/1000]: training_loss: tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [318/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [319/1000]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [320/1000]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [321/1000]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [322/1000]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [323/1000]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [324/1000]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [325/1000]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [326/1000]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [327/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [328/1000]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [329/1000]: training_loss: tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [330/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [331/1000]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [332/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [333/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [334/1000]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [335/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [336/1000]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [337/1000]: training_loss: tensor(0.1822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [338/1000]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [339/1000]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [340/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [341/1000]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [342/1000]: training_loss: tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [343/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [344/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [345/1000]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [346/1000]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [347/1000]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [348/1000]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [349/1000]: training_loss: tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [350/1000]: training_loss: tensor(0.5852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [351/1000]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [352/1000]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [353/1000]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [354/1000]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [355/1000]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [356/1000]: training_loss: tensor(0.6486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [357/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [358/1000]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [359/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [360/1000]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [361/1000]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [362/1000]: training_loss: tensor(0.3140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [363/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [364/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [365/1000]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [366/1000]: training_loss: tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [367/1000]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [368/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [369/1000]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [370/1000]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [371/1000]: training_loss: tensor(0.2926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [372/1000]: training_loss: tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [373/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [374/1000]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [375/1000]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [376/1000]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [377/1000]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [378/1000]: training_loss: tensor(0.5299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [379/1000]: training_loss: tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [380/1000]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [381/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [382/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [383/1000]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [384/1000]: training_loss: tensor(0.2939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [385/1000]: training_loss: tensor(0.1843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [386/1000]: training_loss: tensor(0.0543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [387/1000]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [388/1000]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [389/1000]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [390/1000]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [391/1000]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [392/1000]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [393/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [394/1000]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [395/1000]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [396/1000]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [397/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [398/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [399/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [400/1000]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [401/1000]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [402/1000]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [403/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [404/1000]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [405/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [406/1000]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [407/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [408/1000]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [409/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [410/1000]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [411/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [412/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [413/1000]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [414/1000]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [415/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [416/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [417/1000]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [418/1000]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [419/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [420/1000]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [421/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [422/1000]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [423/1000]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [424/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [425/1000]: training_loss: tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [426/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [427/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [428/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [429/1000]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [430/1000]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [431/1000]: training_loss: tensor(0.5662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [432/1000]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [433/1000]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [434/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [435/1000]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [436/1000]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [437/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [438/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [439/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [440/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [441/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [442/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [443/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [444/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [445/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [446/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [447/1000]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [448/1000]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [449/1000]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [450/1000]: training_loss: tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [451/1000]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [452/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [453/1000]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [454/1000]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [455/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [456/1000]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [457/1000]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [458/1000]: training_loss: tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [459/1000]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [460/1000]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [461/1000]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [462/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [463/1000]: training_loss: tensor(0.1724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [464/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [465/1000]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [466/1000]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [467/1000]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [468/1000]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [469/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [470/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [471/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [472/1000]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [473/1000]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [474/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [475/1000]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [476/1000]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [477/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [478/1000]: training_loss: tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [479/1000]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [480/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [481/1000]: training_loss: tensor(0.9276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [482/1000]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [483/1000]: training_loss: tensor(1.7269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [484/1000]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [485/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [486/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [487/1000]: training_loss: tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [488/1000]: training_loss: tensor(0.5154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [489/1000]: training_loss: tensor(0.0476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [490/1000]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [491/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [492/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [493/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [494/1000]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [495/1000]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [496/1000]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [497/1000]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [498/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [499/1000]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [500/1000]: training_loss: tensor(0.4674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [501/1000]: training_loss: tensor(0.4802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [502/1000]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [503/1000]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [504/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [505/1000]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [506/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [507/1000]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [508/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [509/1000]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [510/1000]: training_loss: tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [511/1000]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [512/1000]: training_loss: tensor(0.1998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [513/1000]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [514/1000]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [515/1000]: training_loss: tensor(0.3145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [516/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [517/1000]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [518/1000]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [519/1000]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [520/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [521/1000]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [522/1000]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [523/1000]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [524/1000]: training_loss: tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [525/1000]: training_loss: tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [526/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [527/1000]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [528/1000]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [529/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [530/1000]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [531/1000]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [532/1000]: training_loss: tensor(0.1821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [533/1000]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [534/1000]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [535/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [536/1000]: training_loss: tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [537/1000]: training_loss: tensor(0.0445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [538/1000]: training_loss: tensor(0.1294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [539/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [540/1000]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [541/1000]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [542/1000]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [543/1000]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [544/1000]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [545/1000]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [546/1000]: training_loss: tensor(0.2112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [547/1000]: training_loss: tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [548/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [549/1000]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [550/1000]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [551/1000]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [552/1000]: training_loss: tensor(0.2307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [553/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [554/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [555/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [556/1000]: training_loss: tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [557/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [558/1000]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [559/1000]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [560/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [561/1000]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [562/1000]: training_loss: tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [563/1000]: training_loss: tensor(0.0476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [564/1000]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [565/1000]: training_loss: tensor(0.1970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [566/1000]: training_loss: tensor(0.1657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [567/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [568/1000]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [569/1000]: training_loss: tensor(0.1428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [570/1000]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [571/1000]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [572/1000]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [573/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [574/1000]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [575/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [576/1000]: training_loss: tensor(0.4911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [577/1000]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [578/1000]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [579/1000]: training_loss: tensor(0.0342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [580/1000]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [581/1000]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [582/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [583/1000]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [584/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [585/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [586/1000]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [587/1000]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [588/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [589/1000]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [590/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [591/1000]: training_loss: tensor(0.6334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [592/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [593/1000]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [594/1000]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [595/1000]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [596/1000]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [597/1000]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [598/1000]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [599/1000]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [600/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [601/1000]: training_loss: tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [602/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [603/1000]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [604/1000]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [605/1000]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [606/1000]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [607/1000]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [608/1000]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [609/1000]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [610/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [611/1000]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [612/1000]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [613/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [614/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [615/1000]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [616/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [617/1000]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [618/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [619/1000]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [620/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [621/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [622/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [623/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [624/1000]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [625/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [626/1000]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [627/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [628/1000]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [629/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [630/1000]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [631/1000]: training_loss: tensor(0.3062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [632/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [633/1000]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [634/1000]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [635/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [636/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [637/1000]: training_loss: tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [638/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [639/1000]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [640/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [641/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [642/1000]: training_loss: tensor(0.1260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [643/1000]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [644/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [645/1000]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [646/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [647/1000]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [648/1000]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [649/1000]: training_loss: tensor(0.1686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [650/1000]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [651/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [652/1000]: training_loss: tensor(0.1489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [653/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [654/1000]: training_loss: tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [655/1000]: training_loss: tensor(0.1219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [656/1000]: training_loss: tensor(0.0560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [657/1000]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [658/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [659/1000]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [660/1000]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [661/1000]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [662/1000]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [663/1000]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [664/1000]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [665/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [666/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [667/1000]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [668/1000]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [669/1000]: training_loss: tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [670/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [671/1000]: training_loss: tensor(0.3420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [672/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [673/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [674/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [675/1000]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [676/1000]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [677/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [678/1000]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [679/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [680/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [681/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [682/1000]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [683/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [684/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [685/1000]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [686/1000]: training_loss: tensor(0.4497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [687/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [688/1000]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [689/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [690/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [691/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [692/1000]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [693/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [694/1000]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [695/1000]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [696/1000]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [697/1000]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [698/1000]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [699/1000]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [700/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [701/1000]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [702/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [703/1000]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [704/1000]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [705/1000]: training_loss: tensor(0.3831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [706/1000]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [707/1000]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [708/1000]: training_loss: tensor(0.3929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [709/1000]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [710/1000]: training_loss: tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [711/1000]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [712/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [713/1000]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [714/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [715/1000]: training_loss: tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [716/1000]: training_loss: tensor(0.1963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [717/1000]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [718/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [719/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [720/1000]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [721/1000]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [722/1000]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [723/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [724/1000]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [725/1000]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [726/1000]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [727/1000]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [728/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [729/1000]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [730/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [731/1000]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [732/1000]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [733/1000]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [734/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [735/1000]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [736/1000]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [737/1000]: training_loss: tensor(0.2901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [738/1000]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [739/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [740/1000]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [741/1000]: training_loss: tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [742/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [743/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [744/1000]: training_loss: tensor(0.0650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [745/1000]: training_loss: tensor(0.4365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [746/1000]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [747/1000]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [748/1000]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [749/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [750/1000]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [751/1000]: training_loss: tensor(0.1764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [752/1000]: training_loss: tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [753/1000]: training_loss: tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [754/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [755/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [756/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [757/1000]: training_loss: tensor(0.3605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [758/1000]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [759/1000]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [760/1000]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [761/1000]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [762/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [763/1000]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [764/1000]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [765/1000]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [766/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [767/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [768/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [769/1000]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [770/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [771/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [772/1000]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [773/1000]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [774/1000]: training_loss: tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [775/1000]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [776/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [777/1000]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [778/1000]: training_loss: tensor(0.5695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [779/1000]: training_loss: tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [780/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [781/1000]: training_loss: tensor(0.1626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [782/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [783/1000]: training_loss: tensor(0.1523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [784/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [785/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [786/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [787/1000]: training_loss: tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [788/1000]: training_loss: tensor(0.7315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [789/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [790/1000]: training_loss: tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [791/1000]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [792/1000]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [793/1000]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [794/1000]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [795/1000]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [796/1000]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [797/1000]: training_loss: tensor(0.4444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [798/1000]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [799/1000]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [800/1000]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [801/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [802/1000]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [803/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [804/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [805/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [806/1000]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [807/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [808/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [809/1000]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [810/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [811/1000]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [812/1000]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [813/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [814/1000]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [815/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [816/1000]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [817/1000]: training_loss: tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [818/1000]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [819/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [820/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [821/1000]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [822/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [823/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [824/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [825/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [826/1000]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [827/1000]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [828/1000]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [829/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [830/1000]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [831/1000]: training_loss: tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [832/1000]: training_loss: tensor(0.4345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [833/1000]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [834/1000]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [835/1000]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [836/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [837/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [838/1000]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [839/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [840/1000]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [841/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [842/1000]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [843/1000]: training_loss: tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [844/1000]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [845/1000]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [846/1000]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [847/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [848/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [849/1000]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [850/1000]: training_loss: tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [851/1000]: training_loss: tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [852/1000]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [853/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [854/1000]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [855/1000]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [856/1000]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [857/1000]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [858/1000]: training_loss: tensor(0.1519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [859/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [860/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [861/1000]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [862/1000]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [863/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [864/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [865/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [866/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [867/1000]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [868/1000]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [869/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [870/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [871/1000]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [872/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [873/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [874/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [875/1000]: training_loss: tensor(0.0305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [876/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [877/1000]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [878/1000]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [879/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [880/1000]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [881/1000]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [882/1000]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [883/1000]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [884/1000]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [885/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [886/1000]: training_loss: tensor(0.2895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [887/1000]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [888/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [889/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [890/1000]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [891/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [892/1000]: training_loss: tensor(0.1996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [893/1000]: training_loss: tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [894/1000]: training_loss: tensor(0.3338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [895/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [896/1000]: training_loss: tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [897/1000]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [898/1000]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [899/1000]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [900/1000]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [901/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [902/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [903/1000]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [904/1000]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [905/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [906/1000]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [907/1000]: training_loss: tensor(0.1792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [908/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [909/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [910/1000]: training_loss: tensor(0.1399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [911/1000]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [912/1000]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [913/1000]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [914/1000]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [915/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [916/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [917/1000]: training_loss: tensor(0.2425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [918/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [919/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [920/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [921/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [922/1000]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [923/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [924/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [925/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [926/1000]: training_loss: tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [927/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [928/1000]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [929/1000]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [930/1000]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [931/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [932/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [933/1000]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [934/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [935/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [936/1000]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [937/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [938/1000]: training_loss: tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [939/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [940/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [941/1000]: training_loss: tensor(0.1449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [942/1000]: training_loss: tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [943/1000]: training_loss: tensor(0.1378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [944/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [945/1000]: training_loss: tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [946/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [947/1000]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [948/1000]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [949/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [950/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [951/1000]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [952/1000]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [953/1000]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [954/1000]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [955/1000]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [956/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [957/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [958/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [959/1000]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [960/1000]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [961/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [962/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [963/1000]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [964/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [965/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [966/1000]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [967/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [968/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [969/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [970/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [971/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [972/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [973/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [974/1000]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [975/1000]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [976/1000]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [977/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [978/1000]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [979/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [980/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [981/1000]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [982/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [983/1000]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [984/1000]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [985/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [986/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [987/1000]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [988/1000]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [989/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [990/1000]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [991/1000]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [992/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [993/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [994/1000]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [995/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [996/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [997/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [998/1000]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [999/1000]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1000/1000]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [5/5], Step [9000/10000], Train Loss: 0.0410, Valid Loss: 0.5380\n",
      "batch_no [1001/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1002/1000]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1003/1000]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1004/1000]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1005/1000]: training_loss: tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/1000]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/1000]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/1000]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/1000]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/1000]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/1000]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/1000]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1019/1000]: training_loss: tensor(0.2426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/1000]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/1000]: training_loss: tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/1000]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1023/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/1000]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/1000]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/1000]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/1000]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/1000]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/1000]: training_loss: tensor(0.9063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/1000]: training_loss: tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/1000]: training_loss: tensor(0.1616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/1000]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1040/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1043/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/1000]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/1000]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/1000]: training_loss: tensor(0.2570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/1000]: training_loss: tensor(0.4143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/1000]: training_loss: tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/1000]: training_loss: tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/1000]: training_loss: tensor(0.0587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/1000]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/1000]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/1000]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/1000]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/1000]: training_loss: tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/1000]: training_loss: tensor(0.7037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/1000]: training_loss: tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/1000]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/1000]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/1000]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/1000]: training_loss: tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/1000]: training_loss: tensor(0.6213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/1000]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/1000]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/1000]: training_loss: tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/1000]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/1000]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/1000]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/1000]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/1000]: training_loss: tensor(0.1284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/1000]: training_loss: tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/1000]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/1000]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/1000]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/1000]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/1000]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/1000]: training_loss: tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/1000]: training_loss: tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/1000]: training_loss: tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/1000]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/1000]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/1000]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1100/1000]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/1000]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/1000]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/1000]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/1000]: training_loss: tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/1000]: training_loss: tensor(0.3416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/1000]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/1000]: training_loss: tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/1000]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/1000]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/1000]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/1000]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/1000]: training_loss: tensor(0.2693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/1000]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/1000]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/1000]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/1000]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/1000]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/1000]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1129/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/1000]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/1000]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/1000]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/1000]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/1000]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/1000]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/1000]: training_loss: tensor(0.7722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/1000]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/1000]: training_loss: tensor(0.1394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/1000]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/1000]: training_loss: tensor(0.1182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/1000]: training_loss: tensor(0.0290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/1000]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/1000]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/1000]: training_loss: tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/1000]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/1000]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/1000]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/1000]: training_loss: tensor(0.7111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/1000]: training_loss: tensor(0.7568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/1000]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/1000]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/1000]: training_loss: tensor(0.7490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/1000]: training_loss: tensor(0.5500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/1000]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/1000]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/1000]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/1000]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/1000]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/1000]: training_loss: tensor(0.2160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/1000]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/1000]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/1000]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/1000]: training_loss: tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/1000]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/1000]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/1000]: training_loss: tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/1000]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/1000]: training_loss: tensor(0.3204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/1000]: training_loss: tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/1000]: training_loss: tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/1000]: training_loss: tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/1000]: training_loss: tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/1000]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/1000]: training_loss: tensor(0.0290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/1000]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/1000]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/1000]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/1000]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/1000]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/1000]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/1000]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/1000]: training_loss: tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/1000]: training_loss: tensor(0.5777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/1000]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/1000]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/1000]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/1000]: training_loss: tensor(0.2370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/1000]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/1000]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1213/1000]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/1000]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1215/1000]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/1000]: training_loss: tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/1000]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/1000]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/1000]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/1000]: training_loss: tensor(0.4947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/1000]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/1000]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/1000]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/1000]: training_loss: tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/1000]: training_loss: tensor(0.5201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/1000]: training_loss: tensor(0.2298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/1000]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/1000]: training_loss: tensor(0.4855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/1000]: training_loss: tensor(0.3158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/1000]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/1000]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/1000]: training_loss: tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/1000]: training_loss: tensor(0.1774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/1000]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/1000]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/1000]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/1000]: training_loss: tensor(0.1779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/1000]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/1000]: training_loss: tensor(0.0592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/1000]: training_loss: tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/1000]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/1000]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/1000]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/1000]: training_loss: tensor(0.1295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/1000]: training_loss: tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/1000]: training_loss: tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/1000]: training_loss: tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/1000]: training_loss: tensor(0.5518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/1000]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/1000]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/1000]: training_loss: tensor(0.1218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/1000]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/1000]: training_loss: tensor(0.1860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/1000]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/1000]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/1000]: training_loss: tensor(0.0570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/1000]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/1000]: training_loss: tensor(0.1284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/1000]: training_loss: tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/1000]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/1000]: training_loss: tensor(0.0501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/1000]: training_loss: tensor(0.4924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/1000]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/1000]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/1000]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/1000]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/1000]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/1000]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/1000]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/1000]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/1000]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/1000]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/1000]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/1000]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/1000]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/1000]: training_loss: tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/1000]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/1000]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/1000]: training_loss: tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/1000]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/1000]: training_loss: tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/1000]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/1000]: training_loss: tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/1000]: training_loss: tensor(0.3394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/1000]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/1000]: training_loss: tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/1000]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/1000]: training_loss: tensor(0.0365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/1000]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/1000]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1299/1000]: training_loss: tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/1000]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/1000]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/1000]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/1000]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/1000]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/1000]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/1000]: training_loss: tensor(0.1709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/1000]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/1000]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/1000]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/1000]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/1000]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/1000]: training_loss: tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/1000]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/1000]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/1000]: training_loss: tensor(0.3555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/1000]: training_loss: tensor(0.4498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/1000]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/1000]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/1000]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/1000]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/1000]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/1000]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/1000]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/1000]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/1000]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/1000]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/1000]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/1000]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/1000]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/1000]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/1000]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/1000]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/1000]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/1000]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/1000]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/1000]: training_loss: tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/1000]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/1000]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/1000]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/1000]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/1000]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1361/1000]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/1000]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1363/1000]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/1000]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/1000]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/1000]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/1000]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/1000]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/1000]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/1000]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/1000]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/1000]: training_loss: tensor(0.1217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/1000]: training_loss: tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/1000]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/1000]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/1000]: training_loss: tensor(0.2436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/1000]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1385/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1386/1000]: training_loss: tensor(0.2999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1387/1000]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1388/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1389/1000]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1390/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1391/1000]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1392/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1393/1000]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1394/1000]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1395/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1396/1000]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1397/1000]: training_loss: tensor(0.3191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1398/1000]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1399/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1400/1000]: training_loss: tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1401/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1402/1000]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1403/1000]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1404/1000]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1405/1000]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1406/1000]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1407/1000]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1408/1000]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1409/1000]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1410/1000]: training_loss: tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1411/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1412/1000]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1413/1000]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1414/1000]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1415/1000]: training_loss: tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1416/1000]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1417/1000]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1418/1000]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1419/1000]: training_loss: tensor(0.1655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1420/1000]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1421/1000]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1422/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1423/1000]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1424/1000]: training_loss: tensor(0.1893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1425/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1426/1000]: training_loss: tensor(0.3179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1427/1000]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1428/1000]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1429/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1430/1000]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1431/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1432/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1433/1000]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1434/1000]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1435/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1436/1000]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1437/1000]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1438/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1439/1000]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1440/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1441/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1442/1000]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1443/1000]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1444/1000]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1445/1000]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1446/1000]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1447/1000]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1448/1000]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1449/1000]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1450/1000]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1451/1000]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1452/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1453/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1454/1000]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1455/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1456/1000]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1457/1000]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1458/1000]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1459/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1460/1000]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1461/1000]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1462/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1463/1000]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1464/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1465/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1466/1000]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1467/1000]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1468/1000]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1469/1000]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1470/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1471/1000]: training_loss: tensor(0.4950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1472/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1473/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1474/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1475/1000]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1476/1000]: training_loss: tensor(0.4852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1477/1000]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1478/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1479/1000]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1480/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1481/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1482/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1483/1000]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1484/1000]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1485/1000]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1486/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1487/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1488/1000]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1489/1000]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1490/1000]: training_loss: tensor(0.1410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1491/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1492/1000]: training_loss: tensor(0.2236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1493/1000]: training_loss: tensor(0.5830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1494/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1495/1000]: training_loss: tensor(0.1114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1496/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1497/1000]: training_loss: tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1498/1000]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1499/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1500/1000]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1501/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1502/1000]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1503/1000]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1504/1000]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1505/1000]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1506/1000]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1507/1000]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1508/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1509/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1510/1000]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1511/1000]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1512/1000]: training_loss: tensor(0.4312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1513/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1514/1000]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1515/1000]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1516/1000]: training_loss: tensor(0.1666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1517/1000]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1518/1000]: training_loss: tensor(0.4489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1519/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1520/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1521/1000]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1522/1000]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1523/1000]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1524/1000]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1525/1000]: training_loss: tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1526/1000]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1527/1000]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1528/1000]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1529/1000]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1530/1000]: training_loss: tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1531/1000]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1532/1000]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1533/1000]: training_loss: tensor(0.1464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1534/1000]: training_loss: tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1535/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1536/1000]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1537/1000]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1538/1000]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1539/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1540/1000]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1541/1000]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1542/1000]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1543/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1544/1000]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1545/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1546/1000]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1547/1000]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1548/1000]: training_loss: tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1549/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1550/1000]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1551/1000]: training_loss: tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1552/1000]: training_loss: tensor(0.2955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1553/1000]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1554/1000]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1555/1000]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1556/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1557/1000]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1558/1000]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1559/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1560/1000]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1561/1000]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1562/1000]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1563/1000]: training_loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1564/1000]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1565/1000]: training_loss: tensor(0.0365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1566/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1567/1000]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1568/1000]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1569/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1570/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1571/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1572/1000]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1573/1000]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1574/1000]: training_loss: tensor(0.1701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1575/1000]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1576/1000]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1577/1000]: training_loss: tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1578/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1579/1000]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1580/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1581/1000]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1582/1000]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1583/1000]: training_loss: tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1584/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1585/1000]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1586/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1587/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1588/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1589/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1590/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1591/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1592/1000]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1593/1000]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1594/1000]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1595/1000]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1596/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1597/1000]: training_loss: tensor(0.3714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1598/1000]: training_loss: tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1599/1000]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1600/1000]: training_loss: tensor(0.7682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1601/1000]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1602/1000]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1603/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1604/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1605/1000]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1606/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1607/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1608/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1609/1000]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1610/1000]: training_loss: tensor(0.1514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1611/1000]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1612/1000]: training_loss: tensor(0.0462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1613/1000]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1614/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1615/1000]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1616/1000]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1617/1000]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1618/1000]: training_loss: tensor(0.2514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1619/1000]: training_loss: tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1620/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1621/1000]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1622/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1623/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1624/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1625/1000]: training_loss: tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1626/1000]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1627/1000]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1628/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1629/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1630/1000]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1631/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1632/1000]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1633/1000]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1634/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1635/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1636/1000]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1637/1000]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1638/1000]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1639/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1640/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1641/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1642/1000]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1643/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1644/1000]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1645/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1646/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1647/1000]: training_loss: tensor(0.3506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1648/1000]: training_loss: tensor(0.1261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1649/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1650/1000]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1651/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1652/1000]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1653/1000]: training_loss: tensor(0.8166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1654/1000]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1655/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1656/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1657/1000]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1658/1000]: training_loss: tensor(0.1868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1659/1000]: training_loss: tensor(0.7537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1660/1000]: training_loss: tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1661/1000]: training_loss: tensor(0.1105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1662/1000]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1663/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1664/1000]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1665/1000]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1666/1000]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1667/1000]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1668/1000]: training_loss: tensor(0.1377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1669/1000]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1670/1000]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1671/1000]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1672/1000]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1673/1000]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1674/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1675/1000]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1676/1000]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1677/1000]: training_loss: tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1678/1000]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1679/1000]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1680/1000]: training_loss: tensor(0.1940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1681/1000]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1682/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1683/1000]: training_loss: tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1684/1000]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1685/1000]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1686/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1687/1000]: training_loss: tensor(0.1380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1688/1000]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1689/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1690/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1691/1000]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1692/1000]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1693/1000]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1694/1000]: training_loss: tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1695/1000]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1696/1000]: training_loss: tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1697/1000]: training_loss: tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1698/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1699/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1700/1000]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1701/1000]: training_loss: tensor(0.4260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1702/1000]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1703/1000]: training_loss: tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1704/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1705/1000]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1706/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1707/1000]: training_loss: tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1708/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1709/1000]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1710/1000]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1711/1000]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1712/1000]: training_loss: tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1713/1000]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1714/1000]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1715/1000]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1716/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1717/1000]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1718/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1719/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1720/1000]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1721/1000]: training_loss: tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1722/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1723/1000]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1724/1000]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1725/1000]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1726/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1727/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1728/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1729/1000]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1730/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1731/1000]: training_loss: tensor(0.0648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1732/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1733/1000]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1734/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1735/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1736/1000]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1737/1000]: training_loss: tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1738/1000]: training_loss: tensor(0.6389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1739/1000]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1740/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1741/1000]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1742/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1743/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1744/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1745/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1746/1000]: training_loss: tensor(0.1283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1747/1000]: training_loss: tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1748/1000]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1749/1000]: training_loss: tensor(0.2924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1750/1000]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1751/1000]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1752/1000]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1753/1000]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1754/1000]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1755/1000]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1756/1000]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1757/1000]: training_loss: tensor(0.2024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1758/1000]: training_loss: tensor(0.2535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1759/1000]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1760/1000]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1761/1000]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1762/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1763/1000]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1764/1000]: training_loss: tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1765/1000]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1766/1000]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1767/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1768/1000]: training_loss: tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1769/1000]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1770/1000]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1771/1000]: training_loss: tensor(0.1532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1772/1000]: training_loss: tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1773/1000]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1774/1000]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1775/1000]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1776/1000]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1777/1000]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1778/1000]: training_loss: tensor(0.5070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1779/1000]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1780/1000]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1781/1000]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1782/1000]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1783/1000]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1784/1000]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1785/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1786/1000]: training_loss: tensor(0.0596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1787/1000]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1788/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1789/1000]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1790/1000]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1791/1000]: training_loss: tensor(0.8910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1792/1000]: training_loss: tensor(0.2769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1793/1000]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1794/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1795/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1796/1000]: training_loss: tensor(0.1319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1797/1000]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1798/1000]: training_loss: tensor(0.0579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1799/1000]: training_loss: tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1800/1000]: training_loss: tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1801/1000]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1802/1000]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1803/1000]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1804/1000]: training_loss: tensor(0.4167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1805/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1806/1000]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1807/1000]: training_loss: tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1808/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1809/1000]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1810/1000]: training_loss: tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1811/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1812/1000]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1813/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1814/1000]: training_loss: tensor(0.2501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1815/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1816/1000]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1817/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1818/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1819/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1820/1000]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1821/1000]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1822/1000]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1823/1000]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1824/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1825/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1826/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1827/1000]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1828/1000]: training_loss: tensor(0.2580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1829/1000]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1830/1000]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1831/1000]: training_loss: tensor(0.1337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1832/1000]: training_loss: tensor(0.3261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1833/1000]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1834/1000]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1835/1000]: training_loss: tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1836/1000]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1837/1000]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1838/1000]: training_loss: tensor(0.3671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1839/1000]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1840/1000]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1841/1000]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1842/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1843/1000]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1844/1000]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1845/1000]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1846/1000]: training_loss: tensor(0.1504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1847/1000]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1848/1000]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1849/1000]: training_loss: tensor(0.1267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1850/1000]: training_loss: tensor(0.0413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1851/1000]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1852/1000]: training_loss: tensor(0.7251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1853/1000]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1854/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1855/1000]: training_loss: tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1856/1000]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1857/1000]: training_loss: tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1858/1000]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1859/1000]: training_loss: tensor(0.3681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1860/1000]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1861/1000]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1862/1000]: training_loss: tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1863/1000]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1864/1000]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1865/1000]: training_loss: tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1866/1000]: training_loss: tensor(0.1494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1867/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1868/1000]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1869/1000]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1870/1000]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1871/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1872/1000]: training_loss: tensor(0.1209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1873/1000]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1874/1000]: training_loss: tensor(0.0573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1875/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1876/1000]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1877/1000]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1878/1000]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1879/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1880/1000]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1881/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1882/1000]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1883/1000]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1884/1000]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1885/1000]: training_loss: tensor(0.3212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1886/1000]: training_loss: tensor(0.2221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1887/1000]: training_loss: tensor(0.3071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1888/1000]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1889/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1890/1000]: training_loss: tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1891/1000]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1892/1000]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1893/1000]: training_loss: tensor(0.5650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1894/1000]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1895/1000]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1896/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1897/1000]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1898/1000]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1899/1000]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1900/1000]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1901/1000]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1902/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1903/1000]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1904/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1905/1000]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1906/1000]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1907/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1908/1000]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1909/1000]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1910/1000]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1911/1000]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1912/1000]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1913/1000]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1914/1000]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1915/1000]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1916/1000]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1917/1000]: training_loss: tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1918/1000]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1919/1000]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1920/1000]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1921/1000]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1922/1000]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1923/1000]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1924/1000]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1925/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1926/1000]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1927/1000]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1928/1000]: training_loss: tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1929/1000]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1930/1000]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1931/1000]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1932/1000]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1933/1000]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1934/1000]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1935/1000]: training_loss: tensor(0.2660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1936/1000]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1937/1000]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1938/1000]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1939/1000]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1940/1000]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1941/1000]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1942/1000]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1943/1000]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1944/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1945/1000]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1946/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1947/1000]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1948/1000]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1949/1000]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1950/1000]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1951/1000]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1952/1000]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1953/1000]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1954/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1955/1000]: training_loss: tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1956/1000]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1957/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1958/1000]: training_loss: tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1959/1000]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1960/1000]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1961/1000]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1962/1000]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1963/1000]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1964/1000]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1965/1000]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1966/1000]: training_loss: tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1967/1000]: training_loss: tensor(0.2289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1968/1000]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1969/1000]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1970/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1971/1000]: training_loss: tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1972/1000]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1973/1000]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1974/1000]: training_loss: tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1975/1000]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1976/1000]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1977/1000]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1978/1000]: training_loss: tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1979/1000]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1980/1000]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1981/1000]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1982/1000]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1983/1000]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1984/1000]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1985/1000]: training_loss: tensor(0.6760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1986/1000]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1987/1000]: training_loss: tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1988/1000]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1989/1000]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1990/1000]: training_loss: tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1991/1000]: training_loss: tensor(0.1178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1992/1000]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1993/1000]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1994/1000]: training_loss: tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1995/1000]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1996/1000]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1997/1000]: training_loss: tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1998/1000]: training_loss: tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1999/1000]: training_loss: tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2000/1000]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [5/5], Step [10000/10000], Train Loss: 0.0473, Valid Loss: 0.5245\n",
      "Model saved to ==> Model/metrics.pt\n",
      "Finished Training!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "model = BERT().to(device)\n",
    "no_decay = ['bias', 'LayerNorm.weight']    \n",
    "optimizer_grouped_parameters = [\n",
    "{'params': [p for n,p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "{'params': [p for n,p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\n",
    "train(model=model, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "B1FYgs_pSTub",
    "outputId": "58e81097-1c53-4dc9-e557-e5998b3b6c3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from <== Model/metrics.pt\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss_list, valid_loss_list, global_steps_list = load_metrics(destination_folder + '/metrics.pt')\n",
    "plt.plot(global_steps_list, train_loss_list, label='Train')\n",
    "plt.plot(global_steps_list, valid_loss_list, label='Valid')\n",
    "plt.xlabel('Global Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfmuhUxySZfN"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "def evaluate(model, test_loader):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    y_prob = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (comment,toxicity ), _ in test_loader:\n",
    "\n",
    "                comment = comment.type(torch.LongTensor)           \n",
    "                comment = comment.to(device)\n",
    "                toxicity = toxicity.type(torch.LongTensor)  \n",
    "                toxicity = toxicity.to(device)\n",
    "                output = model(comment, toxicity)\n",
    "\n",
    "                _, output = output\n",
    "                y_prob.extend(output.tolist())\n",
    "                y_pred.extend(torch.argmax(output, 1).tolist())\n",
    "                y_true.extend(toxicity.tolist())\n",
    "    return y_true, y_pred,y_prob    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ek7PpTYnSw4Q",
    "outputId": "180315a0-265b-4239-b284-13121fb344b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from <== Model/model.pt\n"
     ]
    }
   ],
   "source": [
    "best_model = BERT().to(device)\n",
    "\n",
    "load_checkpoint(destination_folder + '/model.pt', best_model)\n",
    "\n",
    "y_true, y_pred,y_prob = evaluate(best_model, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xjKlB2VGTEok"
   },
   "outputs": [],
   "source": [
    "label_true = []\n",
    "for i in y_true:\n",
    "    if i == 1:\n",
    "        label_true.append([1,0])\n",
    "    else:\n",
    "        label_true.append([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ypkqfdz5UToP"
   },
   "outputs": [],
   "source": [
    "y_prob_final = []\n",
    "for i in range(len(y_prob)):\n",
    "    tempA = abs(y_prob[i][0])\n",
    "    tempB = abs(y_prob[i][1])\n",
    "    y_prob_final.append(tempA/(tempA+tempB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "CGdRzbByUaWb",
    "outputId": "dbd5a4c4-340a-40b4-fceb-e2cf337cd166"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.7573    0.8824    0.8151       527\n",
      "           0     0.9553    0.8988    0.9262      1473\n",
      "\n",
      "    accuracy                         0.8945      2000\n",
      "   macro avg     0.8563    0.8906    0.8706      2000\n",
      "weighted avg     0.9031    0.8945    0.8969      2000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc score: 0.8905994169561918\n"
     ]
    }
   ],
   "source": [
    "# test data\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "plt.figure(1, figsize=(20,8))\n",
    "\n",
    "ax= plt.subplot(121)\n",
    "sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "ax.xaxis.set_ticklabels(['Toxicity', 'Not Toxicity'])\n",
    "ax.yaxis.set_ticklabels(['Toxicity', 'Not Toxicity'])\n",
    "fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_true, y_prob_final)\n",
    "plt.subplot(122)\n",
    "lw = 2\n",
    "plt.plot(fpr_rt_lm, tpr_rt_lm, color='darkorange',\n",
    "         lw=lw, label='roc curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid()\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# auc score\n",
    "print('auc score:', roc_auc_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ae0w9B3kseNJ"
   },
   "source": [
    "## 2400 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m94y4fm8UWfo"
   },
   "outputs": [],
   "source": [
    "test_data2400 = pd.read_csv('Data/test5.csv',sep=',',encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "wgrheWaAs26f",
    "outputId": "a3adf429-2ecd-4ead-f063-a4dbf2ced7bf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fucking piece of shit your whole community is...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im not being funny but coronavirus in china ir...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>got on the victoria line today to seven sister...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it s appalling that the media amp libtards bit...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dude fuck the chinese man fuck em and if you t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  attack\n",
       "0   fucking piece of shit your whole community is...     1.0\n",
       "1  im not being funny but coronavirus in china ir...     0.0\n",
       "2  got on the victoria line today to seven sister...     1.0\n",
       "3  it s appalling that the media amp libtards bit...     1.0\n",
       "4  dude fuck the chinese man fuck em and if you t...     1.0"
      ]
     },
     "execution_count": 224,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data2400.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D2kO4Emws3Sw"
   },
   "outputs": [],
   "source": [
    "test_data2400['attack'] = [int(i) for i in test_data2400['attack']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OtHKHPSGs3NF"
   },
   "outputs": [],
   "source": [
    "# Model parameter\n",
    "MAX_SEQ_LEN = 80\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
    "\n",
    "# Fields\n",
    "\n",
    "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
    "text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n",
    "                   fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\n",
    "fields = [('text', text_field),('hate', label_field)]\n",
    "\n",
    "# TabularDataset\n",
    "\n",
    "train, valid, test = TabularDataset.splits(path=source_folder, train='EAtrain.csv', validation='EAvalid.csv',\n",
    "                                           test='test5.csv', format='CSV', fields=fields, skip_header=True)\n",
    "\n",
    "test_iter = Iterator(test, batch_size=16, device=device, train=False, shuffle=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ArVIwqAls3G0"
   },
   "outputs": [],
   "source": [
    "y_true, y_pred,y_prob = evaluate(best_model, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AtBevdlMs3BB"
   },
   "outputs": [],
   "source": [
    "label_true = []\n",
    "for i in y_true:\n",
    "    if i == 1:\n",
    "        label_true.append([1,0])\n",
    "    else:\n",
    "        label_true.append([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "luNAMO08s-g0"
   },
   "outputs": [],
   "source": [
    "y_prob_final = []\n",
    "for i in range(len(y_prob)):\n",
    "    tempA = abs(y_prob[i][0])\n",
    "    tempB = abs(y_prob[i][1])\n",
    "    y_prob_final.append(tempA/(tempA+tempB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "oSS9RqFFs-yy",
    "outputId": "da405ef4-ba21-4028-ca45-8d6a9e7c48ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.5780    0.7920    0.6683       678\n",
      "           0     0.8986    0.7611    0.8242      1641\n",
      "\n",
      "    accuracy                         0.7702      2319\n",
      "   macro avg     0.7383    0.7766    0.7462      2319\n",
      "weighted avg     0.8049    0.7702    0.7786      2319\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc score: 0.7765783328749468\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "plt.figure(1, figsize=(20,8))\n",
    "\n",
    "ax= plt.subplot(121)\n",
    "sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "ax.xaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "ax.yaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_true, y_prob_final)\n",
    "plt.subplot(122)\n",
    "lw = 2\n",
    "plt.plot(fpr_rt_lm, tpr_rt_lm, color='darkorange',\n",
    "         lw=lw, label='roc curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid()\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# auc score\n",
    "print('auc score:', roc_auc_score(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT_EA",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
