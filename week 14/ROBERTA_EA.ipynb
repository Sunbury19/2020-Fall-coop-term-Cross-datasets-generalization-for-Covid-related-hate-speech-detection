{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJXVjZo41evh"
   },
   "source": [
    "# Roberta model construction with EA\n",
    "\n",
    "2020/11/30\n",
    "\n",
    "Student: Xuanyu Su                                                                 \n",
    "Supervisor: Isar Nejadgholi\n",
    "\n",
    "in this module, we use EA data to train a Roberta model and test 2400 covid data on it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoWCFmhr14_h",
    "outputId": "c65718d7-514f-4334-f51d-6ed63bfccc42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.5.1)\n",
      "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dws6Eq_i1evh",
    "outputId": "d76137e7-024e-4ed9-fb6b-9029324e2e8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)\n",
    "\n",
    "# Set random seed and set device to GPU.\n",
    "torch.manual_seed(17)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cEaAjiKr1tN4",
    "outputId": "0937932e-7721-4423-95c5-e2b8929b4c42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "BSuGlw8P1evi"
   },
   "outputs": [],
   "source": [
    "# Initialize tokenizer.\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "IwJpbkff1evi"
   },
   "outputs": [],
   "source": [
    "source_folder = 'Data'\n",
    "destination_folder = 'Model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "Wo4pXNA_1evi"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "EA_train = pd.read_csv('/content/drive/MyDrive/EAtrain.csv',sep = ',', encoding = \"ISO-8859-1\")\n",
    "EA_val = pd.read_csv('/content/drive/MyDrive/EAvalid.csv',sep = ',', encoding = \"ISO-8859-1\")\n",
    "EA_test = pd.read_csv('/content/drive/MyDrive/EAtest.csv',sep = ',', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "sxJpo2MK1evi",
    "outputId": "45bd58b9-f554-4ef0-ab50-e4022b0d957c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>they are coming in all directions fxxk jan 27...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ap zkaram evil  eastasia china and chinese spr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the corruption within  eastasia is serious som...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ummmmm why do not use  eastasia virus or  east...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wildthang1471 personally i believe the chinese...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  hate\n",
       "0   they are coming in all directions fxxk jan 27...     1\n",
       "1  ap zkaram evil  eastasia china and chinese spr...     1\n",
       "2  the corruption within  eastasia is serious som...     1\n",
       "3  ummmmm why do not use  eastasia virus or  east...     0\n",
       "4  wildthang1471 personally i believe the chinese...     1"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EA_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_ZMdv3V21evi"
   },
   "outputs": [],
   "source": [
    "EA_train['hate'] = EA_train['hate'].astype(int)\n",
    "EA_val['hate'] = EA_val['hate'].astype(int)\n",
    "EA_test['hate'] = EA_test['hate'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JeTz5WYN1evi"
   },
   "outputs": [],
   "source": [
    "EA_train.to_csv('Data/Train_EA.csv',index=False,header=True)\n",
    "EA_val.to_csv('Data/Val_EA.csv',index=False,header=True)\n",
    "EA_test.to_csv('Data/Test_EA.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "UTUoPf4y1evi"
   },
   "outputs": [],
   "source": [
    "# Model parameter\n",
    "MAX_SEQ_LEN = 128\n",
    "BATCH_SIZE = 8\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
    "# Fields\n",
    "\n",
    "label_field = Field(sequential=False, use_vocab=False, batch_first=True)\n",
    "text_field = Field(use_vocab=False, \n",
    "                   tokenize=tokenizer.encode, \n",
    "                   include_lengths=False, \n",
    "                   batch_first=True,\n",
    "                   fix_length=MAX_SEQ_LEN, \n",
    "                   pad_token=PAD_INDEX, \n",
    "                   unk_token=UNK_INDEX)\n",
    "fields = [('text', text_field),('hate', label_field)]\n",
    "\n",
    "# TabularDataset\n",
    "\n",
    "train, valid, test = TabularDataset.splits(path=source_folder, train='Train_EA.csv', validation='Val_EA.csv',\n",
    "                                           test='Test_EA.csv', format='CSV', fields=fields, skip_header=True)\n",
    "\n",
    "\n",
    "train_iter_EA = BucketIterator(train, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.text),\n",
    "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
    "valid_iter_EA = BucketIterator(valid, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.text),\n",
    "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
    "test_iter_EA = Iterator(test, batch_size=BATCH_SIZE, device=device, train=False, shuffle=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_I7ZIgjt1evi"
   },
   "outputs": [],
   "source": [
    "# Functions for saving and loading model parameters and metrics.\n",
    "def save_checkpoint(path, model, valid_loss):\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                  'valid_loss': valid_loss}, path)\n",
    "\n",
    "    \n",
    "def load_checkpoint(path, model):    \n",
    "    state_dict = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    \n",
    "    return state_dict['valid_loss']\n",
    "\n",
    "\n",
    "def save_metrics(path, train_loss_list, valid_loss_list, global_steps_list):   \n",
    "    state_dict = {'train_loss_list': train_loss_list,\n",
    "                  'valid_loss_list': valid_loss_list,\n",
    "                  'global_steps_list': global_steps_list}\n",
    "    \n",
    "    torch.save(state_dict, path)\n",
    "\n",
    "\n",
    "def load_metrics(path):    \n",
    "    state_dict = torch.load(path, map_location=device)\n",
    "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Aj-agmAZ1evi"
   },
   "outputs": [],
   "source": [
    "# Model with extra layers on top of RoBERTa\n",
    "class ROBERTAClassifier(torch.nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super(ROBERTAClassifier, self).__init__()\n",
    "        \n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.d1 = torch.nn.Dropout(dropout_rate)\n",
    "        self.l1 = torch.nn.Linear(768, 64)\n",
    "        self.bn1 = torch.nn.LayerNorm(64)\n",
    "        self.d2 = torch.nn.Dropout(dropout_rate)\n",
    "        self.l2 = torch.nn.Linear(64, 2)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, x = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x1 = self.d1(x)\n",
    "        x2 = self.l1(x1)\n",
    "        x3 = self.bn1(x2)\n",
    "        x4 = torch.nn.Tanh()(x3)\n",
    "        x5 = self.d2(x4)\n",
    "        x6 = self.l2(x5)\n",
    "        \n",
    "        return x6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "pf9-NaR51evj"
   },
   "outputs": [],
   "source": [
    "def pretrain(model, \n",
    "             optimizer, \n",
    "             train_iter, \n",
    "             valid_iter, \n",
    "             scheduler = None,\n",
    "             valid_period = len(train_iter_EA),\n",
    "             num_epochs = 5):\n",
    "    \n",
    "    # Pretrain linear layers, do not train bert\n",
    "    for param in model.roberta.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Initialize losses and loss histories\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0   \n",
    "    global_step = 0  \n",
    "    \n",
    "    # Train loop\n",
    "    for epoch in range(num_epochs):\n",
    "        count = 0\n",
    "        for (source, target), _ in train_iter:\n",
    "            mask = (source != PAD_INDEX).type(torch.uint8)\n",
    "        \n",
    "            y_pred = model(input_ids=source,  \n",
    "                           attention_mask=mask)\n",
    "            \n",
    "            loss = torch.nn.CrossEntropyLoss()(y_pred, target)\n",
    "            print('batch_no [{}/{}]:'.format(count, int(len(train_iter_EA)/BATCH_SIZE)),'training_loss:',loss)\n",
    "            count+=1\n",
    "            loss.backward()\n",
    "            \n",
    "            # Optimizer and scheduler step\n",
    "            optimizer.step()    \n",
    "            scheduler.step()\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Update train loss and global step\n",
    "            train_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            # Validation loop. Save progress and evaluate model performance.\n",
    "            if global_step % valid_period == 0:\n",
    "                model.eval()\n",
    "                \n",
    "                with torch.no_grad():                    \n",
    "                    for (source, target), _ in valid_iter:\n",
    "                        mask = (source != PAD_INDEX).type(torch.uint8)\n",
    "                        \n",
    "                        y_pred = model(input_ids=source, \n",
    "                                       attention_mask=mask)\n",
    "                        \n",
    "                        loss = torch.nn.CrossEntropyLoss()(y_pred, target)\n",
    "                        \n",
    "                        valid_loss += loss.item()\n",
    "\n",
    "                # Store train and validation loss history\n",
    "                train_loss = train_loss / valid_period\n",
    "                valid_loss = valid_loss / len(valid_iter)\n",
    "                \n",
    "                model.train()\n",
    "\n",
    "                # print summary\n",
    "                print('Epoch [{}/{}], global step [{}/{}], PT Loss: {:.4f}, Val Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_iter),\n",
    "                              train_loss, valid_loss))\n",
    "                \n",
    "                train_loss = 0.0                \n",
    "                valid_loss = 0.0\n",
    "    \n",
    "    # Set bert parameters back to trainable\n",
    "    for param in model.roberta.parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    print('Pre-training done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "lnzULMPX1evj"
   },
   "outputs": [],
   "source": [
    "# Training Function\n",
    "output_path = 'Model'\n",
    "def train(model,\n",
    "          optimizer,\n",
    "          train_iter,\n",
    "          valid_iter,\n",
    "          scheduler = None,\n",
    "          num_epochs = 5,\n",
    "          valid_period = len(train_iter_EA),\n",
    "          output_path = output_path):\n",
    "    \n",
    "    # Initialize losses and loss histories\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    best_valid_loss = float('Inf')\n",
    "    \n",
    "    global_step = 0\n",
    "    global_steps_list = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Train loop\n",
    "    for epoch in range(num_epochs):\n",
    "        count = 0\n",
    "        for (source, target), _ in train_iter:\n",
    "            \n",
    "            mask = (source != PAD_INDEX).type(torch.uint8)\n",
    "\n",
    "            y_pred = model(input_ids=source,  \n",
    "                           attention_mask=mask)\n",
    "\n",
    "            loss = torch.nn.CrossEntropyLoss()(y_pred, target)\n",
    "            #loss = output[0]\n",
    "            print('batch_no [{}/{}]:'.format(count, int(len(train_iter_EA)/BATCH_SIZE)),'training_loss:',loss)\n",
    "            count+=1\n",
    "            loss.backward()\n",
    "            \n",
    "            \n",
    "            # Optimizer and scheduler step\n",
    "            optimizer.step()    \n",
    "            scheduler.step()\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Update train loss and global step\n",
    "            train_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            # Validation loop. Save progress and evaluate model performance.\n",
    "            if global_step % valid_period == 0:\n",
    "                model.eval()\n",
    "                \n",
    "                with torch.no_grad():                    \n",
    "                    for (source, target), _ in valid_iter:\n",
    "                        mask = (source != PAD_INDEX).type(torch.uint8)\n",
    "\n",
    "                        y_pred = model(input_ids=source, \n",
    "                                       attention_mask=mask)\n",
    "\n",
    "                        \n",
    "                        loss = torch.nn.CrossEntropyLoss()(y_pred, target)\n",
    "\n",
    "                        \n",
    "                        valid_loss += loss.item()\n",
    "\n",
    "                # Store train and validation loss history\n",
    "                train_loss = train_loss / valid_period\n",
    "                valid_loss = valid_loss / len(valid_iter)\n",
    "                train_loss_list.append(train_loss)\n",
    "                valid_loss_list.append(valid_loss)\n",
    "                global_steps_list.append(global_step)\n",
    "\n",
    "                # print summary\n",
    "                print('Epoch [{}/{}], global step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_iter),\n",
    "                              train_loss, valid_loss))\n",
    "                \n",
    "                # checkpoint\n",
    "                if best_valid_loss > valid_loss:\n",
    "                    best_valid_loss = valid_loss\n",
    "                    save_checkpoint(output_path + '/model.pkl', model, best_valid_loss)\n",
    "                    save_metrics(output_path + '/metric.pkl', train_loss_list, valid_loss_list, global_steps_list)\n",
    "                        \n",
    "                train_loss = 0.0                \n",
    "                valid_loss = 0.0\n",
    "                model.train()\n",
    "    \n",
    "    save_metrics(output_path + '/metric.pkl', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    print('Training done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mtZeTdaf1evj",
    "outputId": "25b97f17-d99b-4f99-f274-7b602ae4630c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "batch_no [1004/250]: training_loss: tensor(0.5296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1005/250]: training_loss: tensor(0.5891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/250]: training_loss: tensor(0.3657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/250]: training_loss: tensor(0.6648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/250]: training_loss: tensor(0.3850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/250]: training_loss: tensor(0.4925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/250]: training_loss: tensor(0.7466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/250]: training_loss: tensor(0.4771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/250]: training_loss: tensor(0.4523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/250]: training_loss: tensor(0.3404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/250]: training_loss: tensor(0.2125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/250]: training_loss: tensor(0.2680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/250]: training_loss: tensor(0.5297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/250]: training_loss: tensor(0.6540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/250]: training_loss: tensor(0.2556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1019/250]: training_loss: tensor(0.7543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/250]: training_loss: tensor(0.4614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/250]: training_loss: tensor(0.3325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/250]: training_loss: tensor(0.3883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1023/250]: training_loss: tensor(0.3481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/250]: training_loss: tensor(0.4617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/250]: training_loss: tensor(0.4741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/250]: training_loss: tensor(0.3384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/250]: training_loss: tensor(0.5978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/250]: training_loss: tensor(0.7446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/250]: training_loss: tensor(0.5580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/250]: training_loss: tensor(0.7320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/250]: training_loss: tensor(0.3908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/250]: training_loss: tensor(0.4736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/250]: training_loss: tensor(0.5167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/250]: training_loss: tensor(0.4872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/250]: training_loss: tensor(0.3770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/250]: training_loss: tensor(0.6871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/250]: training_loss: tensor(0.4476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/250]: training_loss: tensor(0.7447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/250]: training_loss: tensor(0.3933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1040/250]: training_loss: tensor(0.3368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/250]: training_loss: tensor(0.3011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/250]: training_loss: tensor(0.4743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1043/250]: training_loss: tensor(0.4348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/250]: training_loss: tensor(1.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/250]: training_loss: tensor(0.2426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/250]: training_loss: tensor(0.5569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/250]: training_loss: tensor(0.4872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/250]: training_loss: tensor(0.4884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/250]: training_loss: tensor(0.5844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/250]: training_loss: tensor(0.4137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/250]: training_loss: tensor(0.6777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/250]: training_loss: tensor(0.6852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/250]: training_loss: tensor(0.5826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/250]: training_loss: tensor(0.5964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/250]: training_loss: tensor(0.3447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/250]: training_loss: tensor(0.7243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/250]: training_loss: tensor(0.4387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/250]: training_loss: tensor(0.5758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/250]: training_loss: tensor(0.4452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/250]: training_loss: tensor(0.4189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/250]: training_loss: tensor(0.5490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/250]: training_loss: tensor(0.3251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/250]: training_loss: tensor(0.5652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/250]: training_loss: tensor(0.4953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/250]: training_loss: tensor(0.5938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/250]: training_loss: tensor(0.4634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/250]: training_loss: tensor(0.4713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/250]: training_loss: tensor(0.5237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/250]: training_loss: tensor(0.5005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/250]: training_loss: tensor(0.6794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/250]: training_loss: tensor(0.4915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/250]: training_loss: tensor(0.5496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/250]: training_loss: tensor(0.6131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/250]: training_loss: tensor(0.3243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/250]: training_loss: tensor(0.3074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/250]: training_loss: tensor(0.4804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/250]: training_loss: tensor(0.5544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/250]: training_loss: tensor(0.6506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/250]: training_loss: tensor(0.4154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/250]: training_loss: tensor(0.3015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/250]: training_loss: tensor(0.2798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/250]: training_loss: tensor(0.6200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/250]: training_loss: tensor(0.4494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/250]: training_loss: tensor(0.2579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/250]: training_loss: tensor(0.4079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/250]: training_loss: tensor(0.3724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/250]: training_loss: tensor(0.6837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/250]: training_loss: tensor(0.5623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/250]: training_loss: tensor(0.4060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/250]: training_loss: tensor(0.1917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/250]: training_loss: tensor(0.6819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/250]: training_loss: tensor(0.2192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/250]: training_loss: tensor(0.5155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/250]: training_loss: tensor(0.6850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/250]: training_loss: tensor(0.2998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/250]: training_loss: tensor(0.4631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/250]: training_loss: tensor(0.5151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/250]: training_loss: tensor(0.3370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/250]: training_loss: tensor(0.5367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1100/250]: training_loss: tensor(0.5698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/250]: training_loss: tensor(0.3939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/250]: training_loss: tensor(0.4391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/250]: training_loss: tensor(0.4060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/250]: training_loss: tensor(0.2851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/250]: training_loss: tensor(0.5706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/250]: training_loss: tensor(0.6674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/250]: training_loss: tensor(0.4766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/250]: training_loss: tensor(0.4533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/250]: training_loss: tensor(0.2197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/250]: training_loss: tensor(0.4941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/250]: training_loss: tensor(0.3517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/250]: training_loss: tensor(0.3077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/250]: training_loss: tensor(0.2898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/250]: training_loss: tensor(0.4386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/250]: training_loss: tensor(0.4764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/250]: training_loss: tensor(0.3893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/250]: training_loss: tensor(0.4970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/250]: training_loss: tensor(0.2879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/250]: training_loss: tensor(0.4230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/250]: training_loss: tensor(0.3583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/250]: training_loss: tensor(0.5826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/250]: training_loss: tensor(0.5068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/250]: training_loss: tensor(0.3911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/250]: training_loss: tensor(0.5272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/250]: training_loss: tensor(0.1892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/250]: training_loss: tensor(0.4030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/250]: training_loss: tensor(0.5641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/250]: training_loss: tensor(0.2880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1129/250]: training_loss: tensor(0.4905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/250]: training_loss: tensor(0.2254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/250]: training_loss: tensor(0.3299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/250]: training_loss: tensor(0.1990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/250]: training_loss: tensor(0.3177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/250]: training_loss: tensor(0.3996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/250]: training_loss: tensor(0.2974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/250]: training_loss: tensor(0.6242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/250]: training_loss: tensor(0.4914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/250]: training_loss: tensor(0.4433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/250]: training_loss: tensor(0.4241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/250]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/250]: training_loss: tensor(0.4170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/250]: training_loss: tensor(0.3091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/250]: training_loss: tensor(0.6258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/250]: training_loss: tensor(0.2458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/250]: training_loss: tensor(0.5107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/250]: training_loss: tensor(0.3935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/250]: training_loss: tensor(0.3108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/250]: training_loss: tensor(0.2980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/250]: training_loss: tensor(0.4833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/250]: training_loss: tensor(0.3869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/250]: training_loss: tensor(0.2807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/250]: training_loss: tensor(0.6006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/250]: training_loss: tensor(0.4239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/250]: training_loss: tensor(0.3697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/250]: training_loss: tensor(0.4954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/250]: training_loss: tensor(0.3059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/250]: training_loss: tensor(0.2709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/250]: training_loss: tensor(0.1762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/250]: training_loss: tensor(0.6726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/250]: training_loss: tensor(0.2360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/250]: training_loss: tensor(0.4414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/250]: training_loss: tensor(0.1873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/250]: training_loss: tensor(0.4041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/250]: training_loss: tensor(0.3515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/250]: training_loss: tensor(0.2320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/250]: training_loss: tensor(0.3757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/250]: training_loss: tensor(0.2416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/250]: training_loss: tensor(0.3773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/250]: training_loss: tensor(0.5023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/250]: training_loss: tensor(0.2392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/250]: training_loss: tensor(0.3992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/250]: training_loss: tensor(0.3055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/250]: training_loss: tensor(0.5679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/250]: training_loss: tensor(0.2587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/250]: training_loss: tensor(0.1635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/250]: training_loss: tensor(0.1999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/250]: training_loss: tensor(0.4088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/250]: training_loss: tensor(0.1815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/250]: training_loss: tensor(0.2579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/250]: training_loss: tensor(0.1999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/250]: training_loss: tensor(0.2022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/250]: training_loss: tensor(1.1026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/250]: training_loss: tensor(0.3612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/250]: training_loss: tensor(0.1224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/250]: training_loss: tensor(0.5017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/250]: training_loss: tensor(0.5249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/250]: training_loss: tensor(0.2190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/250]: training_loss: tensor(0.3400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/250]: training_loss: tensor(0.6253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/250]: training_loss: tensor(0.1736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/250]: training_loss: tensor(0.3716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/250]: training_loss: tensor(0.3397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/250]: training_loss: tensor(0.1953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/250]: training_loss: tensor(0.1953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/250]: training_loss: tensor(0.8009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/250]: training_loss: tensor(0.3187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/250]: training_loss: tensor(0.6252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/250]: training_loss: tensor(1.1171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/250]: training_loss: tensor(0.2273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/250]: training_loss: tensor(0.7610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/250]: training_loss: tensor(1.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/250]: training_loss: tensor(0.5549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/250]: training_loss: tensor(0.7626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/250]: training_loss: tensor(0.4005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/250]: training_loss: tensor(0.3305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/250]: training_loss: tensor(0.4533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/250]: training_loss: tensor(0.3048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/250]: training_loss: tensor(0.6016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/250]: training_loss: tensor(0.2925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/250]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/250]: training_loss: tensor(0.3939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/250]: training_loss: tensor(0.3106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1213/250]: training_loss: tensor(0.3651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/250]: training_loss: tensor(0.2979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1215/250]: training_loss: tensor(0.4430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/250]: training_loss: tensor(0.5665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/250]: training_loss: tensor(0.3935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/250]: training_loss: tensor(0.6845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/250]: training_loss: tensor(0.3684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/250]: training_loss: tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/250]: training_loss: tensor(0.5340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/250]: training_loss: tensor(0.2660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/250]: training_loss: tensor(0.3080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/250]: training_loss: tensor(0.1931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/250]: training_loss: tensor(0.2518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/250]: training_loss: tensor(0.3311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/250]: training_loss: tensor(0.9487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/250]: training_loss: tensor(0.3397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/250]: training_loss: tensor(0.3675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/250]: training_loss: tensor(0.4821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/250]: training_loss: tensor(0.2528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/250]: training_loss: tensor(0.6574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/250]: training_loss: tensor(0.4461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/250]: training_loss: tensor(0.5458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/250]: training_loss: tensor(0.5765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/250]: training_loss: tensor(0.7371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/250]: training_loss: tensor(0.6452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/250]: training_loss: tensor(0.1989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/250]: training_loss: tensor(0.1538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/250]: training_loss: tensor(0.1459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/250]: training_loss: tensor(0.5996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/250]: training_loss: tensor(0.3006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/250]: training_loss: tensor(0.2381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/250]: training_loss: tensor(0.3011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/250]: training_loss: tensor(0.4738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/250]: training_loss: tensor(0.3631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/250]: training_loss: tensor(0.3524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/250]: training_loss: tensor(0.3504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/250]: training_loss: tensor(0.2377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/250]: training_loss: tensor(0.1988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/250]: training_loss: tensor(0.4187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/250]: training_loss: tensor(0.4928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/250]: training_loss: tensor(0.5224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/250]: training_loss: tensor(0.4534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/250]: training_loss: tensor(0.3672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/250]: training_loss: tensor(0.1512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/250]: training_loss: tensor(0.5548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/250]: training_loss: tensor(0.6284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/250]: training_loss: tensor(0.2293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/250]: training_loss: tensor(0.3568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/250]: training_loss: tensor(0.3193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/250]: training_loss: tensor(0.2975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/250]: training_loss: tensor(0.3835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/250]: training_loss: tensor(0.5392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/250]: training_loss: tensor(0.5479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/250]: training_loss: tensor(0.2499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/250]: training_loss: tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/250]: training_loss: tensor(0.5401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/250]: training_loss: tensor(0.2407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/250]: training_loss: tensor(0.5887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/250]: training_loss: tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/250]: training_loss: tensor(0.1276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/250]: training_loss: tensor(0.4210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/250]: training_loss: tensor(0.1983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/250]: training_loss: tensor(0.9075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/250]: training_loss: tensor(0.7081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/250]: training_loss: tensor(0.2889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/250]: training_loss: tensor(0.3689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/250]: training_loss: tensor(0.8070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/250]: training_loss: tensor(0.3538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/250]: training_loss: tensor(0.3560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/250]: training_loss: tensor(0.1285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/250]: training_loss: tensor(0.2324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/250]: training_loss: tensor(0.3888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/250]: training_loss: tensor(0.7511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/250]: training_loss: tensor(0.5665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/250]: training_loss: tensor(0.3426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/250]: training_loss: tensor(0.4680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/250]: training_loss: tensor(0.4297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/250]: training_loss: tensor(0.5995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/250]: training_loss: tensor(0.3097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/250]: training_loss: tensor(0.2202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/250]: training_loss: tensor(0.1948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/250]: training_loss: tensor(0.2331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/250]: training_loss: tensor(0.7233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/250]: training_loss: tensor(0.7668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/250]: training_loss: tensor(0.3213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/250]: training_loss: tensor(0.4114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1299/250]: training_loss: tensor(0.6683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/250]: training_loss: tensor(0.5985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/250]: training_loss: tensor(0.4871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/250]: training_loss: tensor(0.3172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/250]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/250]: training_loss: tensor(0.3445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/250]: training_loss: tensor(0.2900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/250]: training_loss: tensor(0.2957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/250]: training_loss: tensor(0.1678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/250]: training_loss: tensor(0.3529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/250]: training_loss: tensor(0.4490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/250]: training_loss: tensor(0.4290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/250]: training_loss: tensor(0.2997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/250]: training_loss: tensor(0.3377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/250]: training_loss: tensor(0.6368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/250]: training_loss: tensor(0.4054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/250]: training_loss: tensor(0.2805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/250]: training_loss: tensor(0.3298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/250]: training_loss: tensor(0.5982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/250]: training_loss: tensor(0.2275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/250]: training_loss: tensor(0.4473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/250]: training_loss: tensor(0.6101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/250]: training_loss: tensor(0.2795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/250]: training_loss: tensor(0.2238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/250]: training_loss: tensor(0.6878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/250]: training_loss: tensor(0.3115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/250]: training_loss: tensor(0.6487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/250]: training_loss: tensor(0.4632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/250]: training_loss: tensor(0.1402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/250]: training_loss: tensor(0.3879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/250]: training_loss: tensor(0.2428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/250]: training_loss: tensor(0.2595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/250]: training_loss: tensor(0.3228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/250]: training_loss: tensor(0.6893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/250]: training_loss: tensor(0.4795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/250]: training_loss: tensor(0.5925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/250]: training_loss: tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/250]: training_loss: tensor(0.3856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/250]: training_loss: tensor(0.2856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/250]: training_loss: tensor(0.5567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/250]: training_loss: tensor(0.3858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/250]: training_loss: tensor(0.4069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/250]: training_loss: tensor(1.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/250]: training_loss: tensor(0.6611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/250]: training_loss: tensor(0.2347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/250]: training_loss: tensor(0.3557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/250]: training_loss: tensor(1.1254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/250]: training_loss: tensor(0.2997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/250]: training_loss: tensor(0.1905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/250]: training_loss: tensor(0.4866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/250]: training_loss: tensor(0.2638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/250]: training_loss: tensor(0.5470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/250]: training_loss: tensor(0.2540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/250]: training_loss: tensor(0.3891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/250]: training_loss: tensor(0.1935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/250]: training_loss: tensor(0.2524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/250]: training_loss: tensor(0.2921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/250]: training_loss: tensor(0.2486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/250]: training_loss: tensor(0.4019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/250]: training_loss: tensor(0.5030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/250]: training_loss: tensor(0.2735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/250]: training_loss: tensor(0.2008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1361/250]: training_loss: tensor(0.5558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/250]: training_loss: tensor(0.2786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1363/250]: training_loss: tensor(0.2306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/250]: training_loss: tensor(0.4581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/250]: training_loss: tensor(0.2183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/250]: training_loss: tensor(0.2147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/250]: training_loss: tensor(0.3341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/250]: training_loss: tensor(0.1535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/250]: training_loss: tensor(0.5221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/250]: training_loss: tensor(0.3416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/250]: training_loss: tensor(0.3022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/250]: training_loss: tensor(0.1717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/250]: training_loss: tensor(0.7287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/250]: training_loss: tensor(0.1448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/250]: training_loss: tensor(0.5206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/250]: training_loss: tensor(0.5166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/250]: training_loss: tensor(0.4844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/250]: training_loss: tensor(0.7062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/250]: training_loss: tensor(0.3102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/250]: training_loss: tensor(0.7029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/250]: training_loss: tensor(0.4185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/250]: training_loss: tensor(0.2130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/250]: training_loss: tensor(0.6665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/250]: training_loss: tensor(0.5686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1385/250]: training_loss: tensor(0.3301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1386/250]: training_loss: tensor(0.4130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1387/250]: training_loss: tensor(0.4095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1388/250]: training_loss: tensor(0.4363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1389/250]: training_loss: tensor(0.2375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1390/250]: training_loss: tensor(0.2869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1391/250]: training_loss: tensor(0.1364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1392/250]: training_loss: tensor(0.6079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1393/250]: training_loss: tensor(0.2760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1394/250]: training_loss: tensor(0.2906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1395/250]: training_loss: tensor(0.2487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1396/250]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1397/250]: training_loss: tensor(0.7153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1398/250]: training_loss: tensor(0.5381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1399/250]: training_loss: tensor(0.1932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1400/250]: training_loss: tensor(0.3367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1401/250]: training_loss: tensor(0.3058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1402/250]: training_loss: tensor(0.3005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1403/250]: training_loss: tensor(0.3055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1404/250]: training_loss: tensor(0.3331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1405/250]: training_loss: tensor(0.3539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1406/250]: training_loss: tensor(0.2714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1407/250]: training_loss: tensor(0.4559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1408/250]: training_loss: tensor(0.3446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1409/250]: training_loss: tensor(0.2768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1410/250]: training_loss: tensor(0.1091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1411/250]: training_loss: tensor(0.4003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1412/250]: training_loss: tensor(0.2836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1413/250]: training_loss: tensor(0.4030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1414/250]: training_loss: tensor(0.5670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1415/250]: training_loss: tensor(0.2065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1416/250]: training_loss: tensor(0.1795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1417/250]: training_loss: tensor(0.3989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1418/250]: training_loss: tensor(0.2174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1419/250]: training_loss: tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1420/250]: training_loss: tensor(0.2770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1421/250]: training_loss: tensor(0.2181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1422/250]: training_loss: tensor(0.7274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1423/250]: training_loss: tensor(0.5835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1424/250]: training_loss: tensor(0.4918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1425/250]: training_loss: tensor(0.4726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1426/250]: training_loss: tensor(0.3063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1427/250]: training_loss: tensor(0.3672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1428/250]: training_loss: tensor(0.2191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1429/250]: training_loss: tensor(0.5474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1430/250]: training_loss: tensor(0.1900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1431/250]: training_loss: tensor(0.5240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1432/250]: training_loss: tensor(0.4003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1433/250]: training_loss: tensor(0.3949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1434/250]: training_loss: tensor(0.2626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1435/250]: training_loss: tensor(0.5059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1436/250]: training_loss: tensor(0.2968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1437/250]: training_loss: tensor(0.3170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1438/250]: training_loss: tensor(0.1647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1439/250]: training_loss: tensor(0.3495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1440/250]: training_loss: tensor(0.2486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1441/250]: training_loss: tensor(0.2349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1442/250]: training_loss: tensor(0.3575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1443/250]: training_loss: tensor(0.2367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1444/250]: training_loss: tensor(0.1739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1445/250]: training_loss: tensor(0.2882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1446/250]: training_loss: tensor(0.4284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1447/250]: training_loss: tensor(0.6038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1448/250]: training_loss: tensor(0.5215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1449/250]: training_loss: tensor(0.1251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1450/250]: training_loss: tensor(0.5251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1451/250]: training_loss: tensor(0.3274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1452/250]: training_loss: tensor(0.6810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1453/250]: training_loss: tensor(0.1971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1454/250]: training_loss: tensor(0.2345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1455/250]: training_loss: tensor(0.2688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1456/250]: training_loss: tensor(0.2073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1457/250]: training_loss: tensor(0.3304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1458/250]: training_loss: tensor(0.4176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1459/250]: training_loss: tensor(0.5765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1460/250]: training_loss: tensor(0.1469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1461/250]: training_loss: tensor(0.4736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1462/250]: training_loss: tensor(0.6683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1463/250]: training_loss: tensor(0.4007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1464/250]: training_loss: tensor(0.2829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1465/250]: training_loss: tensor(0.7539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1466/250]: training_loss: tensor(0.2502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1467/250]: training_loss: tensor(0.1747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1468/250]: training_loss: tensor(0.1969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1469/250]: training_loss: tensor(0.6751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1470/250]: training_loss: tensor(0.2322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1471/250]: training_loss: tensor(1.0317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1472/250]: training_loss: tensor(0.3235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1473/250]: training_loss: tensor(0.8839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1474/250]: training_loss: tensor(0.5886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1475/250]: training_loss: tensor(0.4115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1476/250]: training_loss: tensor(0.2507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1477/250]: training_loss: tensor(0.7274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1478/250]: training_loss: tensor(0.1942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1479/250]: training_loss: tensor(0.4322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1480/250]: training_loss: tensor(0.3717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1481/250]: training_loss: tensor(0.2228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1482/250]: training_loss: tensor(0.6073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1483/250]: training_loss: tensor(0.3866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1484/250]: training_loss: tensor(0.2185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1485/250]: training_loss: tensor(0.4310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1486/250]: training_loss: tensor(0.5356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1487/250]: training_loss: tensor(0.8838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1488/250]: training_loss: tensor(0.1331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1489/250]: training_loss: tensor(0.3200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1490/250]: training_loss: tensor(0.2169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1491/250]: training_loss: tensor(0.3156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1492/250]: training_loss: tensor(0.3991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1493/250]: training_loss: tensor(0.1112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1494/250]: training_loss: tensor(0.2922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1495/250]: training_loss: tensor(0.5188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1496/250]: training_loss: tensor(0.1980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1497/250]: training_loss: tensor(0.6528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1498/250]: training_loss: tensor(0.2782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1499/250]: training_loss: tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1500/250]: training_loss: tensor(0.1730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1501/250]: training_loss: tensor(0.6503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1502/250]: training_loss: tensor(0.3495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1503/250]: training_loss: tensor(0.1375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1504/250]: training_loss: tensor(0.2762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1505/250]: training_loss: tensor(0.4227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1506/250]: training_loss: tensor(0.2794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1507/250]: training_loss: tensor(0.5681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1508/250]: training_loss: tensor(0.4067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1509/250]: training_loss: tensor(0.1587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1510/250]: training_loss: tensor(0.3981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1511/250]: training_loss: tensor(0.4312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1512/250]: training_loss: tensor(0.4143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1513/250]: training_loss: tensor(0.2064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1514/250]: training_loss: tensor(0.2016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1515/250]: training_loss: tensor(0.1571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1516/250]: training_loss: tensor(0.1973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1517/250]: training_loss: tensor(0.2564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1518/250]: training_loss: tensor(0.1944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1519/250]: training_loss: tensor(0.6161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1520/250]: training_loss: tensor(0.4566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1521/250]: training_loss: tensor(0.6024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1522/250]: training_loss: tensor(0.2070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1523/250]: training_loss: tensor(0.5634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1524/250]: training_loss: tensor(0.4580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1525/250]: training_loss: tensor(0.5742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1526/250]: training_loss: tensor(0.6164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1527/250]: training_loss: tensor(0.2609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1528/250]: training_loss: tensor(0.2310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1529/250]: training_loss: tensor(0.3483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1530/250]: training_loss: tensor(0.3948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1531/250]: training_loss: tensor(0.4984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1532/250]: training_loss: tensor(0.3352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1533/250]: training_loss: tensor(0.3055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1534/250]: training_loss: tensor(0.1862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1535/250]: training_loss: tensor(0.5305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1536/250]: training_loss: tensor(0.2993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1537/250]: training_loss: tensor(0.2450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1538/250]: training_loss: tensor(0.3825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1539/250]: training_loss: tensor(0.2922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1540/250]: training_loss: tensor(0.3863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1541/250]: training_loss: tensor(0.4090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1542/250]: training_loss: tensor(0.5424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1543/250]: training_loss: tensor(0.2030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1544/250]: training_loss: tensor(0.3996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1545/250]: training_loss: tensor(0.3960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1546/250]: training_loss: tensor(0.4367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1547/250]: training_loss: tensor(0.9639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1548/250]: training_loss: tensor(0.4526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1549/250]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1550/250]: training_loss: tensor(0.1676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1551/250]: training_loss: tensor(0.3285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1552/250]: training_loss: tensor(0.5634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1553/250]: training_loss: tensor(0.4214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1554/250]: training_loss: tensor(0.3024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1555/250]: training_loss: tensor(0.4118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1556/250]: training_loss: tensor(0.6340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1557/250]: training_loss: tensor(0.2749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1558/250]: training_loss: tensor(0.1471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1559/250]: training_loss: tensor(0.2593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1560/250]: training_loss: tensor(0.4425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1561/250]: training_loss: tensor(0.5846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1562/250]: training_loss: tensor(0.3801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1563/250]: training_loss: tensor(0.2861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1564/250]: training_loss: tensor(0.7907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1565/250]: training_loss: tensor(0.2634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1566/250]: training_loss: tensor(0.1697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1567/250]: training_loss: tensor(0.3364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1568/250]: training_loss: tensor(0.3717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1569/250]: training_loss: tensor(0.4513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1570/250]: training_loss: tensor(0.7981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1571/250]: training_loss: tensor(0.1662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1572/250]: training_loss: tensor(0.2669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1573/250]: training_loss: tensor(0.3105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1574/250]: training_loss: tensor(0.4155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1575/250]: training_loss: tensor(0.4005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1576/250]: training_loss: tensor(0.6675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1577/250]: training_loss: tensor(0.3004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1578/250]: training_loss: tensor(0.1664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1579/250]: training_loss: tensor(0.3469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1580/250]: training_loss: tensor(0.2932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1581/250]: training_loss: tensor(0.4064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1582/250]: training_loss: tensor(0.1853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1583/250]: training_loss: tensor(0.1982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1584/250]: training_loss: tensor(0.4736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1585/250]: training_loss: tensor(0.1668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1586/250]: training_loss: tensor(0.3710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1587/250]: training_loss: tensor(0.1829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1588/250]: training_loss: tensor(0.1606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1589/250]: training_loss: tensor(0.4350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1590/250]: training_loss: tensor(0.4938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1591/250]: training_loss: tensor(0.3373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1592/250]: training_loss: tensor(0.1668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1593/250]: training_loss: tensor(0.1225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1594/250]: training_loss: tensor(0.1518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1595/250]: training_loss: tensor(0.2392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1596/250]: training_loss: tensor(0.3272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1597/250]: training_loss: tensor(0.3203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1598/250]: training_loss: tensor(0.5551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1599/250]: training_loss: tensor(0.1789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1600/250]: training_loss: tensor(0.1266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1601/250]: training_loss: tensor(0.5405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1602/250]: training_loss: tensor(0.2353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1603/250]: training_loss: tensor(0.1887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1604/250]: training_loss: tensor(0.2324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1605/250]: training_loss: tensor(0.6703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1606/250]: training_loss: tensor(0.3958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1607/250]: training_loss: tensor(0.8816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1608/250]: training_loss: tensor(0.2878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1609/250]: training_loss: tensor(0.1169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1610/250]: training_loss: tensor(0.1881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1611/250]: training_loss: tensor(0.6630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1612/250]: training_loss: tensor(0.1953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1613/250]: training_loss: tensor(0.2802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1614/250]: training_loss: tensor(0.3137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1615/250]: training_loss: tensor(0.3382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1616/250]: training_loss: tensor(0.4001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1617/250]: training_loss: tensor(0.3725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1618/250]: training_loss: tensor(0.3714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1619/250]: training_loss: tensor(0.3834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1620/250]: training_loss: tensor(0.4660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1621/250]: training_loss: tensor(0.2366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1622/250]: training_loss: tensor(0.3822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1623/250]: training_loss: tensor(0.7039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1624/250]: training_loss: tensor(0.5873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1625/250]: training_loss: tensor(0.4569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1626/250]: training_loss: tensor(0.2409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1627/250]: training_loss: tensor(0.5256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1628/250]: training_loss: tensor(0.6672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1629/250]: training_loss: tensor(0.2231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1630/250]: training_loss: tensor(0.2790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1631/250]: training_loss: tensor(0.2256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1632/250]: training_loss: tensor(0.3979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1633/250]: training_loss: tensor(0.7237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1634/250]: training_loss: tensor(0.2979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1635/250]: training_loss: tensor(0.1681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1636/250]: training_loss: tensor(0.3652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1637/250]: training_loss: tensor(0.3105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1638/250]: training_loss: tensor(0.7604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1639/250]: training_loss: tensor(0.8030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1640/250]: training_loss: tensor(0.3944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1641/250]: training_loss: tensor(1.2207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1642/250]: training_loss: tensor(0.3954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1643/250]: training_loss: tensor(0.4905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1644/250]: training_loss: tensor(0.1994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1645/250]: training_loss: tensor(1.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1646/250]: training_loss: tensor(0.1866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1647/250]: training_loss: tensor(0.2316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1648/250]: training_loss: tensor(0.2842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1649/250]: training_loss: tensor(0.2053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1650/250]: training_loss: tensor(0.1632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1651/250]: training_loss: tensor(0.4174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1652/250]: training_loss: tensor(0.6610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1653/250]: training_loss: tensor(0.1782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1654/250]: training_loss: tensor(0.3273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1655/250]: training_loss: tensor(0.5069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1656/250]: training_loss: tensor(0.4516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1657/250]: training_loss: tensor(0.5101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1658/250]: training_loss: tensor(0.5539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1659/250]: training_loss: tensor(0.8059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1660/250]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1661/250]: training_loss: tensor(0.3937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1662/250]: training_loss: tensor(0.2069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1663/250]: training_loss: tensor(0.1460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1664/250]: training_loss: tensor(0.4429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1665/250]: training_loss: tensor(0.2799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1666/250]: training_loss: tensor(0.4501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1667/250]: training_loss: tensor(0.4926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1668/250]: training_loss: tensor(0.2848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1669/250]: training_loss: tensor(0.2548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1670/250]: training_loss: tensor(0.3206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1671/250]: training_loss: tensor(0.3466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1672/250]: training_loss: tensor(0.1176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1673/250]: training_loss: tensor(0.2526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1674/250]: training_loss: tensor(0.3999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1675/250]: training_loss: tensor(0.3117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1676/250]: training_loss: tensor(0.4473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1677/250]: training_loss: tensor(0.6591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1678/250]: training_loss: tensor(0.4887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1679/250]: training_loss: tensor(0.4173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1680/250]: training_loss: tensor(0.5935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1681/250]: training_loss: tensor(0.2615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1682/250]: training_loss: tensor(0.3656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1683/250]: training_loss: tensor(0.3251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1684/250]: training_loss: tensor(0.5439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1685/250]: training_loss: tensor(0.4253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1686/250]: training_loss: tensor(0.4648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1687/250]: training_loss: tensor(0.2464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1688/250]: training_loss: tensor(0.3455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1689/250]: training_loss: tensor(0.3030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1690/250]: training_loss: tensor(0.2284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1691/250]: training_loss: tensor(0.8946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1692/250]: training_loss: tensor(0.4431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1693/250]: training_loss: tensor(0.1920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1694/250]: training_loss: tensor(0.1880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1695/250]: training_loss: tensor(0.5426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1696/250]: training_loss: tensor(0.1712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1697/250]: training_loss: tensor(0.2723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1698/250]: training_loss: tensor(0.3068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1699/250]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1700/250]: training_loss: tensor(0.4048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1701/250]: training_loss: tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1702/250]: training_loss: tensor(0.7666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1703/250]: training_loss: tensor(0.2521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1704/250]: training_loss: tensor(0.5156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1705/250]: training_loss: tensor(0.3845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1706/250]: training_loss: tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1707/250]: training_loss: tensor(0.7804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1708/250]: training_loss: tensor(0.7484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1709/250]: training_loss: tensor(0.2062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1710/250]: training_loss: tensor(0.3207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1711/250]: training_loss: tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1712/250]: training_loss: tensor(0.3399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1713/250]: training_loss: tensor(0.4047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1714/250]: training_loss: tensor(0.3148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1715/250]: training_loss: tensor(0.2227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1716/250]: training_loss: tensor(0.1551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1717/250]: training_loss: tensor(0.4630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1718/250]: training_loss: tensor(0.7820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1719/250]: training_loss: tensor(0.1837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1720/250]: training_loss: tensor(0.2951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1721/250]: training_loss: tensor(0.2188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1722/250]: training_loss: tensor(1.1502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1723/250]: training_loss: tensor(0.5381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1724/250]: training_loss: tensor(0.2565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1725/250]: training_loss: tensor(0.4754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1726/250]: training_loss: tensor(0.7527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1727/250]: training_loss: tensor(0.6428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1728/250]: training_loss: tensor(0.4847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1729/250]: training_loss: tensor(0.2502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1730/250]: training_loss: tensor(0.2165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1731/250]: training_loss: tensor(0.1257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1732/250]: training_loss: tensor(0.6398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1733/250]: training_loss: tensor(0.1832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1734/250]: training_loss: tensor(0.3155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1735/250]: training_loss: tensor(0.4414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1736/250]: training_loss: tensor(0.4213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1737/250]: training_loss: tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1738/250]: training_loss: tensor(0.3100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1739/250]: training_loss: tensor(0.6756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1740/250]: training_loss: tensor(0.4805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1741/250]: training_loss: tensor(0.4107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1742/250]: training_loss: tensor(0.1934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1743/250]: training_loss: tensor(0.4602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1744/250]: training_loss: tensor(0.5265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1745/250]: training_loss: tensor(0.3291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1746/250]: training_loss: tensor(0.2893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1747/250]: training_loss: tensor(0.3556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1748/250]: training_loss: tensor(0.5627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1749/250]: training_loss: tensor(0.2476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1750/250]: training_loss: tensor(0.3603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1751/250]: training_loss: tensor(0.3975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1752/250]: training_loss: tensor(0.4434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1753/250]: training_loss: tensor(0.3787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1754/250]: training_loss: tensor(0.6240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1755/250]: training_loss: tensor(0.2120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1756/250]: training_loss: tensor(0.4297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1757/250]: training_loss: tensor(0.5962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1758/250]: training_loss: tensor(0.9749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1759/250]: training_loss: tensor(0.4780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1760/250]: training_loss: tensor(0.7223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1761/250]: training_loss: tensor(0.5573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1762/250]: training_loss: tensor(0.1501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1763/250]: training_loss: tensor(0.4399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1764/250]: training_loss: tensor(0.3023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1765/250]: training_loss: tensor(0.6384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1766/250]: training_loss: tensor(0.3608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1767/250]: training_loss: tensor(0.1511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1768/250]: training_loss: tensor(0.2777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1769/250]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1770/250]: training_loss: tensor(0.4532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1771/250]: training_loss: tensor(0.1338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1772/250]: training_loss: tensor(0.1777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1773/250]: training_loss: tensor(0.6302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1774/250]: training_loss: tensor(0.3594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1775/250]: training_loss: tensor(0.3971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1776/250]: training_loss: tensor(0.8849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1777/250]: training_loss: tensor(0.3197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1778/250]: training_loss: tensor(0.3211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1779/250]: training_loss: tensor(0.2948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1780/250]: training_loss: tensor(0.4508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1781/250]: training_loss: tensor(0.4608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1782/250]: training_loss: tensor(0.4322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1783/250]: training_loss: tensor(0.3874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1784/250]: training_loss: tensor(0.2601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1785/250]: training_loss: tensor(0.2284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1786/250]: training_loss: tensor(0.3785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1787/250]: training_loss: tensor(0.3549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1788/250]: training_loss: tensor(0.3641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1789/250]: training_loss: tensor(0.5506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1790/250]: training_loss: tensor(0.3182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1791/250]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1792/250]: training_loss: tensor(0.5071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1793/250]: training_loss: tensor(0.6232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1794/250]: training_loss: tensor(0.4811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1795/250]: training_loss: tensor(0.3316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1796/250]: training_loss: tensor(0.2499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1797/250]: training_loss: tensor(0.2146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1798/250]: training_loss: tensor(0.5415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1799/250]: training_loss: tensor(0.7858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1800/250]: training_loss: tensor(0.3488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1801/250]: training_loss: tensor(0.9389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1802/250]: training_loss: tensor(0.3868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1803/250]: training_loss: tensor(0.1814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1804/250]: training_loss: tensor(0.3896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1805/250]: training_loss: tensor(0.5818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1806/250]: training_loss: tensor(0.1525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1807/250]: training_loss: tensor(0.2242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1808/250]: training_loss: tensor(0.1734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1809/250]: training_loss: tensor(0.2475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1810/250]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1811/250]: training_loss: tensor(0.1304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1812/250]: training_loss: tensor(0.3020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1813/250]: training_loss: tensor(0.5110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1814/250]: training_loss: tensor(0.6483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1815/250]: training_loss: tensor(0.4430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1816/250]: training_loss: tensor(0.2657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1817/250]: training_loss: tensor(0.3032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1818/250]: training_loss: tensor(0.2741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1819/250]: training_loss: tensor(0.3055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1820/250]: training_loss: tensor(0.3866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1821/250]: training_loss: tensor(0.2449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1822/250]: training_loss: tensor(0.4001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1823/250]: training_loss: tensor(0.4292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1824/250]: training_loss: tensor(0.4531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1825/250]: training_loss: tensor(0.2316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1826/250]: training_loss: tensor(0.2640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1827/250]: training_loss: tensor(0.2963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1828/250]: training_loss: tensor(0.2698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1829/250]: training_loss: tensor(0.1281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1830/250]: training_loss: tensor(0.2025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1831/250]: training_loss: tensor(0.1518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1832/250]: training_loss: tensor(0.3186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1833/250]: training_loss: tensor(0.2856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1834/250]: training_loss: tensor(0.1643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1835/250]: training_loss: tensor(0.4574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1836/250]: training_loss: tensor(0.5046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1837/250]: training_loss: tensor(0.2821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1838/250]: training_loss: tensor(0.1941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1839/250]: training_loss: tensor(0.3363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1840/250]: training_loss: tensor(0.4287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1841/250]: training_loss: tensor(0.3249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1842/250]: training_loss: tensor(0.2473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1843/250]: training_loss: tensor(0.3050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1844/250]: training_loss: tensor(0.2611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1845/250]: training_loss: tensor(0.1453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1846/250]: training_loss: tensor(0.2518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1847/250]: training_loss: tensor(0.5865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1848/250]: training_loss: tensor(0.9147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1849/250]: training_loss: tensor(0.3007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1850/250]: training_loss: tensor(0.2879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1851/250]: training_loss: tensor(0.4591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1852/250]: training_loss: tensor(0.3339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1853/250]: training_loss: tensor(0.5040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1854/250]: training_loss: tensor(0.5298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1855/250]: training_loss: tensor(0.4272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1856/250]: training_loss: tensor(0.2974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1857/250]: training_loss: tensor(0.3723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1858/250]: training_loss: tensor(0.2426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1859/250]: training_loss: tensor(0.6669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1860/250]: training_loss: tensor(0.6465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1861/250]: training_loss: tensor(0.4416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1862/250]: training_loss: tensor(0.5019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1863/250]: training_loss: tensor(0.4070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1864/250]: training_loss: tensor(0.3243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1865/250]: training_loss: tensor(0.4992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1866/250]: training_loss: tensor(0.3055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1867/250]: training_loss: tensor(0.2774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1868/250]: training_loss: tensor(0.4520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1869/250]: training_loss: tensor(0.5401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1870/250]: training_loss: tensor(0.1106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1871/250]: training_loss: tensor(0.4221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1872/250]: training_loss: tensor(0.7077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1873/250]: training_loss: tensor(0.1726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1874/250]: training_loss: tensor(0.4855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1875/250]: training_loss: tensor(0.4243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1876/250]: training_loss: tensor(0.2157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1877/250]: training_loss: tensor(0.4477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1878/250]: training_loss: tensor(0.4587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1879/250]: training_loss: tensor(0.2542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1880/250]: training_loss: tensor(0.3217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1881/250]: training_loss: tensor(0.4563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1882/250]: training_loss: tensor(0.8455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1883/250]: training_loss: tensor(0.1368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1884/250]: training_loss: tensor(0.7612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1885/250]: training_loss: tensor(0.3769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1886/250]: training_loss: tensor(0.2848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1887/250]: training_loss: tensor(0.4170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1888/250]: training_loss: tensor(0.4425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1889/250]: training_loss: tensor(0.2426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1890/250]: training_loss: tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1891/250]: training_loss: tensor(0.4002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1892/250]: training_loss: tensor(0.2894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1893/250]: training_loss: tensor(0.4266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1894/250]: training_loss: tensor(0.3281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1895/250]: training_loss: tensor(1.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1896/250]: training_loss: tensor(0.4740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1897/250]: training_loss: tensor(0.2049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1898/250]: training_loss: tensor(0.8601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1899/250]: training_loss: tensor(0.2634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1900/250]: training_loss: tensor(0.5159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1901/250]: training_loss: tensor(0.2576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1902/250]: training_loss: tensor(0.5468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1903/250]: training_loss: tensor(0.4211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1904/250]: training_loss: tensor(0.3683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1905/250]: training_loss: tensor(0.6035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1906/250]: training_loss: tensor(0.3370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1907/250]: training_loss: tensor(0.3508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1908/250]: training_loss: tensor(0.1637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1909/250]: training_loss: tensor(0.5862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1910/250]: training_loss: tensor(0.2282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1911/250]: training_loss: tensor(0.3591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1912/250]: training_loss: tensor(0.2427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1913/250]: training_loss: tensor(0.3788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1914/250]: training_loss: tensor(0.2623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1915/250]: training_loss: tensor(0.3270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1916/250]: training_loss: tensor(0.3316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1917/250]: training_loss: tensor(0.6626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1918/250]: training_loss: tensor(0.3784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1919/250]: training_loss: tensor(0.6813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1920/250]: training_loss: tensor(0.2163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1921/250]: training_loss: tensor(0.5252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1922/250]: training_loss: tensor(0.5248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1923/250]: training_loss: tensor(0.5533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1924/250]: training_loss: tensor(0.1702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1925/250]: training_loss: tensor(0.6758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1926/250]: training_loss: tensor(0.4408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1927/250]: training_loss: tensor(0.4152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1928/250]: training_loss: tensor(0.3812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1929/250]: training_loss: tensor(0.3988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1930/250]: training_loss: tensor(0.7306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1931/250]: training_loss: tensor(0.8096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1932/250]: training_loss: tensor(0.3967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1933/250]: training_loss: tensor(0.3603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1934/250]: training_loss: tensor(0.3367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1935/250]: training_loss: tensor(0.4624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1936/250]: training_loss: tensor(0.8916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1937/250]: training_loss: tensor(0.7008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1938/250]: training_loss: tensor(0.2429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1939/250]: training_loss: tensor(0.5246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1940/250]: training_loss: tensor(0.3316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1941/250]: training_loss: tensor(0.4380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1942/250]: training_loss: tensor(0.5003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1943/250]: training_loss: tensor(0.5343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1944/250]: training_loss: tensor(0.2739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1945/250]: training_loss: tensor(0.3627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1946/250]: training_loss: tensor(0.4075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1947/250]: training_loss: tensor(0.4075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1948/250]: training_loss: tensor(1.0290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1949/250]: training_loss: tensor(0.2652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1950/250]: training_loss: tensor(0.5831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1951/250]: training_loss: tensor(0.2399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1952/250]: training_loss: tensor(0.5419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1953/250]: training_loss: tensor(0.3629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1954/250]: training_loss: tensor(0.3187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1955/250]: training_loss: tensor(0.4354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1956/250]: training_loss: tensor(0.1809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1957/250]: training_loss: tensor(0.4725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1958/250]: training_loss: tensor(0.2068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1959/250]: training_loss: tensor(0.3415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1960/250]: training_loss: tensor(0.2310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1961/250]: training_loss: tensor(0.5479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1962/250]: training_loss: tensor(0.2938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1963/250]: training_loss: tensor(0.4531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1964/250]: training_loss: tensor(0.5965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1965/250]: training_loss: tensor(0.1946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1966/250]: training_loss: tensor(0.1512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1967/250]: training_loss: tensor(0.7780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1968/250]: training_loss: tensor(0.4666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1969/250]: training_loss: tensor(0.2920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1970/250]: training_loss: tensor(0.5063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1971/250]: training_loss: tensor(0.6182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1972/250]: training_loss: tensor(0.2160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1973/250]: training_loss: tensor(0.5152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1974/250]: training_loss: tensor(0.2426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1975/250]: training_loss: tensor(0.2480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1976/250]: training_loss: tensor(0.7687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1977/250]: training_loss: tensor(0.8680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1978/250]: training_loss: tensor(0.4224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1979/250]: training_loss: tensor(0.8168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1980/250]: training_loss: tensor(0.3359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1981/250]: training_loss: tensor(0.1830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1982/250]: training_loss: tensor(0.7892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1983/250]: training_loss: tensor(0.5522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1984/250]: training_loss: tensor(0.3492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1985/250]: training_loss: tensor(0.3660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1986/250]: training_loss: tensor(0.3613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1987/250]: training_loss: tensor(0.4444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1988/250]: training_loss: tensor(0.3845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1989/250]: training_loss: tensor(0.2376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1990/250]: training_loss: tensor(0.1499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1991/250]: training_loss: tensor(0.2817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1992/250]: training_loss: tensor(0.3013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1993/250]: training_loss: tensor(0.2938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1994/250]: training_loss: tensor(0.2753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1995/250]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1996/250]: training_loss: tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1997/250]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1998/250]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1999/250]: training_loss: tensor(0.5020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [1/3], global step [2000/6000], Train Loss: 0.4740, Valid Loss: 0.3612\n",
      "batch_no [0/250]: training_loss: tensor(0.1745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1/250]: training_loss: tensor(0.1376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2/250]: training_loss: tensor(0.4143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3/250]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4/250]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [5/250]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [6/250]: training_loss: tensor(0.4148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [7/250]: training_loss: tensor(0.1918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [8/250]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [9/250]: training_loss: tensor(0.1934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [10/250]: training_loss: tensor(0.1472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [11/250]: training_loss: tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [12/250]: training_loss: tensor(0.1345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [13/250]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [14/250]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [15/250]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [16/250]: training_loss: tensor(0.2577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [17/250]: training_loss: tensor(0.3425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [18/250]: training_loss: tensor(0.1595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [19/250]: training_loss: tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [20/250]: training_loss: tensor(0.6739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [21/250]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [22/250]: training_loss: tensor(0.1521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [23/250]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [24/250]: training_loss: tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [25/250]: training_loss: tensor(0.1256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [26/250]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [27/250]: training_loss: tensor(0.3456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [28/250]: training_loss: tensor(0.4492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [29/250]: training_loss: tensor(0.3268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [30/250]: training_loss: tensor(0.3764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [31/250]: training_loss: tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [32/250]: training_loss: tensor(0.2815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [33/250]: training_loss: tensor(0.1326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [34/250]: training_loss: tensor(0.2153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [35/250]: training_loss: tensor(0.5211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [36/250]: training_loss: tensor(0.1361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [37/250]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [38/250]: training_loss: tensor(0.4760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [39/250]: training_loss: tensor(0.2542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [40/250]: training_loss: tensor(0.4926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [41/250]: training_loss: tensor(0.2900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [42/250]: training_loss: tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [43/250]: training_loss: tensor(0.2868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [44/250]: training_loss: tensor(0.1660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [45/250]: training_loss: tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [46/250]: training_loss: tensor(0.1204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [47/250]: training_loss: tensor(0.2119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [48/250]: training_loss: tensor(0.2947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [49/250]: training_loss: tensor(0.3475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [50/250]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [51/250]: training_loss: tensor(0.5630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [52/250]: training_loss: tensor(0.3748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [53/250]: training_loss: tensor(0.2960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [54/250]: training_loss: tensor(0.3918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [55/250]: training_loss: tensor(0.3502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [56/250]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [57/250]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [58/250]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [59/250]: training_loss: tensor(0.2647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [60/250]: training_loss: tensor(0.1820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [61/250]: training_loss: tensor(0.5158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [62/250]: training_loss: tensor(0.2352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [63/250]: training_loss: tensor(0.1763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [64/250]: training_loss: tensor(0.4336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [65/250]: training_loss: tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [66/250]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [67/250]: training_loss: tensor(0.1994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [68/250]: training_loss: tensor(0.1548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [69/250]: training_loss: tensor(0.2358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [70/250]: training_loss: tensor(0.1780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [71/250]: training_loss: tensor(0.3365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [72/250]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [73/250]: training_loss: tensor(0.4977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [74/250]: training_loss: tensor(0.0676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [75/250]: training_loss: tensor(0.5945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [76/250]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [77/250]: training_loss: tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [78/250]: training_loss: tensor(0.4753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [79/250]: training_loss: tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [80/250]: training_loss: tensor(0.4889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [81/250]: training_loss: tensor(0.4447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [82/250]: training_loss: tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [83/250]: training_loss: tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [84/250]: training_loss: tensor(0.1273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [85/250]: training_loss: tensor(0.4279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [86/250]: training_loss: tensor(0.1897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [87/250]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [88/250]: training_loss: tensor(0.1686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [89/250]: training_loss: tensor(0.6798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [90/250]: training_loss: tensor(0.4067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [91/250]: training_loss: tensor(0.2187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [92/250]: training_loss: tensor(0.1281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [93/250]: training_loss: tensor(0.3084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [94/250]: training_loss: tensor(0.1721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [95/250]: training_loss: tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [96/250]: training_loss: tensor(0.4790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [97/250]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [98/250]: training_loss: tensor(0.4373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [99/250]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [100/250]: training_loss: tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [101/250]: training_loss: tensor(0.3119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [102/250]: training_loss: tensor(0.3221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [103/250]: training_loss: tensor(0.1507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [104/250]: training_loss: tensor(0.1468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [105/250]: training_loss: tensor(0.5446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [106/250]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [107/250]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [108/250]: training_loss: tensor(0.1300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [109/250]: training_loss: tensor(0.5218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [110/250]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [111/250]: training_loss: tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [112/250]: training_loss: tensor(0.0686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [113/250]: training_loss: tensor(0.1755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [114/250]: training_loss: tensor(0.6295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [115/250]: training_loss: tensor(0.2887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [116/250]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [117/250]: training_loss: tensor(0.2103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [118/250]: training_loss: tensor(0.1389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [119/250]: training_loss: tensor(0.2122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [120/250]: training_loss: tensor(0.1798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [121/250]: training_loss: tensor(0.8414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [122/250]: training_loss: tensor(1.0355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [123/250]: training_loss: tensor(0.1974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [124/250]: training_loss: tensor(0.8725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [125/250]: training_loss: tensor(0.1817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [126/250]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [127/250]: training_loss: tensor(0.2179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [128/250]: training_loss: tensor(0.2605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [129/250]: training_loss: tensor(0.3548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [130/250]: training_loss: tensor(0.3543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [131/250]: training_loss: tensor(0.1612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [132/250]: training_loss: tensor(0.1386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [133/250]: training_loss: tensor(0.1641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [134/250]: training_loss: tensor(0.1611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [135/250]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [136/250]: training_loss: tensor(0.4366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [137/250]: training_loss: tensor(0.2794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [138/250]: training_loss: tensor(0.1185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [139/250]: training_loss: tensor(0.1543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [140/250]: training_loss: tensor(0.4719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [141/250]: training_loss: tensor(0.2047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [142/250]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [143/250]: training_loss: tensor(0.1365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [144/250]: training_loss: tensor(0.3529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [145/250]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [146/250]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [147/250]: training_loss: tensor(1.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [148/250]: training_loss: tensor(0.5180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [149/250]: training_loss: tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [150/250]: training_loss: tensor(0.1561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [151/250]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [152/250]: training_loss: tensor(0.4800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [153/250]: training_loss: tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [154/250]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [155/250]: training_loss: tensor(0.1093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [156/250]: training_loss: tensor(0.1797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [157/250]: training_loss: tensor(0.7513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [158/250]: training_loss: tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [159/250]: training_loss: tensor(0.2082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [160/250]: training_loss: tensor(0.1406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [161/250]: training_loss: tensor(0.4746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [162/250]: training_loss: tensor(0.1924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [163/250]: training_loss: tensor(0.1759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [164/250]: training_loss: tensor(0.2301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [165/250]: training_loss: tensor(0.1309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [166/250]: training_loss: tensor(0.4618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [167/250]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [168/250]: training_loss: tensor(0.2218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [169/250]: training_loss: tensor(0.5947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [170/250]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [171/250]: training_loss: tensor(0.4399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [172/250]: training_loss: tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [173/250]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [174/250]: training_loss: tensor(0.5456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [175/250]: training_loss: tensor(0.3349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [176/250]: training_loss: tensor(0.1698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [177/250]: training_loss: tensor(0.5655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [178/250]: training_loss: tensor(0.1307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [179/250]: training_loss: tensor(0.1186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [180/250]: training_loss: tensor(0.2310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [181/250]: training_loss: tensor(0.2787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [182/250]: training_loss: tensor(1.1546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [183/250]: training_loss: tensor(0.5567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [184/250]: training_loss: tensor(0.3888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [185/250]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [186/250]: training_loss: tensor(0.2688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [187/250]: training_loss: tensor(0.5592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [188/250]: training_loss: tensor(0.1416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [189/250]: training_loss: tensor(0.1562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [190/250]: training_loss: tensor(0.4851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [191/250]: training_loss: tensor(0.3415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [192/250]: training_loss: tensor(0.5132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [193/250]: training_loss: tensor(0.1214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [194/250]: training_loss: tensor(0.1936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [195/250]: training_loss: tensor(0.2134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [196/250]: training_loss: tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [197/250]: training_loss: tensor(0.3013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [198/250]: training_loss: tensor(0.4697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [199/250]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [200/250]: training_loss: tensor(0.4337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [201/250]: training_loss: tensor(0.3652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [202/250]: training_loss: tensor(0.3610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [203/250]: training_loss: tensor(0.3110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [204/250]: training_loss: tensor(0.5809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [205/250]: training_loss: tensor(0.3136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [206/250]: training_loss: tensor(0.3510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [207/250]: training_loss: tensor(0.2185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [208/250]: training_loss: tensor(0.3634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [209/250]: training_loss: tensor(0.2860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [210/250]: training_loss: tensor(0.5548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [211/250]: training_loss: tensor(0.6746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [212/250]: training_loss: tensor(0.2444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [213/250]: training_loss: tensor(0.5389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [214/250]: training_loss: tensor(0.7375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [215/250]: training_loss: tensor(0.1853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [216/250]: training_loss: tensor(0.1998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [217/250]: training_loss: tensor(0.6803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [218/250]: training_loss: tensor(0.1476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [219/250]: training_loss: tensor(0.5227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [220/250]: training_loss: tensor(0.3819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [221/250]: training_loss: tensor(0.3095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [222/250]: training_loss: tensor(0.1066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [223/250]: training_loss: tensor(0.5401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [224/250]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [225/250]: training_loss: tensor(0.3638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [226/250]: training_loss: tensor(0.6279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [227/250]: training_loss: tensor(0.2803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [228/250]: training_loss: tensor(0.6800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [229/250]: training_loss: tensor(0.5183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [230/250]: training_loss: tensor(0.2500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [231/250]: training_loss: tensor(0.3139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [232/250]: training_loss: tensor(0.2003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [233/250]: training_loss: tensor(0.7174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [234/250]: training_loss: tensor(0.3483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [235/250]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [236/250]: training_loss: tensor(0.6585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [237/250]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [238/250]: training_loss: tensor(0.3285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [239/250]: training_loss: tensor(0.2209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [240/250]: training_loss: tensor(0.4127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [241/250]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [242/250]: training_loss: tensor(0.2139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [243/250]: training_loss: tensor(0.1337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [244/250]: training_loss: tensor(0.3084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [245/250]: training_loss: tensor(0.7635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [246/250]: training_loss: tensor(0.2056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [247/250]: training_loss: tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [248/250]: training_loss: tensor(0.1874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [249/250]: training_loss: tensor(0.2076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [250/250]: training_loss: tensor(0.1670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [251/250]: training_loss: tensor(0.2852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [252/250]: training_loss: tensor(0.2682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [253/250]: training_loss: tensor(0.1833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [254/250]: training_loss: tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [255/250]: training_loss: tensor(0.1573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [256/250]: training_loss: tensor(0.4433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [257/250]: training_loss: tensor(0.2067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [258/250]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [259/250]: training_loss: tensor(0.2699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [260/250]: training_loss: tensor(0.4250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [261/250]: training_loss: tensor(0.1504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [262/250]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [263/250]: training_loss: tensor(0.1861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [264/250]: training_loss: tensor(0.1103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [265/250]: training_loss: tensor(0.6999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [266/250]: training_loss: tensor(0.3466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [267/250]: training_loss: tensor(0.1970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [268/250]: training_loss: tensor(0.2704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [269/250]: training_loss: tensor(0.6786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [270/250]: training_loss: tensor(0.1278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [271/250]: training_loss: tensor(0.7934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [272/250]: training_loss: tensor(0.1749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [273/250]: training_loss: tensor(0.4989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [274/250]: training_loss: tensor(0.2691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [275/250]: training_loss: tensor(0.1334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [276/250]: training_loss: tensor(0.3164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [277/250]: training_loss: tensor(0.3932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [278/250]: training_loss: tensor(0.5233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [279/250]: training_loss: tensor(0.1391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [280/250]: training_loss: tensor(0.2661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [281/250]: training_loss: tensor(0.6995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [282/250]: training_loss: tensor(0.3832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [283/250]: training_loss: tensor(0.3013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [284/250]: training_loss: tensor(0.7255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [285/250]: training_loss: tensor(0.3277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [286/250]: training_loss: tensor(0.1230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [287/250]: training_loss: tensor(0.6725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [288/250]: training_loss: tensor(0.3916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [289/250]: training_loss: tensor(0.4266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [290/250]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [291/250]: training_loss: tensor(0.2811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [292/250]: training_loss: tensor(0.6483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [293/250]: training_loss: tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [294/250]: training_loss: tensor(0.2021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [295/250]: training_loss: tensor(0.1363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [296/250]: training_loss: tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [297/250]: training_loss: tensor(0.4576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [298/250]: training_loss: tensor(0.4290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [299/250]: training_loss: tensor(0.4432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [300/250]: training_loss: tensor(0.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [301/250]: training_loss: tensor(0.2836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [302/250]: training_loss: tensor(0.5743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [303/250]: training_loss: tensor(0.4292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [304/250]: training_loss: tensor(0.7045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [305/250]: training_loss: tensor(0.1595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [306/250]: training_loss: tensor(0.1451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [307/250]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [308/250]: training_loss: tensor(0.1243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [309/250]: training_loss: tensor(0.4795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [310/250]: training_loss: tensor(0.2677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [311/250]: training_loss: tensor(0.2641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [312/250]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [313/250]: training_loss: tensor(0.3292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [314/250]: training_loss: tensor(0.2156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [315/250]: training_loss: tensor(0.7409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [316/250]: training_loss: tensor(0.1823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [317/250]: training_loss: tensor(0.3199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [318/250]: training_loss: tensor(0.1485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [319/250]: training_loss: tensor(0.3101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [320/250]: training_loss: tensor(0.3868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [321/250]: training_loss: tensor(0.2197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [322/250]: training_loss: tensor(0.2190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [323/250]: training_loss: tensor(0.1573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [324/250]: training_loss: tensor(0.3491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [325/250]: training_loss: tensor(0.2627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [326/250]: training_loss: tensor(0.3602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [327/250]: training_loss: tensor(0.1296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [328/250]: training_loss: tensor(0.2406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [329/250]: training_loss: tensor(0.1043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [330/250]: training_loss: tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [331/250]: training_loss: tensor(0.6345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [332/250]: training_loss: tensor(0.1965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [333/250]: training_loss: tensor(0.3380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [334/250]: training_loss: tensor(0.5064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [335/250]: training_loss: tensor(0.1856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [336/250]: training_loss: tensor(0.2290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [337/250]: training_loss: tensor(0.4699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [338/250]: training_loss: tensor(0.7105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [339/250]: training_loss: tensor(0.0621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [340/250]: training_loss: tensor(0.1918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [341/250]: training_loss: tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [342/250]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [343/250]: training_loss: tensor(0.2515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [344/250]: training_loss: tensor(0.1401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [345/250]: training_loss: tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [346/250]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [347/250]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [348/250]: training_loss: tensor(0.1493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [349/250]: training_loss: tensor(0.8425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [350/250]: training_loss: tensor(0.1601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [351/250]: training_loss: tensor(0.1185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [352/250]: training_loss: tensor(0.8913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [353/250]: training_loss: tensor(0.1324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [354/250]: training_loss: tensor(0.2236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [355/250]: training_loss: tensor(0.1114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [356/250]: training_loss: tensor(0.1648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [357/250]: training_loss: tensor(0.2198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [358/250]: training_loss: tensor(0.8180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [359/250]: training_loss: tensor(0.3827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [360/250]: training_loss: tensor(0.1190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [361/250]: training_loss: tensor(0.3401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [362/250]: training_loss: tensor(0.3239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [363/250]: training_loss: tensor(0.2231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [364/250]: training_loss: tensor(0.1972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [365/250]: training_loss: tensor(0.2392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [366/250]: training_loss: tensor(0.8794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [367/250]: training_loss: tensor(0.1354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [368/250]: training_loss: tensor(0.3275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [369/250]: training_loss: tensor(0.1740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [370/250]: training_loss: tensor(0.2078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [371/250]: training_loss: tensor(0.3676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [372/250]: training_loss: tensor(0.3467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [373/250]: training_loss: tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [374/250]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [375/250]: training_loss: tensor(0.1152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [376/250]: training_loss: tensor(0.4647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [377/250]: training_loss: tensor(0.2189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [378/250]: training_loss: tensor(0.1986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [379/250]: training_loss: tensor(0.2117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [380/250]: training_loss: tensor(0.1451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [381/250]: training_loss: tensor(0.1262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [382/250]: training_loss: tensor(0.4950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [383/250]: training_loss: tensor(0.2820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [384/250]: training_loss: tensor(0.2064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [385/250]: training_loss: tensor(0.4656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [386/250]: training_loss: tensor(0.1648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [387/250]: training_loss: tensor(0.2686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [388/250]: training_loss: tensor(0.1416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [389/250]: training_loss: tensor(0.1556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [390/250]: training_loss: tensor(0.1590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [391/250]: training_loss: tensor(0.2439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [392/250]: training_loss: tensor(0.2721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [393/250]: training_loss: tensor(0.5539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [394/250]: training_loss: tensor(0.6454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [395/250]: training_loss: tensor(0.3531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [396/250]: training_loss: tensor(0.5495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [397/250]: training_loss: tensor(0.5014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [398/250]: training_loss: tensor(0.2186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [399/250]: training_loss: tensor(0.9060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [400/250]: training_loss: tensor(0.2200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [401/250]: training_loss: tensor(0.5351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [402/250]: training_loss: tensor(0.3061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [403/250]: training_loss: tensor(0.4739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [404/250]: training_loss: tensor(0.4029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [405/250]: training_loss: tensor(0.1690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [406/250]: training_loss: tensor(0.3514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [407/250]: training_loss: tensor(0.4192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [408/250]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [409/250]: training_loss: tensor(0.4399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [410/250]: training_loss: tensor(0.4957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [411/250]: training_loss: tensor(0.2971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [412/250]: training_loss: tensor(0.4285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [413/250]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [414/250]: training_loss: tensor(0.2059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [415/250]: training_loss: tensor(0.4143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [416/250]: training_loss: tensor(0.2520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [417/250]: training_loss: tensor(0.2627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [418/250]: training_loss: tensor(0.2789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [419/250]: training_loss: tensor(0.2719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [420/250]: training_loss: tensor(0.2100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [421/250]: training_loss: tensor(0.1606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [422/250]: training_loss: tensor(0.2438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [423/250]: training_loss: tensor(0.4761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [424/250]: training_loss: tensor(0.1487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [425/250]: training_loss: tensor(0.5311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [426/250]: training_loss: tensor(0.4334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [427/250]: training_loss: tensor(0.4477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [428/250]: training_loss: tensor(0.3772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [429/250]: training_loss: tensor(0.8960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [430/250]: training_loss: tensor(0.5142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [431/250]: training_loss: tensor(0.3187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [432/250]: training_loss: tensor(0.5592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [433/250]: training_loss: tensor(0.1676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [434/250]: training_loss: tensor(0.5632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [435/250]: training_loss: tensor(0.7611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [436/250]: training_loss: tensor(0.6867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [437/250]: training_loss: tensor(0.2903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [438/250]: training_loss: tensor(0.6425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [439/250]: training_loss: tensor(0.4446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [440/250]: training_loss: tensor(0.5075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [441/250]: training_loss: tensor(0.4547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [442/250]: training_loss: tensor(0.4052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [443/250]: training_loss: tensor(0.4090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [444/250]: training_loss: tensor(0.2103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [445/250]: training_loss: tensor(0.3476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [446/250]: training_loss: tensor(0.2810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [447/250]: training_loss: tensor(0.1752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [448/250]: training_loss: tensor(0.3153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [449/250]: training_loss: tensor(0.3229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [450/250]: training_loss: tensor(0.2607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [451/250]: training_loss: tensor(0.2525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [452/250]: training_loss: tensor(0.3844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [453/250]: training_loss: tensor(0.2901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [454/250]: training_loss: tensor(0.2647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [455/250]: training_loss: tensor(0.1258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [456/250]: training_loss: tensor(0.3381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [457/250]: training_loss: tensor(0.7374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [458/250]: training_loss: tensor(0.1688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [459/250]: training_loss: tensor(0.1859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [460/250]: training_loss: tensor(0.3565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [461/250]: training_loss: tensor(0.1461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [462/250]: training_loss: tensor(0.5688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [463/250]: training_loss: tensor(0.2753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [464/250]: training_loss: tensor(0.3169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [465/250]: training_loss: tensor(0.1399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [466/250]: training_loss: tensor(0.0683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [467/250]: training_loss: tensor(0.5497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [468/250]: training_loss: tensor(0.8459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [469/250]: training_loss: tensor(0.2059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [470/250]: training_loss: tensor(0.1454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [471/250]: training_loss: tensor(0.2010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [472/250]: training_loss: tensor(0.1738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [473/250]: training_loss: tensor(0.2522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [474/250]: training_loss: tensor(0.4613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [475/250]: training_loss: tensor(0.4815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [476/250]: training_loss: tensor(0.2119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [477/250]: training_loss: tensor(0.2453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [478/250]: training_loss: tensor(0.3861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [479/250]: training_loss: tensor(0.1192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [480/250]: training_loss: tensor(0.5012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [481/250]: training_loss: tensor(0.3381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [482/250]: training_loss: tensor(0.2202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [483/250]: training_loss: tensor(0.4030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [484/250]: training_loss: tensor(0.1319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [485/250]: training_loss: tensor(0.4080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [486/250]: training_loss: tensor(0.2190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [487/250]: training_loss: tensor(0.7091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [488/250]: training_loss: tensor(0.3990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [489/250]: training_loss: tensor(0.3821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [490/250]: training_loss: tensor(0.5308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [491/250]: training_loss: tensor(0.1379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [492/250]: training_loss: tensor(0.7579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [493/250]: training_loss: tensor(0.1537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [494/250]: training_loss: tensor(0.3535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [495/250]: training_loss: tensor(0.4328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [496/250]: training_loss: tensor(0.2265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [497/250]: training_loss: tensor(0.1416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [498/250]: training_loss: tensor(0.2999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [499/250]: training_loss: tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [500/250]: training_loss: tensor(0.2279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [501/250]: training_loss: tensor(0.4594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [502/250]: training_loss: tensor(0.3647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [503/250]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [504/250]: training_loss: tensor(0.2610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [505/250]: training_loss: tensor(0.4272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [506/250]: training_loss: tensor(0.1585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [507/250]: training_loss: tensor(0.1635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [508/250]: training_loss: tensor(0.2277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [509/250]: training_loss: tensor(0.5162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [510/250]: training_loss: tensor(0.5372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [511/250]: training_loss: tensor(0.1400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [512/250]: training_loss: tensor(0.1707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [513/250]: training_loss: tensor(0.5065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [514/250]: training_loss: tensor(0.2837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [515/250]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [516/250]: training_loss: tensor(0.3272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [517/250]: training_loss: tensor(0.5935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [518/250]: training_loss: tensor(0.1640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [519/250]: training_loss: tensor(0.6557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [520/250]: training_loss: tensor(0.6372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [521/250]: training_loss: tensor(0.1350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [522/250]: training_loss: tensor(0.3455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [523/250]: training_loss: tensor(0.1044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [524/250]: training_loss: tensor(0.1390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [525/250]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [526/250]: training_loss: tensor(0.3977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [527/250]: training_loss: tensor(0.2253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [528/250]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [529/250]: training_loss: tensor(0.9557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [530/250]: training_loss: tensor(0.2581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [531/250]: training_loss: tensor(0.8539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [532/250]: training_loss: tensor(0.1521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [533/250]: training_loss: tensor(0.3119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [534/250]: training_loss: tensor(0.6750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [535/250]: training_loss: tensor(0.6768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [536/250]: training_loss: tensor(0.5157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [537/250]: training_loss: tensor(0.2712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [538/250]: training_loss: tensor(0.3587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [539/250]: training_loss: tensor(0.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [540/250]: training_loss: tensor(0.4588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [541/250]: training_loss: tensor(0.1469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [542/250]: training_loss: tensor(0.4781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [543/250]: training_loss: tensor(0.5520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [544/250]: training_loss: tensor(0.2143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [545/250]: training_loss: tensor(0.3008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [546/250]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [547/250]: training_loss: tensor(0.4679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [548/250]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [549/250]: training_loss: tensor(0.2631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [550/250]: training_loss: tensor(0.5482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [551/250]: training_loss: tensor(0.3464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [552/250]: training_loss: tensor(0.4303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [553/250]: training_loss: tensor(0.5550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [554/250]: training_loss: tensor(0.3498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [555/250]: training_loss: tensor(0.3140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [556/250]: training_loss: tensor(0.4129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [557/250]: training_loss: tensor(0.2882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [558/250]: training_loss: tensor(0.0436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [559/250]: training_loss: tensor(0.7237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [560/250]: training_loss: tensor(0.7526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [561/250]: training_loss: tensor(0.3671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [562/250]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [563/250]: training_loss: tensor(0.1617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [564/250]: training_loss: tensor(0.5324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [565/250]: training_loss: tensor(0.1794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [566/250]: training_loss: tensor(0.3375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [567/250]: training_loss: tensor(0.0576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [568/250]: training_loss: tensor(0.6628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [569/250]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [570/250]: training_loss: tensor(0.1849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [571/250]: training_loss: tensor(0.2611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [572/250]: training_loss: tensor(0.4931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [573/250]: training_loss: tensor(0.2575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [574/250]: training_loss: tensor(0.1862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [575/250]: training_loss: tensor(0.3506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [576/250]: training_loss: tensor(0.2829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [577/250]: training_loss: tensor(0.4399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [578/250]: training_loss: tensor(0.3154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [579/250]: training_loss: tensor(0.9171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [580/250]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [581/250]: training_loss: tensor(0.2663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [582/250]: training_loss: tensor(0.5261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [583/250]: training_loss: tensor(0.2885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [584/250]: training_loss: tensor(0.3240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [585/250]: training_loss: tensor(0.5357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [586/250]: training_loss: tensor(0.2273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [587/250]: training_loss: tensor(0.2723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [588/250]: training_loss: tensor(0.1355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [589/250]: training_loss: tensor(0.1625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [590/250]: training_loss: tensor(0.1404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [591/250]: training_loss: tensor(0.5789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [592/250]: training_loss: tensor(0.2618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [593/250]: training_loss: tensor(0.2610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [594/250]: training_loss: tensor(0.4784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [595/250]: training_loss: tensor(0.2678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [596/250]: training_loss: tensor(0.2988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [597/250]: training_loss: tensor(0.5180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [598/250]: training_loss: tensor(0.3981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [599/250]: training_loss: tensor(1.0460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [600/250]: training_loss: tensor(0.1548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [601/250]: training_loss: tensor(0.4146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [602/250]: training_loss: tensor(0.3767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [603/250]: training_loss: tensor(0.1875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [604/250]: training_loss: tensor(0.7077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [605/250]: training_loss: tensor(0.3507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [606/250]: training_loss: tensor(0.1338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [607/250]: training_loss: tensor(0.1254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [608/250]: training_loss: tensor(0.1233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [609/250]: training_loss: tensor(0.1994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [610/250]: training_loss: tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [611/250]: training_loss: tensor(0.1623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [612/250]: training_loss: tensor(0.3405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [613/250]: training_loss: tensor(0.5152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [614/250]: training_loss: tensor(0.1112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [615/250]: training_loss: tensor(0.7304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [616/250]: training_loss: tensor(0.2453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [617/250]: training_loss: tensor(0.2791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [618/250]: training_loss: tensor(0.2058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [619/250]: training_loss: tensor(0.3758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [620/250]: training_loss: tensor(0.2971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [621/250]: training_loss: tensor(0.4539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [622/250]: training_loss: tensor(0.1389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [623/250]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [624/250]: training_loss: tensor(0.2085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [625/250]: training_loss: tensor(0.1343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [626/250]: training_loss: tensor(0.5958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [627/250]: training_loss: tensor(0.1112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [628/250]: training_loss: tensor(0.5283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [629/250]: training_loss: tensor(0.2464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [630/250]: training_loss: tensor(0.3005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [631/250]: training_loss: tensor(0.3906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [632/250]: training_loss: tensor(0.1113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [633/250]: training_loss: tensor(0.2754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [634/250]: training_loss: tensor(0.4008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [635/250]: training_loss: tensor(0.4280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [636/250]: training_loss: tensor(0.2482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [637/250]: training_loss: tensor(0.3324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [638/250]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [639/250]: training_loss: tensor(0.1556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [640/250]: training_loss: tensor(0.4774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [641/250]: training_loss: tensor(0.3171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [642/250]: training_loss: tensor(0.2382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [643/250]: training_loss: tensor(0.1788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [644/250]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [645/250]: training_loss: tensor(0.1331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [646/250]: training_loss: tensor(0.2336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [647/250]: training_loss: tensor(0.3985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [648/250]: training_loss: tensor(0.2611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [649/250]: training_loss: tensor(0.4624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [650/250]: training_loss: tensor(0.6424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [651/250]: training_loss: tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [652/250]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [653/250]: training_loss: tensor(0.6164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [654/250]: training_loss: tensor(0.1264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [655/250]: training_loss: tensor(0.5637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [656/250]: training_loss: tensor(0.2089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [657/250]: training_loss: tensor(0.7189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [658/250]: training_loss: tensor(0.7014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [659/250]: training_loss: tensor(0.1393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [660/250]: training_loss: tensor(0.1617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [661/250]: training_loss: tensor(0.1302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [662/250]: training_loss: tensor(0.4340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [663/250]: training_loss: tensor(0.1369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [664/250]: training_loss: tensor(0.4829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [665/250]: training_loss: tensor(0.4857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [666/250]: training_loss: tensor(0.3820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [667/250]: training_loss: tensor(0.5347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [668/250]: training_loss: tensor(0.2329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [669/250]: training_loss: tensor(0.5694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [670/250]: training_loss: tensor(0.3451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [671/250]: training_loss: tensor(0.5350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [672/250]: training_loss: tensor(0.6201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [673/250]: training_loss: tensor(0.4332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [674/250]: training_loss: tensor(0.2450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [675/250]: training_loss: tensor(0.4442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [676/250]: training_loss: tensor(0.3171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [677/250]: training_loss: tensor(0.3533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [678/250]: training_loss: tensor(0.6093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [679/250]: training_loss: tensor(0.3992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [680/250]: training_loss: tensor(0.6846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [681/250]: training_loss: tensor(0.4807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [682/250]: training_loss: tensor(0.1550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [683/250]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [684/250]: training_loss: tensor(0.4809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [685/250]: training_loss: tensor(0.3961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [686/250]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [687/250]: training_loss: tensor(0.4836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [688/250]: training_loss: tensor(0.2381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [689/250]: training_loss: tensor(0.2101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [690/250]: training_loss: tensor(0.3813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [691/250]: training_loss: tensor(0.3059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [692/250]: training_loss: tensor(0.6808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [693/250]: training_loss: tensor(0.1845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [694/250]: training_loss: tensor(0.1524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [695/250]: training_loss: tensor(0.5964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [696/250]: training_loss: tensor(0.6360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [697/250]: training_loss: tensor(0.3884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [698/250]: training_loss: tensor(0.1913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [699/250]: training_loss: tensor(0.2917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [700/250]: training_loss: tensor(0.2666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [701/250]: training_loss: tensor(0.4478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [702/250]: training_loss: tensor(0.6191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [703/250]: training_loss: tensor(0.5908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [704/250]: training_loss: tensor(0.3153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [705/250]: training_loss: tensor(0.3480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [706/250]: training_loss: tensor(0.4381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [707/250]: training_loss: tensor(0.2273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [708/250]: training_loss: tensor(0.2695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [709/250]: training_loss: tensor(0.3207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [710/250]: training_loss: tensor(0.2473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [711/250]: training_loss: tensor(0.2108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [712/250]: training_loss: tensor(0.2135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [713/250]: training_loss: tensor(0.1694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [714/250]: training_loss: tensor(0.4345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [715/250]: training_loss: tensor(0.3495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [716/250]: training_loss: tensor(0.3687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [717/250]: training_loss: tensor(0.1885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [718/250]: training_loss: tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [719/250]: training_loss: tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [720/250]: training_loss: tensor(0.2156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [721/250]: training_loss: tensor(0.2802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [722/250]: training_loss: tensor(0.1588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [723/250]: training_loss: tensor(0.3235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [724/250]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [725/250]: training_loss: tensor(0.4295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [726/250]: training_loss: tensor(0.1722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [727/250]: training_loss: tensor(0.3322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [728/250]: training_loss: tensor(0.3983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [729/250]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [730/250]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [731/250]: training_loss: tensor(0.4363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [732/250]: training_loss: tensor(0.5494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [733/250]: training_loss: tensor(0.5556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [734/250]: training_loss: tensor(0.2749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [735/250]: training_loss: tensor(0.1965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [736/250]: training_loss: tensor(0.4008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [737/250]: training_loss: tensor(0.3710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [738/250]: training_loss: tensor(0.5689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [739/250]: training_loss: tensor(0.1361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [740/250]: training_loss: tensor(0.2682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [741/250]: training_loss: tensor(0.5565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [742/250]: training_loss: tensor(0.4195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [743/250]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [744/250]: training_loss: tensor(0.4245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [745/250]: training_loss: tensor(0.3429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [746/250]: training_loss: tensor(0.1976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [747/250]: training_loss: tensor(0.1773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [748/250]: training_loss: tensor(0.3296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [749/250]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [750/250]: training_loss: tensor(0.6395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [751/250]: training_loss: tensor(0.3039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [752/250]: training_loss: tensor(0.2850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [753/250]: training_loss: tensor(0.9551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [754/250]: training_loss: tensor(0.4034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [755/250]: training_loss: tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [756/250]: training_loss: tensor(0.2210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [757/250]: training_loss: tensor(0.5208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [758/250]: training_loss: tensor(0.1443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [759/250]: training_loss: tensor(0.7200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [760/250]: training_loss: tensor(0.3916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [761/250]: training_loss: tensor(0.3973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [762/250]: training_loss: tensor(0.2335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [763/250]: training_loss: tensor(0.1983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [764/250]: training_loss: tensor(0.3476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [765/250]: training_loss: tensor(0.1357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [766/250]: training_loss: tensor(0.3543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [767/250]: training_loss: tensor(0.7451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [768/250]: training_loss: tensor(0.6214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [769/250]: training_loss: tensor(0.5253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [770/250]: training_loss: tensor(0.2215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [771/250]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [772/250]: training_loss: tensor(0.2240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [773/250]: training_loss: tensor(0.9095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [774/250]: training_loss: tensor(0.3876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [775/250]: training_loss: tensor(0.5587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [776/250]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [777/250]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [778/250]: training_loss: tensor(0.1908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [779/250]: training_loss: tensor(0.3788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [780/250]: training_loss: tensor(0.2434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [781/250]: training_loss: tensor(0.5312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [782/250]: training_loss: tensor(0.5027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [783/250]: training_loss: tensor(0.5253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [784/250]: training_loss: tensor(0.4152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [785/250]: training_loss: tensor(0.2116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [786/250]: training_loss: tensor(0.5144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [787/250]: training_loss: tensor(0.2507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [788/250]: training_loss: tensor(0.6790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [789/250]: training_loss: tensor(0.4225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [790/250]: training_loss: tensor(0.2024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [791/250]: training_loss: tensor(0.4487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [792/250]: training_loss: tensor(0.1306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [793/250]: training_loss: tensor(0.5079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [794/250]: training_loss: tensor(0.1411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [795/250]: training_loss: tensor(0.2657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [796/250]: training_loss: tensor(0.3782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [797/250]: training_loss: tensor(0.1387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [798/250]: training_loss: tensor(0.4458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [799/250]: training_loss: tensor(0.5314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [800/250]: training_loss: tensor(0.4109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [801/250]: training_loss: tensor(0.8499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [802/250]: training_loss: tensor(0.4208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [803/250]: training_loss: tensor(0.1337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [804/250]: training_loss: tensor(0.1486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [805/250]: training_loss: tensor(0.1778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [806/250]: training_loss: tensor(0.2772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [807/250]: training_loss: tensor(0.1572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [808/250]: training_loss: tensor(0.1519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [809/250]: training_loss: tensor(0.1592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [810/250]: training_loss: tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [811/250]: training_loss: tensor(1.0400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [812/250]: training_loss: tensor(0.3237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [813/250]: training_loss: tensor(0.5166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [814/250]: training_loss: tensor(0.5993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [815/250]: training_loss: tensor(0.1965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [816/250]: training_loss: tensor(0.2549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [817/250]: training_loss: tensor(0.4280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [818/250]: training_loss: tensor(0.1772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [819/250]: training_loss: tensor(0.3012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [820/250]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [821/250]: training_loss: tensor(0.4319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [822/250]: training_loss: tensor(0.3670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [823/250]: training_loss: tensor(0.2444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [824/250]: training_loss: tensor(0.7091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [825/250]: training_loss: tensor(0.2726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [826/250]: training_loss: tensor(0.2621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [827/250]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [828/250]: training_loss: tensor(0.3249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [829/250]: training_loss: tensor(0.1964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [830/250]: training_loss: tensor(0.4478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [831/250]: training_loss: tensor(0.3144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [832/250]: training_loss: tensor(0.4410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [833/250]: training_loss: tensor(0.4539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [834/250]: training_loss: tensor(0.3148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [835/250]: training_loss: tensor(0.1373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [836/250]: training_loss: tensor(0.2061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [837/250]: training_loss: tensor(0.4605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [838/250]: training_loss: tensor(0.2264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [839/250]: training_loss: tensor(0.2498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [840/250]: training_loss: tensor(0.3738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [841/250]: training_loss: tensor(0.7874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [842/250]: training_loss: tensor(0.5539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [843/250]: training_loss: tensor(0.3978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [844/250]: training_loss: tensor(0.3179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [845/250]: training_loss: tensor(0.3034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [846/250]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [847/250]: training_loss: tensor(0.4270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [848/250]: training_loss: tensor(0.1735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [849/250]: training_loss: tensor(0.2575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [850/250]: training_loss: tensor(0.5566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [851/250]: training_loss: tensor(0.4468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [852/250]: training_loss: tensor(0.2462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [853/250]: training_loss: tensor(0.4553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [854/250]: training_loss: tensor(0.3577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [855/250]: training_loss: tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [856/250]: training_loss: tensor(0.3209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [857/250]: training_loss: tensor(0.1856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [858/250]: training_loss: tensor(0.2549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [859/250]: training_loss: tensor(0.2502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [860/250]: training_loss: tensor(0.4643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [861/250]: training_loss: tensor(0.4853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [862/250]: training_loss: tensor(0.2577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [863/250]: training_loss: tensor(0.1927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [864/250]: training_loss: tensor(0.5062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [865/250]: training_loss: tensor(0.4963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [866/250]: training_loss: tensor(0.8191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [867/250]: training_loss: tensor(0.3387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [868/250]: training_loss: tensor(0.2365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [869/250]: training_loss: tensor(0.3882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [870/250]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [871/250]: training_loss: tensor(0.4785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [872/250]: training_loss: tensor(0.3580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [873/250]: training_loss: tensor(0.2666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [874/250]: training_loss: tensor(0.3412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [875/250]: training_loss: tensor(0.3809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [876/250]: training_loss: tensor(0.1365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [877/250]: training_loss: tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [878/250]: training_loss: tensor(0.2215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [879/250]: training_loss: tensor(0.4447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [880/250]: training_loss: tensor(0.8435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [881/250]: training_loss: tensor(0.4776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [882/250]: training_loss: tensor(0.4713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [883/250]: training_loss: tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [884/250]: training_loss: tensor(0.8073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [885/250]: training_loss: tensor(0.6419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [886/250]: training_loss: tensor(0.2669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [887/250]: training_loss: tensor(0.2749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [888/250]: training_loss: tensor(0.3582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [889/250]: training_loss: tensor(0.2806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [890/250]: training_loss: tensor(0.3633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [891/250]: training_loss: tensor(0.2458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [892/250]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [893/250]: training_loss: tensor(0.2187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [894/250]: training_loss: tensor(0.6029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [895/250]: training_loss: tensor(0.4003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [896/250]: training_loss: tensor(0.5803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [897/250]: training_loss: tensor(0.1685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [898/250]: training_loss: tensor(0.2115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [899/250]: training_loss: tensor(0.3527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [900/250]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [901/250]: training_loss: tensor(0.2873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [902/250]: training_loss: tensor(0.3283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [903/250]: training_loss: tensor(0.2641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [904/250]: training_loss: tensor(0.3128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [905/250]: training_loss: tensor(0.6593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [906/250]: training_loss: tensor(0.6288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [907/250]: training_loss: tensor(0.1489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [908/250]: training_loss: tensor(0.2302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [909/250]: training_loss: tensor(0.1854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [910/250]: training_loss: tensor(0.4369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [911/250]: training_loss: tensor(0.1291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [912/250]: training_loss: tensor(0.4756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [913/250]: training_loss: tensor(0.5938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [914/250]: training_loss: tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [915/250]: training_loss: tensor(0.1353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [916/250]: training_loss: tensor(0.4376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [917/250]: training_loss: tensor(0.1449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [918/250]: training_loss: tensor(0.3049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [919/250]: training_loss: tensor(0.2579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [920/250]: training_loss: tensor(0.1890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [921/250]: training_loss: tensor(0.4029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [922/250]: training_loss: tensor(0.3596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [923/250]: training_loss: tensor(0.4314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [924/250]: training_loss: tensor(0.2365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [925/250]: training_loss: tensor(0.4036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [926/250]: training_loss: tensor(0.4224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [927/250]: training_loss: tensor(0.6027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [928/250]: training_loss: tensor(0.4945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [929/250]: training_loss: tensor(0.3273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [930/250]: training_loss: tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [931/250]: training_loss: tensor(0.5983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [932/250]: training_loss: tensor(0.1131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [933/250]: training_loss: tensor(0.4148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [934/250]: training_loss: tensor(0.2779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [935/250]: training_loss: tensor(0.5836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [936/250]: training_loss: tensor(0.2430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [937/250]: training_loss: tensor(0.3856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [938/250]: training_loss: tensor(0.2667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [939/250]: training_loss: tensor(0.3431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [940/250]: training_loss: tensor(0.1673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [941/250]: training_loss: tensor(0.2404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [942/250]: training_loss: tensor(0.1623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [943/250]: training_loss: tensor(0.2808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [944/250]: training_loss: tensor(0.3894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [945/250]: training_loss: tensor(0.6242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [946/250]: training_loss: tensor(0.3645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [947/250]: training_loss: tensor(0.8583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [948/250]: training_loss: tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [949/250]: training_loss: tensor(0.6048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [950/250]: training_loss: tensor(0.2543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [951/250]: training_loss: tensor(0.1683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [952/250]: training_loss: tensor(0.6599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [953/250]: training_loss: tensor(0.5885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [954/250]: training_loss: tensor(0.1595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [955/250]: training_loss: tensor(0.3243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [956/250]: training_loss: tensor(0.4368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [957/250]: training_loss: tensor(0.2529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [958/250]: training_loss: tensor(0.1522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [959/250]: training_loss: tensor(0.4304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [960/250]: training_loss: tensor(0.5598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [961/250]: training_loss: tensor(0.2881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [962/250]: training_loss: tensor(0.2234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [963/250]: training_loss: tensor(0.5421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [964/250]: training_loss: tensor(0.4569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [965/250]: training_loss: tensor(0.2087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [966/250]: training_loss: tensor(0.3712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [967/250]: training_loss: tensor(0.2513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [968/250]: training_loss: tensor(0.6413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [969/250]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [970/250]: training_loss: tensor(0.5554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [971/250]: training_loss: tensor(0.4277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [972/250]: training_loss: tensor(0.2168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [973/250]: training_loss: tensor(0.1607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [974/250]: training_loss: tensor(0.1770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [975/250]: training_loss: tensor(0.4202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [976/250]: training_loss: tensor(0.4187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [977/250]: training_loss: tensor(0.2575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [978/250]: training_loss: tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [979/250]: training_loss: tensor(0.7958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [980/250]: training_loss: tensor(0.1400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [981/250]: training_loss: tensor(0.2451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [982/250]: training_loss: tensor(0.3065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [983/250]: training_loss: tensor(0.3651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [984/250]: training_loss: tensor(0.3436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [985/250]: training_loss: tensor(0.3171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [986/250]: training_loss: tensor(0.2956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [987/250]: training_loss: tensor(0.2146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [988/250]: training_loss: tensor(0.1320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [989/250]: training_loss: tensor(0.6125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [990/250]: training_loss: tensor(0.3488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [991/250]: training_loss: tensor(0.2344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [992/250]: training_loss: tensor(0.2720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [993/250]: training_loss: tensor(0.5436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [994/250]: training_loss: tensor(0.1351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [995/250]: training_loss: tensor(0.2613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [996/250]: training_loss: tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [997/250]: training_loss: tensor(0.6387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [998/250]: training_loss: tensor(0.3561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [999/250]: training_loss: tensor(0.1336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1000/250]: training_loss: tensor(0.5274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1001/250]: training_loss: tensor(0.2455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1002/250]: training_loss: tensor(0.2062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1003/250]: training_loss: tensor(0.4987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1004/250]: training_loss: tensor(0.1498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1005/250]: training_loss: tensor(0.7882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/250]: training_loss: tensor(0.5250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/250]: training_loss: tensor(0.4429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/250]: training_loss: tensor(0.2550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/250]: training_loss: tensor(0.3988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/250]: training_loss: tensor(0.5515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/250]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/250]: training_loss: tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/250]: training_loss: tensor(0.5547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/250]: training_loss: tensor(0.2047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/250]: training_loss: tensor(0.1304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/250]: training_loss: tensor(0.2736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/250]: training_loss: tensor(0.2724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/250]: training_loss: tensor(0.1912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1019/250]: training_loss: tensor(0.4885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/250]: training_loss: tensor(0.5118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/250]: training_loss: tensor(0.1530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/250]: training_loss: tensor(0.2412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1023/250]: training_loss: tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/250]: training_loss: tensor(0.2969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/250]: training_loss: tensor(0.4729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/250]: training_loss: tensor(0.1237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/250]: training_loss: tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/250]: training_loss: tensor(0.2760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/250]: training_loss: tensor(0.6221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/250]: training_loss: tensor(0.4096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/250]: training_loss: tensor(0.2318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/250]: training_loss: tensor(0.2025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/250]: training_loss: tensor(0.4592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/250]: training_loss: tensor(0.5962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/250]: training_loss: tensor(0.1936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/250]: training_loss: tensor(0.4611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/250]: training_loss: tensor(0.4697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/250]: training_loss: tensor(0.6301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/250]: training_loss: tensor(0.1592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1040/250]: training_loss: tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/250]: training_loss: tensor(0.1129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/250]: training_loss: tensor(0.2952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1043/250]: training_loss: tensor(0.5417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/250]: training_loss: tensor(0.7858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/250]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/250]: training_loss: tensor(0.4789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/250]: training_loss: tensor(0.3172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/250]: training_loss: tensor(0.3515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/250]: training_loss: tensor(0.4271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/250]: training_loss: tensor(0.1938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/250]: training_loss: tensor(0.3980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/250]: training_loss: tensor(0.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/250]: training_loss: tensor(0.2425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/250]: training_loss: tensor(0.8616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/250]: training_loss: tensor(0.4484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/250]: training_loss: tensor(0.3160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/250]: training_loss: tensor(0.1640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/250]: training_loss: tensor(0.3545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/250]: training_loss: tensor(0.3135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/250]: training_loss: tensor(0.2668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/250]: training_loss: tensor(0.5063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/250]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/250]: training_loss: tensor(0.3182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/250]: training_loss: tensor(0.1730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/250]: training_loss: tensor(0.1746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/250]: training_loss: tensor(0.3992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/250]: training_loss: tensor(0.3098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/250]: training_loss: tensor(0.4520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/250]: training_loss: tensor(0.5766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/250]: training_loss: tensor(0.3473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/250]: training_loss: tensor(0.1495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/250]: training_loss: tensor(0.2710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/250]: training_loss: tensor(0.4690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/250]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/250]: training_loss: tensor(0.1494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/250]: training_loss: tensor(0.3282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/250]: training_loss: tensor(0.3722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/250]: training_loss: tensor(0.5817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/250]: training_loss: tensor(0.1105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/250]: training_loss: tensor(0.1150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/250]: training_loss: tensor(0.1252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/250]: training_loss: tensor(0.2718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/250]: training_loss: tensor(0.4683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/250]: training_loss: tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/250]: training_loss: tensor(0.5948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/250]: training_loss: tensor(0.1465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/250]: training_loss: tensor(0.6143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/250]: training_loss: tensor(0.8131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/250]: training_loss: tensor(0.2614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/250]: training_loss: tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/250]: training_loss: tensor(0.2147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/250]: training_loss: tensor(0.1202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/250]: training_loss: tensor(0.1645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/250]: training_loss: tensor(0.4017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/250]: training_loss: tensor(0.2408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/250]: training_loss: tensor(0.1877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/250]: training_loss: tensor(0.3374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/250]: training_loss: tensor(0.4623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/250]: training_loss: tensor(0.1678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1100/250]: training_loss: tensor(0.5564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/250]: training_loss: tensor(0.1531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/250]: training_loss: tensor(0.2272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/250]: training_loss: tensor(0.2048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/250]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/250]: training_loss: tensor(0.1457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/250]: training_loss: tensor(0.5598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/250]: training_loss: tensor(0.1495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/250]: training_loss: tensor(0.1184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/250]: training_loss: tensor(0.1801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/250]: training_loss: tensor(0.2651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/250]: training_loss: tensor(0.1376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/250]: training_loss: tensor(0.1674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/250]: training_loss: tensor(0.4449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/250]: training_loss: tensor(0.2548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/250]: training_loss: tensor(0.4413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/250]: training_loss: tensor(0.1381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/250]: training_loss: tensor(0.1678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/250]: training_loss: tensor(0.0575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/250]: training_loss: tensor(0.2747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/250]: training_loss: tensor(0.3421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/250]: training_loss: tensor(0.1601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/250]: training_loss: tensor(0.2288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/250]: training_loss: tensor(0.1147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/250]: training_loss: tensor(0.4098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/250]: training_loss: tensor(0.2069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/250]: training_loss: tensor(0.1145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/250]: training_loss: tensor(0.3498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/250]: training_loss: tensor(0.1091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1129/250]: training_loss: tensor(0.1429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/250]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/250]: training_loss: tensor(0.1085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/250]: training_loss: tensor(0.1914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/250]: training_loss: tensor(0.1407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/250]: training_loss: tensor(0.3573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/250]: training_loss: tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/250]: training_loss: tensor(0.6733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/250]: training_loss: tensor(0.4521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/250]: training_loss: tensor(0.2878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/250]: training_loss: tensor(0.3207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/250]: training_loss: tensor(0.1508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/250]: training_loss: tensor(0.1439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/250]: training_loss: tensor(0.2316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/250]: training_loss: tensor(0.4414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/250]: training_loss: tensor(0.1214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/250]: training_loss: tensor(0.3157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/250]: training_loss: tensor(0.2366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/250]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/250]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/250]: training_loss: tensor(0.2513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/250]: training_loss: tensor(0.2859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/250]: training_loss: tensor(0.4498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/250]: training_loss: tensor(0.4863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/250]: training_loss: tensor(0.6770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/250]: training_loss: tensor(0.5440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/250]: training_loss: tensor(0.1322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/250]: training_loss: tensor(0.1992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/250]: training_loss: tensor(0.2559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/250]: training_loss: tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/250]: training_loss: tensor(0.1152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/250]: training_loss: tensor(0.3775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/250]: training_loss: tensor(0.3851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/250]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/250]: training_loss: tensor(0.1857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/250]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/250]: training_loss: tensor(0.3370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/250]: training_loss: tensor(0.2227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/250]: training_loss: tensor(0.1363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/250]: training_loss: tensor(0.2035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/250]: training_loss: tensor(0.4013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/250]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/250]: training_loss: tensor(0.3444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/250]: training_loss: tensor(0.2592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/250]: training_loss: tensor(0.2062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/250]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/250]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/250]: training_loss: tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/250]: training_loss: tensor(0.3643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/250]: training_loss: tensor(0.1352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/250]: training_loss: tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/250]: training_loss: tensor(0.2697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/250]: training_loss: tensor(0.2546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/250]: training_loss: tensor(0.2996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/250]: training_loss: tensor(0.2066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/250]: training_loss: tensor(0.0545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/250]: training_loss: tensor(0.3417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/250]: training_loss: tensor(0.3000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/250]: training_loss: tensor(0.1840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/250]: training_loss: tensor(0.1325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/250]: training_loss: tensor(0.2145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/250]: training_loss: tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/250]: training_loss: tensor(0.3550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/250]: training_loss: tensor(0.3862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/250]: training_loss: tensor(0.1258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/250]: training_loss: tensor(0.2550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/250]: training_loss: tensor(0.6334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/250]: training_loss: tensor(0.3045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/250]: training_loss: tensor(0.3662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/250]: training_loss: tensor(0.4987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/250]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/250]: training_loss: tensor(0.2853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/250]: training_loss: tensor(0.8863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/250]: training_loss: tensor(0.4836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/250]: training_loss: tensor(0.6489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/250]: training_loss: tensor(0.2033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/250]: training_loss: tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/250]: training_loss: tensor(0.4373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/250]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/250]: training_loss: tensor(0.2833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/250]: training_loss: tensor(0.1935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/250]: training_loss: tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/250]: training_loss: tensor(0.3818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/250]: training_loss: tensor(0.3301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1213/250]: training_loss: tensor(0.1837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/250]: training_loss: tensor(0.2360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1215/250]: training_loss: tensor(0.1333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/250]: training_loss: tensor(0.2595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/250]: training_loss: tensor(0.2629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/250]: training_loss: tensor(0.4470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/250]: training_loss: tensor(0.5183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/250]: training_loss: tensor(0.1923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/250]: training_loss: tensor(0.5210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/250]: training_loss: tensor(0.1399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/250]: training_loss: tensor(0.2526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/250]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/250]: training_loss: tensor(0.2070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/250]: training_loss: tensor(0.2347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/250]: training_loss: tensor(0.5129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/250]: training_loss: tensor(0.1797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/250]: training_loss: tensor(0.1114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/250]: training_loss: tensor(0.5466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/250]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/250]: training_loss: tensor(0.7521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/250]: training_loss: tensor(0.7438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/250]: training_loss: tensor(0.3465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/250]: training_loss: tensor(0.3945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/250]: training_loss: tensor(0.3365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/250]: training_loss: tensor(0.1700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/250]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/250]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/250]: training_loss: tensor(0.1189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/250]: training_loss: tensor(0.2352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/250]: training_loss: tensor(0.2861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/250]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/250]: training_loss: tensor(0.1318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/250]: training_loss: tensor(0.3411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/250]: training_loss: tensor(0.2924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/250]: training_loss: tensor(0.1427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/250]: training_loss: tensor(0.1661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/250]: training_loss: tensor(0.1349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/250]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/250]: training_loss: tensor(0.1798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/250]: training_loss: tensor(0.3892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/250]: training_loss: tensor(0.4582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/250]: training_loss: tensor(0.3572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/250]: training_loss: tensor(0.3128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/250]: training_loss: tensor(0.1244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/250]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/250]: training_loss: tensor(0.6015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/250]: training_loss: tensor(0.1369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/250]: training_loss: tensor(0.1278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/250]: training_loss: tensor(0.1226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/250]: training_loss: tensor(0.2302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/250]: training_loss: tensor(0.2199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/250]: training_loss: tensor(0.4739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/250]: training_loss: tensor(0.3802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/250]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/250]: training_loss: tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/250]: training_loss: tensor(0.3809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/250]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/250]: training_loss: tensor(0.4401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/250]: training_loss: tensor(0.2115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/250]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/250]: training_loss: tensor(0.2225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/250]: training_loss: tensor(0.3108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/250]: training_loss: tensor(0.5625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/250]: training_loss: tensor(0.3726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/250]: training_loss: tensor(0.5821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/250]: training_loss: tensor(0.1787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/250]: training_loss: tensor(0.8259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/250]: training_loss: tensor(0.3406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/250]: training_loss: tensor(0.2823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/250]: training_loss: tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/250]: training_loss: tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/250]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/250]: training_loss: tensor(0.4604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/250]: training_loss: tensor(0.2262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/250]: training_loss: tensor(0.1295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/250]: training_loss: tensor(0.4203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/250]: training_loss: tensor(0.1666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/250]: training_loss: tensor(0.5919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/250]: training_loss: tensor(0.3248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/250]: training_loss: tensor(0.2353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/250]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/250]: training_loss: tensor(0.1223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/250]: training_loss: tensor(0.8460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/250]: training_loss: tensor(0.9676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/250]: training_loss: tensor(0.4239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/250]: training_loss: tensor(0.6422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1299/250]: training_loss: tensor(0.3819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/250]: training_loss: tensor(0.4280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/250]: training_loss: tensor(0.3978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/250]: training_loss: tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/250]: training_loss: tensor(0.1745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/250]: training_loss: tensor(0.3478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/250]: training_loss: tensor(0.1804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/250]: training_loss: tensor(0.1399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/250]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/250]: training_loss: tensor(0.3371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/250]: training_loss: tensor(0.2461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/250]: training_loss: tensor(0.2204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/250]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/250]: training_loss: tensor(0.1527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/250]: training_loss: tensor(0.4805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/250]: training_loss: tensor(0.4467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/250]: training_loss: tensor(0.2272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/250]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/250]: training_loss: tensor(0.3669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/250]: training_loss: tensor(0.1531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/250]: training_loss: tensor(0.2257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/250]: training_loss: tensor(0.2751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/250]: training_loss: tensor(0.3393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/250]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/250]: training_loss: tensor(0.7048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/250]: training_loss: tensor(0.2006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/250]: training_loss: tensor(0.2948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/250]: training_loss: tensor(0.1388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/250]: training_loss: tensor(0.2475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/250]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/250]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/250]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/250]: training_loss: tensor(0.1443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/250]: training_loss: tensor(0.3199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/250]: training_loss: tensor(0.2515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/250]: training_loss: tensor(0.7608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/250]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/250]: training_loss: tensor(0.5295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/250]: training_loss: tensor(0.1429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/250]: training_loss: tensor(0.7873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/250]: training_loss: tensor(0.4766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/250]: training_loss: tensor(0.3814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/250]: training_loss: tensor(0.8799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/250]: training_loss: tensor(0.2549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/250]: training_loss: tensor(0.1852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/250]: training_loss: tensor(0.3402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/250]: training_loss: tensor(1.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/250]: training_loss: tensor(0.1369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/250]: training_loss: tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/250]: training_loss: tensor(0.1585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/250]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/250]: training_loss: tensor(0.5455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/250]: training_loss: tensor(0.2740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/250]: training_loss: tensor(0.3591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/250]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/250]: training_loss: tensor(0.1381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/250]: training_loss: tensor(0.1593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/250]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/250]: training_loss: tensor(0.1508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/250]: training_loss: tensor(0.5019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/250]: training_loss: tensor(0.1233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/250]: training_loss: tensor(0.1788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1361/250]: training_loss: tensor(0.5899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/250]: training_loss: tensor(0.2671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1363/250]: training_loss: tensor(0.1987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/250]: training_loss: tensor(0.1525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/250]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/250]: training_loss: tensor(0.2426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/250]: training_loss: tensor(0.3183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/250]: training_loss: tensor(0.1496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/250]: training_loss: tensor(0.7420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/250]: training_loss: tensor(0.1511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/250]: training_loss: tensor(0.1071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/250]: training_loss: tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/250]: training_loss: tensor(0.7164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/250]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/250]: training_loss: tensor(0.4726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/250]: training_loss: tensor(0.5430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/250]: training_loss: tensor(0.3929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/250]: training_loss: tensor(0.2009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/250]: training_loss: tensor(0.4509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/250]: training_loss: tensor(0.8557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/250]: training_loss: tensor(0.1978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/250]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/250]: training_loss: tensor(0.3318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/250]: training_loss: tensor(0.2126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1385/250]: training_loss: tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1386/250]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1387/250]: training_loss: tensor(0.2160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1388/250]: training_loss: tensor(0.2152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1389/250]: training_loss: tensor(0.2494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1390/250]: training_loss: tensor(0.1796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1391/250]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1392/250]: training_loss: tensor(0.4366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1393/250]: training_loss: tensor(0.1442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1394/250]: training_loss: tensor(0.2788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1395/250]: training_loss: tensor(0.3712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1396/250]: training_loss: tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1397/250]: training_loss: tensor(0.7631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1398/250]: training_loss: tensor(0.6582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1399/250]: training_loss: tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1400/250]: training_loss: tensor(0.4623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1401/250]: training_loss: tensor(0.2007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1402/250]: training_loss: tensor(0.1740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1403/250]: training_loss: tensor(0.3155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1404/250]: training_loss: tensor(0.1131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1405/250]: training_loss: tensor(0.2368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1406/250]: training_loss: tensor(0.1905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1407/250]: training_loss: tensor(0.1682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1408/250]: training_loss: tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1409/250]: training_loss: tensor(0.3405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1410/250]: training_loss: tensor(0.1395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1411/250]: training_loss: tensor(0.3225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1412/250]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1413/250]: training_loss: tensor(0.8359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1414/250]: training_loss: tensor(0.3006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1415/250]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1416/250]: training_loss: tensor(0.4279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1417/250]: training_loss: tensor(0.6224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1418/250]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1419/250]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1420/250]: training_loss: tensor(0.1266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1421/250]: training_loss: tensor(0.1184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1422/250]: training_loss: tensor(0.7841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1423/250]: training_loss: tensor(0.5394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1424/250]: training_loss: tensor(0.4257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1425/250]: training_loss: tensor(0.3978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1426/250]: training_loss: tensor(0.2820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1427/250]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1428/250]: training_loss: tensor(0.2426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1429/250]: training_loss: tensor(0.4144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1430/250]: training_loss: tensor(0.2152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1431/250]: training_loss: tensor(0.3386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1432/250]: training_loss: tensor(0.4060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1433/250]: training_loss: tensor(0.1639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1434/250]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1435/250]: training_loss: tensor(0.6558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1436/250]: training_loss: tensor(0.2980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1437/250]: training_loss: tensor(0.2888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1438/250]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1439/250]: training_loss: tensor(0.2415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1440/250]: training_loss: tensor(0.3334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1441/250]: training_loss: tensor(0.1570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1442/250]: training_loss: tensor(0.2760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1443/250]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1444/250]: training_loss: tensor(0.2562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1445/250]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1446/250]: training_loss: tensor(0.4810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1447/250]: training_loss: tensor(0.4151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1448/250]: training_loss: tensor(0.1704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1449/250]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1450/250]: training_loss: tensor(0.3641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1451/250]: training_loss: tensor(0.4029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1452/250]: training_loss: tensor(0.4140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1453/250]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1454/250]: training_loss: tensor(0.1596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1455/250]: training_loss: tensor(0.1663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1456/250]: training_loss: tensor(0.1518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1457/250]: training_loss: tensor(0.3147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1458/250]: training_loss: tensor(0.2442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1459/250]: training_loss: tensor(0.6805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1460/250]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1461/250]: training_loss: tensor(0.1791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1462/250]: training_loss: tensor(0.6351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1463/250]: training_loss: tensor(0.4904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1464/250]: training_loss: tensor(0.4641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1465/250]: training_loss: tensor(0.3176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1466/250]: training_loss: tensor(0.4678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1467/250]: training_loss: tensor(0.1620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1468/250]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1469/250]: training_loss: tensor(0.7597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1470/250]: training_loss: tensor(0.1266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1471/250]: training_loss: tensor(1.3269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1472/250]: training_loss: tensor(0.2277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1473/250]: training_loss: tensor(0.7457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1474/250]: training_loss: tensor(0.6642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1475/250]: training_loss: tensor(0.1351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1476/250]: training_loss: tensor(0.1479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1477/250]: training_loss: tensor(0.4223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1478/250]: training_loss: tensor(0.1617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1479/250]: training_loss: tensor(0.3162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1480/250]: training_loss: tensor(0.2422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1481/250]: training_loss: tensor(0.0560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1482/250]: training_loss: tensor(0.4198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1483/250]: training_loss: tensor(0.3211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1484/250]: training_loss: tensor(0.1993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1485/250]: training_loss: tensor(0.1588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1486/250]: training_loss: tensor(0.5141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1487/250]: training_loss: tensor(0.6760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1488/250]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1489/250]: training_loss: tensor(0.1324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1490/250]: training_loss: tensor(0.1598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1491/250]: training_loss: tensor(0.1714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1492/250]: training_loss: tensor(0.1202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1493/250]: training_loss: tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1494/250]: training_loss: tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1495/250]: training_loss: tensor(0.1653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1496/250]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1497/250]: training_loss: tensor(0.5412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1498/250]: training_loss: tensor(0.2226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1499/250]: training_loss: tensor(0.1108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1500/250]: training_loss: tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1501/250]: training_loss: tensor(0.3030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1502/250]: training_loss: tensor(0.1127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1503/250]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1504/250]: training_loss: tensor(0.3066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1505/250]: training_loss: tensor(0.5323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1506/250]: training_loss: tensor(0.2330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1507/250]: training_loss: tensor(0.4617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1508/250]: training_loss: tensor(0.5178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1509/250]: training_loss: tensor(0.1220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1510/250]: training_loss: tensor(0.3947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1511/250]: training_loss: tensor(0.1695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1512/250]: training_loss: tensor(0.7551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1513/250]: training_loss: tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1514/250]: training_loss: tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1515/250]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1516/250]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1517/250]: training_loss: tensor(0.1571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1518/250]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1519/250]: training_loss: tensor(0.3393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1520/250]: training_loss: tensor(0.4565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1521/250]: training_loss: tensor(0.1397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1522/250]: training_loss: tensor(0.3454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1523/250]: training_loss: tensor(0.5322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1524/250]: training_loss: tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1525/250]: training_loss: tensor(0.7341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1526/250]: training_loss: tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1527/250]: training_loss: tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1528/250]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1529/250]: training_loss: tensor(0.2600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1530/250]: training_loss: tensor(0.4030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1531/250]: training_loss: tensor(0.5946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1532/250]: training_loss: tensor(0.3911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1533/250]: training_loss: tensor(0.2647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1534/250]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1535/250]: training_loss: tensor(0.1793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1536/250]: training_loss: tensor(0.1630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1537/250]: training_loss: tensor(0.2292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1538/250]: training_loss: tensor(0.2938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1539/250]: training_loss: tensor(0.1520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1540/250]: training_loss: tensor(0.2629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1541/250]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1542/250]: training_loss: tensor(0.5153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1543/250]: training_loss: tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1544/250]: training_loss: tensor(0.1638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1545/250]: training_loss: tensor(0.2737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1546/250]: training_loss: tensor(0.4274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1547/250]: training_loss: tensor(0.9000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1548/250]: training_loss: tensor(0.3795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1549/250]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1550/250]: training_loss: tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1551/250]: training_loss: tensor(0.2317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1552/250]: training_loss: tensor(0.2777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1553/250]: training_loss: tensor(0.3004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1554/250]: training_loss: tensor(0.2545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1555/250]: training_loss: tensor(0.4631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1556/250]: training_loss: tensor(0.7867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1557/250]: training_loss: tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1558/250]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1559/250]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1560/250]: training_loss: tensor(0.4870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1561/250]: training_loss: tensor(0.5049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1562/250]: training_loss: tensor(0.3507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1563/250]: training_loss: tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1564/250]: training_loss: tensor(0.4330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1565/250]: training_loss: tensor(0.1819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1566/250]: training_loss: tensor(0.1702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1567/250]: training_loss: tensor(0.2384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1568/250]: training_loss: tensor(0.2835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1569/250]: training_loss: tensor(0.2175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1570/250]: training_loss: tensor(0.5840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1571/250]: training_loss: tensor(0.1137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1572/250]: training_loss: tensor(0.3010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1573/250]: training_loss: tensor(0.1597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1574/250]: training_loss: tensor(0.8639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1575/250]: training_loss: tensor(0.2238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1576/250]: training_loss: tensor(0.5393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1577/250]: training_loss: tensor(0.2857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1578/250]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1579/250]: training_loss: tensor(0.2598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1580/250]: training_loss: tensor(0.8235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1581/250]: training_loss: tensor(0.5536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1582/250]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1583/250]: training_loss: tensor(0.1195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1584/250]: training_loss: tensor(0.3686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1585/250]: training_loss: tensor(0.1366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1586/250]: training_loss: tensor(0.3685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1587/250]: training_loss: tensor(0.1078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1588/250]: training_loss: tensor(0.2279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1589/250]: training_loss: tensor(0.3313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1590/250]: training_loss: tensor(0.1853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1591/250]: training_loss: tensor(0.4721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1592/250]: training_loss: tensor(0.1498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1593/250]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1594/250]: training_loss: tensor(0.1224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1595/250]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1596/250]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1597/250]: training_loss: tensor(0.1127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1598/250]: training_loss: tensor(0.6089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1599/250]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1600/250]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1601/250]: training_loss: tensor(0.5308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1602/250]: training_loss: tensor(0.1294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1603/250]: training_loss: tensor(0.1273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1604/250]: training_loss: tensor(0.3963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1605/250]: training_loss: tensor(0.7515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1606/250]: training_loss: tensor(0.1871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1607/250]: training_loss: tensor(0.8819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1608/250]: training_loss: tensor(0.3666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1609/250]: training_loss: tensor(0.0502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1610/250]: training_loss: tensor(0.1557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1611/250]: training_loss: tensor(0.5527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1612/250]: training_loss: tensor(0.2761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1613/250]: training_loss: tensor(0.1288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1614/250]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1615/250]: training_loss: tensor(0.2290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1616/250]: training_loss: tensor(0.4570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1617/250]: training_loss: tensor(0.1358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1618/250]: training_loss: tensor(0.3603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1619/250]: training_loss: tensor(0.3337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1620/250]: training_loss: tensor(0.3850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1621/250]: training_loss: tensor(0.1333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1622/250]: training_loss: tensor(0.6117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1623/250]: training_loss: tensor(0.5841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1624/250]: training_loss: tensor(0.2250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1625/250]: training_loss: tensor(0.3781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1626/250]: training_loss: tensor(0.2094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1627/250]: training_loss: tensor(0.1242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1628/250]: training_loss: tensor(0.8055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1629/250]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1630/250]: training_loss: tensor(0.2926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1631/250]: training_loss: tensor(0.2409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1632/250]: training_loss: tensor(0.1612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1633/250]: training_loss: tensor(0.5114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1634/250]: training_loss: tensor(0.2291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1635/250]: training_loss: tensor(0.1114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1636/250]: training_loss: tensor(0.3962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1637/250]: training_loss: tensor(0.2504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1638/250]: training_loss: tensor(0.6447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1639/250]: training_loss: tensor(0.4821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1640/250]: training_loss: tensor(0.2483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1641/250]: training_loss: tensor(0.9101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1642/250]: training_loss: tensor(0.4911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1643/250]: training_loss: tensor(0.3913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1644/250]: training_loss: tensor(0.2394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1645/250]: training_loss: tensor(0.6271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1646/250]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1647/250]: training_loss: tensor(0.1983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1648/250]: training_loss: tensor(0.3010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1649/250]: training_loss: tensor(0.1159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1650/250]: training_loss: tensor(0.3276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1651/250]: training_loss: tensor(0.4501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1652/250]: training_loss: tensor(0.7001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1653/250]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1654/250]: training_loss: tensor(0.1710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1655/250]: training_loss: tensor(0.5615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1656/250]: training_loss: tensor(0.3469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1657/250]: training_loss: tensor(0.3811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1658/250]: training_loss: tensor(0.1878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1659/250]: training_loss: tensor(0.5133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1660/250]: training_loss: tensor(0.2582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1661/250]: training_loss: tensor(0.3460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1662/250]: training_loss: tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1663/250]: training_loss: tensor(0.2082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1664/250]: training_loss: tensor(0.4951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1665/250]: training_loss: tensor(0.2049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1666/250]: training_loss: tensor(0.2756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1667/250]: training_loss: tensor(0.4948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1668/250]: training_loss: tensor(0.1720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1669/250]: training_loss: tensor(0.3267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1670/250]: training_loss: tensor(0.2443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1671/250]: training_loss: tensor(0.2675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1672/250]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1673/250]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1674/250]: training_loss: tensor(0.4112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1675/250]: training_loss: tensor(0.1589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1676/250]: training_loss: tensor(0.4514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1677/250]: training_loss: tensor(0.7205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1678/250]: training_loss: tensor(0.3990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1679/250]: training_loss: tensor(0.1225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1680/250]: training_loss: tensor(0.5559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1681/250]: training_loss: tensor(0.1143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1682/250]: training_loss: tensor(0.3837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1683/250]: training_loss: tensor(0.1398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1684/250]: training_loss: tensor(0.2954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1685/250]: training_loss: tensor(0.1560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1686/250]: training_loss: tensor(0.4203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1687/250]: training_loss: tensor(0.2484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1688/250]: training_loss: tensor(0.1383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1689/250]: training_loss: tensor(0.3329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1690/250]: training_loss: tensor(0.1881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1691/250]: training_loss: tensor(0.5992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1692/250]: training_loss: tensor(0.2954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1693/250]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1694/250]: training_loss: tensor(0.1842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1695/250]: training_loss: tensor(0.6946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1696/250]: training_loss: tensor(0.1762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1697/250]: training_loss: tensor(0.3391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1698/250]: training_loss: tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1699/250]: training_loss: tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1700/250]: training_loss: tensor(0.2155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1701/250]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1702/250]: training_loss: tensor(0.6966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1703/250]: training_loss: tensor(0.1983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1704/250]: training_loss: tensor(0.3135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1705/250]: training_loss: tensor(0.2819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1706/250]: training_loss: tensor(0.8169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1707/250]: training_loss: tensor(0.4644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1708/250]: training_loss: tensor(0.6482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1709/250]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1710/250]: training_loss: tensor(0.4939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1711/250]: training_loss: tensor(0.1961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1712/250]: training_loss: tensor(0.2162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1713/250]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1714/250]: training_loss: tensor(0.2533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1715/250]: training_loss: tensor(0.2120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1716/250]: training_loss: tensor(0.1346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1717/250]: training_loss: tensor(0.2058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1718/250]: training_loss: tensor(0.4144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1719/250]: training_loss: tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1720/250]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1721/250]: training_loss: tensor(0.1250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1722/250]: training_loss: tensor(0.8853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1723/250]: training_loss: tensor(0.5998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1724/250]: training_loss: tensor(0.1043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1725/250]: training_loss: tensor(0.4100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1726/250]: training_loss: tensor(0.6087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1727/250]: training_loss: tensor(0.6609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1728/250]: training_loss: tensor(0.4475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1729/250]: training_loss: tensor(0.1489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1730/250]: training_loss: tensor(0.3613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1731/250]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1732/250]: training_loss: tensor(0.7015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1733/250]: training_loss: tensor(0.2185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1734/250]: training_loss: tensor(0.3267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1735/250]: training_loss: tensor(0.2456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1736/250]: training_loss: tensor(0.3947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1737/250]: training_loss: tensor(0.2247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1738/250]: training_loss: tensor(0.1879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1739/250]: training_loss: tensor(0.3504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1740/250]: training_loss: tensor(0.2246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1741/250]: training_loss: tensor(0.3264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1742/250]: training_loss: tensor(0.2725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1743/250]: training_loss: tensor(0.5057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1744/250]: training_loss: tensor(0.4615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1745/250]: training_loss: tensor(0.2491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1746/250]: training_loss: tensor(0.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1747/250]: training_loss: tensor(0.2574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1748/250]: training_loss: tensor(0.4269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1749/250]: training_loss: tensor(0.1398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1750/250]: training_loss: tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1751/250]: training_loss: tensor(0.1659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1752/250]: training_loss: tensor(0.4400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1753/250]: training_loss: tensor(0.2594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1754/250]: training_loss: tensor(0.4532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1755/250]: training_loss: tensor(0.1453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1756/250]: training_loss: tensor(0.4690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1757/250]: training_loss: tensor(0.5872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1758/250]: training_loss: tensor(0.8878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1759/250]: training_loss: tensor(0.3518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1760/250]: training_loss: tensor(0.5158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1761/250]: training_loss: tensor(0.5139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1762/250]: training_loss: tensor(0.1494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1763/250]: training_loss: tensor(0.3510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1764/250]: training_loss: tensor(0.1271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1765/250]: training_loss: tensor(0.3024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1766/250]: training_loss: tensor(0.2751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1767/250]: training_loss: tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1768/250]: training_loss: tensor(0.3507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1769/250]: training_loss: tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1770/250]: training_loss: tensor(0.2917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1771/250]: training_loss: tensor(0.2549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1772/250]: training_loss: tensor(0.1225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1773/250]: training_loss: tensor(0.4235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1774/250]: training_loss: tensor(0.4829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1775/250]: training_loss: tensor(0.3426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1776/250]: training_loss: tensor(0.6975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1777/250]: training_loss: tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1778/250]: training_loss: tensor(0.1371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1779/250]: training_loss: tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1780/250]: training_loss: tensor(0.1589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1781/250]: training_loss: tensor(0.3102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1782/250]: training_loss: tensor(0.4741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1783/250]: training_loss: tensor(0.3966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1784/250]: training_loss: tensor(0.1914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1785/250]: training_loss: tensor(0.1568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1786/250]: training_loss: tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1787/250]: training_loss: tensor(0.1384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1788/250]: training_loss: tensor(0.1881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1789/250]: training_loss: tensor(0.6555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1790/250]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1791/250]: training_loss: tensor(0.2124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1792/250]: training_loss: tensor(0.5854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1793/250]: training_loss: tensor(0.4080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1794/250]: training_loss: tensor(0.2045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1795/250]: training_loss: tensor(0.3808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1796/250]: training_loss: tensor(0.2662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1797/250]: training_loss: tensor(0.1447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1798/250]: training_loss: tensor(0.4578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1799/250]: training_loss: tensor(0.5627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1800/250]: training_loss: tensor(0.4551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1801/250]: training_loss: tensor(0.6475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1802/250]: training_loss: tensor(0.2436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1803/250]: training_loss: tensor(0.1043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1804/250]: training_loss: tensor(0.2845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1805/250]: training_loss: tensor(0.7579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1806/250]: training_loss: tensor(0.1319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1807/250]: training_loss: tensor(0.3020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1808/250]: training_loss: tensor(0.1386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1809/250]: training_loss: tensor(0.1693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1810/250]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1811/250]: training_loss: tensor(0.2162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1812/250]: training_loss: tensor(0.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1813/250]: training_loss: tensor(0.4487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1814/250]: training_loss: tensor(0.2470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1815/250]: training_loss: tensor(0.3948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1816/250]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1817/250]: training_loss: tensor(0.1241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1818/250]: training_loss: tensor(0.2727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1819/250]: training_loss: tensor(0.3057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1820/250]: training_loss: tensor(0.1185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1821/250]: training_loss: tensor(0.3210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1822/250]: training_loss: tensor(0.2431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1823/250]: training_loss: tensor(0.2991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1824/250]: training_loss: tensor(0.2138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1825/250]: training_loss: tensor(0.3202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1826/250]: training_loss: tensor(0.2940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1827/250]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1828/250]: training_loss: tensor(0.1295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1829/250]: training_loss: tensor(0.1566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1830/250]: training_loss: tensor(0.1149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1831/250]: training_loss: tensor(0.2066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1832/250]: training_loss: tensor(0.4439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1833/250]: training_loss: tensor(0.2672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1834/250]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1835/250]: training_loss: tensor(0.3682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1836/250]: training_loss: tensor(0.3212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1837/250]: training_loss: tensor(0.1110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1838/250]: training_loss: tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1839/250]: training_loss: tensor(0.4152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1840/250]: training_loss: tensor(0.1457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1841/250]: training_loss: tensor(0.2190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1842/250]: training_loss: tensor(0.1592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1843/250]: training_loss: tensor(0.3929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1844/250]: training_loss: tensor(0.2738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1845/250]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1846/250]: training_loss: tensor(0.1925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1847/250]: training_loss: tensor(0.5980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1848/250]: training_loss: tensor(0.7308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1849/250]: training_loss: tensor(0.1804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1850/250]: training_loss: tensor(0.1794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1851/250]: training_loss: tensor(0.4712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1852/250]: training_loss: tensor(0.3587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1853/250]: training_loss: tensor(0.4944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1854/250]: training_loss: tensor(0.1870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1855/250]: training_loss: tensor(0.2146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1856/250]: training_loss: tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1857/250]: training_loss: tensor(0.2093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1858/250]: training_loss: tensor(0.1654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1859/250]: training_loss: tensor(0.6126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1860/250]: training_loss: tensor(0.7748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1861/250]: training_loss: tensor(0.1868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1862/250]: training_loss: tensor(0.4198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1863/250]: training_loss: tensor(0.3837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1864/250]: training_loss: tensor(0.2516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1865/250]: training_loss: tensor(0.3594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1866/250]: training_loss: tensor(0.2121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1867/250]: training_loss: tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1868/250]: training_loss: tensor(0.8640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1869/250]: training_loss: tensor(0.2411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1870/250]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1871/250]: training_loss: tensor(0.2316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1872/250]: training_loss: tensor(0.3658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1873/250]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1874/250]: training_loss: tensor(0.6686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1875/250]: training_loss: tensor(0.2228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1876/250]: training_loss: tensor(0.1751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1877/250]: training_loss: tensor(0.2608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1878/250]: training_loss: tensor(0.2703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1879/250]: training_loss: tensor(0.1412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1880/250]: training_loss: tensor(0.1434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1881/250]: training_loss: tensor(0.5685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1882/250]: training_loss: tensor(0.7517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1883/250]: training_loss: tensor(0.2056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1884/250]: training_loss: tensor(0.4411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1885/250]: training_loss: tensor(0.2573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1886/250]: training_loss: tensor(0.2714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1887/250]: training_loss: tensor(0.3349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1888/250]: training_loss: tensor(0.4669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1889/250]: training_loss: tensor(0.1182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1890/250]: training_loss: tensor(0.2183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1891/250]: training_loss: tensor(0.3328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1892/250]: training_loss: tensor(0.1559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1893/250]: training_loss: tensor(0.5128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1894/250]: training_loss: tensor(0.1452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1895/250]: training_loss: tensor(1.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1896/250]: training_loss: tensor(0.2129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1897/250]: training_loss: tensor(0.2805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1898/250]: training_loss: tensor(0.8818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1899/250]: training_loss: tensor(0.3901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1900/250]: training_loss: tensor(0.3564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1901/250]: training_loss: tensor(0.6114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1902/250]: training_loss: tensor(0.3733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1903/250]: training_loss: tensor(0.3272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1904/250]: training_loss: tensor(0.3733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1905/250]: training_loss: tensor(0.2697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1906/250]: training_loss: tensor(0.1272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1907/250]: training_loss: tensor(0.5725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1908/250]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1909/250]: training_loss: tensor(0.5267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1910/250]: training_loss: tensor(0.1168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1911/250]: training_loss: tensor(0.3203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1912/250]: training_loss: tensor(0.2636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1913/250]: training_loss: tensor(0.3177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1914/250]: training_loss: tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1915/250]: training_loss: tensor(0.1852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1916/250]: training_loss: tensor(0.3116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1917/250]: training_loss: tensor(0.6536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1918/250]: training_loss: tensor(0.3178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1919/250]: training_loss: tensor(0.5369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1920/250]: training_loss: tensor(0.1800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1921/250]: training_loss: tensor(0.1549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1922/250]: training_loss: tensor(0.4221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1923/250]: training_loss: tensor(0.3621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1924/250]: training_loss: tensor(0.2064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1925/250]: training_loss: tensor(0.7877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1926/250]: training_loss: tensor(0.5049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1927/250]: training_loss: tensor(0.3492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1928/250]: training_loss: tensor(0.2474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1929/250]: training_loss: tensor(0.3003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1930/250]: training_loss: tensor(0.7068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1931/250]: training_loss: tensor(0.9334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1932/250]: training_loss: tensor(0.4171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1933/250]: training_loss: tensor(0.3246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1934/250]: training_loss: tensor(0.2409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1935/250]: training_loss: tensor(0.2748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1936/250]: training_loss: tensor(0.7045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1937/250]: training_loss: tensor(0.4781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1938/250]: training_loss: tensor(0.1958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1939/250]: training_loss: tensor(0.2595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1940/250]: training_loss: tensor(0.3074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1941/250]: training_loss: tensor(0.4773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1942/250]: training_loss: tensor(0.2876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1943/250]: training_loss: tensor(0.3749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1944/250]: training_loss: tensor(0.2191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1945/250]: training_loss: tensor(0.4258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1946/250]: training_loss: tensor(0.3239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1947/250]: training_loss: tensor(0.3724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1948/250]: training_loss: tensor(0.9529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1949/250]: training_loss: tensor(0.1697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1950/250]: training_loss: tensor(0.7200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1951/250]: training_loss: tensor(0.1674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1952/250]: training_loss: tensor(0.4430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1953/250]: training_loss: tensor(0.4670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1954/250]: training_loss: tensor(0.1634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1955/250]: training_loss: tensor(0.4364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1956/250]: training_loss: tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1957/250]: training_loss: tensor(0.4581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1958/250]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1959/250]: training_loss: tensor(0.4813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1960/250]: training_loss: tensor(0.1581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1961/250]: training_loss: tensor(0.4690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1962/250]: training_loss: tensor(0.2138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1963/250]: training_loss: tensor(0.4550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1964/250]: training_loss: tensor(0.5012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1965/250]: training_loss: tensor(0.2230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1966/250]: training_loss: tensor(0.1495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1967/250]: training_loss: tensor(0.5979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1968/250]: training_loss: tensor(0.3691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1969/250]: training_loss: tensor(0.2775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1970/250]: training_loss: tensor(0.6410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1971/250]: training_loss: tensor(0.4161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1972/250]: training_loss: tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1973/250]: training_loss: tensor(0.3377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1974/250]: training_loss: tensor(0.2444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1975/250]: training_loss: tensor(0.2521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1976/250]: training_loss: tensor(0.5717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1977/250]: training_loss: tensor(0.7368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1978/250]: training_loss: tensor(0.2873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1979/250]: training_loss: tensor(1.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1980/250]: training_loss: tensor(0.2676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1981/250]: training_loss: tensor(0.1365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1982/250]: training_loss: tensor(0.3299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1983/250]: training_loss: tensor(0.3288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1984/250]: training_loss: tensor(0.3152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1985/250]: training_loss: tensor(0.3487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1986/250]: training_loss: tensor(0.3986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1987/250]: training_loss: tensor(0.2533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1988/250]: training_loss: tensor(0.2984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1989/250]: training_loss: tensor(0.2323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1990/250]: training_loss: tensor(0.1695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1991/250]: training_loss: tensor(0.2823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1992/250]: training_loss: tensor(0.2241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1993/250]: training_loss: tensor(0.2827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1994/250]: training_loss: tensor(0.1182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1995/250]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1996/250]: training_loss: tensor(0.1677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1997/250]: training_loss: tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1998/250]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1999/250]: training_loss: tensor(0.4399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [2/3], global step [4000/6000], Train Loss: 0.3112, Valid Loss: 0.3078\n",
      "batch_no [0/250]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1/250]: training_loss: tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2/250]: training_loss: tensor(0.5151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3/250]: training_loss: tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4/250]: training_loss: tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [5/250]: training_loss: tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [6/250]: training_loss: tensor(0.5102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [7/250]: training_loss: tensor(0.3959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [8/250]: training_loss: tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [9/250]: training_loss: tensor(0.1184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [10/250]: training_loss: tensor(0.1229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [11/250]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [12/250]: training_loss: tensor(0.0594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [13/250]: training_loss: tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [14/250]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [15/250]: training_loss: tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [16/250]: training_loss: tensor(0.1467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [17/250]: training_loss: tensor(0.3192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [18/250]: training_loss: tensor(0.1566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [19/250]: training_loss: tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [20/250]: training_loss: tensor(0.8523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [21/250]: training_loss: tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [22/250]: training_loss: tensor(0.1550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [23/250]: training_loss: tensor(0.0487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [24/250]: training_loss: tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [25/250]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [26/250]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [27/250]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [28/250]: training_loss: tensor(0.2503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [29/250]: training_loss: tensor(0.4683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [30/250]: training_loss: tensor(0.1771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [31/250]: training_loss: tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [32/250]: training_loss: tensor(0.5617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [33/250]: training_loss: tensor(0.2868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [34/250]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [35/250]: training_loss: tensor(0.2321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [36/250]: training_loss: tensor(0.2563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [37/250]: training_loss: tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [38/250]: training_loss: tensor(0.2583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [39/250]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [40/250]: training_loss: tensor(0.4840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [41/250]: training_loss: tensor(0.2943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [42/250]: training_loss: tensor(0.0495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [43/250]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [44/250]: training_loss: tensor(0.1238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [45/250]: training_loss: tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [46/250]: training_loss: tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [47/250]: training_loss: tensor(0.2897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [48/250]: training_loss: tensor(0.3571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [49/250]: training_loss: tensor(0.3414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [50/250]: training_loss: tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [51/250]: training_loss: tensor(0.5761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [52/250]: training_loss: tensor(0.1482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [53/250]: training_loss: tensor(0.2304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [54/250]: training_loss: tensor(0.1751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [55/250]: training_loss: tensor(0.4410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [56/250]: training_loss: tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [57/250]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [58/250]: training_loss: tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [59/250]: training_loss: tensor(0.3918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [60/250]: training_loss: tensor(0.2807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [61/250]: training_loss: tensor(0.5687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [62/250]: training_loss: tensor(0.1541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [63/250]: training_loss: tensor(0.1859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [64/250]: training_loss: tensor(0.5443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [65/250]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [66/250]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [67/250]: training_loss: tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [68/250]: training_loss: tensor(0.1773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [69/250]: training_loss: tensor(0.3270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [70/250]: training_loss: tensor(0.1316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [71/250]: training_loss: tensor(0.1602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [72/250]: training_loss: tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [73/250]: training_loss: tensor(0.4112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [74/250]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [75/250]: training_loss: tensor(0.4990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [76/250]: training_loss: tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [77/250]: training_loss: tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [78/250]: training_loss: tensor(0.4430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [79/250]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [80/250]: training_loss: tensor(0.4724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [81/250]: training_loss: tensor(0.4598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [82/250]: training_loss: tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [83/250]: training_loss: tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [84/250]: training_loss: tensor(0.2198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [85/250]: training_loss: tensor(0.4877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [86/250]: training_loss: tensor(0.0635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [87/250]: training_loss: tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [88/250]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [89/250]: training_loss: tensor(0.5232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [90/250]: training_loss: tensor(0.4058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [91/250]: training_loss: tensor(0.1325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [92/250]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [93/250]: training_loss: tensor(0.4039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [94/250]: training_loss: tensor(0.2169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [95/250]: training_loss: tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [96/250]: training_loss: tensor(0.4126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [97/250]: training_loss: tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [98/250]: training_loss: tensor(0.3553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [99/250]: training_loss: tensor(0.1149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [100/250]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [101/250]: training_loss: tensor(0.3937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [102/250]: training_loss: tensor(0.2245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [103/250]: training_loss: tensor(0.2252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [104/250]: training_loss: tensor(0.2958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [105/250]: training_loss: tensor(0.5692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [106/250]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [107/250]: training_loss: tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [108/250]: training_loss: tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [109/250]: training_loss: tensor(0.4892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [110/250]: training_loss: tensor(0.1076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [111/250]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [112/250]: training_loss: tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [113/250]: training_loss: tensor(0.2226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [114/250]: training_loss: tensor(0.7353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [115/250]: training_loss: tensor(0.2516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [116/250]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [117/250]: training_loss: tensor(0.5750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [118/250]: training_loss: tensor(0.1657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [119/250]: training_loss: tensor(0.1132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [120/250]: training_loss: tensor(0.2046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [121/250]: training_loss: tensor(0.7449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [122/250]: training_loss: tensor(0.7644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [123/250]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [124/250]: training_loss: tensor(0.7367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [125/250]: training_loss: tensor(0.1443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [126/250]: training_loss: tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [127/250]: training_loss: tensor(0.3385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [128/250]: training_loss: tensor(0.2031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [129/250]: training_loss: tensor(0.2805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [130/250]: training_loss: tensor(0.3645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [131/250]: training_loss: tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [132/250]: training_loss: tensor(0.1091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [133/250]: training_loss: tensor(0.2316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [134/250]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [135/250]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [136/250]: training_loss: tensor(0.4184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [137/250]: training_loss: tensor(0.2607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [138/250]: training_loss: tensor(0.1784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [139/250]: training_loss: tensor(0.1448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [140/250]: training_loss: tensor(0.3509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [141/250]: training_loss: tensor(0.1200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [142/250]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [143/250]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [144/250]: training_loss: tensor(0.1380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [145/250]: training_loss: tensor(0.1754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [146/250]: training_loss: tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [147/250]: training_loss: tensor(0.8179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [148/250]: training_loss: tensor(0.5255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [149/250]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [150/250]: training_loss: tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [151/250]: training_loss: tensor(0.1150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [152/250]: training_loss: tensor(0.1616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [153/250]: training_loss: tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [154/250]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [155/250]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [156/250]: training_loss: tensor(0.1941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [157/250]: training_loss: tensor(0.5679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [158/250]: training_loss: tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [159/250]: training_loss: tensor(0.2445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [160/250]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [161/250]: training_loss: tensor(0.5131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [162/250]: training_loss: tensor(0.3027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [163/250]: training_loss: tensor(0.1666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [164/250]: training_loss: tensor(0.1175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [165/250]: training_loss: tensor(0.1167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [166/250]: training_loss: tensor(0.3966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [167/250]: training_loss: tensor(0.0586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [168/250]: training_loss: tensor(0.1784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [169/250]: training_loss: tensor(0.2248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [170/250]: training_loss: tensor(0.1252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [171/250]: training_loss: tensor(0.2506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [172/250]: training_loss: tensor(0.0389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [173/250]: training_loss: tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [174/250]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [175/250]: training_loss: tensor(0.1696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [176/250]: training_loss: tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [177/250]: training_loss: tensor(0.4690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [178/250]: training_loss: tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [179/250]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [180/250]: training_loss: tensor(0.1354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [181/250]: training_loss: tensor(0.2496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [182/250]: training_loss: tensor(0.8491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [183/250]: training_loss: tensor(0.3250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [184/250]: training_loss: tensor(0.2872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [185/250]: training_loss: tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [186/250]: training_loss: tensor(0.2652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [187/250]: training_loss: tensor(0.1959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [188/250]: training_loss: tensor(0.2486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [189/250]: training_loss: tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [190/250]: training_loss: tensor(0.5873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [191/250]: training_loss: tensor(0.1442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [192/250]: training_loss: tensor(0.4725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [193/250]: training_loss: tensor(0.1200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [194/250]: training_loss: tensor(0.1420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [195/250]: training_loss: tensor(0.1351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [196/250]: training_loss: tensor(0.1379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [197/250]: training_loss: tensor(0.1358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [198/250]: training_loss: tensor(0.4563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [199/250]: training_loss: tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [200/250]: training_loss: tensor(0.2195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [201/250]: training_loss: tensor(0.2796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [202/250]: training_loss: tensor(0.4935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [203/250]: training_loss: tensor(0.1653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [204/250]: training_loss: tensor(0.7180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [205/250]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [206/250]: training_loss: tensor(0.3993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [207/250]: training_loss: tensor(0.1974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [208/250]: training_loss: tensor(0.4890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [209/250]: training_loss: tensor(0.1738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [210/250]: training_loss: tensor(0.3419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [211/250]: training_loss: tensor(0.6575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [212/250]: training_loss: tensor(0.1114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [213/250]: training_loss: tensor(0.4097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [214/250]: training_loss: tensor(0.6140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [215/250]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [216/250]: training_loss: tensor(0.4913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [217/250]: training_loss: tensor(0.2479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [218/250]: training_loss: tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [219/250]: training_loss: tensor(0.5026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [220/250]: training_loss: tensor(0.1994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [221/250]: training_loss: tensor(0.3236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [222/250]: training_loss: tensor(0.1093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [223/250]: training_loss: tensor(0.4766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [224/250]: training_loss: tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [225/250]: training_loss: tensor(0.2884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [226/250]: training_loss: tensor(0.7602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [227/250]: training_loss: tensor(0.2204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [228/250]: training_loss: tensor(0.5772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [229/250]: training_loss: tensor(0.4598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [230/250]: training_loss: tensor(0.3504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [231/250]: training_loss: tensor(0.2185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [232/250]: training_loss: tensor(0.2614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [233/250]: training_loss: tensor(0.6447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [234/250]: training_loss: tensor(0.5188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [235/250]: training_loss: tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [236/250]: training_loss: tensor(0.3812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [237/250]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [238/250]: training_loss: tensor(0.2478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [239/250]: training_loss: tensor(0.2314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [240/250]: training_loss: tensor(0.4232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [241/250]: training_loss: tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [242/250]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [243/250]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [244/250]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [245/250]: training_loss: tensor(1.1790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [246/250]: training_loss: tensor(0.3456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [247/250]: training_loss: tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [248/250]: training_loss: tensor(0.1334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [249/250]: training_loss: tensor(0.1333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [250/250]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [251/250]: training_loss: tensor(0.1693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [252/250]: training_loss: tensor(0.3797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [253/250]: training_loss: tensor(0.1368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [254/250]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [255/250]: training_loss: tensor(0.4259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [256/250]: training_loss: tensor(0.2981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [257/250]: training_loss: tensor(0.1471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [258/250]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [259/250]: training_loss: tensor(0.1845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [260/250]: training_loss: tensor(0.6002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [261/250]: training_loss: tensor(0.1319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [262/250]: training_loss: tensor(0.0439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [263/250]: training_loss: tensor(0.1282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [264/250]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [265/250]: training_loss: tensor(0.6959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [266/250]: training_loss: tensor(0.3317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [267/250]: training_loss: tensor(0.3452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [268/250]: training_loss: tensor(0.2033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [269/250]: training_loss: tensor(0.4647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [270/250]: training_loss: tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [271/250]: training_loss: tensor(0.4656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [272/250]: training_loss: tensor(0.1663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [273/250]: training_loss: tensor(0.1554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [274/250]: training_loss: tensor(0.2151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [275/250]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [276/250]: training_loss: tensor(0.2521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [277/250]: training_loss: tensor(0.1805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [278/250]: training_loss: tensor(0.7422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [279/250]: training_loss: tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [280/250]: training_loss: tensor(0.1943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [281/250]: training_loss: tensor(0.3941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [282/250]: training_loss: tensor(0.3766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [283/250]: training_loss: tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [284/250]: training_loss: tensor(0.6623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [285/250]: training_loss: tensor(0.1876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [286/250]: training_loss: tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [287/250]: training_loss: tensor(0.5020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [288/250]: training_loss: tensor(0.3716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [289/250]: training_loss: tensor(0.2803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [290/250]: training_loss: tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [291/250]: training_loss: tensor(0.1794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [292/250]: training_loss: tensor(0.6001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [293/250]: training_loss: tensor(0.0436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [294/250]: training_loss: tensor(0.1384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [295/250]: training_loss: tensor(0.1240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [296/250]: training_loss: tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [297/250]: training_loss: tensor(0.5018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [298/250]: training_loss: tensor(0.5281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [299/250]: training_loss: tensor(0.3924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [300/250]: training_loss: tensor(0.2328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [301/250]: training_loss: tensor(0.1965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [302/250]: training_loss: tensor(0.4491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [303/250]: training_loss: tensor(0.4575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [304/250]: training_loss: tensor(0.3699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [305/250]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [306/250]: training_loss: tensor(0.1324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [307/250]: training_loss: tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [308/250]: training_loss: tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [309/250]: training_loss: tensor(0.4607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [310/250]: training_loss: tensor(0.3345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [311/250]: training_loss: tensor(0.1833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [312/250]: training_loss: tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [313/250]: training_loss: tensor(0.3630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [314/250]: training_loss: tensor(0.3623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [315/250]: training_loss: tensor(0.3119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [316/250]: training_loss: tensor(0.2799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [317/250]: training_loss: tensor(0.2032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [318/250]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [319/250]: training_loss: tensor(0.4355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [320/250]: training_loss: tensor(0.2565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [321/250]: training_loss: tensor(0.1359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [322/250]: training_loss: tensor(0.3521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [323/250]: training_loss: tensor(0.1712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [324/250]: training_loss: tensor(0.1595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [325/250]: training_loss: tensor(0.2260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [326/250]: training_loss: tensor(0.3347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [327/250]: training_loss: tensor(0.1479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [328/250]: training_loss: tensor(0.2077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [329/250]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [330/250]: training_loss: tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [331/250]: training_loss: tensor(0.5540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [332/250]: training_loss: tensor(0.3609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [333/250]: training_loss: tensor(0.5776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [334/250]: training_loss: tensor(0.5720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [335/250]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [336/250]: training_loss: tensor(0.3240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [337/250]: training_loss: tensor(0.3201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [338/250]: training_loss: tensor(0.3539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [339/250]: training_loss: tensor(0.1537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [340/250]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [341/250]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [342/250]: training_loss: tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [343/250]: training_loss: tensor(0.1288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [344/250]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [345/250]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [346/250]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [347/250]: training_loss: tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [348/250]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [349/250]: training_loss: tensor(0.7169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [350/250]: training_loss: tensor(0.1270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [351/250]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [352/250]: training_loss: tensor(0.4797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [353/250]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [354/250]: training_loss: tensor(0.1524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [355/250]: training_loss: tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [356/250]: training_loss: tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [357/250]: training_loss: tensor(0.1833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [358/250]: training_loss: tensor(0.3491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [359/250]: training_loss: tensor(0.3955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [360/250]: training_loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [361/250]: training_loss: tensor(0.3653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [362/250]: training_loss: tensor(0.1609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [363/250]: training_loss: tensor(0.1486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [364/250]: training_loss: tensor(0.3150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [365/250]: training_loss: tensor(0.1920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [366/250]: training_loss: tensor(0.6009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [367/250]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [368/250]: training_loss: tensor(0.3351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [369/250]: training_loss: tensor(0.2375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [370/250]: training_loss: tensor(0.1442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [371/250]: training_loss: tensor(0.4499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [372/250]: training_loss: tensor(0.3884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [373/250]: training_loss: tensor(0.1404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [374/250]: training_loss: tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [375/250]: training_loss: tensor(0.1411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [376/250]: training_loss: tensor(0.6291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [377/250]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [378/250]: training_loss: tensor(0.1332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [379/250]: training_loss: tensor(0.1591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [380/250]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [381/250]: training_loss: tensor(0.1413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [382/250]: training_loss: tensor(0.3556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [383/250]: training_loss: tensor(0.1848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [384/250]: training_loss: tensor(0.1342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [385/250]: training_loss: tensor(0.2116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [386/250]: training_loss: tensor(0.1470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [387/250]: training_loss: tensor(0.1949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [388/250]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [389/250]: training_loss: tensor(0.3051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [390/250]: training_loss: tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [391/250]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [392/250]: training_loss: tensor(0.1729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [393/250]: training_loss: tensor(0.6013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [394/250]: training_loss: tensor(0.3599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [395/250]: training_loss: tensor(0.4119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [396/250]: training_loss: tensor(0.1596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [397/250]: training_loss: tensor(0.5363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [398/250]: training_loss: tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [399/250]: training_loss: tensor(1.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [400/250]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [401/250]: training_loss: tensor(0.4816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [402/250]: training_loss: tensor(0.3986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [403/250]: training_loss: tensor(0.5024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [404/250]: training_loss: tensor(0.2635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [405/250]: training_loss: tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [406/250]: training_loss: tensor(0.3968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [407/250]: training_loss: tensor(0.4210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [408/250]: training_loss: tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [409/250]: training_loss: tensor(0.4520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [410/250]: training_loss: tensor(0.2408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [411/250]: training_loss: tensor(0.1169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [412/250]: training_loss: tensor(0.2560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [413/250]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [414/250]: training_loss: tensor(0.1298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [415/250]: training_loss: tensor(0.1144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [416/250]: training_loss: tensor(0.3990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [417/250]: training_loss: tensor(0.2382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [418/250]: training_loss: tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [419/250]: training_loss: tensor(0.4165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [420/250]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [421/250]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [422/250]: training_loss: tensor(0.4199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [423/250]: training_loss: tensor(0.4499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [424/250]: training_loss: tensor(0.1830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [425/250]: training_loss: tensor(0.2999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [426/250]: training_loss: tensor(0.4620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [427/250]: training_loss: tensor(0.4153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [428/250]: training_loss: tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [429/250]: training_loss: tensor(0.9483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [430/250]: training_loss: tensor(0.4309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [431/250]: training_loss: tensor(0.1592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [432/250]: training_loss: tensor(0.5929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [433/250]: training_loss: tensor(0.1537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [434/250]: training_loss: tensor(0.2245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [435/250]: training_loss: tensor(1.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [436/250]: training_loss: tensor(0.7861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [437/250]: training_loss: tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [438/250]: training_loss: tensor(0.1634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [439/250]: training_loss: tensor(0.4883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [440/250]: training_loss: tensor(0.2407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [441/250]: training_loss: tensor(0.5090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [442/250]: training_loss: tensor(0.1710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [443/250]: training_loss: tensor(0.2241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [444/250]: training_loss: tensor(0.2599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [445/250]: training_loss: tensor(0.3423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [446/250]: training_loss: tensor(0.2695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [447/250]: training_loss: tensor(0.1486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [448/250]: training_loss: tensor(0.1844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [449/250]: training_loss: tensor(0.2102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [450/250]: training_loss: tensor(0.3623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [451/250]: training_loss: tensor(0.1585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [452/250]: training_loss: tensor(0.3011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [453/250]: training_loss: tensor(0.1966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [454/250]: training_loss: tensor(0.1487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [455/250]: training_loss: tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [456/250]: training_loss: tensor(0.3538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [457/250]: training_loss: tensor(0.5426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [458/250]: training_loss: tensor(0.1261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [459/250]: training_loss: tensor(0.2902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [460/250]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [461/250]: training_loss: tensor(0.1263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [462/250]: training_loss: tensor(0.1768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [463/250]: training_loss: tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [464/250]: training_loss: tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [465/250]: training_loss: tensor(0.1463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [466/250]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [467/250]: training_loss: tensor(0.4976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [468/250]: training_loss: tensor(0.5393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [469/250]: training_loss: tensor(0.3500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [470/250]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [471/250]: training_loss: tensor(0.2179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [472/250]: training_loss: tensor(0.2081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [473/250]: training_loss: tensor(0.2962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [474/250]: training_loss: tensor(0.3471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [475/250]: training_loss: tensor(0.2408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [476/250]: training_loss: tensor(0.2679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [477/250]: training_loss: tensor(0.2643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [478/250]: training_loss: tensor(0.3117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [479/250]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [480/250]: training_loss: tensor(0.7164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [481/250]: training_loss: tensor(0.5293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [482/250]: training_loss: tensor(0.1762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [483/250]: training_loss: tensor(0.1619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [484/250]: training_loss: tensor(0.1611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [485/250]: training_loss: tensor(0.2747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [486/250]: training_loss: tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [487/250]: training_loss: tensor(0.3746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [488/250]: training_loss: tensor(0.2400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [489/250]: training_loss: tensor(0.1464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [490/250]: training_loss: tensor(0.3072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [491/250]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [492/250]: training_loss: tensor(0.7758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [493/250]: training_loss: tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [494/250]: training_loss: tensor(0.1699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [495/250]: training_loss: tensor(0.1892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [496/250]: training_loss: tensor(0.1305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [497/250]: training_loss: tensor(0.1928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [498/250]: training_loss: tensor(0.2454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [499/250]: training_loss: tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [500/250]: training_loss: tensor(0.2008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [501/250]: training_loss: tensor(0.3326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [502/250]: training_loss: tensor(0.4907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [503/250]: training_loss: tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [504/250]: training_loss: tensor(0.1946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [505/250]: training_loss: tensor(0.4840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [506/250]: training_loss: tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [507/250]: training_loss: tensor(0.2370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [508/250]: training_loss: tensor(0.2563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [509/250]: training_loss: tensor(0.7337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [510/250]: training_loss: tensor(0.1690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [511/250]: training_loss: tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [512/250]: training_loss: tensor(0.2516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [513/250]: training_loss: tensor(0.7559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [514/250]: training_loss: tensor(0.1642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [515/250]: training_loss: tensor(0.2723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [516/250]: training_loss: tensor(0.5570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [517/250]: training_loss: tensor(0.5241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [518/250]: training_loss: tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [519/250]: training_loss: tensor(0.3161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [520/250]: training_loss: tensor(0.6335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [521/250]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [522/250]: training_loss: tensor(0.3354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [523/250]: training_loss: tensor(0.1116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [524/250]: training_loss: tensor(0.2347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [525/250]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [526/250]: training_loss: tensor(0.3131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [527/250]: training_loss: tensor(0.1677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [528/250]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [529/250]: training_loss: tensor(0.3809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [530/250]: training_loss: tensor(0.3928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [531/250]: training_loss: tensor(0.7048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [532/250]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [533/250]: training_loss: tensor(0.2197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [534/250]: training_loss: tensor(0.5499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [535/250]: training_loss: tensor(0.5941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [536/250]: training_loss: tensor(0.3582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [537/250]: training_loss: tensor(0.1865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [538/250]: training_loss: tensor(0.2675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [539/250]: training_loss: tensor(0.1205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [540/250]: training_loss: tensor(0.3941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [541/250]: training_loss: tensor(0.1553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [542/250]: training_loss: tensor(0.3170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [543/250]: training_loss: tensor(0.4322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [544/250]: training_loss: tensor(0.1363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [545/250]: training_loss: tensor(0.3399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [546/250]: training_loss: tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [547/250]: training_loss: tensor(0.4175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [548/250]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [549/250]: training_loss: tensor(0.2428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [550/250]: training_loss: tensor(0.5485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [551/250]: training_loss: tensor(0.2576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [552/250]: training_loss: tensor(0.2986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [553/250]: training_loss: tensor(0.3908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [554/250]: training_loss: tensor(0.2827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [555/250]: training_loss: tensor(0.2963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [556/250]: training_loss: tensor(0.3174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [557/250]: training_loss: tensor(0.2291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [558/250]: training_loss: tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [559/250]: training_loss: tensor(0.3058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [560/250]: training_loss: tensor(0.4863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [561/250]: training_loss: tensor(0.3727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [562/250]: training_loss: tensor(0.1164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [563/250]: training_loss: tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [564/250]: training_loss: tensor(0.4124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [565/250]: training_loss: tensor(0.1825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [566/250]: training_loss: tensor(0.2931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [567/250]: training_loss: tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [568/250]: training_loss: tensor(0.4605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [569/250]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [570/250]: training_loss: tensor(0.1854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [571/250]: training_loss: tensor(0.2130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [572/250]: training_loss: tensor(0.4355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [573/250]: training_loss: tensor(0.2803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [574/250]: training_loss: tensor(0.2335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [575/250]: training_loss: tensor(0.5068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [576/250]: training_loss: tensor(0.1713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [577/250]: training_loss: tensor(0.4598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [578/250]: training_loss: tensor(0.1274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [579/250]: training_loss: tensor(0.7788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [580/250]: training_loss: tensor(0.1368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [581/250]: training_loss: tensor(0.1908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [582/250]: training_loss: tensor(0.2428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [583/250]: training_loss: tensor(0.2339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [584/250]: training_loss: tensor(0.2481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [585/250]: training_loss: tensor(0.4097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [586/250]: training_loss: tensor(0.1603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [587/250]: training_loss: tensor(0.2468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [588/250]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [589/250]: training_loss: tensor(0.1555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [590/250]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [591/250]: training_loss: tensor(0.5170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [592/250]: training_loss: tensor(0.1412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [593/250]: training_loss: tensor(0.2138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [594/250]: training_loss: tensor(0.1530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [595/250]: training_loss: tensor(0.2985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [596/250]: training_loss: tensor(0.2375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [597/250]: training_loss: tensor(0.4985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [598/250]: training_loss: tensor(0.2264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [599/250]: training_loss: tensor(0.8247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [600/250]: training_loss: tensor(0.1695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [601/250]: training_loss: tensor(0.2972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [602/250]: training_loss: tensor(0.1173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [603/250]: training_loss: tensor(0.1678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [604/250]: training_loss: tensor(0.7436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [605/250]: training_loss: tensor(0.3157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [606/250]: training_loss: tensor(0.1365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [607/250]: training_loss: tensor(0.1610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [608/250]: training_loss: tensor(0.1068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [609/250]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [610/250]: training_loss: tensor(0.1241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [611/250]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [612/250]: training_loss: tensor(0.3701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [613/250]: training_loss: tensor(0.4216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [614/250]: training_loss: tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [615/250]: training_loss: tensor(0.7524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [616/250]: training_loss: tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [617/250]: training_loss: tensor(0.2325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [618/250]: training_loss: tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [619/250]: training_loss: tensor(0.3804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [620/250]: training_loss: tensor(0.2067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [621/250]: training_loss: tensor(0.3393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [622/250]: training_loss: tensor(0.1085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [623/250]: training_loss: tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [624/250]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [625/250]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [626/250]: training_loss: tensor(0.6127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [627/250]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [628/250]: training_loss: tensor(0.5615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [629/250]: training_loss: tensor(0.2186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [630/250]: training_loss: tensor(0.2585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [631/250]: training_loss: tensor(0.2055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [632/250]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [633/250]: training_loss: tensor(0.2982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [634/250]: training_loss: tensor(0.1711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [635/250]: training_loss: tensor(0.3780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [636/250]: training_loss: tensor(0.3227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [637/250]: training_loss: tensor(0.1851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [638/250]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [639/250]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [640/250]: training_loss: tensor(0.4410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [641/250]: training_loss: tensor(0.2998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [642/250]: training_loss: tensor(0.1240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [643/250]: training_loss: tensor(0.1624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [644/250]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [645/250]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [646/250]: training_loss: tensor(0.2253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [647/250]: training_loss: tensor(0.3881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [648/250]: training_loss: tensor(0.1901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [649/250]: training_loss: tensor(0.4909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [650/250]: training_loss: tensor(0.4122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [651/250]: training_loss: tensor(0.1367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [652/250]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [653/250]: training_loss: tensor(0.5694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [654/250]: training_loss: tensor(0.1188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [655/250]: training_loss: tensor(0.5566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [656/250]: training_loss: tensor(0.1846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [657/250]: training_loss: tensor(0.6638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [658/250]: training_loss: tensor(0.5399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [659/250]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [660/250]: training_loss: tensor(0.1201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [661/250]: training_loss: tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [662/250]: training_loss: tensor(0.3287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [663/250]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [664/250]: training_loss: tensor(0.3414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [665/250]: training_loss: tensor(0.2908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [666/250]: training_loss: tensor(0.4056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [667/250]: training_loss: tensor(0.3590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [668/250]: training_loss: tensor(0.1402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [669/250]: training_loss: tensor(0.4442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [670/250]: training_loss: tensor(0.2968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [671/250]: training_loss: tensor(0.4292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [672/250]: training_loss: tensor(0.6005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [673/250]: training_loss: tensor(0.3921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [674/250]: training_loss: tensor(0.2970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [675/250]: training_loss: tensor(0.3068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [676/250]: training_loss: tensor(0.2690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [677/250]: training_loss: tensor(0.4982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [678/250]: training_loss: tensor(0.8816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [679/250]: training_loss: tensor(0.2403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [680/250]: training_loss: tensor(0.6856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [681/250]: training_loss: tensor(0.4222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [682/250]: training_loss: tensor(0.1809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [683/250]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [684/250]: training_loss: tensor(0.4235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [685/250]: training_loss: tensor(0.1739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [686/250]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [687/250]: training_loss: tensor(0.2698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [688/250]: training_loss: tensor(0.2204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [689/250]: training_loss: tensor(0.2261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [690/250]: training_loss: tensor(0.3013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [691/250]: training_loss: tensor(0.2850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [692/250]: training_loss: tensor(0.4468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [693/250]: training_loss: tensor(0.1111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [694/250]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [695/250]: training_loss: tensor(0.2692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [696/250]: training_loss: tensor(0.4304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [697/250]: training_loss: tensor(0.2495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [698/250]: training_loss: tensor(0.1776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [699/250]: training_loss: tensor(0.1990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [700/250]: training_loss: tensor(0.1501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [701/250]: training_loss: tensor(0.3141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [702/250]: training_loss: tensor(0.4506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [703/250]: training_loss: tensor(0.8236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [704/250]: training_loss: tensor(0.2501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [705/250]: training_loss: tensor(0.4553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [706/250]: training_loss: tensor(0.5145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [707/250]: training_loss: tensor(0.2845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [708/250]: training_loss: tensor(0.1401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [709/250]: training_loss: tensor(0.4225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [710/250]: training_loss: tensor(0.2485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [711/250]: training_loss: tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [712/250]: training_loss: tensor(0.1147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [713/250]: training_loss: tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [714/250]: training_loss: tensor(0.4836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [715/250]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [716/250]: training_loss: tensor(0.4522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [717/250]: training_loss: tensor(0.1256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [718/250]: training_loss: tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [719/250]: training_loss: tensor(0.1206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [720/250]: training_loss: tensor(0.3133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [721/250]: training_loss: tensor(0.1782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [722/250]: training_loss: tensor(0.2119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [723/250]: training_loss: tensor(0.1593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [724/250]: training_loss: tensor(0.1241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [725/250]: training_loss: tensor(0.7171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [726/250]: training_loss: tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [727/250]: training_loss: tensor(0.3012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [728/250]: training_loss: tensor(0.3022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [729/250]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [730/250]: training_loss: tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [731/250]: training_loss: tensor(0.3255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [732/250]: training_loss: tensor(0.2896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [733/250]: training_loss: tensor(0.4989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [734/250]: training_loss: tensor(0.1951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [735/250]: training_loss: tensor(0.2116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [736/250]: training_loss: tensor(0.1578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [737/250]: training_loss: tensor(0.1558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [738/250]: training_loss: tensor(0.2316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [739/250]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [740/250]: training_loss: tensor(0.1788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [741/250]: training_loss: tensor(0.4451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [742/250]: training_loss: tensor(0.3130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [743/250]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [744/250]: training_loss: tensor(0.4149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [745/250]: training_loss: tensor(0.2690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [746/250]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [747/250]: training_loss: tensor(0.1373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [748/250]: training_loss: tensor(0.3257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [749/250]: training_loss: tensor(0.1459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [750/250]: training_loss: tensor(0.4619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [751/250]: training_loss: tensor(0.2040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [752/250]: training_loss: tensor(0.1819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [753/250]: training_loss: tensor(1.0647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [754/250]: training_loss: tensor(0.2941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [755/250]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [756/250]: training_loss: tensor(0.2973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [757/250]: training_loss: tensor(0.4164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [758/250]: training_loss: tensor(0.1344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [759/250]: training_loss: tensor(0.6524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [760/250]: training_loss: tensor(0.3285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [761/250]: training_loss: tensor(0.2679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [762/250]: training_loss: tensor(0.1387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [763/250]: training_loss: tensor(0.1568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [764/250]: training_loss: tensor(0.2224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [765/250]: training_loss: tensor(0.1682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [766/250]: training_loss: tensor(0.2327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [767/250]: training_loss: tensor(0.6481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [768/250]: training_loss: tensor(0.6063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [769/250]: training_loss: tensor(0.1812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [770/250]: training_loss: tensor(0.1312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [771/250]: training_loss: tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [772/250]: training_loss: tensor(0.1612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [773/250]: training_loss: tensor(0.4230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [774/250]: training_loss: tensor(0.3517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [775/250]: training_loss: tensor(0.2014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [776/250]: training_loss: tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [777/250]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [778/250]: training_loss: tensor(0.1132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [779/250]: training_loss: tensor(0.3437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [780/250]: training_loss: tensor(0.1307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [781/250]: training_loss: tensor(0.4171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [782/250]: training_loss: tensor(0.4849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [783/250]: training_loss: tensor(0.3727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [784/250]: training_loss: tensor(0.2810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [785/250]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [786/250]: training_loss: tensor(0.5292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [787/250]: training_loss: tensor(0.2664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [788/250]: training_loss: tensor(0.5253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [789/250]: training_loss: tensor(0.2537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [790/250]: training_loss: tensor(0.1895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [791/250]: training_loss: tensor(0.1391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [792/250]: training_loss: tensor(0.2482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [793/250]: training_loss: tensor(0.1906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [794/250]: training_loss: tensor(0.1108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [795/250]: training_loss: tensor(0.2049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [796/250]: training_loss: tensor(0.4164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [797/250]: training_loss: tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [798/250]: training_loss: tensor(0.3186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [799/250]: training_loss: tensor(0.2211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [800/250]: training_loss: tensor(0.5013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [801/250]: training_loss: tensor(0.8637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [802/250]: training_loss: tensor(0.4233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [803/250]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [804/250]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [805/250]: training_loss: tensor(0.1904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [806/250]: training_loss: tensor(0.2458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [807/250]: training_loss: tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [808/250]: training_loss: tensor(0.2473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [809/250]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [810/250]: training_loss: tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [811/250]: training_loss: tensor(1.0429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [812/250]: training_loss: tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [813/250]: training_loss: tensor(0.7998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [814/250]: training_loss: tensor(0.7260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [815/250]: training_loss: tensor(0.2216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [816/250]: training_loss: tensor(0.2305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [817/250]: training_loss: tensor(0.2478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [818/250]: training_loss: tensor(0.1338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [819/250]: training_loss: tensor(0.2015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [820/250]: training_loss: tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [821/250]: training_loss: tensor(0.3964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [822/250]: training_loss: tensor(0.2489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [823/250]: training_loss: tensor(0.3011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [824/250]: training_loss: tensor(0.9307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [825/250]: training_loss: tensor(0.3457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [826/250]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [827/250]: training_loss: tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [828/250]: training_loss: tensor(0.2187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [829/250]: training_loss: tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [830/250]: training_loss: tensor(0.4045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [831/250]: training_loss: tensor(0.4329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [832/250]: training_loss: tensor(0.2951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [833/250]: training_loss: tensor(0.3048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [834/250]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [835/250]: training_loss: tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [836/250]: training_loss: tensor(0.1663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [837/250]: training_loss: tensor(0.6626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [838/250]: training_loss: tensor(0.1651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [839/250]: training_loss: tensor(0.1760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [840/250]: training_loss: tensor(0.3084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [841/250]: training_loss: tensor(0.4928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [842/250]: training_loss: tensor(0.4641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [843/250]: training_loss: tensor(0.1948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [844/250]: training_loss: tensor(0.3477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [845/250]: training_loss: tensor(0.3704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [846/250]: training_loss: tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [847/250]: training_loss: tensor(0.4710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [848/250]: training_loss: tensor(0.2690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [849/250]: training_loss: tensor(0.4665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [850/250]: training_loss: tensor(0.4008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [851/250]: training_loss: tensor(0.4899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [852/250]: training_loss: tensor(0.2530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [853/250]: training_loss: tensor(0.1270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [854/250]: training_loss: tensor(0.2234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [855/250]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [856/250]: training_loss: tensor(0.3251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [857/250]: training_loss: tensor(0.2873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [858/250]: training_loss: tensor(0.3504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [859/250]: training_loss: tensor(0.2852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [860/250]: training_loss: tensor(0.3277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [861/250]: training_loss: tensor(0.4080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [862/250]: training_loss: tensor(0.1569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [863/250]: training_loss: tensor(0.1948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [864/250]: training_loss: tensor(0.5146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [865/250]: training_loss: tensor(0.3756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [866/250]: training_loss: tensor(0.4756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [867/250]: training_loss: tensor(0.3122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [868/250]: training_loss: tensor(0.1600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [869/250]: training_loss: tensor(0.1794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [870/250]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [871/250]: training_loss: tensor(0.2488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [872/250]: training_loss: tensor(0.2818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [873/250]: training_loss: tensor(0.3910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [874/250]: training_loss: tensor(0.1618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [875/250]: training_loss: tensor(0.4118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [876/250]: training_loss: tensor(0.1744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [877/250]: training_loss: tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [878/250]: training_loss: tensor(0.2777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [879/250]: training_loss: tensor(0.3328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [880/250]: training_loss: tensor(0.7755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [881/250]: training_loss: tensor(0.3412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [882/250]: training_loss: tensor(0.3195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [883/250]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [884/250]: training_loss: tensor(0.5879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [885/250]: training_loss: tensor(0.3050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [886/250]: training_loss: tensor(0.1430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [887/250]: training_loss: tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [888/250]: training_loss: tensor(0.2117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [889/250]: training_loss: tensor(0.1596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [890/250]: training_loss: tensor(0.3932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [891/250]: training_loss: tensor(0.1738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [892/250]: training_loss: tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [893/250]: training_loss: tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [894/250]: training_loss: tensor(0.4756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [895/250]: training_loss: tensor(0.1808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [896/250]: training_loss: tensor(0.6839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [897/250]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [898/250]: training_loss: tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [899/250]: training_loss: tensor(0.2045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [900/250]: training_loss: tensor(0.1244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [901/250]: training_loss: tensor(0.2573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [902/250]: training_loss: tensor(0.2625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [903/250]: training_loss: tensor(0.3116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [904/250]: training_loss: tensor(0.2309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [905/250]: training_loss: tensor(0.8113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [906/250]: training_loss: tensor(0.4208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [907/250]: training_loss: tensor(0.1427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [908/250]: training_loss: tensor(0.3225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [909/250]: training_loss: tensor(0.2039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [910/250]: training_loss: tensor(0.2901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [911/250]: training_loss: tensor(0.1794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [912/250]: training_loss: tensor(0.3471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [913/250]: training_loss: tensor(0.4463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [914/250]: training_loss: tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [915/250]: training_loss: tensor(0.2665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [916/250]: training_loss: tensor(0.4343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [917/250]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [918/250]: training_loss: tensor(0.4816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [919/250]: training_loss: tensor(0.1411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [920/250]: training_loss: tensor(0.3498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [921/250]: training_loss: tensor(0.3404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [922/250]: training_loss: tensor(0.2503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [923/250]: training_loss: tensor(0.2702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [924/250]: training_loss: tensor(0.1826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [925/250]: training_loss: tensor(0.2013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [926/250]: training_loss: tensor(0.5039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [927/250]: training_loss: tensor(0.4921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [928/250]: training_loss: tensor(0.4356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [929/250]: training_loss: tensor(0.3636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [930/250]: training_loss: tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [931/250]: training_loss: tensor(0.6715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [932/250]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [933/250]: training_loss: tensor(0.3693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [934/250]: training_loss: tensor(0.2831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [935/250]: training_loss: tensor(0.1783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [936/250]: training_loss: tensor(0.2444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [937/250]: training_loss: tensor(0.1745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [938/250]: training_loss: tensor(0.1968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [939/250]: training_loss: tensor(0.2508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [940/250]: training_loss: tensor(0.1436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [941/250]: training_loss: tensor(0.1649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [942/250]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [943/250]: training_loss: tensor(0.2869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [944/250]: training_loss: tensor(0.1838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [945/250]: training_loss: tensor(0.5249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [946/250]: training_loss: tensor(0.3596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [947/250]: training_loss: tensor(0.5463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [948/250]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [949/250]: training_loss: tensor(0.6962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [950/250]: training_loss: tensor(0.3208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [951/250]: training_loss: tensor(0.1332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [952/250]: training_loss: tensor(0.3278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [953/250]: training_loss: tensor(0.4010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [954/250]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [955/250]: training_loss: tensor(0.2677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [956/250]: training_loss: tensor(0.2918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [957/250]: training_loss: tensor(0.1952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [958/250]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [959/250]: training_loss: tensor(0.3398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [960/250]: training_loss: tensor(0.4223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [961/250]: training_loss: tensor(0.2774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [962/250]: training_loss: tensor(0.3787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [963/250]: training_loss: tensor(0.4614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [964/250]: training_loss: tensor(0.4743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [965/250]: training_loss: tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [966/250]: training_loss: tensor(0.1886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [967/250]: training_loss: tensor(0.3514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [968/250]: training_loss: tensor(0.4652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [969/250]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [970/250]: training_loss: tensor(0.7552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [971/250]: training_loss: tensor(0.4131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [972/250]: training_loss: tensor(0.2175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [973/250]: training_loss: tensor(0.1465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [974/250]: training_loss: tensor(0.1500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [975/250]: training_loss: tensor(0.1043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [976/250]: training_loss: tensor(0.3294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [977/250]: training_loss: tensor(0.3860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [978/250]: training_loss: tensor(0.2929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [979/250]: training_loss: tensor(0.6241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [980/250]: training_loss: tensor(0.1575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [981/250]: training_loss: tensor(0.1501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [982/250]: training_loss: tensor(0.3024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [983/250]: training_loss: tensor(0.4674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [984/250]: training_loss: tensor(0.4048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [985/250]: training_loss: tensor(0.2590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [986/250]: training_loss: tensor(0.3122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [987/250]: training_loss: tensor(0.2332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [988/250]: training_loss: tensor(0.2436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [989/250]: training_loss: tensor(0.5515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [990/250]: training_loss: tensor(0.2295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [991/250]: training_loss: tensor(0.1916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [992/250]: training_loss: tensor(0.1302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [993/250]: training_loss: tensor(0.4236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [994/250]: training_loss: tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [995/250]: training_loss: tensor(0.3863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [996/250]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [997/250]: training_loss: tensor(0.3774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [998/250]: training_loss: tensor(0.3700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [999/250]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1000/250]: training_loss: tensor(0.4357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1001/250]: training_loss: tensor(0.1614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1002/250]: training_loss: tensor(0.1171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1003/250]: training_loss: tensor(0.3057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1004/250]: training_loss: tensor(0.1147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1005/250]: training_loss: tensor(0.9441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/250]: training_loss: tensor(0.3051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/250]: training_loss: tensor(0.4979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/250]: training_loss: tensor(0.2523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/250]: training_loss: tensor(0.3726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/250]: training_loss: tensor(0.2092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/250]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/250]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/250]: training_loss: tensor(0.4527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/250]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/250]: training_loss: tensor(0.0620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/250]: training_loss: tensor(0.1913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/250]: training_loss: tensor(0.2811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/250]: training_loss: tensor(0.1541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1019/250]: training_loss: tensor(0.3036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/250]: training_loss: tensor(0.3941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/250]: training_loss: tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/250]: training_loss: tensor(0.1764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1023/250]: training_loss: tensor(0.1064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/250]: training_loss: tensor(0.2563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/250]: training_loss: tensor(0.2566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/250]: training_loss: tensor(0.2481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/250]: training_loss: tensor(0.5692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/250]: training_loss: tensor(0.2749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/250]: training_loss: tensor(0.3268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/250]: training_loss: tensor(0.5044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/250]: training_loss: tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/250]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/250]: training_loss: tensor(0.3898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/250]: training_loss: tensor(0.3843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/250]: training_loss: tensor(0.1890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/250]: training_loss: tensor(0.2991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/250]: training_loss: tensor(0.5375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/250]: training_loss: tensor(0.5821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/250]: training_loss: tensor(0.1346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1040/250]: training_loss: tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/250]: training_loss: tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/250]: training_loss: tensor(0.1473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1043/250]: training_loss: tensor(0.7353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/250]: training_loss: tensor(0.5024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/250]: training_loss: tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/250]: training_loss: tensor(0.2812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/250]: training_loss: tensor(0.1939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/250]: training_loss: tensor(0.1637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/250]: training_loss: tensor(0.4531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/250]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/250]: training_loss: tensor(0.3984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/250]: training_loss: tensor(0.2052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/250]: training_loss: tensor(0.2408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/250]: training_loss: tensor(0.5508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/250]: training_loss: tensor(0.4380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/250]: training_loss: tensor(0.3482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/250]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/250]: training_loss: tensor(0.2921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/250]: training_loss: tensor(0.3303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/250]: training_loss: tensor(0.2772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/250]: training_loss: tensor(0.3072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/250]: training_loss: tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/250]: training_loss: tensor(0.2655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/250]: training_loss: tensor(0.1177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/250]: training_loss: tensor(0.2022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/250]: training_loss: tensor(0.3323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/250]: training_loss: tensor(0.1960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/250]: training_loss: tensor(0.5416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/250]: training_loss: tensor(0.4872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/250]: training_loss: tensor(0.2467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/250]: training_loss: tensor(0.1043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/250]: training_loss: tensor(0.3637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/250]: training_loss: tensor(0.3012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/250]: training_loss: tensor(0.0611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/250]: training_loss: tensor(0.1557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/250]: training_loss: tensor(0.1437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/250]: training_loss: tensor(0.2822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/250]: training_loss: tensor(0.3381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/250]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/250]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/250]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/250]: training_loss: tensor(0.5719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/250]: training_loss: tensor(0.5402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/250]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/250]: training_loss: tensor(0.4790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/250]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/250]: training_loss: tensor(0.6629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/250]: training_loss: tensor(0.5055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/250]: training_loss: tensor(0.2770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/250]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/250]: training_loss: tensor(0.1271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/250]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/250]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/250]: training_loss: tensor(0.3629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/250]: training_loss: tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/250]: training_loss: tensor(0.2892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/250]: training_loss: tensor(0.3728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/250]: training_loss: tensor(0.4083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/250]: training_loss: tensor(0.1946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1100/250]: training_loss: tensor(0.5125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/250]: training_loss: tensor(0.1789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/250]: training_loss: tensor(0.2103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/250]: training_loss: tensor(0.1576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/250]: training_loss: tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/250]: training_loss: tensor(0.1234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/250]: training_loss: tensor(0.3937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/250]: training_loss: tensor(0.2352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/250]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/250]: training_loss: tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/250]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/250]: training_loss: tensor(0.1566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/250]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/250]: training_loss: tensor(0.5311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/250]: training_loss: tensor(0.2252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/250]: training_loss: tensor(0.2826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/250]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/250]: training_loss: tensor(0.1964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/250]: training_loss: tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/250]: training_loss: tensor(0.1283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/250]: training_loss: tensor(0.2196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/250]: training_loss: tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/250]: training_loss: tensor(0.1691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/250]: training_loss: tensor(0.0666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/250]: training_loss: tensor(0.4530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/250]: training_loss: tensor(0.1500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/250]: training_loss: tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/250]: training_loss: tensor(0.2053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/250]: training_loss: tensor(0.1066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1129/250]: training_loss: tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/250]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/250]: training_loss: tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/250]: training_loss: tensor(0.2045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/250]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/250]: training_loss: tensor(0.2257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/250]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/250]: training_loss: tensor(0.3758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/250]: training_loss: tensor(0.3187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/250]: training_loss: tensor(0.1743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/250]: training_loss: tensor(0.4562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/250]: training_loss: tensor(0.1904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/250]: training_loss: tensor(0.1382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/250]: training_loss: tensor(0.2275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/250]: training_loss: tensor(0.5006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/250]: training_loss: tensor(0.0648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/250]: training_loss: tensor(0.2306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/250]: training_loss: tensor(0.4123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/250]: training_loss: tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/250]: training_loss: tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/250]: training_loss: tensor(0.1711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/250]: training_loss: tensor(0.2377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/250]: training_loss: tensor(0.4301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/250]: training_loss: tensor(0.4612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/250]: training_loss: tensor(0.5498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/250]: training_loss: tensor(0.2913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/250]: training_loss: tensor(0.1579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/250]: training_loss: tensor(0.2668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/250]: training_loss: tensor(0.2723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/250]: training_loss: tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/250]: training_loss: tensor(0.1281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/250]: training_loss: tensor(0.5136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/250]: training_loss: tensor(0.3067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/250]: training_loss: tensor(0.2141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/250]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/250]: training_loss: tensor(0.0650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/250]: training_loss: tensor(0.1685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/250]: training_loss: tensor(0.1436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/250]: training_loss: tensor(0.1511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/250]: training_loss: tensor(0.2812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/250]: training_loss: tensor(0.2873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/250]: training_loss: tensor(0.1237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/250]: training_loss: tensor(0.2552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/250]: training_loss: tensor(0.2984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/250]: training_loss: tensor(0.1665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/250]: training_loss: tensor(0.0494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/250]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/250]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/250]: training_loss: tensor(0.2570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/250]: training_loss: tensor(0.3326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/250]: training_loss: tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/250]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/250]: training_loss: tensor(0.1956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/250]: training_loss: tensor(0.1765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/250]: training_loss: tensor(0.1595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/250]: training_loss: tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/250]: training_loss: tensor(0.4092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/250]: training_loss: tensor(0.1998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/250]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/250]: training_loss: tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/250]: training_loss: tensor(0.1704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/250]: training_loss: tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/250]: training_loss: tensor(0.2086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/250]: training_loss: tensor(0.4215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/250]: training_loss: tensor(0.1597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/250]: training_loss: tensor(0.2259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/250]: training_loss: tensor(0.6186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/250]: training_loss: tensor(0.1762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/250]: training_loss: tensor(0.4152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/250]: training_loss: tensor(0.5773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/250]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/250]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/250]: training_loss: tensor(1.1795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/250]: training_loss: tensor(0.3705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/250]: training_loss: tensor(0.3742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/250]: training_loss: tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/250]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/250]: training_loss: tensor(0.5850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/250]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/250]: training_loss: tensor(0.2508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/250]: training_loss: tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/250]: training_loss: tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/250]: training_loss: tensor(0.2559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/250]: training_loss: tensor(0.3211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1213/250]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/250]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1215/250]: training_loss: tensor(0.1225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/250]: training_loss: tensor(0.1656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/250]: training_loss: tensor(0.2006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/250]: training_loss: tensor(0.1408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/250]: training_loss: tensor(0.4132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/250]: training_loss: tensor(0.1809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/250]: training_loss: tensor(0.5653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/250]: training_loss: tensor(0.1355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/250]: training_loss: tensor(0.1676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/250]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/250]: training_loss: tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/250]: training_loss: tensor(0.1433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/250]: training_loss: tensor(0.5612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/250]: training_loss: tensor(0.2682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/250]: training_loss: tensor(0.1805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/250]: training_loss: tensor(0.3879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/250]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/250]: training_loss: tensor(0.7464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/250]: training_loss: tensor(0.6493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/250]: training_loss: tensor(0.3333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/250]: training_loss: tensor(0.2054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/250]: training_loss: tensor(0.3043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/250]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/250]: training_loss: tensor(0.1416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/250]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/250]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/250]: training_loss: tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/250]: training_loss: tensor(0.2451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/250]: training_loss: tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/250]: training_loss: tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/250]: training_loss: tensor(0.4396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/250]: training_loss: tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/250]: training_loss: tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/250]: training_loss: tensor(0.2154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/250]: training_loss: tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/250]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/250]: training_loss: tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/250]: training_loss: tensor(0.4516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/250]: training_loss: tensor(0.3274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/250]: training_loss: tensor(0.3318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/250]: training_loss: tensor(0.1617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/250]: training_loss: tensor(0.2196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/250]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/250]: training_loss: tensor(0.7579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/250]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/250]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/250]: training_loss: tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/250]: training_loss: tensor(0.0575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/250]: training_loss: tensor(0.1957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/250]: training_loss: tensor(0.2853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/250]: training_loss: tensor(0.3193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/250]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/250]: training_loss: tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/250]: training_loss: tensor(0.1748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/250]: training_loss: tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/250]: training_loss: tensor(0.2235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/250]: training_loss: tensor(0.1391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/250]: training_loss: tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/250]: training_loss: tensor(0.1983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/250]: training_loss: tensor(0.2049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/250]: training_loss: tensor(0.5477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/250]: training_loss: tensor(0.2896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/250]: training_loss: tensor(0.4961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/250]: training_loss: tensor(0.1596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/250]: training_loss: tensor(0.6071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/250]: training_loss: tensor(0.1857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/250]: training_loss: tensor(0.2256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/250]: training_loss: tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/250]: training_loss: tensor(0.4181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/250]: training_loss: tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/250]: training_loss: tensor(0.2255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/250]: training_loss: tensor(0.1474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/250]: training_loss: tensor(0.1577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/250]: training_loss: tensor(0.5168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/250]: training_loss: tensor(0.1743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/250]: training_loss: tensor(0.5568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/250]: training_loss: tensor(0.1338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/250]: training_loss: tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/250]: training_loss: tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/250]: training_loss: tensor(0.2091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/250]: training_loss: tensor(0.5376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/250]: training_loss: tensor(0.6398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/250]: training_loss: tensor(0.3431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/250]: training_loss: tensor(0.6241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1299/250]: training_loss: tensor(0.2994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/250]: training_loss: tensor(0.3554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/250]: training_loss: tensor(0.3013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/250]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/250]: training_loss: tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/250]: training_loss: tensor(0.1574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/250]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/250]: training_loss: tensor(0.1339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/250]: training_loss: tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/250]: training_loss: tensor(0.2063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/250]: training_loss: tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/250]: training_loss: tensor(0.1184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/250]: training_loss: tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/250]: training_loss: tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/250]: training_loss: tensor(0.4317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/250]: training_loss: tensor(0.5327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/250]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/250]: training_loss: tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/250]: training_loss: tensor(0.2097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/250]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/250]: training_loss: tensor(0.3809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/250]: training_loss: tensor(0.4107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/250]: training_loss: tensor(0.2064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/250]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/250]: training_loss: tensor(0.5890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/250]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/250]: training_loss: tensor(0.1925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/250]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/250]: training_loss: tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/250]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/250]: training_loss: tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/250]: training_loss: tensor(0.2286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/250]: training_loss: tensor(0.2199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/250]: training_loss: tensor(0.5165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/250]: training_loss: tensor(0.4249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/250]: training_loss: tensor(0.5221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/250]: training_loss: tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/250]: training_loss: tensor(0.2162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/250]: training_loss: tensor(0.1125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/250]: training_loss: tensor(0.5238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/250]: training_loss: tensor(0.2876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/250]: training_loss: tensor(0.3377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/250]: training_loss: tensor(0.5739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/250]: training_loss: tensor(0.1197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/250]: training_loss: tensor(0.2061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/250]: training_loss: tensor(0.3879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/250]: training_loss: tensor(0.9187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/250]: training_loss: tensor(0.1793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/250]: training_loss: tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/250]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/250]: training_loss: tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/250]: training_loss: tensor(0.4078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/250]: training_loss: tensor(0.1480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/250]: training_loss: tensor(0.1956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/250]: training_loss: tensor(0.2624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/250]: training_loss: tensor(0.0566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/250]: training_loss: tensor(0.1885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/250]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/250]: training_loss: tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/250]: training_loss: tensor(0.2839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/250]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/250]: training_loss: tensor(0.1770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1361/250]: training_loss: tensor(0.1316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/250]: training_loss: tensor(0.1482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1363/250]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/250]: training_loss: tensor(0.1584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/250]: training_loss: tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/250]: training_loss: tensor(0.1681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/250]: training_loss: tensor(0.4899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/250]: training_loss: tensor(0.0629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/250]: training_loss: tensor(0.7136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/250]: training_loss: tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/250]: training_loss: tensor(0.2300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/250]: training_loss: tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/250]: training_loss: tensor(0.5691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/250]: training_loss: tensor(0.0587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/250]: training_loss: tensor(0.3924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/250]: training_loss: tensor(0.2596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/250]: training_loss: tensor(0.2365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/250]: training_loss: tensor(0.2031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/250]: training_loss: tensor(0.2274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/250]: training_loss: tensor(0.8658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/250]: training_loss: tensor(0.1137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/250]: training_loss: tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/250]: training_loss: tensor(0.2474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/250]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1385/250]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1386/250]: training_loss: tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1387/250]: training_loss: tensor(0.1748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1388/250]: training_loss: tensor(0.5968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1389/250]: training_loss: tensor(0.2660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1390/250]: training_loss: tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1391/250]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1392/250]: training_loss: tensor(0.7409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1393/250]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1394/250]: training_loss: tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1395/250]: training_loss: tensor(0.1466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1396/250]: training_loss: tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1397/250]: training_loss: tensor(0.6062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1398/250]: training_loss: tensor(0.3989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1399/250]: training_loss: tensor(0.1285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1400/250]: training_loss: tensor(0.2690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1401/250]: training_loss: tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1402/250]: training_loss: tensor(0.1374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1403/250]: training_loss: tensor(0.2058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1404/250]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1405/250]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1406/250]: training_loss: tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1407/250]: training_loss: tensor(0.1455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1408/250]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1409/250]: training_loss: tensor(0.3075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1410/250]: training_loss: tensor(0.0676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1411/250]: training_loss: tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1412/250]: training_loss: tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1413/250]: training_loss: tensor(0.8091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1414/250]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1415/250]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1416/250]: training_loss: tensor(0.4192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1417/250]: training_loss: tensor(0.2876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1418/250]: training_loss: tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1419/250]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1420/250]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1421/250]: training_loss: tensor(0.2196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1422/250]: training_loss: tensor(0.8521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1423/250]: training_loss: tensor(0.2127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1424/250]: training_loss: tensor(0.4261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1425/250]: training_loss: tensor(0.4233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1426/250]: training_loss: tensor(0.2090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1427/250]: training_loss: tensor(0.1076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1428/250]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1429/250]: training_loss: tensor(0.5007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1430/250]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1431/250]: training_loss: tensor(0.1909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1432/250]: training_loss: tensor(0.3537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1433/250]: training_loss: tensor(0.2171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1434/250]: training_loss: tensor(0.1577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1435/250]: training_loss: tensor(0.2280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1436/250]: training_loss: tensor(0.2617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1437/250]: training_loss: tensor(0.2396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1438/250]: training_loss: tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1439/250]: training_loss: tensor(0.2704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1440/250]: training_loss: tensor(0.3481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1441/250]: training_loss: tensor(0.1645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1442/250]: training_loss: tensor(0.3822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1443/250]: training_loss: tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1444/250]: training_loss: tensor(0.2678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1445/250]: training_loss: tensor(0.1459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1446/250]: training_loss: tensor(0.5093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1447/250]: training_loss: tensor(0.1781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1448/250]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1449/250]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1450/250]: training_loss: tensor(0.2673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1451/250]: training_loss: tensor(0.4310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1452/250]: training_loss: tensor(0.3199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1453/250]: training_loss: tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1454/250]: training_loss: tensor(0.1344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1455/250]: training_loss: tensor(0.1652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1456/250]: training_loss: tensor(0.1338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1457/250]: training_loss: tensor(0.3991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1458/250]: training_loss: tensor(0.3449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1459/250]: training_loss: tensor(0.4796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1460/250]: training_loss: tensor(0.1452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1461/250]: training_loss: tensor(0.1900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1462/250]: training_loss: tensor(0.2831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1463/250]: training_loss: tensor(0.2118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1464/250]: training_loss: tensor(0.3505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1465/250]: training_loss: tensor(0.1875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1466/250]: training_loss: tensor(0.2441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1467/250]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1468/250]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1469/250]: training_loss: tensor(0.8054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1470/250]: training_loss: tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1471/250]: training_loss: tensor(0.7609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1472/250]: training_loss: tensor(0.3238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1473/250]: training_loss: tensor(0.2900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1474/250]: training_loss: tensor(0.5211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1475/250]: training_loss: tensor(0.1190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1476/250]: training_loss: tensor(0.1852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1477/250]: training_loss: tensor(0.4164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1478/250]: training_loss: tensor(0.0493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1479/250]: training_loss: tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1480/250]: training_loss: tensor(0.2553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1481/250]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1482/250]: training_loss: tensor(0.1707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1483/250]: training_loss: tensor(0.1511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1484/250]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1485/250]: training_loss: tensor(0.1654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1486/250]: training_loss: tensor(0.2968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1487/250]: training_loss: tensor(0.5401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1488/250]: training_loss: tensor(0.1149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1489/250]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1490/250]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1491/250]: training_loss: tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1492/250]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1493/250]: training_loss: tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1494/250]: training_loss: tensor(0.0603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1495/250]: training_loss: tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1496/250]: training_loss: tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1497/250]: training_loss: tensor(0.3042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1498/250]: training_loss: tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1499/250]: training_loss: tensor(0.1279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1500/250]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1501/250]: training_loss: tensor(0.1701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1502/250]: training_loss: tensor(0.0534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1503/250]: training_loss: tensor(0.0555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1504/250]: training_loss: tensor(0.3733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1505/250]: training_loss: tensor(0.1632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1506/250]: training_loss: tensor(0.1757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1507/250]: training_loss: tensor(0.3240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1508/250]: training_loss: tensor(0.2186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1509/250]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1510/250]: training_loss: tensor(0.3215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1511/250]: training_loss: tensor(0.1864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1512/250]: training_loss: tensor(0.7593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1513/250]: training_loss: tensor(0.1466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1514/250]: training_loss: tensor(0.0447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1515/250]: training_loss: tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1516/250]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1517/250]: training_loss: tensor(0.5032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1518/250]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1519/250]: training_loss: tensor(0.2696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1520/250]: training_loss: tensor(0.4126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1521/250]: training_loss: tensor(0.1043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1522/250]: training_loss: tensor(0.2954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1523/250]: training_loss: tensor(0.4356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1524/250]: training_loss: tensor(0.3388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1525/250]: training_loss: tensor(0.6443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1526/250]: training_loss: tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1527/250]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1528/250]: training_loss: tensor(0.1479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1529/250]: training_loss: tensor(0.2451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1530/250]: training_loss: tensor(0.3129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1531/250]: training_loss: tensor(0.7074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1532/250]: training_loss: tensor(0.5966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1533/250]: training_loss: tensor(0.2302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1534/250]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1535/250]: training_loss: tensor(0.2112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1536/250]: training_loss: tensor(0.1885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1537/250]: training_loss: tensor(0.1727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1538/250]: training_loss: tensor(0.1669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1539/250]: training_loss: tensor(0.1307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1540/250]: training_loss: tensor(0.3666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1541/250]: training_loss: tensor(0.1147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1542/250]: training_loss: tensor(0.4475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1543/250]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1544/250]: training_loss: tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1545/250]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1546/250]: training_loss: tensor(0.4697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1547/250]: training_loss: tensor(0.7281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1548/250]: training_loss: tensor(0.4110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1549/250]: training_loss: tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1550/250]: training_loss: tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1551/250]: training_loss: tensor(0.2498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1552/250]: training_loss: tensor(0.2717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1553/250]: training_loss: tensor(0.3017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1554/250]: training_loss: tensor(0.1573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1555/250]: training_loss: tensor(0.4206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1556/250]: training_loss: tensor(0.5467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1557/250]: training_loss: tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1558/250]: training_loss: tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1559/250]: training_loss: tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1560/250]: training_loss: tensor(0.5156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1561/250]: training_loss: tensor(0.4746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1562/250]: training_loss: tensor(0.3676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1563/250]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1564/250]: training_loss: tensor(0.3044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1565/250]: training_loss: tensor(0.1193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1566/250]: training_loss: tensor(0.3566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1567/250]: training_loss: tensor(0.2972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1568/250]: training_loss: tensor(0.2445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1569/250]: training_loss: tensor(0.2046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1570/250]: training_loss: tensor(0.5437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1571/250]: training_loss: tensor(0.1439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1572/250]: training_loss: tensor(0.1930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1573/250]: training_loss: tensor(0.1131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1574/250]: training_loss: tensor(0.8500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1575/250]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1576/250]: training_loss: tensor(0.4786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1577/250]: training_loss: tensor(0.2150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1578/250]: training_loss: tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1579/250]: training_loss: tensor(0.5344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1580/250]: training_loss: tensor(0.6141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1581/250]: training_loss: tensor(0.4726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1582/250]: training_loss: tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1583/250]: training_loss: tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1584/250]: training_loss: tensor(0.1689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1585/250]: training_loss: tensor(0.1869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1586/250]: training_loss: tensor(0.4049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1587/250]: training_loss: tensor(0.1230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1588/250]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1589/250]: training_loss: tensor(0.3418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1590/250]: training_loss: tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1591/250]: training_loss: tensor(0.2632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1592/250]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1593/250]: training_loss: tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1594/250]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1595/250]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1596/250]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1597/250]: training_loss: tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1598/250]: training_loss: tensor(0.5898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1599/250]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1600/250]: training_loss: tensor(0.1460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1601/250]: training_loss: tensor(0.6470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1602/250]: training_loss: tensor(0.1222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1603/250]: training_loss: tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1604/250]: training_loss: tensor(0.2224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1605/250]: training_loss: tensor(0.7471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1606/250]: training_loss: tensor(0.1142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1607/250]: training_loss: tensor(1.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1608/250]: training_loss: tensor(0.5146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1609/250]: training_loss: tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1610/250]: training_loss: tensor(0.1775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1611/250]: training_loss: tensor(0.6071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1612/250]: training_loss: tensor(0.1298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1613/250]: training_loss: tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1614/250]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1615/250]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1616/250]: training_loss: tensor(0.6314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1617/250]: training_loss: tensor(0.1414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1618/250]: training_loss: tensor(0.3147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1619/250]: training_loss: tensor(0.1441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1620/250]: training_loss: tensor(0.5512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1621/250]: training_loss: tensor(0.1950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1622/250]: training_loss: tensor(0.4840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1623/250]: training_loss: tensor(0.8012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1624/250]: training_loss: tensor(0.2846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1625/250]: training_loss: tensor(0.5562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1626/250]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1627/250]: training_loss: tensor(0.1672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1628/250]: training_loss: tensor(0.3176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1629/250]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1630/250]: training_loss: tensor(0.3912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1631/250]: training_loss: tensor(0.1282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1632/250]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1633/250]: training_loss: tensor(0.3878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1634/250]: training_loss: tensor(0.1746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1635/250]: training_loss: tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1636/250]: training_loss: tensor(0.2668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1637/250]: training_loss: tensor(0.2142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1638/250]: training_loss: tensor(0.5824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1639/250]: training_loss: tensor(0.3232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1640/250]: training_loss: tensor(0.4477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1641/250]: training_loss: tensor(0.9806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1642/250]: training_loss: tensor(0.3472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1643/250]: training_loss: tensor(0.3217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1644/250]: training_loss: tensor(0.2050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1645/250]: training_loss: tensor(0.5329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1646/250]: training_loss: tensor(0.1182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1647/250]: training_loss: tensor(0.2827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1648/250]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1649/250]: training_loss: tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1650/250]: training_loss: tensor(0.2677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1651/250]: training_loss: tensor(0.4351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1652/250]: training_loss: tensor(0.5649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1653/250]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1654/250]: training_loss: tensor(0.1337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1655/250]: training_loss: tensor(0.5598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1656/250]: training_loss: tensor(0.3477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1657/250]: training_loss: tensor(0.4882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1658/250]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1659/250]: training_loss: tensor(0.6520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1660/250]: training_loss: tensor(0.3351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1661/250]: training_loss: tensor(0.4382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1662/250]: training_loss: tensor(0.0676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1663/250]: training_loss: tensor(0.1454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1664/250]: training_loss: tensor(0.4571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1665/250]: training_loss: tensor(0.1450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1666/250]: training_loss: tensor(0.3122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1667/250]: training_loss: tensor(0.5039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1668/250]: training_loss: tensor(0.1114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1669/250]: training_loss: tensor(0.5662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1670/250]: training_loss: tensor(0.2066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1671/250]: training_loss: tensor(0.1599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1672/250]: training_loss: tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1673/250]: training_loss: tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1674/250]: training_loss: tensor(0.3338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1675/250]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1676/250]: training_loss: tensor(0.2131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1677/250]: training_loss: tensor(0.5039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1678/250]: training_loss: tensor(0.2911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1679/250]: training_loss: tensor(0.0541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1680/250]: training_loss: tensor(0.3879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1681/250]: training_loss: tensor(0.1139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1682/250]: training_loss: tensor(0.2455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1683/250]: training_loss: tensor(0.1205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1684/250]: training_loss: tensor(0.2705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1685/250]: training_loss: tensor(0.2109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1686/250]: training_loss: tensor(0.4979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1687/250]: training_loss: tensor(0.3094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1688/250]: training_loss: tensor(0.1929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1689/250]: training_loss: tensor(0.2381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1690/250]: training_loss: tensor(0.2070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1691/250]: training_loss: tensor(0.4750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1692/250]: training_loss: tensor(0.2111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1693/250]: training_loss: tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1694/250]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1695/250]: training_loss: tensor(0.6091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1696/250]: training_loss: tensor(0.1863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1697/250]: training_loss: tensor(0.4323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1698/250]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1699/250]: training_loss: tensor(0.0362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1700/250]: training_loss: tensor(0.2009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1701/250]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1702/250]: training_loss: tensor(0.4007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1703/250]: training_loss: tensor(0.2489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1704/250]: training_loss: tensor(0.2035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1705/250]: training_loss: tensor(0.1861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1706/250]: training_loss: tensor(0.6800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1707/250]: training_loss: tensor(0.3001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1708/250]: training_loss: tensor(0.4930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1709/250]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1710/250]: training_loss: tensor(0.4205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1711/250]: training_loss: tensor(0.2028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1712/250]: training_loss: tensor(0.2955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1713/250]: training_loss: tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1714/250]: training_loss: tensor(0.1686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1715/250]: training_loss: tensor(0.4159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1716/250]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1717/250]: training_loss: tensor(0.1410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1718/250]: training_loss: tensor(0.3345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1719/250]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1720/250]: training_loss: tensor(0.1258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1721/250]: training_loss: tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1722/250]: training_loss: tensor(0.7574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1723/250]: training_loss: tensor(0.4542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1724/250]: training_loss: tensor(0.1286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1725/250]: training_loss: tensor(0.2219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1726/250]: training_loss: tensor(0.5988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1727/250]: training_loss: tensor(0.5489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1728/250]: training_loss: tensor(0.1960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1729/250]: training_loss: tensor(0.1181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1730/250]: training_loss: tensor(0.2556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1731/250]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1732/250]: training_loss: tensor(0.5160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1733/250]: training_loss: tensor(0.1019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1734/250]: training_loss: tensor(0.1651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1735/250]: training_loss: tensor(0.2651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1736/250]: training_loss: tensor(0.4826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1737/250]: training_loss: tensor(0.1527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1738/250]: training_loss: tensor(0.1539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1739/250]: training_loss: tensor(0.2439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1740/250]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1741/250]: training_loss: tensor(0.2030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1742/250]: training_loss: tensor(0.1196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1743/250]: training_loss: tensor(0.1264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1744/250]: training_loss: tensor(0.3965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1745/250]: training_loss: tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1746/250]: training_loss: tensor(0.1584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1747/250]: training_loss: tensor(0.1398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1748/250]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1749/250]: training_loss: tensor(0.1262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1750/250]: training_loss: tensor(0.2206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1751/250]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1752/250]: training_loss: tensor(0.4472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1753/250]: training_loss: tensor(0.2344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1754/250]: training_loss: tensor(0.2118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1755/250]: training_loss: tensor(0.1402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1756/250]: training_loss: tensor(0.3713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1757/250]: training_loss: tensor(0.5222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1758/250]: training_loss: tensor(0.9312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1759/250]: training_loss: tensor(0.4780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1760/250]: training_loss: tensor(0.5944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1761/250]: training_loss: tensor(0.5905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1762/250]: training_loss: tensor(0.1267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1763/250]: training_loss: tensor(0.3540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1764/250]: training_loss: tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1765/250]: training_loss: tensor(0.3445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1766/250]: training_loss: tensor(0.3014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1767/250]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1768/250]: training_loss: tensor(0.3835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1769/250]: training_loss: tensor(0.0620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1770/250]: training_loss: tensor(0.1520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1771/250]: training_loss: tensor(0.2626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1772/250]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1773/250]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1774/250]: training_loss: tensor(0.1493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1775/250]: training_loss: tensor(0.2794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1776/250]: training_loss: tensor(0.8077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1777/250]: training_loss: tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1778/250]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1779/250]: training_loss: tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1780/250]: training_loss: tensor(0.1355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1781/250]: training_loss: tensor(0.2263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1782/250]: training_loss: tensor(0.6134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1783/250]: training_loss: tensor(0.2352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1784/250]: training_loss: tensor(0.2686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1785/250]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1786/250]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1787/250]: training_loss: tensor(0.1617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1788/250]: training_loss: tensor(0.2720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1789/250]: training_loss: tensor(0.5040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1790/250]: training_loss: tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1791/250]: training_loss: tensor(0.1757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1792/250]: training_loss: tensor(0.2821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1793/250]: training_loss: tensor(0.2413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1794/250]: training_loss: tensor(0.2855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1795/250]: training_loss: tensor(0.4258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1796/250]: training_loss: tensor(0.1778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1797/250]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1798/250]: training_loss: tensor(0.2998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1799/250]: training_loss: tensor(0.4219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1800/250]: training_loss: tensor(0.4415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1801/250]: training_loss: tensor(0.5079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1802/250]: training_loss: tensor(0.1223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1803/250]: training_loss: tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1804/250]: training_loss: tensor(0.4513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1805/250]: training_loss: tensor(0.1449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1806/250]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1807/250]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1808/250]: training_loss: tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1809/250]: training_loss: tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1810/250]: training_loss: tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1811/250]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1812/250]: training_loss: tensor(0.2239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1813/250]: training_loss: tensor(0.3072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1814/250]: training_loss: tensor(0.2813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1815/250]: training_loss: tensor(0.3811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1816/250]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1817/250]: training_loss: tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1818/250]: training_loss: tensor(0.2745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1819/250]: training_loss: tensor(0.1785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1820/250]: training_loss: tensor(0.1277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1821/250]: training_loss: tensor(0.3395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1822/250]: training_loss: tensor(0.2018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1823/250]: training_loss: tensor(0.2099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1824/250]: training_loss: tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1825/250]: training_loss: tensor(0.2250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1826/250]: training_loss: tensor(0.2458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1827/250]: training_loss: tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1828/250]: training_loss: tensor(0.1344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1829/250]: training_loss: tensor(0.1588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1830/250]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1831/250]: training_loss: tensor(0.2130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1832/250]: training_loss: tensor(0.2617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1833/250]: training_loss: tensor(0.2288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1834/250]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1835/250]: training_loss: tensor(0.1328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1836/250]: training_loss: tensor(0.2490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1837/250]: training_loss: tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1838/250]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1839/250]: training_loss: tensor(0.3534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1840/250]: training_loss: tensor(0.1680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1841/250]: training_loss: tensor(0.1328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1842/250]: training_loss: tensor(0.1716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1843/250]: training_loss: tensor(0.3890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1844/250]: training_loss: tensor(0.2972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1845/250]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1846/250]: training_loss: tensor(0.1083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1847/250]: training_loss: tensor(0.5277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1848/250]: training_loss: tensor(0.8499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1849/250]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1850/250]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1851/250]: training_loss: tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1852/250]: training_loss: tensor(0.2491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1853/250]: training_loss: tensor(0.1456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1854/250]: training_loss: tensor(0.3698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1855/250]: training_loss: tensor(0.2355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1856/250]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1857/250]: training_loss: tensor(0.1195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1858/250]: training_loss: tensor(0.1408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1859/250]: training_loss: tensor(0.4466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1860/250]: training_loss: tensor(0.5211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1861/250]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1862/250]: training_loss: tensor(0.4023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1863/250]: training_loss: tensor(0.4708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1864/250]: training_loss: tensor(0.1654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1865/250]: training_loss: tensor(0.3940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1866/250]: training_loss: tensor(0.1819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1867/250]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1868/250]: training_loss: tensor(0.6986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1869/250]: training_loss: tensor(0.3278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1870/250]: training_loss: tensor(0.1706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1871/250]: training_loss: tensor(0.3915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1872/250]: training_loss: tensor(0.1865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1873/250]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1874/250]: training_loss: tensor(0.4963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1875/250]: training_loss: tensor(0.2503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1876/250]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1877/250]: training_loss: tensor(0.2658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1878/250]: training_loss: tensor(0.2361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1879/250]: training_loss: tensor(0.1406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1880/250]: training_loss: tensor(0.2075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1881/250]: training_loss: tensor(0.3725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1882/250]: training_loss: tensor(0.5036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1883/250]: training_loss: tensor(0.2333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1884/250]: training_loss: tensor(0.4880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1885/250]: training_loss: tensor(0.2550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1886/250]: training_loss: tensor(0.1356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1887/250]: training_loss: tensor(0.3544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1888/250]: training_loss: tensor(0.4954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1889/250]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1890/250]: training_loss: tensor(0.2790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1891/250]: training_loss: tensor(0.3433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1892/250]: training_loss: tensor(0.2466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1893/250]: training_loss: tensor(0.5806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1894/250]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1895/250]: training_loss: tensor(0.8237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1896/250]: training_loss: tensor(0.1211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1897/250]: training_loss: tensor(0.3204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1898/250]: training_loss: tensor(0.1428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1899/250]: training_loss: tensor(0.2676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1900/250]: training_loss: tensor(0.3811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1901/250]: training_loss: tensor(0.2605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1902/250]: training_loss: tensor(0.4630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1903/250]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1904/250]: training_loss: tensor(0.3265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1905/250]: training_loss: tensor(0.2348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1906/250]: training_loss: tensor(0.1750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1907/250]: training_loss: tensor(0.5995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1908/250]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1909/250]: training_loss: tensor(0.4691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1910/250]: training_loss: tensor(0.1764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1911/250]: training_loss: tensor(0.1625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1912/250]: training_loss: tensor(0.2436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1913/250]: training_loss: tensor(0.1676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1914/250]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1915/250]: training_loss: tensor(0.2266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1916/250]: training_loss: tensor(0.1872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1917/250]: training_loss: tensor(0.6656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1918/250]: training_loss: tensor(0.3409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1919/250]: training_loss: tensor(0.4640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1920/250]: training_loss: tensor(0.1826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1921/250]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1922/250]: training_loss: tensor(0.4178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1923/250]: training_loss: tensor(0.1669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1924/250]: training_loss: tensor(0.1538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1925/250]: training_loss: tensor(0.9652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1926/250]: training_loss: tensor(0.2350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1927/250]: training_loss: tensor(0.3327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1928/250]: training_loss: tensor(0.3916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1929/250]: training_loss: tensor(0.2134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1930/250]: training_loss: tensor(0.7196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1931/250]: training_loss: tensor(0.8272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1932/250]: training_loss: tensor(0.4164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1933/250]: training_loss: tensor(0.1860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1934/250]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1935/250]: training_loss: tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1936/250]: training_loss: tensor(0.4108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1937/250]: training_loss: tensor(0.3763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1938/250]: training_loss: tensor(0.1938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1939/250]: training_loss: tensor(0.1336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1940/250]: training_loss: tensor(0.2339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1941/250]: training_loss: tensor(0.2301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1942/250]: training_loss: tensor(0.2212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1943/250]: training_loss: tensor(0.1519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1944/250]: training_loss: tensor(0.1155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1945/250]: training_loss: tensor(0.4043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1946/250]: training_loss: tensor(0.1922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1947/250]: training_loss: tensor(0.3143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1948/250]: training_loss: tensor(0.7264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1949/250]: training_loss: tensor(0.1891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1950/250]: training_loss: tensor(0.7763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1951/250]: training_loss: tensor(0.3043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1952/250]: training_loss: tensor(0.2453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1953/250]: training_loss: tensor(0.2779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1954/250]: training_loss: tensor(0.1829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1955/250]: training_loss: tensor(0.5095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1956/250]: training_loss: tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1957/250]: training_loss: tensor(0.3880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1958/250]: training_loss: tensor(0.1294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1959/250]: training_loss: tensor(0.4021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1960/250]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1961/250]: training_loss: tensor(0.4805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1962/250]: training_loss: tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1963/250]: training_loss: tensor(0.3451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1964/250]: training_loss: tensor(0.5315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1965/250]: training_loss: tensor(0.3053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1966/250]: training_loss: tensor(0.1351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1967/250]: training_loss: tensor(0.4690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1968/250]: training_loss: tensor(0.3121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1969/250]: training_loss: tensor(0.2368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1970/250]: training_loss: tensor(0.6114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1971/250]: training_loss: tensor(0.3429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1972/250]: training_loss: tensor(0.1500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1973/250]: training_loss: tensor(0.2981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1974/250]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1975/250]: training_loss: tensor(0.1494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1976/250]: training_loss: tensor(0.2514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1977/250]: training_loss: tensor(0.8459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1978/250]: training_loss: tensor(0.2967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1979/250]: training_loss: tensor(0.7624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1980/250]: training_loss: tensor(0.3287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1981/250]: training_loss: tensor(0.1112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1982/250]: training_loss: tensor(0.6257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1983/250]: training_loss: tensor(0.3079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1984/250]: training_loss: tensor(0.1749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1985/250]: training_loss: tensor(0.1402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1986/250]: training_loss: tensor(0.3043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1987/250]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1988/250]: training_loss: tensor(0.3277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1989/250]: training_loss: tensor(0.1478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1990/250]: training_loss: tensor(0.1632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1991/250]: training_loss: tensor(0.2551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1992/250]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1993/250]: training_loss: tensor(0.2008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1994/250]: training_loss: tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1995/250]: training_loss: tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1996/250]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1997/250]: training_loss: tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1998/250]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1999/250]: training_loss: tensor(0.4394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [3/3], global step [6000/6000], Train Loss: 0.2598, Valid Loss: 0.3051\n",
      "Training done!\n"
     ]
    }
   ],
   "source": [
    "# Main training loop\n",
    "NUM_EPOCHS = 2\n",
    "steps_per_epoch = len(train_iter_EA)\n",
    "\n",
    "model = ROBERTAClassifier(0.3)\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"======================= Start pretraining ==============================\")\n",
    "\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=steps_per_epoch*1, \n",
    "                                            num_training_steps=steps_per_epoch*NUM_EPOCHS)\n",
    "pretrain(model=model,\n",
    "         train_iter=train_iter_EA,\n",
    "         valid_iter=valid_iter_EA,\n",
    "         optimizer=optimizer,\n",
    "         scheduler=scheduler,\n",
    "         num_epochs=NUM_EPOCHS)\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "print(\"======================= Start training =================================\")\n",
    "optimizer = AdamW(model.parameters(), lr=2e-6)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=steps_per_epoch*2, \n",
    "                                            num_training_steps=steps_per_epoch*NUM_EPOCHS)\n",
    "\n",
    "train(model=model, \n",
    "      train_iter=train_iter_EA, \n",
    "      valid_iter=valid_iter_EA, \n",
    "      optimizer=optimizer, \n",
    "      scheduler=scheduler, \n",
    "      num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Tucur-Tf1evl"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "# Evaluation Function\n",
    "import seaborn as sns\n",
    "def evaluate(model, test_loader):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    y_prob = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (source, target), _ in test_loader:\n",
    "                mask = (source != PAD_INDEX).type(torch.uint8)\n",
    "                \n",
    "                output = model(source, attention_mask=mask)\n",
    "                y_prob.extend(output.tolist())\n",
    "                y_pred.extend(torch.argmax(output, axis=-1).tolist())\n",
    "                y_true.extend(target.tolist())\n",
    "    \n",
    "    label_true = []\n",
    "    for i in y_true:\n",
    "        if i == 1:\n",
    "            label_true.append([1,0])\n",
    "        else:\n",
    "            label_true.append([0,1])\n",
    "    y_prob_final = []\n",
    "    for i in range(len(y_prob)):\n",
    "        tempA = abs(y_prob[i][0])\n",
    "        tempB = abs(y_prob[i][1])\n",
    "        y_prob_final.append(tempA/(tempA+tempB))\n",
    "\n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "    plt.figure(1, figsize=(20,8))\n",
    "\n",
    "    ax= plt.subplot(121)\n",
    "    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ax.set_xlabel('Predicted Labels')\n",
    "    ax.set_ylabel('True Labels')\n",
    "    ax.xaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "    ax.yaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "    fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_true, y_prob_final)\n",
    "    plt.subplot(122)\n",
    "    lw = 2\n",
    "    plt.plot(fpr_rt_lm, tpr_rt_lm, color='darkorange',\n",
    "             lw=lw, label='roc curve')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.grid()\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 699
    },
    "id": "_FEpBmcc1evl",
    "outputId": "3058fe48-7b00-486b-c1e1-25bd007b9f7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.7488    0.8824    0.8101       527\n",
      "           0     0.9550    0.8941    0.9236      1473\n",
      "\n",
      "    accuracy                         0.8910      2000\n",
      "   macro avg     0.8519    0.8882    0.8668      2000\n",
      "weighted avg     0.9007    0.8910    0.8937      2000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate(model, test_iter_EA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "810yAD8YDu4T"
   },
   "source": [
    "# Test in 2400 Covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "vWItWKHlIxaV"
   },
   "outputs": [],
   "source": [
    "test_data2400 = pd.read_csv('Data/test5.csv',sep=',',encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "QmppNuRAIxPe",
    "outputId": "9633ad7a-8f0d-4763-c179-e8670b60dc50"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fucking piece of shit your whole community is...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im not being funny but coronavirus in china ir...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>got on the victoria line today to seven sister...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it s appalling that the media amp libtards bit...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dude fuck the chinese man fuck em and if you t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  attack\n",
       "0   fucking piece of shit your whole community is...     1.0\n",
       "1  im not being funny but coronavirus in china ir...     0.0\n",
       "2  got on the victoria line today to seven sister...     1.0\n",
       "3  it s appalling that the media amp libtards bit...     1.0\n",
       "4  dude fuck the chinese man fuck em and if you t...     1.0"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data2400.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "Ymcatr_DIxLp"
   },
   "outputs": [],
   "source": [
    "test_data2400['attack'] = [int(i) for i in test_data2400['attack']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "sfDukeoJIxBc"
   },
   "outputs": [],
   "source": [
    "# Model parameter\n",
    "MAX_SEQ_LEN = 80\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
    "\n",
    "# Fields\n",
    "\n",
    "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
    "text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n",
    "                   fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\n",
    "fields = [('text', text_field),('hate', label_field)]\n",
    "\n",
    "# TabularDataset\n",
    "\n",
    "train, valid, test = TabularDataset.splits(path=source_folder, train='test5.csv', validation='test5.csv',\n",
    "                                           test='test5.csv', format='CSV', fields=fields, skip_header=True)\n",
    "\n",
    "test_iter_2400 = Iterator(test, batch_size=16, device=device, train=False, shuffle=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "id": "0lJ6Mt6yIw9r",
    "outputId": "bc050040-e5f9-46ad-9ccb-98c2f1ae2a09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.6142    0.7419    0.6720       678\n",
      "           0     0.8833    0.8074    0.8437      1641\n",
      "\n",
      "    accuracy                         0.7883      2319\n",
      "   macro avg     0.7487    0.7747    0.7578      2319\n",
      "weighted avg     0.8046    0.7883    0.7935      2319\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJEAAAHwCAYAAAAW3v7qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd5wV1fnH8c+zS++IgIhgAzH2gr2hRgVREQV7QjTGGHs00ZjYYpox0bSfsUWDDQELgrEbJfYCiQ1BxUpTpElvu+f3x724C4IU793Z8nn7uq+dOTN35gsXd2efOedMpJSQJEmSJEmSvk5J1gEkSZIkSZJU/VlEkiRJkiRJ0mpZRJIkSZIkSdJqWUSSJEmSJEnSallEkiRJkiRJ0mpZRJIkSZIkSdJqWUSSiiAiGkfEgxHxRUTc8w2Oc2JEPF7IbFmIiEciYkDWOSRJkiRJ684ikuq0iDghIkZFxNyImJIvduxdgEP3A9oDbVJK/df1ICmlu1JKBxcgz3IiokdEpIgYtkL79vn2kWt4nCsi4s7V7ZdS6pVSum0d40qSJNV6EfFRRCzIX5d+GhEDI6LZCvvsGRFPRcSc/M3KByNiqxX2aRERf46IT/LHej+/vn7V/okk1UYWkVRnRcT5wJ+B35Ir+HQG/g70KcDhNwbeTSktLcCxiuVzYI+IaFOpbQDwbqFOEDl+n5EkSVozh6eUmgE7ADsCFy/bEBF7AI8Dw4ENgU2B14HnI2Kz/D4NgH8DWwM9gRbAHsB0YNdihY6IesU6tqTqxV/uVCdFREvgSuDMlNL9KaV5KaUlKaUHU0o/ze/TMH/XZnL+9eeIaJjf1iMiJkbEBRExNd+L6eT8tl8ClwHH5u/+fH/FHjsRsUm+x0+9/Pr3IuKD/F2lDyPixErtz1V6354R8Wr+ztOrEbFnpW0jI+JXEfF8/jiPr+aO02LgAeC4/PtLgWOBu1b4u/pLREyIiNkRMToi9sm39wR+XunP+XqlHL+JiOeB+cBm+bZT89uvj4j7Kh3/9xHx74iINf4AJUmSarGU0qfAY+SKSctcDdyeUvpLSmlOSmlGSukS4CXgivw+3yV3Y7RvSuntlFJ5SmlqSulXKaWHV3auiNg6Ip6IiBkR8VlE/DzfPjAifl1pvx4RMbHS+kcRcVFEvAHMyy/fu8Kx/xIRf80vt4yIW/LXzZMi4tf5609JNYhFJNVVewCNgGFfs88vgN3J/fDentzdm0sqbd8AaAl0BL4PXBcRrVNKl5Pr3TQkpdQspXTL1wWJiKbAX4FeKaXmwJ7AayvZbz3gofy+bYBrgYdW6El0AnAy0A5oAPzk684N3E7uYgPgEOAtYPIK+7xK7u9gPWAQcE9ENEopPbrCn3P7Su/5DnAa0Bz4eIXjXQBsmy+Q7UPu725ASimtJqskSVKdEBEbAb2A8fn1JuSuEVc21+ZQ4KD88reBR1NKc9fwPM2BJ4FHyfVu6kKuJ9OaOh7oDbQCBgOH5o+57AblMeSuHwEGAkvz59gROBg4dS3OJakasIikuqoNMG01w81OBK7M3735HPglueLIMkvy25fk7+zMBbqtY55yYJuIaJxSmpJSGrOSfXoD76WU7kgpLU0p3Q2MAw6vtM8/U0rvppQWkLug2GElx/lSSukFYL2I6EaumHT7Sva5M6U0PX/Oa4CGrP7POTClNCb/niUrHG8+ub/Ha4E7gbNTShNXdhBJkqQ65oGImANMAKYCl+fb1yP3u9uUlbxnCrCs93mbVeyzKocBn6aUrkkpLcz3cHp5Ld7/15TShJTSgpTSx8B/gb75bQcA81NKL0VEe+BQ4Lz8CICpwJ/I94iXVHNYRFJdNR1YfzXjtzdk+V40H+fbvjzGCkWo+cBykx+uiZTSPHLDyE4HpkTEQxGx5RrkWZapY6X1T9chzx3AWcD+rKRnVkT8JCLG5ofQzSLX+2p1EzNO+LqN+YuTD4AgV+ySJEkSHJnvmd4D2JKKa66Z5G46dljJezoA0/LL01exz6p0At5fp6Q5K17zDSLXOwlyPeSX9ULaGKhP7lp3Vv6a8kZyvecl1SAWkVRXvQgsAo78mn0mk/uBt0xnvjrUa03NA5pUWt+g8saU0mMppYPI/dAfB9y8BnmWZZq0jpmWuQM4A3g430voS/nhZheS64rcOqXUCviCXPEHYFVD0L52aFpEnEmuR9Pk/PElSZKUl1L6D7nhX3/Mr88jd/26sqf+HkPFELQngUPy0yWsiQnAZqvY9rXXr8uirrB+D9AjPxyvLxVFpAnkrr3XTym1yr9apJS2XsOckqoJi0iqk1JKX5Cb/Pq6iDgyIppERP2I6BURV+d3uxu4JCLa5ieovozc8Kt18Rqwb0R0zk/qXflJG+0jok/+h/0icsPiyldyjIeBLSLihIioFxHHAlsB/1rHTACklD4E9iM3B9SKmpMbu/45UC8iLiP3lI9lPgM2ibV4AltEbAH8GjiJ3LC2CyPia4fdSZIk1UF/Bg6KiGXzTv4MGBAR50RE84honZ/4eg9y0y5A7ubgBOC+iNgyIkoiok1E/DwiDl3JOf4FdIiI8yL3UJnmEbFbfttr5OY4Wi8iNgDOW13g/BQQI4F/Ah+mlMbm26eQe7LcNRHRIp9r84jYbx3+XiRlyCKS6qz8/D7nk5ss+3NyP3DPIvfEMsgVOkYBbwBvkhvj/euvHmmNzvUEMCR/rNEsX/gpyeeYDMwgV9D50UqOMZ3cuPULyHVVvhA4LKU0bcV91yHfcymllfWyeozcRIvvkhs6t5Dluy0vm9xxekT8d3XnyQ8fvBP4fUrp9ZTSe+Se8HZH5J98J0mSpC8LMreTu5FJSuk5cg9COYrcvEcfk5ugeu/8NRUppUXkJtceBzwBzAZeITcs7itzHaWU5pCblPtwctMivEduigPIFaReBz4iVwAasobRB+UzDFqh/bvkHvzyNrnhefeydkPvJFUD4QORJEmSJEmStDr2RJIkSZIkSdJqWUSSJEmSJEnSallEkiRJkiRJ0mpZRJIkSZIkSdJqWUSSJEmSJEnSatXLOsCqPDF2mo+Nk9bAPl3XzzqCVCM0qkcU+xyNdzyrKD+7Fvzv/4qeXVqmVatWqUuXLlnHUCXz5s2jadOmWcfQCvxcqh8/k+rJz6X6GT169LSUUtt1eW+1LSJJkiSp6rVv355Ro0ZlHUOVjBw5kh49emQdQyvwc6l+/EyqJz+X6iciPl7X91pEkiSpUMJR4pIkSaq9vNqVJEmSJEnSatkTSZKkQgmnLpIkSVLtZU8kSZIkSZIkrZY9kSRJKhTnRJIkSVItZhFJkqRCcTibJEmSajFvmUqSJEmSJGm17IkkSVKhOJxNkiRJtZhXu5IkSZIkSVoteyJJklQozokkSZKkWswikiRJheJwNkmSJNViXu1KkiRJkiRpteyJJElSoTicTZIkSbWYPZEkSZIkSZK0WvZEkiSpUJwTSZIkSbWYV7uSJBVKRHFe0kpExK0RMTUi3lrF9oiIv0bE+Ih4IyJ2quqMkiSpdrGIJEmSVDMNBHp+zfZeQNf86zTg+irIJEmSajGHs0mSVCgOZ1MVSik9ExGbfM0ufYDbU0oJeCkiWkVEh5TSlCoJKEmSqpfP34Sp//1Gh7CIJEmSVDt1BCZUWp+Yb7OIJElSXfPuvTz+x5+xw4affqPDWESSJKlQnL9INVREnEZuyBtt27Zl5MiR2QbScubOnetnUg35uVQ/fibVk59L9tpNe4x/DXyWnz18Entv8gnwz3U+lkUkSZKk2mkS0KnS+kb5tq9IKd0E3ATQrVu31KNHj6KH05obOXIkfibVj59L9eNnUj35uWRk2ltwx04soCWn3rEvg/53EAAHHdubZ69a9yKSkzdIklQoUVKcl7RuRgDfzT+lbXfgC+dDkiSpDnj9RrhtWybOaMy+1/Zl0P+2o1nDRQy7Y18u/d3R3+jQ9kSSJKlQLPioCkXE3UAPYP2ImAhcDtQHSCndADwMHAqMB+YDJ2eTVJIkVZn5n8OTp/PiRxvR97bj+GxOMzbdpAUjhh/LNttt+I0PbxFJkiSpBkopHb+a7Qk4s4riSJKkrJWXwcCtAHj0nS58NqcZBxywCUOH9qdNmyYFOYVFJEmSCqXEibUlSZKUkckvwoJpAFx+wRZ0OuZwBgzYnvr1Swt2CvvdS5IkSZIk1WAzZizgOycOYfIXzQEoOfgGTj11p4IWkMCeSJIkFY5zIkmSJKmKjRn9Hn163cj7n6/P7Om9GX5tc4ji9JC3iCRJUqEU6Ye1JEmStDIjRrzDicfexdyFzdmp42T+r+/DsOfYop3PIpIkSZIkSVINklLit1c8wqW/epWUSjluhze55fT3aPL9qVCvYdHOaxFJkqRCcTibJEmSiiwtXcQJxw5i8P2fEJH43aH/5qL9nyP2vauoBSSwiCRJkiRJklS9LZ4D//sbLJxJjPoj28zfh+YN92bQifdx2N6lsOkFsNlhRY9hEUmSpEJxTiRJkiQVWkrwtxYsWFKPxvWXAvDzA5/lO/tNo3PXTaHPA1CvUZVEsYgkSVKhOJxNkiRJhfT5G3D3ntz44s78+sl9ef6y5+m8Tz+ifXc6d96/yuNYRJIkSZIkSapOlsyDN29lyZPnce4Dvbj+xV0AGFb6F87dZffMYllEkiSpUBzOJkmSpG9i1vtw1y6wcCafz21C/9u/y38+2IQG9RI33XgYA07pnmk8i0iSJEmSJElZ++AhGJabHPuNye054p/H8/HMVmzQJhj2wAnsvnfXjANaRJIkqXCcE0mSJElra8orMOVFePo8AKbPa8w+N53B7LmJXXbZkGHDjqVjxxYZh8yxiCRJUqE4nE2SJElrauKz8NgpMGv8cs1tTrqHy5q25rXXPuOmmw6jceP6GQX8KotIkiRJkiRJVWn+VBiy75ercxc14L02Z7Lj7lvAZr05//wEQFSzm5QWkSRJKhSHs0mSJOnrzHgXJj8Po6/9sunDb11Pn4vKmDR5Hq++eiybUf2KR8tYRJIkSZIkSaoK9x0Csz/6cvXp2UfT/7uzmT59Ad26taGsrDy7bGvAW6aSJBVKRHFeqz1t3BoRUyPirUptf4iIcRHxRkQMi4hWlbZdHBHjI+KdiDikUnvPfNv4iPhZwf9+JEmS6roF0wBIW57IdR//hIN+sx3Tpy+gV68uvPzyqXTt2ibjgF/PIpIkSTXfQKDnCm1PANuklLYD3gUuBoiIrYDjgK3z7/l7RJRGRClwHdAL2Ao4Pr+vJEmS1tW4IXBNwP+1yr2WzGXx0lJ+eP9RnPW3ZpSVJS68cE8efPB4WrZslHXa1XI4myRJhZLRnEgppWciYpMV2h6vtPoS0C+/3AcYnFJaBHwYEeOBXfPbxqeUPgCIiMH5fd8uYnRJkqTaZ/5UKFsCn78GDx2Xa1v0xZebX5u/D7fe9haNGtXjH/84nBNP3C6joGvPIpIkSYVSfSfWPgUYkl/uSK6otMzEfBvAhBXadyt+NEmSpBrsk6dg0nMV66/8HpbO/+p+/f8N7XYCYNeGLbhlszfYeut2dO++YRUFLQyLSJIkVXMRcRpwWqWmm1JKN63he38BLAXuKkY2SZKkOmP6WHjlKvjoUShtCCnB3Imr3r/ZhhD1YK8rGfJiO1q2nEbPnl0AGDBghyoKXVgWkSRJKpQiPYo1XzBao6JRZRHxPeAw4MCUUso3TwI6Vdpto3wbX9MuSZJU+yyYAc/8FBbPXf2+M8fB52+sevvul1YsN90Atv0BlNanvDxx6aVP8dvf3kfLlg0ZO/ZMOnRo/s2zZ8QikiRJtVBE9AQuBPZLKVXuUz0CGBQR1wIbAl2BV4AAukbEpuSKR8cBJ1RtakmSpCpSXgZ/X8cnoW3WG/b9I9Rvkltv2gFK639lt9mzF3HSSffz4IPvUloaXHnl/mywQbNvEDp7FpEkSSqUjOZEioi7gR7A+hExEbic3NPYGgJPRK6H1EsppdNTSmMiYii5CbOXAmemlMryxzkLeAwoBW5NKY2p8j+MJElSMc14NzcE7Z4DK9q2OAa6HrX695aUQudvQ6NWq911/PgZHHHE3YwdO43WrRsxdGh/vv3tzb5B8OrBIpIkSYVSpOFsq5NSOn4lzbd8zf6/AX6zkvaHgYcLGE2SJKn6ePkqeO7i5dva7QiHD1n5/uvoqac+pF+/ocycuZCttmrL8OHH0aXLegU9R1YsIkmSJEmSpNqnbAl88C9YOBO+eB9e/m3FtvY7Q9ejYdefFfy0DRuWMnfuYg4/fAvuvPMoWrRoWPBzZMUikiRJhZLRcDZJkqQ6b+lC+OTf8OIvc09EA5jy4sr3PXkcrNetoKcvL0+UlOR6pe+1V2deeOH77LRThy/baguLSJIkSZIkqeaZPxVevwEWfQGjr/36fbc+OTf59Y5nF7yA9Omnc+nXbyg//eme9OmzJQDdu29Y0HNUFxaRJEkqlIzmRJIkSapTUoLHToExA7+6rcXGsOM50GH33Hq9xtBuh6Jdp40aNZkjjxzMpElzmDXrKQ47bAtKS2tv73SLSJIkFUhYRJIkSSqOsiUw/7Pc8v/+tnwBae/f5XoZddgDOu5ZZZHuuusNTj31QRYuXMo++3Tm3nuPqdUFJLCIJEmSJEmSqqtPR+Umx37xlyvffs58qN+4SiOVlZXz85//m6uvfgGA007bib/97VAaNCit0hxZsIgkSVKB2BNJkiSpAO7rBdPeBALmTlx+W7OOua9LF8DxL1R5AQnghz/8F7fc8j/q1Svhr3/tyY9+tEuVZ8iKRSRJkiRJkpS9F38F4wbBjHFf3bbLhbBpL+jUo8pjreiHP9yZxx57nzvu6EuPHptkHadKWUSSJKlQ7IgkSZK09sqWwKg/wAuXLd/+g09yX5u0hXqNqj5XJePHz6BLl/UA2GWXjowffzYNG9a9kkrtnvFJkiRJkiRVb++PgOd+UbF+xH1w1ixo0Sn3yrCAlFLij398gW7d/o+hQ8d82V4XC0hgTyRJkgrGOZEkSZLW0vsPwoP9KtaPfRY22ju7PJUsWLCE0077F3fe+QYAH344M+NE2bOIJElSgVhEkiRJWo2PnoA5+WFqz1wEC6dXbNv7d9WmgDRp0mz69h3Cq69OpmnT+txxR1/69v1W1rEyZxFJkiRJkiQVVnkZjBvEzmN/BZPb5NqmvQVL5q58/z7DYfPDqi7f13jppYn07TuETz+dy6abtmL48OPYdtv2WceqFiwiSZJUIPZEkiRJdc6Md+CtW6F8yfLto/8EQHOA+e999X3bnJL72mZr6H5+USOujaVLyxkw4AE+/XQu+++/CUOH9mf99ZtkHavasIgkSZIkSZLWzsRnYMh+q90tEUSv26HV5rmGKIV2O0Jp/SIHXDf16pUwdGg/brvtdX7/+29Tv35p1pGqFYtIkiQViD2RJElSrbR0EcyfWrE+5p/wwuXL77P9j6BVl+XbWnflPxOa02OrHkWP+E3MnLmA++8fy/e/vxMA22+/Addeu0HGqaoni0iSJBWKNSRJklTbTH8bBm696u19HoBNe0Fpg5VvnzCyKLEK5e23P6dPn8GMHz+Dxo3rc8IJ22YdqVqziCRJkiRJknJmT4DpY3LLcyfB46dWbGu2UcXy0nnwvbehac3tsfPgg+9w4on3M2fOYnbYYQP23rtz1pGqPYtIkiQViMPZJElSjTRuMLx7L5QvhfeHr3yfw4ZAt2OqNleRpJT43e+e45JLniIlOOaYrbn11iNo2nQVvan0JYtIkiRJkiTVRXMnw23bwMKZX922ySG5r6kcuv8UNjmoarMVyfz5SzjllOEMGZLrbfWb3xzAxRfv7c3ANWQRSZKkAvHiQ5IkVXsz3oEF03PLg/dafluvO6C0IXTYDVrUzqFdCxcuZdSoyTRr1oC77jqKI47olnWkGsUikiRJBWIRSZIkVTspVSx//ATcd8hX99n2VDjoJqgD1zLrrdeYESOOJ6XE1lu3yzpOjWMRSZIkSZKk2mTW+/DJUzD6WpgxbuX7dNgj93WDXeCAv1Rdtgz84x//5e23P+faa3MFtK22aptxoprLIpIkSQViTyRJkpS5l38Lz/1i1dvrNYJD/glbHld1mTKyZEkZP/7xY1x33asA9O+/FXvs0SnjVDWbRSRJkiRJkmqDDx9ZvoC05fHQegvY/RIoqVu//k+bNp9jjrmHp5/+iAYNSrnhht4WkAqgbv0rkiSpmOyIJEmSsvT+iIrlH06GZh2yy5KhN9/8jD59BvPhh7PYYINm3H//MRaQCsQikiRJkiRJNU3ZEpg3Jbf85i0w8T+5F0D3n9TZAtJzz31Cz553Mm/eErp335Bhw45lo41aZB2r1rCIJElSgTgnkiRJqhJzJ8ONHVe9fbvTqi5LNbPttu3o1KklO+/cgZtvPpzGjetnHalWsYgkSVKBWESSJElF9+Yt8PipFevNNoJl1yAH3wztdoYm62eTLSPz5i2mXr0SGjasR8uWjXjuuZNZb73GXpsVgUUkSZIkSZKqow8ehrdu4cuJFxdMqxiyBrDPVbDrRZlEqy4++mgWffoMZpddNuTmmw8nImjTpknWsWoti0iSJBWId7skSVLBlC2GYb1XvX3AW7D+1lWXpxoaOfIj+vUbyvTpC1i4cCmzZi2kdevGWceq1SwiSZIkSZJU1coWwxs3wat/gPor6Tkzd1LFcs/bKu0TsNG+0KRtlcSsrq6//lXOOedRli4tp2fPLtx999G0atUo61i1nkUkSZIKxY5IkiRpTYy9Cx4+ac327XwgbP3d4uapQRYvLuOccx7hxhtHA/DTn+7J7353IKWlJRknqxssIkmSVCAOZ5MkSas1fdzyBaQGzeHQu6BVl5Xvv6r2Ouq3v32WG28cTcOGpfzjH0dw0knbZR2pTrGIJEmSJElSMaUE09+GJ38Ek56taO/3JGx8YHa5aqCf/GRPXn55Elde2YNddumYdZw6xyKSJEkFYk8kSZL0FbPeh1tW0puo520WkNbQo4+OZ7/9NqZx4/o0a9aARx45MetIdZaDBiVJkiRJKpYnz1h+fevvwRnTnOdoDZSXJy677Gl69bqLH/zgQVJKWUeq8+yJJElSgdgTSZKkOiwleOlXMPEZlnvaxidP5r5u3gf6DAOvF9bInDmL+M53hjF8+DuUlATdu2+YdSRhEUmSpIKxiCRJUh12wwYwf+qqtx9yiwWkNfT++zPo02cwY8Z8TuvWjRgypB8HHbR51rGERSRJkiRJktbOFx/BZ6Pg0e9BgxYw/zNI5RXbj36U5WaPabMVNG5TxSFrpn//+wOOOeZeZsxYwLe+tT4jRhxPly7rZR1LeRaRJEkqFG8uSpJUe5UtgSVzYd4UGLh1RfuSeRXLDVrA2V9UfbZaZODA15kxYwGHH74Fd955FC1aNMw6kiqxiCRJkiRJ0ooWzIBxg2DpAlg8F1668qv7bHIIdO0Hmx2aW2/aoWoz1kI33ngYu+3WkTPO2IWSEu/QVTcWkSRJKhDnRJIkqRYoWwJP/BDG/HPl2xu2yvU+6vEn2PHMqs1WC3322VwuueQp/vznnjRt2oAmTepz1lm7Zh1Lq2ARSZIkSZKk8SNg/P0w5rbl29ffNtfjiIAt+kEHCxyFMnr0ZI48cggTJ86mQYNSrruud9aRtBoWkSRJKhB7IkmSVEMtmg3D+yzf1mg9+O7r0HyjbDLVcnff/SannDKChQuXstdenbjssv2yjqQ1YBFJkqQCsYgkSVINlBIMqVTAOPgf0GJj6Hwg+LO94MrKyvnFL57i979/HoBTT92R667rTYMGpRkn05qwiCRJkiRJqpuWzIenzoHPX8utb9Eftv1+tplqsUWLlnL00UN56KH3KC0N/vKXnpxxxi7eiKtBLCJJklQoXv9IklSzfPAQvHVLxfqhd2WXpQ5o0KCUtm2b0qZNY+65pz/7779p1pG0lkqyDiBJkqR1ExE9I+KdiBgfET9byfbOEfF0RPwvIt6IiEOzyClJ1dYXH1QsnzwOSutnl6UWW7y4DMgN/b/hht6MHn2aBaQayiKSJEkFEhFFeUkrExGlwHVAL2Ar4PiI2GqF3S4BhqaUdgSOA/5etSklqZpKCe7sDs/m6+9bfQfW65ZtploopcTQoRPo3v0mZs9eBEDDhvXYeONWGSfTurKIJElSgVhEUhXbFRifUvogpbQYGAys8GghEtAiv9wSmFyF+SSp+ho/DD4bXbHe7djsstRSCxcuZcCAB7j++g94882pPPzwe1lHUgE4J5IkSVLN1BGYUGl9IrDbCvtcATweEWcDTYFvr+xAEXEacBpA27ZtGTlyZKGz6huYO3eun0k15OdS/azpZ9J0wQfs8nbF5NnP7PgY5Z80gE9W/16tmWnTFnHppWMYN24ODRuWcPHFW7LBBtP8f6YWsIik5Vz2g6Np2LgJJSUllJSWctE1tzJvzmxu/eOlzJj6Keu124Dv//RXNGnWgjdefpZ/DbqZiKCktJR+3z+XzbfaPus/glR0l11yMc/8ZyTrrdeG+4f/C4CfXnAeH3/4IQBz5syhefPmDL1/OJMmTaTv4YeyySa5Md/bbr89l15+ZWbZVVz2GlI1dDwwMKV0TUTsAdwREduklMor75RSugm4CaBbt26pR48eVZ9UqzRy5Ej8TKofP5fqZ40+k7LF8Of9K9b7PsS+mx1c1Fx1zcsvT+Scc4YwZcpcNt64JZdc0oVTTz0s61gqEItI+opzf/03mrWoGKP6xH130G277hx89Hd4/L47ePy+OzlywBl0225ntt11byKCSR+N59Y/XMql192dYXKpavQ58iiOP+EkfnHxRV+2/eGaP3+5/Merr6JZs2Zfrm/UqTND7x9epRkl1QmTgE6V1jfKt1X2faAnQErpxYhoBKwPTK2ShJJUncyfBte3rVjf/RLYzOcNFNJ7701nv/0GsmhRGfvttzH33NOfMWNezTqWCsg5kbRab7zyLLvt3wuA3fbvxRsvPwNAw8ZNvrzrvmjhQvAOvOqInbvvQouWLVe6LaXE4489Qq/e3m2pi5wTSVXsVaBrRGwaEQ3ITZw9YoV9PgEOBIiIbwGNgM+rNKUkVRdv3lSx3Lor7Gnv8ELr2rUNJ5+8A//QuSkAACAASURBVD/6UXeeeOI7tG3bNOtIKrCi9USKiF4ppUdWaDs9pXRDsc6pby4i+L8rfkwQ7HVIH/Y+pA9zZs2k5XrrA9CidRvmzJr55f6vv/QfRtxxA3O+mMnpl/wxq9hStfHf0aNo06YNG2+8yZdtkyZN5Jijj6RZs2acdc557LRz9+wCqris96gKpZSWRsRZwGNAKXBrSmlMRFwJjEopjQAuAG6OiB+Tm2T7eymllF1qSapCqRzGD4f5n+W+fvRorr3ZhnDKu9lmq0VmzlzAjBkL2Hzz9QC47rrelJR4UVRbFXM426URsSil9BRARFwI7A9YRKrGfvy762nVpi1zZs3k/644jw022ni57RGxXI+j7Xffj+1334/xY17joUE3c/aVf6nqyFK18sjD/6LnoRW9kNq2bcdjTz5Nq1ateXvMW5x3zpncP/yh5Ya7SdK6Sik9DDy8QttllZbfBvaq6lySVC188jSMOOqr7YfdU/VZaqmxYz+nT5/BpASvvHIqrVs3toBUyxVzONsRwG8jYp+I+A25p4Ws+NjZ5UTEaRExKiJGPTT09iJG06q0apMbI9y8VWu2221fPnrvbZq3as0XM6YB8MWMaTRv2eor7+uy9Q5M+2wyc2fPqtK8UnWydOlS/v3kE/TsWTG2vkGDBrRq1RqArbbehk6dOvPxRx9mFVFF5nA2SZKqifIyePZnueWWm8J2P4RdLoIzpkHHPbPNVks89NC77LbbP3jvvRk0bVqfuXMXZx1JVaBoRaSU0jRyhaTrgA2Bfimlr/1XlVK6KaXUPaXUvfcx3y1WNK3CooULWLhg3pfL4157hQ07b8a2u+7Ny0/nRia+/PQjbLfrPgB8PmUiy3rET3j/HZYuWUzT5iufJ0aqC15+8QU23XQz2m+wwZdtM2bMoKysDICJEybw8ccfsdFGnVZ1CEmSJH1TZYvh7j3gs1G59c7fhoNugH2vgsZtss1WC6SUuOqq5zj88LuZM2cx/ftvxfPPn0KnTv4uWBcUfDhbRMwhN+Y+8l8bAJsB/SIipZRaFPqcKow5s2Zw81U/B6CsbCnd9z2YrXbanc5dvsWtf7iUF5/8F+u13YBTfvorAF57cSQvP/0IpaX1qN+wIaf85ErvmKtOuOgn5zPq1VeYNWsmBx2wLz8682yOOro/jz7yMD0P7b3cvv8d9SrX/d9fqV+vHlFSwiWX/ZKWrb7am0+1g98DJUmqYu/dD//9C0Qp28+aBVNbwYSnl99n799kk60Wmj9/CaeeOoK7734LgF/9an9+8Yt9vAaqQ6K6zq34xNhp1TOYVM3s03X9rCNINUKjesWf9nrzCx4pys+u96/p5ZWZqky3bt3SO++8k3UMVTJy5Eh69OiRdQytwM+lGihfCn+qv+rtpQ3gtInQpG3VZarlhg0by1FHDaVZswbceWdf+vTZcrXv8f+V6iciRqeU1ulpP8V8Oltf4KmU0hf59VZAj5TSA8U6pyRJWfImnCRJVWDhTJj8Arx+fUVbn+G89vZ4dthhh9x6g+bQvrs/nAusb99vcdVVB9K79xZss027rOMoA8V8OtvlKaVhy1ZSSrMi4nLAIpIkqVayK7ckSVXggT4w6dmK9XpNoMsRzJo4Ejr3yCpVrfXPf/6PnXfekO22aw/ARRftnXEiZamYT2db2bGLWbSSJEmSJNVm44dXFJBabgZb9IPTp2SbqZZasqSMc855hFNOGUGfPoOZN8+nr6m4RZ1REXEtuaezAZwJjC7i+SRJypQdkSRJKoLFc+HOnWHxbJj3aUX7yWNz8x6p4KZPn88xx9zLU099SP36JVxyyT40berftYpbRDobuBQYkl9/glwhSZIkSZKkNfOfC2Dmu8u3nfCSBaQieeutqfTpM5gPPphJ+/ZNuf/+Y9lzz05Zx1I1UbQiUkppHvCzYh1fkqTqxjmRJEkqkPIyeH84PPkjmD8119a8M/R/ElpuCiXOlFIMw4eP46SThjF37mJ23rkDw4YdS6dOLbOOpWqkaHMiRUTbiPhDRDwcEU8texXrfJIkZS2iOK/VnzdujYipEfFWpbb1IuKJiHgv/7V1vj0i4q8RMT4i3oiInSq9Z0B+//ciYkAx/o4kSVojE56GEUdXFJAADr8HWne1gFRE8+cvYe7cxRx//DY888zJFpD0FcWcWPsuYBywKfBL4CPg1SKeT5Kkumog0HOFtp8B/04pdQX+TUXv4F5A1/zrNOB6yBWdgMuB3YBdgcuXFZ4kSapyo6+tWN73aji/HDrsml2eWiyl9OXy8cdvy8iRA7jrrqNo0qR+hqlUXRWziNQmpXQLsCSl9J+U0inAAUU8nyRJmSopiaK8Viel9AwwY4XmPsBt+eXbgCMrtd+ecl4CWkVEB+AQ4ImU0oyU0kxycxmuWJiSJKl4pr8NY+/KvT58JNe29QDY5ac+vaJIPv54FnvueSujRk3+sm2//TZxiL5WqZj9AJfkv06JiN7AZGC9Ip5PkiRVaJ9SWvbM40+B9vnljsCESvtNzLetql2SpOKb9xkM3Pqr7TueXfVZ6ohnnvmYo48eyrRp87nwwid46ilHsmv1illE+nVEtAQuAP4GtADOK+L5JEnKVLFu2kXEaeSGni1zU0rppjV9f0opRURa/Z6SJGXkk39XLG95Qu5rh92h/c7Z5KnlbrhhFGef/QhLl5Zz8MGbM3jw0VlHUg1RzCLSzJTSF8AXwP4AEbFXEc8nSVKmitX1O18wWuOiUd5nEdEhpTQlP1xt2cykk4DKz+ndKN82CeixQvvIdQosSdLaKFsMD5+YW96kJ/S+K9s8tdiSJWWce+6jXH/9KAAuuGAPrrrq29SrV8yZblSbFPNfyt/WsE2SJBXeCGBZv/QBwPBK7d/NP6Vtd+CL/LC3x4CDI6J1fkLtg/NtkiQVx8JZuWFsf25Y0dbZaXSLJaVE375DuP76UTRsWMpttx3JH/94sAUkrZWC90SKiD2APYG2EXF+pU0tgNJCn0+SpOoiqzkoI+Jucr2I1o+IieSesnYVMDQivg98DByT3/1h4FBgPDAfOBkgpTQjIn5FxZNUr0wprThZtyRJhTG8L4x/YPm2ttvlJtFWUUQEp5yyI6+99in3338su+7q1Idae8UYztYAaJY/dvNK7bOBfkU4nyRJdVpK6fhVbDpwJfsm4MxVHOdW4NYCRpMk6ateuGL5AlLDltDpADji3swi1WYTJnxBp04tATjqqG/Rs2cXmjSpn3Eq1VQFLyKllP4D/CciFqSUrq68LSL6A+8V+pySJFUHPg5XkqTV+OIjePGXFevnLYLSBpnFqc3KyxO//OVIfv/753n66QHssUduSkQLSPomijn48biVtF1cxPNJkiRJkqqbBTPgzVtg1DXwj00r2s+cYQGpSObMWUS/fkO58spnWLKknDffnLr6N0lroBhzIvUiN9dCx4j4a6VNzYElhT6fJEnVhT2RJElawaej4K5dvtp+4N+hUeuqz1MHfPDBTPr0Gcxbb02lVatGDB58NIcc0iXrWKolijEn0mRgNHBE/usyG5ObwFOSpFrJGpIkSSsYsm/F8vrbQMd9odsx0Gm/7DLVYk899SH9+9/DjBkL2HLL9Rk+/Di22KJN1rFUixRjTqTXgdcj4i5gG+AEoD/wIXBfoc8nSZIkSaqG3r0Pli7ILe9xOex5RaZxars5cxZ9WUDq3bsrd911FC1bNso6lmqZYgxn2wI4Pv+aBgwBIqW0f6HPJUlSdeJwNkmS8hbPgQcrPZx7159ll6WOaN68IbfffiTPPfcJv/71AZSWFnMKZNVVxRjONg54FjgspTQeICJ+XITzSJIkSZKqo8/+W7F89ONQzx4xxfDZZ3N58cWJHHnklgD07r0FvXtvkXEq1WbFKE0eBUwBno6ImyPiQMBbs5KkWi+iOC9JkmqUofvD0B655XpNYJODMo1TW/33v1Po3v1m+ve/h2ee+TjrOKojCl5ESik9kFI6DtgSeBo4D2gXEddHxMGFPp8kSdVFRBTlJUlSjTBuMPxjM5gwsqLt0Dszi1ObDR78FnvvfSsTJ85m1107Onm2qkwxhrMBkFKaBwwCBkVEa3KTa18EPF6sc0qSJEmSMjDxWXjo+OXbzpkP9Rtnk6eWKisr55JLnuKqq54H4JRTduDvf+9Nw4ZF+9VeWk6V/EtLKc0Ebsq/JEmqlew0JEmqc8rL4NNX4d17KtqOuA82PtgCUoHNnr2IE064j4ceeo/S0uBPfzqEs87a1V7LqlKWKyVJkiRJa2/2x3DzJsu37XQudD0qkzi13eefz+OFFybQunUj7rmnPwceuFnWkVQHWUSSJKlAvBMoSaozFs3+agFpk56w9feySFMnbL75ejzwwHF07NiczTdfL+s4qqMsIkmSVCDWkCRJdcYbN1Ys7/Zz2PNKKCnNLk8tlFLiz39+idLSEs45ZzcA9t1344xTqa6ziCRJkiRJWjuv/T33te0OsPdvss1SCy1cuJTTT/8Xt932OqWlwWGHbcFmm7XOOpZkEUmSpEJxOJskqc5o0RlmfwRbnZR1klpnypQ59O07hJdfnkSTJvUZOLCPBSRVGxaRJEmSJElrbv7nMPGZ3HL77tlmqWVeeWUSffsOYfLkOXTu3JLhw49jhx02yDqW9CWLSJIkFYgdkSRJtV75Uri+XcV6842yy1LLPPjgO/Tvfw+LFpWxzz6duffeY2jXrmnWsaTllGQdQJIkSZJUAyyeC3+qX7G+xTHQavPs8tQy223XnubNG/LDH+7Mk09+1wKSqiV7IkmSVCDOiSRJqpUmPgujr60YwgbQuhv0HpRdplpi7tzFNG1an4hg441b8cYbp9OhQ/OsY0mrZE8kSZIKJKI4L0mSMvVgPxj/ACyckVvvfCCcMg5KSrPNVcONGzeNnXe+id///vkv2ywgqbqziCRJkiRJWrmUYP7U3PLOP4a+/4I+w7LNVAs8/PB77LbbP3j33ekMGTKGxYvLso4krRGHs0mSVCAOZ5Mk1TpL51cs73s1lPgr5DeRUuLqq5/n4ov/TUpw9NHfYuDAI2nQwF5dqhn8DiBJkiRJ+qpFs+GxkyvWLSB9IwsWLOHUUx9k0KA3AfjlL3twySX7UlLiTSjVHH4XkCSpQOyIJEmqNT5/A27fvmK9fffsstQS55zzCIMGvUnTpvW5446+9O37rawjSWvNIpIkSQXicDZJUq0w9XW4Y4eK9VabQ6/bs8tTS1xxRQ/Gjp3G9df3Zttt22cdR1onTqwtSZIkScoZc/vyBaT9roHvj4c29ppZF0888T7l5QmAjh1b8OyzJ1tAUo1mEUmSpAKJiKK8JEmqMu8Mrlg+5Fbofn52WWqwpUvLOe+8Rzn44Du58sr/fNnuz3XVdA5nkyRJkiTBFx/Bh4/klnvdAVudlGmcmmrGjAUce+y9PPnkB9SvX8JGG7XIOpJUMBaRJEkqEG8uSpJqtIeOr1jufEB2OWqwMWOm0qfPYN5/fybt2jXlvvuOYe+9O2cdSyoYi0iSJBWIXdQlSTXa9DG5r1ufDM02zDZLDTRixDuceOL9zJ27mJ126sCwYcfSuXPLrGNJBWURSZIkSZLqqrLFMGYgvPkPWDwn17bnFVkmqpHKyxN/+MMLzJ27mOOO24ZbbjmCJk3qZx1LKjiLSJIkFYgdkSRJNc7Yu+CJHy7f1nyjbLLUYCUlwb339mfIkDGcffau9k5WreXT2SRJkiSpLiovg8dOqVjf+3dw5gwIf01cE5988gU/+cnjlJWVA9C+fTPOOWc3C0iq1eyJJElSgXjRKEmqMRbOgtu2rlg/5mno1COzODXNc899wlFHDeHzz+fTrl1TLrxwr6wjSVXCIpIkSQViDUmSVO3N/gTmfwbTxsDcybm2zQ6HjfbLNlcNcvPNoznzzIdZsqScgw7ajB/8YKesI0lVxiKSJEmSJNUFr1wNz160fFuH3aDviGzy1DBLlpTx4x8/xnXXvQrAj3+8O1dffRD16jn8T3WHRSRJkgqkxK5IkqTqauQFMPraivX23XNzH+14TnaZapAvvlhI375DePrpj2jQoJSbbjqMAQN2yDqWVOUsIkmSJElSbTduUMXy6VOg6QbZZamBmjSpD8AGGzRj2LBj2X13n2CnuskikiRJBWJHJElStTTxOZj3aW755HEWkNZCWVk5paUl1K9fytCh/Vm0aCkdO7bIOpaUGQdvSpIkSVJtlcphyD4V6626ZJelBikvT1xxxUh69bqLpUvLAVh//SYWkFTn2RNJkqQCCbsiSZKqk8Vz4MZKw6563gYlpdnlqSHmzl3MgAEPcP/9YykpCZ555mMOOGDTrGNJ1YJFJEmSCqTEGpIkqTp54oeweHbF+rdOzC5LDfHhhzPp02cwb745lZYtGzJ4cD8LSFIlFpEkSZIkqTYoWwJTXoQF0+CFy2HaW7n2hi3hR1PthbQaI0d+RL9+Q5k+fQHdurVh+PDj6NZt/axjSdWKRSRJkgrE4WySpEzd0gXmfLJ8W0k96PcElDbIJlMN8cILEzjooDtYurScXr26MGjQ0bRq1SjrWFK1YxFJkiRJkmqyJfNg+FHLF5A27QXb/wg67g2NWmeXrYbYbbeOHHjgpmy/fXt++9sDKS31GVTSylhEkiSpQOyIpG8iIpqklOZnnUNSDVO2BP7abPm288sgLIKsztSp84iAtm2bUlpawoMPHk/9+g75k76O31kkSSqQKNJ/qt0iYs+IeBsYl1/fPiL+nnEsSTXFnysNU2vdFc6aZQFpDfzvf1Po3v0mjj56KIsXlwFYQJLWgN9dJEmSsvUn4BBgOkBK6XVg30wTSar+UoK/t6tY3/ggOOXd3CTa+lpDhrzFXnvdyoQJs1mypJw5cxZlHUmqMRzOJklSgZTYaUjrKKU0YYWJ2cuyyiKphhhxNCz4PLfcuC0c/Vi2eWqA8vLEZZc9zW9+8ywA3/veDtxwQ28aNvTXYmlN+X+LJElStiZExJ5Aioj6wLnA2IwzSarOpo2B8cNyy1EKZ0zNNk8NMHv2Ik466X4efPBdSkqCa645mHPP3c0nq0prySKSJEkF4oWo1tHpwF+AjsAk4HHgjEwTSare3rqlYvncBdnlqEH++c//8eCD79K6dSOGDu3Pt7+9WdaRpBrJIpIkSQViDUnrqFtK6cTKDRGxF/B8RnkkVWef/Q9G/ym3vPX3oLR+pnFqirPP3o2PP/6CM87YhS5d1ss6jlRjObG2JElStv62hm2S6rqXfg137lSxvpFz8K9KSokbbhjF5MlzACgpCa699hALSNI3ZE8kSZIKpMSuSFoLEbEHsCfQNiLOr7SpBeBzpiV91ZjbKpb3+jVsPSC7LNXYokVLOf30hxg48DVuu+11nnvuZEpL7T8hFYL/J0mSJGWjAdCM3E295pVes4F+a3KAiOgZEe9ExPiI+Nkq9jkmIt6OiDERMahA2SVVpfIyuPcQmDU+t95nOOz+Cwh/nVvRlClz6NHjNgYOfI3Gjetx3nm7WUCSCsieSJIkFYgdkbQ2Ukr/Af4TEQNTSh+v7fsjohS4DjgImAi8GhEjUkpvV9qnK3AxsFdKaWZEtCtQfElVaep/4ePHK9Y7759dlmps3LjZnHTSzUyaNIdOnVowfPhx7Lhjh6xjSbWKRSRJkqRszY+IPwBbA42WNaaUDljN+3YFxqeUPgCIiMFAH+DtSvv8ALgupTQzf0yfAy7VNOVlMGj3ivWzZkGD5tnlqaYGDXqTc899ncWLy9l7787ce29/2rdvlnUsqdaxiCRJUoGEXZG0bu4ChgCHAacDA4DP1+B9HYEJldYnArutsM8WABHxPLl5lq5IKT264oEi4jTgNIC2bdsycuTItfsTqKjmzp3rZ1INVdXnstFn99IllQMwpU1P3nnxf0U/Z0301FMfs3hxOYcd1oFzztmYsWNHMXZs1qkEfg+rbSwiSZJUINaQtI7apJRuiYhzKw1xe7VAx64HdAV6ABsBz0TEtimlWZV3SindBNwE0K1bt9SjR48CnV6FMHLkSPxMqp8q+1xeeTlXIgY6DHiIDs6DtFL77Zfo2vV+LrzwKG/qVDN+D6td/A4kSZKUrSX5r1MiondE7AisyTOoJwGdKq1vlG+rbCIwIqW0JKX0IfAuuaKSpJrizZtzX3e50Im0K3n33enst99APvnkCyDXG3i33dpYQJKKzO9CkiQVSElEUV6q9X4dES2BC4CfAP8AzluD970KdI2ITSOiAXAcMGKFfR4g1wuJiFif3PC2DwqUW1KxvXETzHo/t1y+5Ov3rUMefXQ8u+56M8888zG/+MVTWceR6hSHs0mSJGUopfSv/OIXwP4AEbHXGrxvaUScBTxGbr6jW1NKYyLiSmBUSmlEftvBEfE2UAb8NKU0vRh/DkkFNO8zuLkzlC2uaNvrN9nlqSZSSlxzzYtcdNGTlJcn+vbdkuuv7511LKlOWW0RKSLOBf4JzCF3Z2xH4Gcppce/9o2SJNUx9hnS2oiIUuAYchNkP5pSeisiDgN+DjQmd831tVJKDwMPr9B2WaXlBJyff0mqKW7qtHzPo5NGQf3G2eWpBhYuXMoPfvAgd975BgBXXLEfl166HyUl/vSVqtKa9EQ6JaX0l4g4BGgNfAe4A7CIJElSJc7DoLV0C7k5jV4B/hoRk4Hu5G7WPZBpMknZ+OhxeO7nFQWkjQ+Gvg9CaYNsc2Vs6dJyDjjgNl58cSJNm9bn9tv7ctRR38o6llQnrUkRadkV8aHAHflu0l4lS5IkfTPdge1SSuUR0Qj4FNjc4WZSHVReBnfvCZ++snz7UQ9BiTOQ1KtXwlFHfYspU+YyfPhxbLdd+6wjSXXWmnxHGh0RjwObAhdHRHOgvLixJEmqeexRr7W0OKVUDpBSWhgRH1hAkuqgjx6D+3ou37bv1bDtqXW+gDR16jzatWsKwAUX7MFpp+1MixYNM04l1W1r8nS27wM/A3ZJKc0HGgAnFzWVJElaKxHx44gYExFvRcTdEdEo/9SulyNifEQMyT/Bi4homF8fn9++Sbbp66wtI+KN/OvNSutvRsQbWYeTVEUqF5CatIMfL4FdfgqNWmeXKWNLl5Zz/vmPsdVW1/HBBzOB3JBxC0hS9lZZ2o6InVZo2sxRbJIkrVpWPycjoiNwDrBVSmlBRAwl97j3Q4E/pZQGR8QN5G4MXZ//OjOl1CUijgN+DxybSfi6zQk9pLpu5nsVywf8Dbb/EZSUZpenGpg5cwHHHnsvTzzxAfXqlTB69GQ226zuFtSk6ubr+kde8zXbEnBAgbNIklSjZXyvpR7QOCKWAE2AKeR+Vp+Q334bcAW5IlKf/DLAvcD/RUTkn+SlKpJS+jjrDJIyNPV1uGOHivUdzsz8B0nWxo79nP9n777jrKjuPo5/ztI7SBEUFFQExYoKIhassSAQBMHYNRJbjFFjNDEaW/IkGo3dEAVLVLou9hYRGxawoiAWqqJIr7vs7nn+uCu7GMoC997Z8nnndV/3nDMzd75kZZf9zZkzvXoN44svFtC8eV1Gjz6Rgw7aPulYkkpZbxEpxnhoNoNIkqR1CyEMAgaVGhocYxz8YyfGOCeEcDMwE1hJ6gmqE4FFMcaC4t1mk3qUPMXvs4qPLQghLAaaAj9k9A8iSUqZ/ToMP6ikf8Q9Vb6A9NRTn/OLX4xm6dJ89t67JU88MZDttmuUdCxJP7HRldpCCHWBS4DtYoyDQgjtgQ4xxqcynk6SpAokU7ezFReMBq9vewihCanZRe2ARcBI4Oj17S9JSlDh6rULSD9/GnY4Nrk85cDs2Us44YQR5OcXcuKJnRg6tDd169ZIOpakdSjLcv9DSV3NPKC4P4fUP04tIkmSVD4cAXwdY5wHEEIYA3QHGocQqhfPRmpN6mc4xe9tgNkhhOpAI8CngiUohFCH1AW7qUlnkZRhj/csaQ98HbbtnlyWcqJ164bccstRLF6cx5VXHpjYGoOSNq4sT2fbMcb4d2A1QPET2vxbLUnST+SEzLzKYCawfwihbkj9y/tw4FPgFaBf8T6nA7nF7bHFfYq3/9f1kJITQjge+AB4rri/VwhhbLKpJKXdoq/gtSthxgupft0WVbqANGvWYsaPL1ka7oILuvCHPxxkAUkq58oyEym/+OpYBAgh7AjkZTSVJEkqsxjj2yGEUcAkoAB4n9Ttb08Dw0IINxSP3V98yP3AwyGEL4AFpJ7kpuT8GegCjAOIMX4QQmiXZCBJm2n+FBj3W4p/dVrb9OfX7p/+cVYilUdvvDGTvn1HkJdXwDvvnMPOOzdNOpKkMipLEekaUlfG2oQQHiE1Pf6MTIaSJKkiSvLqaYzxGlI/s0v7ilRx4qf7rgL6ZyOXymR1jHHxT/77cWaYVNGsWgQP7LLx/ToMhG7XpGYiVUH33TeJ889/mtWrizjiiB1o1qxu0pEkbYKNFpFijC+GECYB+5O6je03MUaf3iJJ0k84AV+baXII4RdAteIHmFwEvJlwJkmb6sl+Je1uf4ZWXf93n+Z7Qv1WWYtUnqxeXcgllzzPnXe+C8DFF3flppuOonr1sqywIqm8KMtMJIBDgANJXRWrATyesUSSJElVy6+BP5JaLuBR4HnghkQTSdp0M19OvTffCw746cTQqm3+/BX07z+SV16ZTs2a1bj33uM488y9k44laTNstIgUQrgb2Al4rHjoVyGEI2KMF2Q0mSRJFUyOi4Fq83SMMf6RVCFJUkX0zKkl7aOHJpejnPr88/m8/vpMtt66Ho8/PoBu3dokHUnSZirLTKTDgF1+fGpLCOFBYHJGU0mSJFUd/wghtARGAcNjjJ8kHUjSJogRPvtPSb/FXsllKae6dWvD8OH92G+/bWndumHScSRtgbLcgPoFsF2pfpviMUmSVEoImXmpcosxHgocCswD/hVC+DiEcFXCsSSV1ZhjS9oXLEwuRzlSVBS57rpXyc2dsmbsQuf+7gAAIABJREFU5z/fxQKSVAmsdyZSCOFJUmsgNQA+CyG8U9zvCryTnXiSJFUcST6dTRVbjHEucHsI4RXgcuBqXBdJKv8+eQCmP5dq12kOtRsnGqc8WLYsnzPOeILRoz+jUaNafP31b2jSpE7SsSSlyYZuZ7s5aykkSZKqqBDCLsAA4ARgPjAcuDTRUJLKZvzvStrnzEguRzkxffoievcexkcffUfDhrV49NETLCBJlcx6i0gxxlezGUSSpIrOiUjaTENIFY5+FmP8JukwkjZB/W1h5Q/Q91moUbWLJa++Op1+/Ubyww8r2HnnpuTmDqRjx2ZJx5KUZmV5Otv+wB3ALkBNoBqwPMboDa2SJElbKMbYLekMkrZQvZZJJ0jUQw99yNlnj6WgoIijj96Jxx47gcaNaycdS1IGlOXpbHcCA4GRwL7AacDOmQwlSVJFlONUJG2CEMKIGOOJIYSPSa07uWYTEGOMeyQUTVIZ5BTlwbwPk45RLnTs2Ixq1QIXX9yN//u/I6hWrSzPb5JUEZWliESM8YsQQrUYYyEwNITwPnBlZqNJklSxWEPSJvpN8XvPRFNI2jTfTICvn+bg90utfV8FZyKtXLmaOnVqANCly7ZMmXIhbdu6sLhU2ZWlRLwihFAT+CCE8PcQwm/LeJwkSZLWI8b4bXHz/BjjjNIv4Pwks0nagMe6wYRSBaQj7qlyRaQPPpjLLrvcxciRk9eMWUCSqoayFINOLd7vQmA50Abom8lQkiRVRCGEjLxU6R25jrFjsp5C0sYtmbmmObv5z2Hg67DnuQkGyr6RIyfTvfsQZsxYzL33TiTGuPGDJFUaG72drfhqGMAq4FqAEMJwUo+izZj2zetn8uOlSqPJfhcmHUGqEFa+f2fSEaS1hBDOIzXjaIcQwkelNjUA3kgmlaQNWv7tmuYX211E6227Jxgmu4qKItdc8wo33PAaAKefvif33tvTix1SFVOmNZHWwaeISJL0E97rrU30KPAs8FfgilLjS2OMC5KJJGmD3vhT6r1ll2RzZNnSpXmceurj5OZOJScncPPNR3LxxftbQJKqoM0tIkmSJGnLxBjj9BDCBT/dEELYykKSVM5Mfx5mvJhq16iXbJYsO+mk0Tz99DQaN67N8OH9OOqoHZOOJCkh6y0ihRA6r28TUCMzcSRJqri8IqtN9CipJ7NNBCKpf2P9KAI7JBFK0joUFcDoo0v6/V6A8a8nlyfLbrjhML77bjmPPtqX9u2bJh1HUoI2NBPpHxvYNiXdQSRJquhyrCFpE8QYexa/t0s6i6T1ePcmmDoCvnuvZOz4kZBTuW/oiDHyxhuzOPDA7QDYa6+WvPPOL71YImn9RaQY46HZDCJJklQVhRC6Ax/EGJeHEE4BOgP/jDHO3MihkjIlfxm8eQ1MvGXt8ZoNoH3lflB1Xl4B55//NEOGfMBDD/Xh1FP3BJxtKymlcpfQJUnKImciaTPdA+wZQtgTuBS4D3gYOCTRVFJVNedNGPaTp66d9CZUrwPN94RKXEyZO3cZJ5wwgjffnEWdOtWpWbNa0pEklTMWkSRJkpJVEGOMIYTewJ0xxvtDCGcnHUqqkua+t3YBqcXecNww2Grn5DJlycSJ39Cnz3Bmz15C69YNyc0dSOfOrZKOJamcsYgkSVKaONVfm2lpCOFK4FTgoBBCDj7ERMq+2a/D8INK+offBXudn1yeLHrssY8566yxrFpVQPfubRg9+kS23rp+0rEklUM5G9shpJwSQri6uL9dCKFL5qNJklSx5ITMvFTpDQDygLNijHOB1sBNyUaSqqDHjy1pH3VflSkgrVpVwNVXj2PVqgLOPntvXn75NAtIktarLDOR7gaKgMOA64ClwGhgvwzmkiRJqhJijHNDCI8A+4UQegLvxBgfSjqXVKUsng75S1PtYx6GXU9JNE421a5dnSeeGMC4cdM5//z9nFUraYM2OhMJ6BpjvABYBRBjXAjUzGgqSZIqoBAy81LlFkI4EXgH6A+cCLwdQuiXbCqpivnwnpJ2x5OSy5El06bN529/e31Nv1OnFlxwQRcLSJI2qiwzkVaHEKoBESCE0JzUzCRJkiRtuT8C+8UYv4c1/9Z6CRiVaCqpqnj2NPj04VR7+6Mgp3I/keyFF75kwIBRLFq0irZtGzNgwG5JR5JUgZSliHQ78DjQIoRwI9APuCqjqSRJqoByvIKrzZPzYwGp2HzKNltc0pZaMLWkgASwf+X9NSfGyK23TuB3v3uRoqJInz4dOfbY9knHklTBbLSIFGN8JIQwETgcCECfGONnGU8mSVIF42/92kzPhRCeBx4r7g8Ankkwj1R15PYpaZ//A9RpmlyWDFq1qoBf/eopHnroQwCuvvpgrrmmBzk+vUHSJtpoESmEsB2wAniy9FiMcWYmg0mSJFUFMcbfhRD6AgcWDw2OMT6eZCapSihYBQumpNr7XFJpC0hz5y6jT59hvP32HOrWrcGDD/ahX79dk44lqYIqy+1sT5NaDykAtYF2wFSgUwZzSZJU4Xg3mzZFCKE9cDOwI/AxcFmMcU6yqaQqIm8x3Nm4pN/9+uSyZFidOtVZtGgV22/fiNzcgey5Z8ukI0mqwMpyO9vupfshhM7A+RlLJEmSVDUMAR4CxgPHA3cAfRNNJFV2McLkB+D5s0rG9roAatRNLFKmFBVFcnICjRrV5plnTqZBg5o0b14v6ViSKriyzERaS4xxUgihaybCSJJUkbmwtjZRgxjjv4vbU0MIkxJNI1UFH/0LXjqvpN/pdDj8zuTyZEBBQRFXXPESS5bk8a9/9SSEwA47NEk6lqRKoixrIl1SqpsDdAa+yVgiSZKkqqF2CGFvUksGANQp3Y8xWlSS0q10Aenkd6HlvsllyYCFC1dy0kmjef75L6lePYff/KYrnTq1SDqWpEqkLDORGpRqF5BaI2l0ZuJIklRxORFJm+hb4JZS/bml+hE4LOuJpMqu7taw4jvo/3KlKyBNmfIDvXo9xrRpC2jWrC6jR59oAUlS2m2wiBRCqEZqqvVlWcojSVKF5ZOStSlijIcmnUGq9ArzYfXyVHvRl6kCEkDTyvV0smeemcZJJ41myZI89txza3JzB7L99o03fqAkbaL1FpFCCNVjjAUhhO7ZDCRJkiRJW2zhNBiy87q31Wyw7vEK6IknptC373BihP79d2Xo0N7Uq1cz6ViSKqkNzUR6h9T6Rx+EEMYCI4HlP26MMY7JcDZJkioUF9aWpHLk7b+UtGsVz8rJWwTHPAw1Ks9Tyo44Ygf22GNr+vXblT/+8SCCP4skZVBZ1kSqDcwndV9+JLXYYwQsIkmSJEkqf5bPhckPpNodT4LjHk00TrrNmbOEpk3rUrt2derXr8nbb/+SWrU2+cHbkrTJcjawrUXxk9k+AT4ufp9c/P5JFrJJklShhJCZlyq3kHJKCOHq4v52IYQuSeeSKqyiQri3VUm/88XJZcmAN9+cxT77DGbQoCeJMQJYQJKUNRv6blMNqE/JY2dLi5mJI0lSxeXC2tpMdwNFpGZ9XwcsJfUk3P2SDCVVWCt/KGkfcC20qjw12SFD3ufcc59i9eoi5sxZysqVBdStWyPpWJKqkA0Vkb6NMV6XtSSSJElVU9cYY+cQwvsAMcaFIQRXxZU21/zJqfe6LaDb1clmSZOCgiIuvfR5br/9HQB+/esu/OMfR1GjRrWEk0mqajZURPJ6qiRJmyD4o1ObZ3UIoRrFM71DCM1JzUyStKnmfwojD0+18xYnmyVN5s9fwYABo3j55a+pUSOHe+45jrPP7px0LElV1IaKSIdnLYUkSVLVdTvwOKn1KG8E+gFXJRtJqoC+eQseO6Ck3/vx5LKk0V/+8hovv/w1LVrUY8yYE+nefbukI0mqwtZbRIoxLshmEEmSKjrXRNLmiDE+EkKYSOoCXgD6xBg/SziWVLGsWrR2Aengv0O7Y5LLk0bXX38YCxeu4tpre9CmTaOk40iq4lzGX5KkNLGIpM0RQtgOWAE8WXosxjgzuVRSBbO01F+XnsOhw4nJZdlCMUYGD57IKafsQb16NalbtwZDhvROOpYkARaRJEmSkvY0qfWQAlAbaAdMBTolGUqqUL5+NvXebPcKXUBavjyfM8/MZeTITxk3bgaPPXZC0pEkaS0WkSRJSpMQnIqkTRdj3L10P4TQGTg/oThSxVOwCl67ItUuWp1sli0wY8YievcexocffkeDBjU5+eTdN36QJGWZRSRJkqRyJMY4KYTQNekcUoUxuE1J+6j7k8uxBcaPn0G/fiOYN28FO+20FWPHDmSXXZonHUuS/odFJEmS0sQ1kbQ5QgiXlOrmAJ2BbxKKI1UsK+fDyh9S7RZ7wzbdks2zGf71r/e48MJnKSgo4qijdmTYsBNo0qRO0rEkaZ1ykg4gSZJUxTUo9apFao0kV9GVymL4wSXtUydBBbutOMbIhAlzKCgo4pJL9ufpp39hAUlSueZMJEmS0qSC/e6iciCEUA1oEGO8LOksUoU0/9PU+9b7JptjM4UQuOee4/j5zzvSq1eHpONI0kY5E0mSpDTJCSEjL1VOIYTqMcZCoHvSWaQKacnMkvYJzyWXYxN9+OFcjjvuUZYuzQOgdu3qFpAkVRgWkSRJkpLxTvH7ByGEsSGEU0MIfX98JZpMKq/ylsDUkTCkI/x7+5Lxmg2Ty7QJRo36lAMOGMIzz0zjL395Lek4krTJvJ1NkqQ0cWFtbabawHzgMCACofh9TJKhpHLp2dPgy9y1xw69DarVSCZPGRUVRa69dhzXXTcegFNP3YNrrumRbChJ2gwWkSRJkpLRovjJbJ9QUjz6UUwmklSOTbqjpICUUwP2vRQOuK7cF5CWLs3j9NOf4PHHp5CTE/j734/gkku6EbxdWVIFZBFJkqQ08fcBbaJqQH3WLh79yCKS9FNTHitpD5oF9bZOLksZLVmSR/fuQ/jkk+9p1KgWw4f342c/2ynpWJK02SwiSZKUJjnrrAVI6/VtjPG6pENIFcKk2+Dbt1Lto+6vEAUkgIYNa9G9extWry5k7NiT2HnnpklHkqQtYhFJkiQpGVYdpbJ65eKS9o7HJ5ejDGKMLFq0iiZN6gBw++3HsHLlaho1qp1wMknacj6dTZKkNAkhMy9VWocnHUCqEJbMKGmf9iHUbZ5clo3Izy9k0KAn2X//+1m0aBUANWtWs4AkqdKwiCRJkpSAGOOCpDNI5VphPnwzAf7dtmSs+R6JxdmY775bxmGHPch9973PzJmLmTjxm6QjSVLaeTubJElpkuOsIUnafD98ArNfS7VfvQQKVq29vesfsp+pjCZN+pY+fYYxa9YStt22AU88MZB9990m6ViSlHYWkSRJSpMc7z1TloUQjgZuI/Wkt/tijP+3nv1OAEYB+8UY38tiRKnsHukCBSvXve3YR2CXX2Q3TxkNH/4JZ56Zy8qVBXTr1poxYwbQsmX9pGNJUkZYRJIkSaqAQgjVgLuAI4HZwLshhLExxk9/sl8D4DfA29lPKZXBNxPg7RtLCki7nAw1G0DT3WDvC5LNthGTJn3LwIGjATjrrL24++7jqFXLX7EkVV5+h5MkKU2ciKQs6wJ8EWP8CiCEMAzoDXz6k/2uB/4G/C678aQyevUy+OaNkv6x/0kuyybq3LkVl17aje22a8Svf92F4A8CSZWcRSRJkqSKaVtgVqn+bKBr6R1CCJ2BNjHGp0MIFpFUPv04A6nLFbDXhclmKYNp0+aTn1+4pn/zzUclmEaSsssikiRJaeKaSCpPQgg5wC3AGWXYdxAwCKB58+aMGzcuo9m0aZYtW1ZpvyZNF73J7t9PAuC9JW1ZNnEaMC3ZUBvw3nsLuPbaz2jQoDo33dSh0n5dKqrK/HelIvPrUrlYRJIkqRIIITQG7gN2AyJwFjAVGA60BaYDJ8YYF4bU/Ra3AccCK4AzYoyTEoitLTMHaFOq37p47EcNSP33MK74FpuWwNgQQq+fLq4dYxwMDAbo0KFD7NGjRwZja1ONGzeOSvk1ee4s+HLomu6+BxwGTdonGGj9Yoz8858T+P3vP6GoKHL44TvSpEn9yvl1qcAq7d+VCs6vS+WSk3QASZIqixAy8yqj24DnYowdgT2Bz4ArgJdjjO2Bl4v7AMcA7Ytfg4B70vh/g7LnXaB9CKFdCKEmMBAY++PGGOPiGGOzGGPbGGNbYALwPwUkKREFeTC5pIBEz+HltoCUl1fAWWeN5ZJLXqCoKHLVVQcxZswA6tb1erykqsfvfJIkpUlSV2ZCCI2Agym+bSnGmA/khxB6Az2Kd3sQGAf8ntTiyw/FGCMwIYTQOITQKsb4bZajawvEGAtCCBcCzwPVgCExxskhhOuA92KMYzf8CVKCRpdaR+jifKhWI7ksG/Dtt0vp23cEEybMpm7dGjzwQG/69++UdCxJSoxFJEmSKr52wDxgaAhhT2AiqUe6b12qMDQX2Lq4va4FmbcFLCJVMDHGZ4BnfjJ29Xr27ZGNTNJGFeTB7PGpdoPtym0BCeD112cyYcJsttuuEbm5A9lrr5ZJR5KkRFlEkiQpTTL1aOfSix4XG1y8hs2PqgOdgV/HGN8OIdxGya1rAMQYYwghZiSgJJVVjPDu30r6p0xMLksZ9O/fiX//O49evTrQokW9pONIUuIsIkmSVM6VXvR4PWYDs2OMbxf3R5EqIn33421qIYRWwPfF2ze2ILMkpd+K7+GerUv6TdpD3WbJ5VmHwsIi/vSnV+jXb1c6d24FwC9/2TnhVJJUfriwtiRJaRIy9NqYGONcYFYIoUPx0OHAp6QWWT69eOx0ILe4PRY4LaTsDyx2PSRJGZW3eO0CEsAJLySTZT0WLVpFz56P8de/vk6/fiPIzy9MOpIklTvORJIkKU1yMnQ7Wxn9Gnik+CldXwFnkrpYNCKEcDYwAzixeN9ngGOBL4AVxftKUmbMHg/Plfo2c8B10O1PyeVZh6lTf6BXr2F8/vl8mjatw5AhvalZs1rSsSSp3LGIJElSJRBj/ADYdx2bDl/HvhG4IOOhJFVtH/4Lxl0CBStKxtr0KHcFpGefncbAgaNZsiSP3XdvQW7uQNq1a5J0LEkqlywiSZKUJonOQ5Kk8iJ/KXyRCy+du/Z4j1uh40nJZFqP229/m4svfo4YoW/fXXjwwT7Ur18z6ViSVG5ZRJIkSZKUHh/dBy+es/bYSW9Cq64Qyt9yrK1bNwTgz38+hD/96RBycrwcIEkbYhFJkqQ0SXZJJElK2Ldvr11Aqrs1HPMwbNMtuUzrkJ9fuGa9o759d+HTTy+gY8fy9ZQ4SSqvyt/lAEmSKqgQQkZeklQhvH9HSfuMyXDeXGh7ZHJ51uGtt2ax8853MGHC7DVjFpAkqewsIkmSJEnacp+PTL3vfCI03TXZLOswdOj79OjxIDNmLOb2299OOo4kVUjeziZJUpp4ZUZSlVVUCIX5qfYe52x43ywrKCjisste4LbbUoWjCy/cj1tu+VnCqSSpYrKIJEmSJGnLrF5W0m5zaHI5fmLBgpUMGDCKl176iho1crjrrmM555x9ko4lSRWWRSRJktLE9YskVVnDDy5p51RLLkcpRUWRI498mEmTvqV587qMGTOAAw/cLulYklShOfNekiRJ0uZZ9BXMfg3mfZTqN98z2Tyl5OQErruuB/vs04r33htkAUmS0sCZSJIkpYnzkCRVKW//FV7/w9pjA19LJkuxGCPvvz+Xzp1bAXDccTtz9NE7Ua2a184lKR38bipJUpqEEDLykqRyJxatXUBqtjv8bAjUbJBYpOXL8xk4cDRdu97H+PEz1oxbQJKk9HEmkiRJkqRN8/noknafJ2HHnsllAWbOXEzv3sP44IO5NGhQk2XL8hPNI0mVlUUkSZLSxGvdkqqEyQ/Bc6eX9Nv+LLkswGuvzeCEE0Ywb94KdtyxCWPHnsSuuzZPNJMkVVb+e1eSJElS2cx5Y+0CUp8noVqNxOIMHjyRww9/iHnzVnDEETvwzjvnWECSpAxyJpIkSWni+kWSKr3P/lPSPmsaNNkpsSjz5i3niiteYvXqIi6+uCs33XQU1at7jVySMskikiRJaWIJSVKlVZgPg9vAiu9T/f2vTrSABNC8eT2GD+/HnDlLOeOMvRLNIklVhUUkSZIkSetXVAgP711SQILEFtL++OPvmDjx2zVFoyOP3DGRHJJUVVlEkiQpTbybTVKl9PaNMP/Tkv4lRYl8w3v88c849dTHWbWqgPbtt6J79+2ynkGSqjpvGpYkSZK0fl8/V9K+cFHWC0hFRZFrrx1H374jWL58NSedtDudO7fKagZJUoozkSRJSpMcV0WSVJnEInjmVPj2rVT/yH9DrUZZjbBsWT5nnPEEo0d/Rk5O4G9/O4JLL+3mgwwkKSEWkSRJShN/p5FUqXz7Dkx5tKTf/udZPf306Yvo3XsYH330HY0a1WLYsH4cfXSyi3lLUlVnEUmSJElSiRhhbF/44omSsQsXZX0WEsA33yylQ4em5OYOpEOHZlk/vyRpbRaRJElKk+DtbJIquoI8eGAXWPx1yViPW7NWQIoxAhBCoG3bxrzwwim0a9eExo1rZ+X8kqQNs4gkSZIkKeX9O9YuIF20HGrUzcqp8/ML+fWvn2HHHbfi8su7A7D33i6gLUnliUUkSZLSxDWRJFVoRYUw/nepdqgGF6+CnOz8uvD998s54YQRvP76TOrWrcHpp+/J1lvXz8q5JUllZxFJkqQ08elskiq0Z04uafd/KWsFpPff/5Y+fYYzc+Zitt22AU88MdACkiSVUxaRJEmSJMGCKSXt1odk5ZQjRkzmjDOeYOXKAvbfvzVjxpxIq1YNsnJuSdKms4gkSVKaeDubpAorRpj3Yao98I2sfEMbPHgiv/rVUwCceeZe3HPPcdSq5a8nklSe+V1akiRJqspWr4C3rivpN9kpK6c99tj2tG7dkMsu68ZFF3UlWImXpHLPIpIkSWni7z+SKpznzoDJD5b0G7WDui0ydro5c5bQqlUDcnICrVs3ZMqUC6hXr2bGzidJSq+cpANIkiRJyrJVi+DjIWsXkGo1ghNfzdgpX3rpK3bf/R5uuGH8mjELSJJUsTgTSZKkNAk+nU1SRZC/FO5qsvbYRcugRr2MnC7GyO23v80ll7xAUVFk4sRvKSqK5OT4PVOSKhqLSJIkpYm/D0mqEL5+tqTdfA84/O6MFZDy8go477ynGTr0AwD+8IcDuf76wywgSVIFZRFJkiRJqiqKCuDDe1Pt5nvAaR9m7FRz5y6jb9/hvPXWbOrUqc7Qob0ZMGC3jJ1PkpR5FpEkSUoTb2eTVO6NORZmvZJq122Z0VP9+tfP8tZbs2nTpiFPPDGQzp1bZfR8kqTMs4gkSZIkVXZFhTD6aJj5UqpfvTYcPTSjp7zjjmPIyQncfvvRbL11/YyeS5KUHT6dTZKkNAkhMy9J2iIrF8Ad9UsKSKEaXLgY6m+T1tMUFhYxdOj7FBYWAdCyZX2GD+9nAUmSKhFnIkmSlCbeziapXPpoMBSsKulfvApy0vtrwKJFq/jFL0bz7LNf8MUXC7jxxsPT+vmSpPLBIpIkSZJUWS3/Dl6/MtWu1QjO/yHtBaTPP59Pr16PMXXqfJo2rcMRR+yQ1s+XJJUfFpEkSUoTn1gtqVwpzId7Sy2e3XNE2gtIzz33BQMHjmLx4jx2370FubkDadeuSVrPIUkqP1wTSZIkSapsZo2DOxuV9HfuD22PStvHxxi5+eY3Oe64R1m8OI+f/7wjb755tgUkSarknIkkSVKauCaSpHLjtStK1kGq3RSOezStH19YGHn66WkUFUWuueYQrr76EHKcjilJlZ5FJK2Rn5fHpRecyerVqyksKOCgQ4/ktF+eT+6ox3h8xCN8O2cWI54eR6PGJVeYPpz0LvfedhMFBatp1LgJN981JME/gZQ5915zMsccvBvzFixl3/5/AeDq84+j5yF7UBQj8xYsZdA1/+HbeYsZeMy+XHLGkYQQWLZiFRf9ZTgffz4HgClPX8vS5XkUFhVRUFjEgSf/Pck/ltLMJ6lJKnf2uQR6/CPtH1u9eg4jR/bnzTdn0atXh7R/viSpfLKIpDVq1KzJ32+/jzp161JQsJpLzjuD/fY/kE577EXX7gdz+YW/XGv/ZUuXcOc//sKN/7ibFi1bsWjh/ISSS5n38JMTuHf4q9x3/Wlrxm598GWuu/tpAM4/6RCuHHQMF904jOnfzOeoX/6TRUtXclT3XbnrqpM4+LSb1xx39KDbmL9oedb/DJKkKiBGmP4crPwh1W/fN20f/fbbs7nrrncZMqQ31avn0KxZXQtIklTFZKyIFELYJ8Y48SdjPWOMT2XqnNoyIQTq1K0LQEFBAYUFBYQAO+28yzr3f+XFZ+l+yOG0aNkKgMZNmmYtq5Rtb0z6ku1abbXW2NLlJY9LrlunFjFGACZ8+PWa8Xc++pptt26cnZBKnBORJCVuzusw5tiSfrVaafnYBx/8gEGDniI/v5CuXbflggu6pOVzJUkVSyZnIv07hHBajPETgBDCScDFgEWkcqywsJALzzqJb+bM5Pi+A+jYaY/17jt75gwKCwv43YVns2LFcvr0P5kjjzk+i2ml5P35guM5uWcXFi9bydGDbv+f7Wf0OYDn3/h0TT/GyJN3X0iMkftHv8GQMW9kM64kqTKLEd68JtVu0AZ2Oxu27rxFH1lQUMTll7/IrbdOAOD88/dl0KB9tjSpJKmCymQRqR8wKoTwC+Ag4DQgfY+EUEZUq1aNex4cwbKlS7j2yt8y/atptN2h/Tr3LSwsYNqUT/nb7YPJy8vj4l+dxi6ddqf1dm2zG1pK0J/vepI/3/Ukl511FOcOOJgb7n1mzbaD923P6X26cfhZt64ZO/zMW/lm3mKaN6nPU/deyNTpc3lj0pdJRFcG5LgokqSkvHsTjL+8pL/d4XDANVv0kQsXrmTAgFG8+OJEteHyAAAgAElEQVRXVK+ew113HWsBSZKquJxMfXCM8StgIDAGOAE4Ksa4eEPHhBAGhRDeCyG89+hD92cqmsqgfoOG7Nl5P96d8OZ692neYmv26XoAtevUpVHjJuy+V2e++uLzLKaUyo/hz7xLn8P3WtPfrf023HP1L+j/28EsWFyy/tE381LfBuctXMbY/37Efp3aZjuqJKky+nxkSbtWIzjwxi36uDlzltCly328+OJXNG9el//+9zQLSJKk9BeRQggfhxA+CiF8BIwCtgLaAW8Xj61XjHFwjHHfGOO+vzjt7HRH00YsWriAZUuXAJCXt4pJ706gzfZt17t/t4MOZfJH71NYUMCqVSuZMvljtmvbLktppeTtuF3zNe2ePfbg8+nfAdCmZROG3XwOZ//pIb6Y+f2aferWrkn9urXWtI/o1pHJX36T3dDKqJChlySVWf//woWLoP42W/QxW29dnx12aMJee7Xk3XfP4aCDtk9TQElSRZaJ29l6ZuAzlQUL5v/AzTdcRVFREUVFRRx82FHs3/0Qnhj5CCMfeYAFC+Zz7mn96dLtQH575Z/Zru0O7Nu1O+ee3p8QAkcf33e9t75JFd2Dfz2Dg/ZpT7PG9fniueu5/t5nOPrATrTfvgVFRZGZ3y7gohuHAXDloGPYqnE9/nnlAAAKCos48OS/06JpA4bfcg4A1atVY/iz7/Him58l9mdSBljxkZSEd2+Gue+m2jXqbfbHxBhZvnw19evXpHr1HIYP70eNGjnUq1czTUElSRVd+PFpQmn/4BD2BybHGJcW9xsCu8QY3y7L8dN/WJWZYFIls8uRlyUdQaoQVr5/Z8ZLPBO+XJSRn13779jY8pSypkOHDnHq1KlJx1Ap48aNo0ePHv+7oagQVi+HOxuVjJ33PdRt/r/7bsSKFas5++yxfPPNUl588VRq1qy2+YGriPV+XZQYvyblk1+X8ieEMDHGuO/mHJvJhbXvAUo/DmLZOsYkSao0glORJGVL3hJ4YFdYNqdk7Lx5ULfZJn/UrFmL6dNnOJMmfUv9+jWZPPl79t67VRrDSpIqi0wWkUIsNc0pxlgUQsjk+SRJkqTKb8HnMLRDSb9GPdipz2YVkN54YyZ9+47g+++Xs+OOTcjNHUinTi3SGFaSVJlksqjzVQjhIlKzjwDOB77K4PkkSUpUcCKSpEwqXA1v3whvXVsy1mEA9By2WR93332TOP/8p1m9uogjjtiB4cP7sdVWddIUVpJUGaX96WylnAscAMwBZgNdgUEZPJ8kSYny6WySMurpgWsXkDpfDMc+slkf9dRTn3POOU+yenURv/lNV5599mQLSJKkjcrYTKQY4/fAwEx9viRJklRl5C2BaWNK+n2fgbZHb/YUyGOPbU+/frty7LE7ceaZe6cppCSpsstYESmEUBs4G+gE1P5xPMZ4VqbOKUlSopw2JCndVq+g3Zz74c5DS8bO+BSa7rLJH/XJJ9/TtGkdWrVqQE5OYMSIfgTvw5UkbYJM3s72MNAS+BnwKtAaWJrB80mSJEmVy/Nns/3c/5T0dzkFtuq4yR+TmzuFbt3up2/fEeTlFQBYQJIkbbJMLqy9U4yxfwihd4zxwRDCo8BrGTyfJEmJCk5FkpRuU0stmn3GZGi66yYdHmPkhhvGc/XV4wDYYYcmFBXFDR8kSdJ6ZLKItLr4fVEIYTdgLuDzQiVJlZYX9SWl1byPStq/mLDJBaTly/M544xcRo36lBDg//7vCH73uwOcgSRJ2myZLCINDiE0Aa4CxgL1gT9l8HySJElSxRcjzBoHIw8DYHnt7ajXqusmfcT06Yvo3XsYH330HQ0b1uKxx07g2GPbZyCsJKkqyWQR6eUY40JgPLADQAihXQbPJ0lSory2LyktHukC3723pjuvySHU28SPGDlyMh999B0779yU3NyBdOzYLL0ZJUlVUiaLSKOBzj8ZGwXsk8FzSpIkSRXXqoVrFZA48t9MX7ATbTfxYy677ABihEGD9qFx49obP0CSpDJIexEphNAR6AQ0CiH0LbWpIeBPMElS5eVUJElb6sl+Je2L86BaTRg3bqOH5ecX8qc//ZcLL+xCmzaNCCFw+eXdM5dTklQlZWImUgegJ9AYOL7U+FLgnAycT5IkSarYlsyAp0+Gb95I9bc9MFVAKoN585bTr99Ixo+fwRtvzOK118508WxJUkakvYgUY8wFckMIB8cYx5feFkLwcogkqdIKTkWStDlmvQojeqw91v/lMh364Ydz6d17GDNmLGabbRpwyy0/s4AkScqYnAx+9j/XMXZHBs8nSVKiQsjMS1IlN7vUddc9z4PzfyjTLKRRoz7lgAOGMGPGYrp23ZZ33z2HLl22zWBQSVJVl4k1kboBBwDNQwiXlNrUEKiW7vNJkqSUEEI14D1gToyxZ/FTUYcBTYGJwKkxxvwQQi3gIVIPu5gPDIgxTk8otqT3i6+zdrkCDvprmQ659tpx/PnPrwJw+ul7cu+9PaldO5PPzJEkKTMzkWoC9UkVqBqUei0B+m3gOEmSKrSQodcm+A3wWan+34BbY4w7AQuBs4vHzwYWFo/fWryfpGxbvRy+yIVaDVP9htuX+dC6dWuQkxO45ZajGDq0twUkSVJWZGJNpFeBV0MID8QYZ6T78yVJ0v8KIbQGjgNuBC4JqUVRDgN+UbzLg8CfgXuA3sVtgFHAnSGEEGOM2cysLRdCOBq4jdRs7/tijP/3k+2XAL8ECoB5wFn++yxBs8fDW9dDKL6OO+OFtbe37LrBwwsLi6hWLXXsZZcdwFFH7ciee7bMRFJJktYpk5csVoQQbgI6AbV/HIwxHpbBc0qSlJxk1y/6J3A5qdm/kLqFbVGMsaC4Pxv4cbGUbYFZADHGghDC4uL9f8heXG2p4tsX7wKOJPX1fTeEMDbG+Gmp3d4H9o0xrgghnAf8HRiQ/bQiFsHwQ9a9rU4z2O0saLHneg//73+/5rzznua5506mXbsmhBAsIEmSsi6TRaRHgOFAT+Bc4HRSV8AkSaqUMvV0thDCIGBQqaHBMcbBpbb3BL6PMU4MIfTISAiVR12AL2KMXwGEEIaRmmW2pogUY3yl1P4TgFOymlAlZr9W0j5yMDTcLtWu0wJa7LXeVfRjjIwZM4e77x5PYWHkjjve4ZZbfpaFwJIk/a9MFpGaxhjvDyH8ptQtbu9m8HySJFVKxQWjwRvYpTvQK4RwLKnZvw1J3eLUOIRQvXg2UmtgTvH+c4A2wOwQQnWgEakFtlWxrJlRVmw2sKH7oc4Gns1oIq3fmKOLGwH2OKdMh+TlFXDBBc9w//1fAHDllQdy/fWHZiigJEkbl8ki0uri929DCMcB3wBbZfB8kiQlaj0TCTIuxnglcGUqQ+gBXBZjPDmEMJLUQy2GkZoRnFt8yNji/lvF2//rekiVWwjhFGBfYJ33U5We7da8eXPGjRuXvXBVQK387+hWsAqAuVsdyZQy/P+7YEE+V189mcmTl1CzZuDyyzty+OHVeO218RlOq7JatmyZf1fKGb8m5ZNfl8olk0WkG0IIjYBLgTtIXRX9bQbPJ0mS1vZ7YFgI4QZSa+PcXzx+P/BwCOELYAEwMKF82jI/zij7UenZZmuEEI4A/ggcEmPMW9cHlZ7t1qFDh9ijR4+0h62Sigph6nB45uQ1Qy1Pf5qWORv+J/jKlavZdde7mT59Ca1bN+Sqq3biV786PtNptYnGjRuHf1fKF78m5ZNfl8olY0WkGONTxc3FgPNuJUmVXrLraqfEGMcB44rbX5FaN+en+6wC+mc1mDLhXaB9CKEdqeLRQEqexgdACGFv4F/A0THG77MfsYr5cULf0lnw7k3wwZ1rb+/6B9hIAQmgTp0aXHLJ/gwbNpkxY07ks8/ey0BYSZI2XSZnIkmSVLWUhyqSqoziJ+tdCDwPVAOGxBgnhxCuA96LMY4FbgLqAyND6n7LmTHGXomFrsyeOgmmDlv/9j5jYcf1zyYqLCxi2rQFdOzYDIALL+zCuefuS40a1fjss3SHlSRp81hEkiRJqqBijM8Az/xk7OpS7SOyHqqqWlcBqdOZsPMJ0O4YCDnrPXTx4lWcfPIY3nhjFu+880vat29KCIEaNaplMLAkSZsuY0WkEEK7GOPXGxuTJKmyCE5FkqqelfPh1ctK+hcuglqNynz4tGnz6dVrGFOm/MBWW9Vh7txltG/fNANBJUnacuu/JLLlRq9jbFQGzydJkiRl1+QHYfIDJf1NKCA9//wXdOlyH1Om/ECnTs15991zOOig7dOfUZKkNEn7TKQQQkegE9AohNC31KaGQO10n0+SpPIiOBFJqlryFsOyb1Ltmg3g9E/KdFiMkVtvncDvfvciRUWRPn068tBDfWjQoFYGw0qStOUycTtbB6An0BgovXrgUuCcDJxPkiRJyq55H8Mj+0Jhfqq/90XQcLsyHfr55/O54oqXKCqKXH31wVxzTQ9ycqxCS5LKv7QXkWKMuUBuCKFbjPGtdH++JEnllb8CSlXIgs9SBaTqdaBh2w0+ee2nOnRoxr/+1ZMGDWrRr9+umcsoSVKaZfLpbLNCCI8D3Yv7rwG/iTHOzuA5JUlKjlUkqWqIEWa9kmrv0BOOH7HRQ955Zw7z56/gmGPaA3DmmXtnMqEkSRmRyYW1hwJjgW2KX08Wj0mSJEkV1zMnw4f3ptor521094cf/pCDDx7KgAGjmDZtfobDSZKUOZksIrWIMQ6NMRYUvx4AmmfwfJIkJSpk6H+SyokFU+GB3WDKYyVjh92x3t0LC4u47LIXOO20J8jLK+SUU/agbdvGWQgqSVJmZPJ2th9CCKcAP/6UPQnw0oskSZIqlhjhv7+GD+4qGavTDM6ZATXqrvOQhQtXctJJo3n++S+pXj2HO+44hnPP3TdLgSVJyoxMFpHOAu4AbgUi8CZwZgbPJ0lSooKThqTKZ/lcuLfV2mOdL4bu1623gDRlyg/06vUY06YtoFmzuowefSIHH7x9FsJKkpRZGSsixRhnAL0y9fmSJJU31pCkSmhIh7X7FyyE2hu+JW3x4lXMnLmYPffcmieeGOgtbJKkSiPtRaQQwtUb2BxjjNen+5ySJElS2uUvg/wlqfZOfaD342U6rGvX1jz77Ml06bIt9erVzGBASZKyKxMLay9fxwvgbOD3GTifJEnlQ8jQS1L2LfoSPrm/pH/8qPXuunLlak45ZQwjR05eM3booe0sIEmSKp20z0SKMf7jx3YIoQHwG1JrIQ0D/rG+4yRJkqRy46E9YXXxtdCtOkJOtXXuNnv2Evr0GcbEid/y0ktfcdxxO1O3bo0sBpUkKXsysiZSCGEr4BLgZOBBoHOMcWEmziVJUnkRnDYkVQ4LPi8pIO3cH3Y5ZZ27vfnmLPr2Hc533y1nhx2akJs70AKSJKlSy8SaSDcBfYHBwO4xxmXpPockSeWRT2eTKok3rylpHz9inbsMGfI+5533NPn5hRx2WDtGjOhH06brflqbJEmVRSZmIl0K5AFXAX8MJf+iDqQW1m6YgXNKkiRJm2f1cijMh3dvgoVTYdqY1Hirbuvc/cYbx3PVVa8AcNFFXfjHP35G9eqZWGpUkqTyJRNrIvkTVJJUJTkRSaqApgyDZ06BWPi/23oOX+chxx7bnptvfoubbz6Ss8/unOGAkiSVHxlZE0mSJEkq94oK4OmTSvq1GkHtpnDw36HZbtCwzZpN33+/nBYt6gGw996tmD79NzRqVDvbiSVJSpSzhiRJSpeQoZekzFgwpaTd9xm4cBH88kvY+QTYqsOaTbm5U9hpp9v5z38+WjNmAUmSVBVZRJIkSVLVNHt86r32VtDumP/ZHGPkxhvH06fPcJYuzefll7/OckBJksoXb2eTJClNgtOGpIpjySx4+YJUO/zvddXly/M588xcRo78lBDgL385nN//vnuWQ0qSVL5YRJIkKU2CNSSpYihYBcMOLOkf9Le1Ns+YsYg+fYbzwQdzadCgJo8+egI9e+6c5ZCSJJU/FpEkSZJUecUIc16D8b+H796DnJpQsKJk+469YbczS+0eGThwNB98MJeddtqKsWMHsssuzRMILklS+eOaSJIkpYnrakvl0PeTYPgh8O2E1NPYSheQmu8BRw9daxphCIF///t4+vbdhXfe+aUFJEmSSnEmkiRJkiqvFd+n3uu1hJ37wwHXQbUaEKpB9dQT1lavLiQ3dyr9+u0KwG67tWD06BOTSixJUrnlTCRJktLFqUhS+VJUCG/8KdVuvhccdjvUbgw16q0pIM2bt5wjj3yY/v1Hcv/9kxIMK0lS+edMJEmS0sSns0nlSIxwT3NYtTDVz1v0P7t89NF39Or1GDNmLKZly/p06tQiyyElSapYLCJJkiSp8pnyWEkBCaBP7lqbR4/+lNNOe4IVK1az337b8PjjA9h224ZZDilJUsViEUmSpDQJTkSSyo83ry5p/3Y15KT+2VtUFLn22nFcd914AE45ZQ8GD+5JnTo1kkgpSVKF4ppIkiRJqlwK82HRl6n2PpesKSABLF+ez/Dhk8nJCdx005E89FAfC0iSJJWRM5EkSUoTJyJJCSpYBdPGQP4SeOm8kvED/7LWbg0a1CI3dyBff72Io4/eKcshJUmq2CwiSZKUJt7OJmXZoq8gf2mq/dFg+PDutbfvehpUr8Urr3zNs89+wd/+dgQhBDp0aEaHDs2yn1eSpArOIpIkSZIqnqkj4KkB6962x6+gzaHEDidy153vcPHFz1FYGOnevQ29e3fMbk5JkioRi0iSJKWNU5GkrBn/+5J28z1S79XrweF3wtadyc8v5IJBT3Lffe8DcPnlB9Cz584JBJUkqfKwiCRJkqSKZ8n01PuBf4WuV6y16bvvlnHCCSN4441Z1K5dnfvuO56TT94j+xklSapkLCJJkpQmrokkZcmir0rau5661qapU3/gyCMfZtasJWy7bQOeeGIg++67TZYDSpJUOeUkHUCSJEkqs/fvhPt3LOk32Hatzdts04CGDWvRrVtr3ntvkAUkSZLSyJlIkiSliRORpAxa+AW8fiV8PqpkbK8LACgqiqxeXUitWtVp0KAWL7xwKk2b1qFWLf+pK0lSOvmTVZKkNPF2NilDVi2EIe3XHjtrGjTZiSVL8jj55DE0aVKbBx/sQwiBbbZpkExOSZIqOYtIkiRJKt8mP1jS3vd3sOe50HgHvvhiAb16PcZnn/1Akya1mTFjMW3bNk4upyRJlZxFJEmS0iR4Q5uUfp89Cp89kmq37AKH/B2AF1/8kgEDRrFw4Sp23bU5Y8cOtIAkSVKGWUSSJElS+RQjPHNySX/H44kxctttb3PppS9QVBTp1asDDz/8cxo2rJVcTkmSqgiLSJIkpYsTkaT0+W4i/Gffkv6xj8JOvfn3vyfx298+D8BVVx3EtdceSk6Of/kkScoGi0iSJKWJv8ZKafLpw/DsaSX9htvDLicBcMope/Dwwx9x0UVd6N+/U0IBJUmqmiwiSZIkqfzIXwbPnVHSP+RmPsgZSPvl+dSrV5O6dWswfvwZBB+HKElS1uUkHUCSpMoihMy8pCrlk6EQi1LtE57nkalHsv8BQznzzFxijAAWkCRJSohFJEmSJJUPk26DVy4CoLAocPndgVNOeZy8vEKaNKlNYWFMOKAkSVWbt7NJkpQmwVWRpE23ZCbM+xBmjYOJtwCwaGVtfvHqbTz70ptUr57D7bcfzXnn7ZdsTkmSZBFJkqS0sYYkbZpYBA/uDvlL1gxN/b4pvUZfwedffkvTpnUYNepEevRom1xGSZK0hkUkSZIkJWP+ZyUFpB16Qk51bv+0H59/+QV77LE1ubkDadu2cbIZJUnSGhaRJElKEyciSZtozDEl7Z8/CcA/flbAVtuM5/e/P5D69WsmFEySJK2LC2tLkiQp++Z9DEtnsXJ1df744TUsXZoHQO3a1bn++sMsIEmSVA45E0mSpDTxqePSJnjzGuYsbkCfh8/gvemBrwue4tFHT0g6lSRJ2gCLSJIkScquRV8y4eV3+fkDg5i7tAFt2zbmyisPTDqVJEnaCItIkiSlSXBVJGnDVq+EUUfyQO5yfjXqDPILq9PjoJaMHHMqzZrVTTqdJKkcWb16NbNn/397dx4nVXXmf/zzBVoRURSMuECAUcQFBAUMiYOCuKBGlKhBonGJE+K+xhljjGM0ZuKa0eC4Ky5E0KCBqAkmgqKiBJVNUaNGfopJBMEQ2QWf3x/3tBZFd1d1U71gfd+8+tV1t3OfW4fbdeq55547n5UrVzZ2KButli1b0qFDByoqKkpWppNIZmZmJeLb2cxq9tm7E7nwltb877MHAXDmsS345ej/oKKieSNHZmZmTc38+fPZYost6Ny5M3Ijq9YigkWLFjF//ny6dOlSsnI9sLaZmZmZ1b9l/0BzbmfZ6k2oaL6W228exMiHfuwEkpmZVWnlypW0a9fOCaQ6kkS7du1K3pPLPZHMzMzMrF599syPaPbSLxAwcmhzfjC0Gb3PuKKxwzIzsybOCaQNUx/vn3simZmZmVnpLfsQ3p7A7267n6+dsIh/rmgJwCZb70Dv03/VyMGZmZlZXbgnkpmZWYn4YplZMvcB4onv8j+T+nPpHw4gYkfueHFvLrrjBtj+a40dnZmZWa1EBBFBs2YN3w9nzZo1tGjRdFI37olkZmZWIqqnf2Ybldm3s3z89xg++hh+/PtBAFz1nQ/44YVfh+32aeTgzMzMijNv3jy6devGiSeeSPfu3Xn//fe56KKL6N69Oz169GDs2LGfr3v11VfTo0cPevbsycUXX7xeWR9++CFDhw6lZ8+e9OzZk6lTpzJv3jy6d+/++TrXXXcdl19+OQADBgzgvPPOo0+fPlx11VV06tSJzz77DIBly5bRsWNHPv30U9555x0GDx5M79696d+/P2+88Ub9vim4J5KZmZmZlco/3+G9h/6To0Z9jxkfbE/rzVvw6weP4YgjujV2ZGZmtjG7vp4uql0YNS5+6623uPfee+nXrx/jxo1j5syZzJo1i48++oi+ffuy3377MXPmTMaPH8+0adNo1aoVixcvXq+cc845h/33359HH32UtWvXsnTpUj7++OMa97169WpeeuklAF555RWeeeYZBg4cyGOPPcYhhxxCRUUFI0aM4NZbb6Vr165MmzaNM844g0mTJtX9/SiCk0hmZmYl4tvZrKy9MZYFY06l742ns2Bpa3bqvDnjHzuRPfbYtrEjMzMzq5NOnTrRr18/AJ577jmGDx9O8+bNad++Pfvvvz/Tp0/nmWee4ZRTTqFVq1YAtG3bdr1yJk2axH333QdA8+bNadOmTcEk0rBhw9Z5PXbsWAYOHMiYMWM444wzWLp0KVOnTuXYY4/9fL1Vq1Zt8DEX4iSSmZmZmW2YFYvg8ePYdgsY1us1Xl+5D2Mnnknbtps1dmRmZvZlUKDHUH3ZfPPN663sFi1afH6LGsDKlSur3feQIUO45JJLWLx4MS+//DIHHHAAy5YtY6uttmLmzJn1FmNVPCaSmZlZiaiefgruV+ooabKkuZJek3Rumt9W0h8lvZV+b53mS9JNkt6WNFvS3qV6D6z8fPr32bx31U6fT99wz3n8fupPnEAyM7Mvlf79+zN27FjWrl3LwoULmTJlCvvssw8HHXQQ99xzD8uXLweo8na2QYMGccsttwCwdu1alixZQvv27VmwYAGLFi1i1apVPPbYY9Xuu3Xr1vTt25dzzz2Xb37zmzRv3pwtt9ySLl268PDDDwPZ4N+zZs2qhyNfl5NIZmZmpdJYWSRYA1wYEbsD/YAzJe0OXAw8FRFdgafSNMChQNf0MwK4pe4HbeXso4+Wc0j/Gxhwy8l8tKwV7HESLXY+mBYVzRs7NDMzs5IaOnQoe+65Jz179uSAAw7gmmuuYbvttmPw4MEMGTKEPn360KtXL6677rr1tr3xxhuZPHkyPXr0oHfv3sydO5eKigouu+yyzxNRu+66a437HzZsGA888MA6t7mNHj2au+66i549e7LHHnswfvz4kh93PkU0TrewQuZ9tLJpBmbWxOx20A8bOwSzjcKKGSPrfcSiT1Z9Vi+fXVts2qxWsUsaD4xMPwMi4u+Stgeejohukm5Lrx9M679ZuV6pY7eNT7du3eLNN98suN6cOR8y5PBRzHt/Je23WMofbtyUXif9FJo5gVRqTz/9NAMGDGjsMCyP66XpcZ00TXWtl9dff53ddtut9AGVmareR0kvR0SfupTnMZHMzMxKREV2G6p1udIIsh5DlW6PiNurWbczsBcwDWifkxj6B9A+vd4ReD9ns/lpnpNIVpRHHnmdE098lGXLPqVPhw949OSxdDhlSWOHZWZmZvXMSSQzM7MmLiWMqkwa5ZLUGhgHnBcR/1LO4+IiIiS5l69tkM9WfsKV59zE5XesAeD4vWdzx7ET2Gyvkxo5MjMzM2sIHhPJzMysRKT6+Slu36ogSyCNjohH0uwP021spN8L0vwPgI45m3dI88yq9+HLTDl/Ty6/Yw1ScM3hT3L/8EfYrGINtOnc2NGZmZlZA3BPJDMzs42csi5HdwGvR8QNOYsmACcBv0i/x+fMP0vSGOBrwBKPh2Q1WjgHHujDgJ3hpwdPpm/HDzj0+6cCR0DrHWD3Exo7QjMz+xKKCFTsFTVbT32Mge0kkpmZWYk0YhNnX+C7wBxJM9O8S8iSRw9JOhX4f8C307IngMOAt4HlwCkNG65tTJ5+eh5bPn8he7fMpi+7YCcYNBFabNq4gZmZ2Zday5YtWbRoEe3atXMiqQ4igkWLFtGyZcuSluskkpmZWak0UvsmIp6rYe+Dqlg/gDPrNSjb6EUEt4x8gXPPn8h2rbsz44Ln2aZbXzjkrsYOzczMykCHDh2YP38+CxcubOxQNlotW7akQ4cOJS3TSSQzMzOzjZSkwcCNQHPgzoj4Rd7yTYH7gN7AImBYRMwrVO7q5cs4+4nRjOAAAA+tSURBVOwnuP3uuUAzhu/1KltvtgKGjCv5MZiZmVWloqKCLl26NHYYlsdJJDMzsxJRY97QZmVHUnPgZuAgYD4wXdKEiJibs9qpwMcRsbOk44CrgWE1lbvZJ29zYPezefbdTmzaYg13HjuBE4btBIcthYpW9XU4ZmZmthFwEsnMzMxs47QP8HZE/BUgDZR+JJCbRDoSuDy9/g0wUpKihpE2X1+wDavXdmKHNp/w25MepO8RQ+DAW+rnCMzMzGyj4iSSmZlZiXjMR2tgOwLv50zPJ3vaXpXrRMQaSUuAdsBH1RW6em1z+vXdlkfGX8D2219X4pDNzMxsY9Zkk0idt2nppngTJGlERNze2HHYF1bMGNnYIVgVfK6Up5YtfD+bbZwkjQBGpMlVL04/49UddjijMUOydW1DDYk/azSul6bHddI0uV6anm513bDJJpGsyRoB+IuxWWE+V8ysvn0AdMyZ7pDmVbXOfEktgDZkA2yvIyW9bweQ9FJE9KmXiK1OXCdNk+ul6XGdNE2ul6ZH0kt13bZZKQMxMzMzswYzHegqqYukTYDjgAl560wATkqvjwEm1TQekpmZmVlN3BPJzMzMbCOUxjg6C5gINAfujojXJF0BvBQRE4C7gPslvQ0sJks0mZmZmdWJk0hWW749x6w4PlfMrN5FxBPAE3nzLst5vRI4tpbF+u9X0+M6aZpcL02P66Rpcr00PXWuE7lHs5mZmZmZmZmZFeIxkczMzMzMzMzMrCAnkcqUpKMkhaRd03QvSYflLB8g6RsbUP7SUsRpVh/S//3rc6Z/KOnyAtscJWn3Wu5nnfOoLmXkbNtZ0qt12dbMrCqSBkt6U9Lbki6uYvmmksam5dMkdW74KMtLEXVygaS5kmZLekpSp8aIs9wUqpec9Y5ObQw/haqeFVMnkr6dzpfXJP26oWMsN0X8/fqqpMmSZqS/YYdVVY6VjqS7JS2o7juEMjelOpstae9iynUSqXwNB55LvwF6Abkn8gCgzkkksyZuFfAtSdvUYpujgNomgAaw7nlUlzLMzEpOUnPgZuBQsr9Lw6tIcp8KfBwROwO/BK5u2CjLS5F1MgPoExF7Ar8BrmnYKMtPkfWCpC2Ac4FpDRth+SmmTiR1BX4E7BsRewDnNXigZaTI8+RS4KGI2IvsIQ//17BRlqVRwOAalh8KdE0/I4BbiinUSaQyJKk18O9kjcPj0mOBrwCGSZop6b+A04Dz03R/SUekq5AzJP1JUvvKsiTdI2lOyl4enbevbSS9IOnwBj5Ms5qsIRtM7vz8BanHz6Scq7xfTb2JhgDXpnNip7xt1js/0hX73PNo//wyJH1f0nRJsySNk9Qqldde0qNp/qz8XoGS/i3tq299vDlmVhb2Ad6OiL9GxGpgDHBk3jpHAvem178BBklSA8ZYbgrWSURMjojlafJFoEMDx1iOijlXAK4kS7SubMjgylQxdfJ94OaI+BggIhY0cIzlppg6CWDL9LoN8LcGjK8sRcQUsiezVudI4L7IvAhsJWn7QuU6iVSejgT+EBF/ARYBPYDLgLER0SsirgZuBX6Zpp8l67XUL2WOxwD/mcr6CbAkInqkq2KTKneSEk2PA5dFxOMNdXBmRboZOF5Sm7z5vwLuTf+fRwM3RcRUYAJwUTon3snbZr3zIyLmse559EwVZTwSEX0joifwOlliF+Am4Jk0f2/gtcodSeoGjANOjojpJXovzKz87Ai8nzM9P82rcp2IWAMsAdo1SHTlqZg6yXUq8Pt6jcigiHpJt4B0dHu3wRRzruwC7CLpeUkvSqqpN4ZtuGLq5HLgBEnzyZ4qenbDhGY1qO3nDgAt6i0ca8qGAzem12PSdKGxVjoAY1NmchPg3TT/QLLuiABUZvuBCuAp4Mz05dmsSYmIf0m6DzgHWJGz6OvAt9Lr+ynuVoHqzo9Cukv6GbAV0BqYmOYfAJyY4lwLLJG0NfAVYDzwrYiYW+Q+zMzsS0bSCUAfYP/GjqXcSWoG3ACc3Mih2LpakN2iM4CsnTZFUo+I+GejRlXehgOjIuJ6SV8H7pfUPSI+a+zArHbcE6nMSGpL9gX1TknzgIuAbwOFuqf/ChgZET2AHwAtC6y/BngZOGSDAjarX/9LdiV38w0sp7bnR6VRwFlpu58Wsd0S4D2y21HNzDbEB0DHnOkOaV6V60hqQXb7waIGia48FVMnSDoQ+DEwJCJWNVBs5axQvWwBdAeeTm3rfsAED65dr4o5V+YDEyLi04h4F/gLWVLJ6kcxdXIq8BBARLxA1u6tzfikVnpFfe7kcxKp/BwD3B8RnSKic0R0JOs18VWyD8FKn+RNt+GL/1An5cz/I3Bm5UTqLQHZPa/fA3ZNYyyZNTkRsZjsw+zUnNlT+aJ33fHAs+l1/jmRq7rzI3+b/OktgL9Lqkj7qvQUcDpkAxXm3HK3GhgKnCjpOzUenJlZzaYDXSV1SWMjHkd2y22uCXzxN+0YYFJERAPGWG4K1omkvYDbyBJIHuOlYdRYLxGxJCK2Se3qzmRjVQ2JiJcaJ9yyUMzfr9+S9UIiPUhlF+CvDRlkmSmmTt4DBgFI2o0sibSwQaO0fBPIvldIUj+yYWr+XmgjJ5HKz3Dg0bx544DtgN3TgL/DgN8BQysH1ia7h/VhSS8DH+Vs+zNga0mvSpoFDKxckG7DGQ4cIOmMejsisw1zPeteBTkbOEXSbOC7ZE9agezWz4vSgNY75ZVxOVWfH/nnUX4ZPyF7isvzwBs5250LDJQ0h6xH3+dPt4iIZcA3yQbsHrIBx21mZSyNcXQW2W20r5M9Mec1SVfk/G25C2gn6W3gAqDaR5vbhiuyTq4lu/354fTZkv8lzUqsyHqxBlRknUwEFkmaC0wmG5PSPSnrSZF1ciHw/fSd8UGy8T19YaIeSXoQeAHoJmm+pFMlnSbptLTKE2TJ1beBO4CivrPL9WZmZmZmZmZmZoW4J5KZmZmZmZmZmRXkJJKZmZmZmZmZmRXkJJKZmZmZmZmZmRXkJJKZmZmZmZmZmRXkJJKZmZmZmZmZmRXkJJJZAZLWpsfovirpYUmtNqCsUZKOSa/vlLR7DesOkPSNOuxjnqRtip1fTRknSxpZiv2amZmZlYucdmPlT+ca1l1agv2NkvRu2tcrkr5ehzI+b5NKuiRv2dQNjTGVk9ue/p2krQqs30vSYaXYt5mVlpNIZoWtiIheEdEdWA2clrtQUou6FBoR/xERc2tYZQBQ6ySSmZmZmTWaynZj5c+8BtjnRRHRC7gYuK22G+e1SS/JW1aqtmhue3oxcGaB9XsBTiKZNUFOIpnVzrPAzqmX0LOSJgBzJTWXdK2k6ZJmS/oBgDIjJb0p6U/AtpUFSXpaUp/0enC6ejRL0lPpqtVpwPnpqk1/SV+RNC7tY7qkfdO27SQ9Kek1SXcCKvZgJO0j6QVJMyRNldQtZ3HHFONbkv47Z5sTJP05xXWbpOZ5ZW4u6fF0LK9KGlbL99jMzMzsS0FS69S2e0XSHElHVrHO9pKm5PTU6Z/mH5zaaa+k3vCtC+xuCrBz2vaCVNarks5L86pso1W2SSX9AtgsxTE6LVuafo+RdHhOzKMkHVNdG7iAF4AdUznrtUUlbQJcAQxLsQxLsd+d2qAzqnofzaxh1KkHhVk5Sj2ODgX+kGbtDXSPiHcljQCWRERfSZsCz0t6EtgL6AbsDrQH5gJ355X7FeAOYL9UVtuIWCzpVmBpRFyX1vs18MuIeE7SV4GJwG7AfwPPRcQV6cP91Foc1htA/4hYI+lA4OfA0WnZPkB3YDkwXdLjwDJgGLBvRHwq6f+A44H7csocDPwtIg5PcbepRTxmZmZmG7PNJM1Mr98FjgWGRsS/lN32/6KkCREROdt8B5gYEVeli3Ot0rqXAgdGxDJJ/wVcQJZcqc4RwBxJvYFTgK+RXVycJukZ4N+ooY0WERdLOiv1aso3Fvg28HhK8gwCTidrd67XBo6Id6sKMB3fIOCuNGu9tmhEHC3pMqBPRJyVtvs5MCkivqfsVrg/S/pTRCyr4f0ws3rgJJJZYbmNgWfJPvS+Afw55wPyYGBPpfGOgDZAV2A/4MGIWAv8TdKkKsrvB0ypLCsiFlcTx4HA7tLnHY22TFek9gO+lbZ9XNLHtTi2NsC9kroCAVTkLPtjRCwCkPQI8O/AGqA3WVIJYDNgQV6Zc4DrJV0NPBYRz9YiHjMzM7ON2YrcJIykCuDnkvYDPiPrgdMe+EfONtOBu9O6v42ImZL2J7sI+Xxqc21C1oOnKtdKuhRYSJbUGQQ8WplgSe24/mQXQuvaRvs9cGNKFA0ma7uukFRdGzg/iVTZnt4ReB34Y8761bVFcx0MDJH0wzTdEvhqKsvMGpCTSGaFrci/IpM+zHOvfAg4OyIm5q1Xynu5mwH9ImJlFbHU1ZXA5IgYquwWuqdzlkXeukF2nPdGxI+qKzAi/iJpb7L72H8m6amIqOmqmZmZmdmX1fHAV4DeqRf3PLIEyOciYkpKMh0OjJJ0A/Ax2QW94UXs46KI+E3lhKRBVa20IW20iFgp6WngELJe6WMqd0cVbeAqrIiIXsoeUDORbEykm6i5LZpLwNER8WYx8ZpZ/fGYSGalMRE4PV1BQtIukjYnuzd9WLpffHtgYBXbvgjsJ6lL2rZtmv8JsEXOek8CZ1dOSKpMbE0h6waNpEOBrWsRdxvgg/T65LxlB0lqK2kz4CjgeeAp4BhJ21bGKqlT7kaSdgCWR8QDwLVkt/2ZmZmZlaM2wIKUQBoIdMpfIbWlPoyIO4A7ydpOLwL7Sqoc42hzSbsUuc9ngaMktUrt0aHAs0W20T6tbM9WYSzZbXKVvZqg+jZwlSJiOXAOcGEaKqK6tmh+O3gicLbS1VNJe1W3DzOrX04imZXGnWTjHb0i6VWyJ2O0AB4F3krL7qOKbsgRsRAYATwiaRbZBzTA74ChaUDB/mQfuH3SoIVz+eIpcT8lS0K9RnZb23s1xDlb0vz0cwNwDfA/kmawfs/EPwPjgNnAuIh4KT2541LgSUmzyboib5+3XQ+y+9Rnko3X9LMa4jEzMzP7MhtN1n6bA5xINgZQvgHArNQeGwbcmNqHJwMPpjbXC8CuxewwIl4BRpG15aYBd0bEDIpro91O1l4cXcWyJ4H9gT9FxOo0r7o2cE3xzSBrXw6n+rboZLJhHGYqGwD8SrJb3WanNu+VNb8LZlZftO6YbmZmZmZmZmZmZutzTyQzMzMzMzMzMyvISSQzMzMzMzMzMyvISSQzMzMzMzMzMyvISSQzMzMzMzMzMyvISSQzMzMzMzMzMyvISSQzMzMzMzMzMyvISSQzMzMzMzMzMyvISSQzMzMzMzMzMyvo/wMa+rs/9qQMOAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-c3ee167f33fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter_2400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "y_true, y_pred,y_prob = evaluate(model, test_iter_2400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBUvAnfyIwzH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ROBERTA_EA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
