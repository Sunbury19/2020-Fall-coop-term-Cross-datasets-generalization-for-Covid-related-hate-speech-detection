{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roberta model construction with Gab data\n",
    "\n",
    "2020/11/16\n",
    "\n",
    "Student: Xuanyu Su                                                                 \n",
    "Supervisor: Isar Nejadgholi\n",
    "\n",
    "In this file, we built an RoBERTa model by using Gab data to compare the distance among different databases, and use the final result to compare with the graph we conducted in week 11.\n",
    "\n",
    "in this version we remove the fine-tuning module(use part of test data pretrain on linear layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)\n",
    "\n",
    "# Set random seed and set device to GPU.\n",
    "torch.manual_seed(17)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer.\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tokenizer hyperparameters.\n",
    "MAX_SEQ_LEN = 128\n",
    "BATCH_SIZE = 8\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
    "\n",
    "\n",
    "# Define columns to read.\n",
    "label_field = Field(sequential=False, use_vocab=False, batch_first=True)\n",
    "text_field = Field(use_vocab=False, \n",
    "                   tokenize=tokenizer.encode, \n",
    "                   include_lengths=False, \n",
    "                   batch_first=True,\n",
    "                   fix_length=MAX_SEQ_LEN, \n",
    "                   pad_token=PAD_INDEX, \n",
    "                   unk_token=UNK_INDEX)\n",
    "\n",
    "fields = {'Text' : ('Text', text_field), 'Hate' : ('Hate', label_field)}\n",
    "\n",
    "\n",
    "# Read preprocessed CSV into TabularDataset and split it into train, test and valid.\n",
    "train_data, valid_data, test_data = TabularDataset(path=\"Data/final_GAB.csv\", \n",
    "                                                   format='CSV', \n",
    "                                                   fields=fields, \n",
    "                                                   skip_header=False).split(split_ratio=[0.70, 0.2, 0.1], \n",
    "                                                                            stratified=True, \n",
    "                                                                            strata_field='Hate')\n",
    "\n",
    "# Create train and validation iterators.\n",
    "train_iter, valid_iter = BucketIterator.splits((train_data, valid_data),\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               device=device,\n",
    "                                               shuffle=True,\n",
    "                                               sort_key=lambda x: len(x.Text), \n",
    "                                               sort=True, \n",
    "                                               sort_within_batch=False)\n",
    "\n",
    "# Test iterator, no shuffling or sorting required.\n",
    "test_iter = Iterator(test_data, batch_size=BATCH_SIZE, device=device, train=False, shuffle=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for saving and loading model parameters and metrics.\n",
    "def save_checkpoint(path, model, valid_loss):\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "                  'valid_loss': valid_loss}, path)\n",
    "\n",
    "    \n",
    "def load_checkpoint(path, model):    \n",
    "    state_dict = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    \n",
    "    return state_dict['valid_loss']\n",
    "\n",
    "\n",
    "def save_metrics(path, train_loss_list, valid_loss_list, global_steps_list):   \n",
    "    state_dict = {'train_loss_list': train_loss_list,\n",
    "                  'valid_loss_list': valid_loss_list,\n",
    "                  'global_steps_list': global_steps_list}\n",
    "    \n",
    "    torch.save(state_dict, path)\n",
    "\n",
    "\n",
    "def load_metrics(path):    \n",
    "    state_dict = torch.load(path, map_location=device)\n",
    "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with extra layers on top of RoBERTa\n",
    "class ROBERTAClassifier(torch.nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super(ROBERTAClassifier, self).__init__()\n",
    "        \n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.d1 = torch.nn.Dropout(dropout_rate)\n",
    "        self.l1 = torch.nn.Linear(768, 64)\n",
    "        self.bn1 = torch.nn.LayerNorm(64)\n",
    "        self.d2 = torch.nn.Dropout(dropout_rate)\n",
    "        self.l2 = torch.nn.Linear(64, 2)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, x = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x1 = self.d1(x)\n",
    "        x2 = self.l1(x1)\n",
    "        x3 = self.bn1(x2)\n",
    "        x4 = torch.nn.Tanh()(x3)\n",
    "        x5 = self.d2(x4)\n",
    "        x6 = self.l2(x5)\n",
    "        \n",
    "        return x6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain(model, \n",
    "             optimizer, \n",
    "             train_iter, \n",
    "             valid_iter, \n",
    "             scheduler = None,\n",
    "             valid_period = len(train_iter),\n",
    "             num_epochs = 5):\n",
    "    \n",
    "    # Pretrain linear layers, do not train bert\n",
    "    for param in model.roberta.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Initialize losses and loss histories\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0   \n",
    "    global_step = 0  \n",
    "    \n",
    "    # Train loop\n",
    "    for epoch in range(num_epochs):\n",
    "        count = 0\n",
    "        for (source, target), _ in train_iter:\n",
    "            mask = (source != PAD_INDEX).type(torch.uint8)\n",
    "        \n",
    "            y_pred = model(input_ids=source,  \n",
    "                           attention_mask=mask)\n",
    "            \n",
    "            loss = torch.nn.CrossEntropyLoss()(y_pred, target)\n",
    "            print('batch_no [{}/{}]:'.format(count, int(len(train_iter)/BATCH_SIZE)),'training_loss:',loss)\n",
    "            count+=1\n",
    "            loss.backward()\n",
    "            \n",
    "            # Optimizer and scheduler step\n",
    "            optimizer.step()    \n",
    "            scheduler.step()\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Update train loss and global step\n",
    "            train_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            # Validation loop. Save progress and evaluate model performance.\n",
    "            if global_step % valid_period == 0:\n",
    "                model.eval()\n",
    "                \n",
    "                with torch.no_grad():                    \n",
    "                    for (source, target), _ in valid_iter:\n",
    "                        mask = (source != PAD_INDEX).type(torch.uint8)\n",
    "                        \n",
    "                        y_pred = model(input_ids=source, \n",
    "                                       attention_mask=mask)\n",
    "                        \n",
    "                        loss = torch.nn.CrossEntropyLoss()(y_pred, target)\n",
    "                        \n",
    "                        valid_loss += loss.item()\n",
    "\n",
    "                # Store train and validation loss history\n",
    "                train_loss = train_loss / valid_period\n",
    "                valid_loss = valid_loss / len(valid_iter)\n",
    "                \n",
    "                model.train()\n",
    "\n",
    "                # print summary\n",
    "                print('Epoch [{}/{}], global step [{}/{}], PT Loss: {:.4f}, Val Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_iter),\n",
    "                              train_loss, valid_loss))\n",
    "                \n",
    "                train_loss = 0.0                \n",
    "                valid_loss = 0.0\n",
    "    \n",
    "    # Set bert parameters back to trainable\n",
    "    for param in model.roberta.parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    print('Pre-training done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "output_path = 'Model_roberta'\n",
    "def train(model,\n",
    "          optimizer,\n",
    "          train_iter,\n",
    "          valid_iter,\n",
    "          scheduler = None,\n",
    "          num_epochs = 5,\n",
    "          valid_period = len(train_iter),\n",
    "          output_path = output_path):\n",
    "    \n",
    "    # Initialize losses and loss histories\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    best_valid_loss = float('Inf')\n",
    "    \n",
    "    global_step = 0\n",
    "    global_steps_list = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Train loop\n",
    "    for epoch in range(num_epochs):\n",
    "        count = 0\n",
    "        for (source, target), _ in train_iter:\n",
    "            \n",
    "            mask = (source != PAD_INDEX).type(torch.uint8)\n",
    "\n",
    "            y_pred = model(input_ids=source,  \n",
    "                           attention_mask=mask)\n",
    "\n",
    "            loss = torch.nn.CrossEntropyLoss()(y_pred, target)\n",
    "            #loss = output[0]\n",
    "            print('batch_no [{}/{}]:'.format(count, int(len(train_iter)/BATCH_SIZE)),'training_loss:',loss)\n",
    "            count+=1\n",
    "            loss.backward()\n",
    "            \n",
    "            \n",
    "            # Optimizer and scheduler step\n",
    "            optimizer.step()    \n",
    "            scheduler.step()\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Update train loss and global step\n",
    "            train_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            # Validation loop. Save progress and evaluate model performance.\n",
    "            if global_step % valid_period == 0:\n",
    "                model.eval()\n",
    "                \n",
    "                with torch.no_grad():                    \n",
    "                    for (source, target), _ in valid_iter:\n",
    "                        mask = (source != PAD_INDEX).type(torch.uint8)\n",
    "\n",
    "                        y_pred = model(input_ids=source, \n",
    "                                       attention_mask=mask)\n",
    "\n",
    "                        \n",
    "                        loss = torch.nn.CrossEntropyLoss()(y_pred, target)\n",
    "\n",
    "                        \n",
    "                        valid_loss += loss.item()\n",
    "\n",
    "                # Store train and validation loss history\n",
    "                train_loss = train_loss / valid_period\n",
    "                valid_loss = valid_loss / len(valid_iter)\n",
    "                train_loss_list.append(train_loss)\n",
    "                valid_loss_list.append(valid_loss)\n",
    "                global_steps_list.append(global_step)\n",
    "\n",
    "                # print summary\n",
    "                print('Epoch [{}/{}], global step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_iter),\n",
    "                              train_loss, valid_loss))\n",
    "                \n",
    "                # checkpoint\n",
    "                if best_valid_loss > valid_loss:\n",
    "                    best_valid_loss = valid_loss\n",
    "                    save_checkpoint(output_path + '/model.pkl', model, best_valid_loss)\n",
    "                    save_metrics(output_path + '/metric.pkl', train_loss_list, valid_loss_list, global_steps_list)\n",
    "                        \n",
    "                train_loss = 0.0                \n",
    "                valid_loss = 0.0\n",
    "                model.train()\n",
    "    \n",
    "    save_metrics(output_path + '/metric.pkl', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    print('Training done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================= Start pretraining ==============================\n",
      "batch_no [0/302]: training_loss: tensor(1.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1/302]: training_loss: tensor(0.7395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2/302]: training_loss: tensor(0.5533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3/302]: training_loss: tensor(0.4738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4/302]: training_loss: tensor(0.5393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [5/302]: training_loss: tensor(0.4750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [6/302]: training_loss: tensor(0.4942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [7/302]: training_loss: tensor(0.5939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [8/302]: training_loss: tensor(0.4486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [9/302]: training_loss: tensor(0.4998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [10/302]: training_loss: tensor(0.6072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [11/302]: training_loss: tensor(0.4898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [12/302]: training_loss: tensor(0.4891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [13/302]: training_loss: tensor(0.4410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [14/302]: training_loss: tensor(0.6195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [15/302]: training_loss: tensor(0.5109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [16/302]: training_loss: tensor(0.5341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [17/302]: training_loss: tensor(0.4615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [18/302]: training_loss: tensor(0.4995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [19/302]: training_loss: tensor(0.4947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [20/302]: training_loss: tensor(0.6218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [21/302]: training_loss: tensor(0.5644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [22/302]: training_loss: tensor(0.4731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [23/302]: training_loss: tensor(0.4858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [24/302]: training_loss: tensor(0.5246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [25/302]: training_loss: tensor(0.4729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [26/302]: training_loss: tensor(0.6420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [27/302]: training_loss: tensor(0.5850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [28/302]: training_loss: tensor(0.9005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [29/302]: training_loss: tensor(0.4059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [30/302]: training_loss: tensor(0.5396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [31/302]: training_loss: tensor(0.4372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [32/302]: training_loss: tensor(0.4525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [33/302]: training_loss: tensor(0.5210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [34/302]: training_loss: tensor(0.4408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [35/302]: training_loss: tensor(0.4361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [36/302]: training_loss: tensor(0.5789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [37/302]: training_loss: tensor(0.7098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [38/302]: training_loss: tensor(0.5710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [39/302]: training_loss: tensor(0.5260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [40/302]: training_loss: tensor(0.7698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [41/302]: training_loss: tensor(1.0402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [42/302]: training_loss: tensor(0.9313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [43/302]: training_loss: tensor(0.4978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [44/302]: training_loss: tensor(0.4139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [45/302]: training_loss: tensor(0.5172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [46/302]: training_loss: tensor(0.4378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [47/302]: training_loss: tensor(0.3909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [48/302]: training_loss: tensor(0.5986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [49/302]: training_loss: tensor(0.4541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [50/302]: training_loss: tensor(0.4661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [51/302]: training_loss: tensor(0.4656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [52/302]: training_loss: tensor(0.5275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [53/302]: training_loss: tensor(0.5968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [54/302]: training_loss: tensor(0.4959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [55/302]: training_loss: tensor(0.5666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [56/302]: training_loss: tensor(0.4971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [57/302]: training_loss: tensor(0.5487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [58/302]: training_loss: tensor(0.4438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [59/302]: training_loss: tensor(0.6521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [60/302]: training_loss: tensor(0.4962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [61/302]: training_loss: tensor(0.4420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [62/302]: training_loss: tensor(0.3913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [63/302]: training_loss: tensor(0.4442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [64/302]: training_loss: tensor(0.4769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [65/302]: training_loss: tensor(0.5306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [66/302]: training_loss: tensor(1.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [67/302]: training_loss: tensor(0.8324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [68/302]: training_loss: tensor(0.5746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [69/302]: training_loss: tensor(0.4503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [70/302]: training_loss: tensor(0.5481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [71/302]: training_loss: tensor(0.4663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [72/302]: training_loss: tensor(0.4532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [73/302]: training_loss: tensor(0.4030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [74/302]: training_loss: tensor(0.3715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [75/302]: training_loss: tensor(0.3906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [76/302]: training_loss: tensor(0.3536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [77/302]: training_loss: tensor(0.3860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [78/302]: training_loss: tensor(0.4068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [79/302]: training_loss: tensor(0.4610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [80/302]: training_loss: tensor(0.3514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [81/302]: training_loss: tensor(0.4567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [82/302]: training_loss: tensor(0.4341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [83/302]: training_loss: tensor(0.5255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [84/302]: training_loss: tensor(0.3576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [85/302]: training_loss: tensor(0.4714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [86/302]: training_loss: tensor(0.4537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [87/302]: training_loss: tensor(0.4660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [88/302]: training_loss: tensor(0.5140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [89/302]: training_loss: tensor(0.3365, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [90/302]: training_loss: tensor(0.3716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [91/302]: training_loss: tensor(0.4498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [92/302]: training_loss: tensor(0.3218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [93/302]: training_loss: tensor(0.5258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [94/302]: training_loss: tensor(1.1513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [95/302]: training_loss: tensor(1.0559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [96/302]: training_loss: tensor(0.9832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [97/302]: training_loss: tensor(0.8723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [98/302]: training_loss: tensor(0.3788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [99/302]: training_loss: tensor(0.3847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [100/302]: training_loss: tensor(0.4124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [101/302]: training_loss: tensor(0.5107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [102/302]: training_loss: tensor(0.4004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [103/302]: training_loss: tensor(0.4571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [104/302]: training_loss: tensor(0.3914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [105/302]: training_loss: tensor(0.4569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [106/302]: training_loss: tensor(0.4111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [107/302]: training_loss: tensor(0.4119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [108/302]: training_loss: tensor(0.4410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [109/302]: training_loss: tensor(0.3783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [110/302]: training_loss: tensor(0.3826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [111/302]: training_loss: tensor(0.3776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [112/302]: training_loss: tensor(0.3499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [113/302]: training_loss: tensor(0.2961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [114/302]: training_loss: tensor(0.3399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [115/302]: training_loss: tensor(0.3591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [116/302]: training_loss: tensor(0.3392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [117/302]: training_loss: tensor(0.3770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [118/302]: training_loss: tensor(0.4037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [119/302]: training_loss: tensor(0.3938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [120/302]: training_loss: tensor(0.3701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [121/302]: training_loss: tensor(0.4401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [122/302]: training_loss: tensor(0.3584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [123/302]: training_loss: tensor(0.3541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [124/302]: training_loss: tensor(0.3825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [125/302]: training_loss: tensor(0.3023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [126/302]: training_loss: tensor(0.3517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [127/302]: training_loss: tensor(0.3564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [128/302]: training_loss: tensor(0.3796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [129/302]: training_loss: tensor(0.3858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [130/302]: training_loss: tensor(0.3638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [131/302]: training_loss: tensor(0.3939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [132/302]: training_loss: tensor(0.3329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [133/302]: training_loss: tensor(0.2913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [134/302]: training_loss: tensor(0.3080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [135/302]: training_loss: tensor(0.2549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [136/302]: training_loss: tensor(1.1262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [137/302]: training_loss: tensor(1.3910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [138/302]: training_loss: tensor(1.1837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [139/302]: training_loss: tensor(1.5258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [140/302]: training_loss: tensor(0.8505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [141/302]: training_loss: tensor(0.3419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [142/302]: training_loss: tensor(0.4406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [143/302]: training_loss: tensor(0.3629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [144/302]: training_loss: tensor(0.3727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [145/302]: training_loss: tensor(0.2574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [146/302]: training_loss: tensor(0.3126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [147/302]: training_loss: tensor(0.3710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [148/302]: training_loss: tensor(0.3417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [149/302]: training_loss: tensor(0.3565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [150/302]: training_loss: tensor(0.2578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [151/302]: training_loss: tensor(0.2157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [152/302]: training_loss: tensor(0.3069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [153/302]: training_loss: tensor(0.3485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [154/302]: training_loss: tensor(0.2719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [155/302]: training_loss: tensor(0.3619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [156/302]: training_loss: tensor(0.3980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [157/302]: training_loss: tensor(0.3341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [158/302]: training_loss: tensor(0.2751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [159/302]: training_loss: tensor(0.2750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [160/302]: training_loss: tensor(0.2492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [161/302]: training_loss: tensor(0.2879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [162/302]: training_loss: tensor(0.2865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [163/302]: training_loss: tensor(0.2796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [164/302]: training_loss: tensor(0.2871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [165/302]: training_loss: tensor(0.2628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [166/302]: training_loss: tensor(0.2634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [167/302]: training_loss: tensor(0.3865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [168/302]: training_loss: tensor(0.2552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [169/302]: training_loss: tensor(0.2355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [170/302]: training_loss: tensor(0.2941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [171/302]: training_loss: tensor(0.2484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [172/302]: training_loss: tensor(0.2208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [173/302]: training_loss: tensor(0.2544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [174/302]: training_loss: tensor(0.2136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [175/302]: training_loss: tensor(0.2816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [176/302]: training_loss: tensor(0.2083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [177/302]: training_loss: tensor(0.2449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [178/302]: training_loss: tensor(0.2244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [179/302]: training_loss: tensor(0.2166, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [180/302]: training_loss: tensor(0.1938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [181/302]: training_loss: tensor(0.2841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [182/302]: training_loss: tensor(0.1980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [183/302]: training_loss: tensor(0.2656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [184/302]: training_loss: tensor(0.2198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [185/302]: training_loss: tensor(0.1943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [186/302]: training_loss: tensor(0.2388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [187/302]: training_loss: tensor(0.2473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [188/302]: training_loss: tensor(0.2948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [189/302]: training_loss: tensor(0.2735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [190/302]: training_loss: tensor(0.1939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [191/302]: training_loss: tensor(0.2392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [192/302]: training_loss: tensor(0.2637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [193/302]: training_loss: tensor(0.2253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [194/302]: training_loss: tensor(0.1987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [195/302]: training_loss: tensor(0.1837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [196/302]: training_loss: tensor(0.2064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [197/302]: training_loss: tensor(1.6057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [198/302]: training_loss: tensor(1.7587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [199/302]: training_loss: tensor(1.8033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [200/302]: training_loss: tensor(1.6063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [201/302]: training_loss: tensor(1.6737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [202/302]: training_loss: tensor(0.8876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [203/302]: training_loss: tensor(0.2797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [204/302]: training_loss: tensor(0.1879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [205/302]: training_loss: tensor(0.2193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [206/302]: training_loss: tensor(0.1736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [207/302]: training_loss: tensor(0.1931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [208/302]: training_loss: tensor(0.1989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [209/302]: training_loss: tensor(0.2439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [210/302]: training_loss: tensor(0.1976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [211/302]: training_loss: tensor(0.2365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [212/302]: training_loss: tensor(0.2741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [213/302]: training_loss: tensor(0.2071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [214/302]: training_loss: tensor(0.2143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [215/302]: training_loss: tensor(0.2118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [216/302]: training_loss: tensor(0.2362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [217/302]: training_loss: tensor(0.2198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [218/302]: training_loss: tensor(0.2082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [219/302]: training_loss: tensor(0.1922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [220/302]: training_loss: tensor(0.2140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [221/302]: training_loss: tensor(0.2018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [222/302]: training_loss: tensor(0.2621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [223/302]: training_loss: tensor(0.2283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [224/302]: training_loss: tensor(0.1780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [225/302]: training_loss: tensor(0.1909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [226/302]: training_loss: tensor(0.2285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [227/302]: training_loss: tensor(0.2347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [228/302]: training_loss: tensor(0.2423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [229/302]: training_loss: tensor(0.1947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [230/302]: training_loss: tensor(0.1575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [231/302]: training_loss: tensor(0.2066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [232/302]: training_loss: tensor(0.2125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [233/302]: training_loss: tensor(0.1746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [234/302]: training_loss: tensor(0.2115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [235/302]: training_loss: tensor(0.1747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [236/302]: training_loss: tensor(0.1828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [237/302]: training_loss: tensor(0.1768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [238/302]: training_loss: tensor(0.1549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [239/302]: training_loss: tensor(0.2165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [240/302]: training_loss: tensor(0.2217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [241/302]: training_loss: tensor(0.1764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [242/302]: training_loss: tensor(0.2014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [243/302]: training_loss: tensor(0.1315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [244/302]: training_loss: tensor(0.1556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [245/302]: training_loss: tensor(0.1606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [246/302]: training_loss: tensor(0.1925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [247/302]: training_loss: tensor(0.1414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [248/302]: training_loss: tensor(0.1647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [249/302]: training_loss: tensor(0.1929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [250/302]: training_loss: tensor(0.2062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [251/302]: training_loss: tensor(0.1377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [252/302]: training_loss: tensor(0.1788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [253/302]: training_loss: tensor(0.1448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [254/302]: training_loss: tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [255/302]: training_loss: tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [256/302]: training_loss: tensor(0.1533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [257/302]: training_loss: tensor(0.1360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [258/302]: training_loss: tensor(0.1943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [259/302]: training_loss: tensor(0.1767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [260/302]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [261/302]: training_loss: tensor(0.1344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [262/302]: training_loss: tensor(0.1290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [263/302]: training_loss: tensor(0.1498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [264/302]: training_loss: tensor(0.1612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [265/302]: training_loss: tensor(0.1670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [266/302]: training_loss: tensor(0.1689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [267/302]: training_loss: tensor(0.1395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [268/302]: training_loss: tensor(0.1321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [269/302]: training_loss: tensor(0.1487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [270/302]: training_loss: tensor(0.1306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [271/302]: training_loss: tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [272/302]: training_loss: tensor(0.1384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [273/302]: training_loss: tensor(0.1622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [274/302]: training_loss: tensor(0.1582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [275/302]: training_loss: tensor(0.1181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [276/302]: training_loss: tensor(1.3254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [277/302]: training_loss: tensor(2.1255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [278/302]: training_loss: tensor(2.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [279/302]: training_loss: tensor(2.3277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [280/302]: training_loss: tensor(2.2295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [281/302]: training_loss: tensor(2.1939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [282/302]: training_loss: tensor(2.0582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [283/302]: training_loss: tensor(1.3678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [284/302]: training_loss: tensor(0.1254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [285/302]: training_loss: tensor(0.1442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [286/302]: training_loss: tensor(0.1346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [287/302]: training_loss: tensor(0.1371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [288/302]: training_loss: tensor(0.1383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [289/302]: training_loss: tensor(0.2075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [290/302]: training_loss: tensor(0.1377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [291/302]: training_loss: tensor(0.1402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [292/302]: training_loss: tensor(0.1397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [293/302]: training_loss: tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [294/302]: training_loss: tensor(0.1627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [295/302]: training_loss: tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [296/302]: training_loss: tensor(0.1476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [297/302]: training_loss: tensor(0.1931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [298/302]: training_loss: tensor(0.1348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [299/302]: training_loss: tensor(0.1651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [300/302]: training_loss: tensor(0.1363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [301/302]: training_loss: tensor(0.1897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [302/302]: training_loss: tensor(0.1419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [303/302]: training_loss: tensor(0.1545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [304/302]: training_loss: tensor(0.1468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [305/302]: training_loss: tensor(0.1995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [306/302]: training_loss: tensor(0.1299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [307/302]: training_loss: tensor(0.1285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [308/302]: training_loss: tensor(0.1299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [309/302]: training_loss: tensor(0.2128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [310/302]: training_loss: tensor(0.1878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [311/302]: training_loss: tensor(0.1495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [312/302]: training_loss: tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [313/302]: training_loss: tensor(0.1275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [314/302]: training_loss: tensor(0.1262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [315/302]: training_loss: tensor(0.1324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [316/302]: training_loss: tensor(0.1450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [317/302]: training_loss: tensor(0.1259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [318/302]: training_loss: tensor(0.1282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [319/302]: training_loss: tensor(0.1322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [320/302]: training_loss: tensor(0.1197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [321/302]: training_loss: tensor(0.1423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [322/302]: training_loss: tensor(0.1394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [323/302]: training_loss: tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [324/302]: training_loss: tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [325/302]: training_loss: tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [326/302]: training_loss: tensor(0.1157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [327/302]: training_loss: tensor(0.1172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [328/302]: training_loss: tensor(0.1107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [329/302]: training_loss: tensor(0.1171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [330/302]: training_loss: tensor(0.1340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [331/302]: training_loss: tensor(0.1384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [332/302]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [333/302]: training_loss: tensor(0.1302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [334/302]: training_loss: tensor(0.1214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [335/302]: training_loss: tensor(0.1069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [336/302]: training_loss: tensor(0.1371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [337/302]: training_loss: tensor(0.1485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [338/302]: training_loss: tensor(0.1281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [339/302]: training_loss: tensor(0.1250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [340/302]: training_loss: tensor(0.1450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [341/302]: training_loss: tensor(0.1118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [342/302]: training_loss: tensor(0.1097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [343/302]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [344/302]: training_loss: tensor(0.1272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [345/302]: training_loss: tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [346/302]: training_loss: tensor(0.1201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [347/302]: training_loss: tensor(0.1171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [348/302]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [349/302]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [350/302]: training_loss: tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [351/302]: training_loss: tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [352/302]: training_loss: tensor(0.1204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [353/302]: training_loss: tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [354/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [355/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [356/302]: training_loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [357/302]: training_loss: tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [358/302]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [359/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [360/302]: training_loss: tensor(0.1182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [361/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [362/302]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [363/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [364/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [365/302]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [366/302]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [367/302]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [368/302]: training_loss: tensor(2.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [369/302]: training_loss: tensor(2.4682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [370/302]: training_loss: tensor(2.5486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [371/302]: training_loss: tensor(2.5576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [372/302]: training_loss: tensor(2.4787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [373/302]: training_loss: tensor(2.5512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [374/302]: training_loss: tensor(2.5399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [375/302]: training_loss: tensor(2.5890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [376/302]: training_loss: tensor(2.3399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [377/302]: training_loss: tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [378/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [379/302]: training_loss: tensor(0.1144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [380/302]: training_loss: tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [381/302]: training_loss: tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [382/302]: training_loss: tensor(0.1233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [383/302]: training_loss: tensor(0.1230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [384/302]: training_loss: tensor(0.1187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [385/302]: training_loss: tensor(0.1186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [386/302]: training_loss: tensor(0.1444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [387/302]: training_loss: tensor(0.1275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [388/302]: training_loss: tensor(0.1175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [389/302]: training_loss: tensor(0.1251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [390/302]: training_loss: tensor(0.1077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [391/302]: training_loss: tensor(0.1260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [392/302]: training_loss: tensor(0.1863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [393/302]: training_loss: tensor(0.1162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [394/302]: training_loss: tensor(0.1625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [395/302]: training_loss: tensor(0.1072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [396/302]: training_loss: tensor(0.1314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [397/302]: training_loss: tensor(0.1709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [398/302]: training_loss: tensor(0.1511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [399/302]: training_loss: tensor(0.1371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [400/302]: training_loss: tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [401/302]: training_loss: tensor(0.1305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [402/302]: training_loss: tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [403/302]: training_loss: tensor(0.1114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [404/302]: training_loss: tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [405/302]: training_loss: tensor(0.1371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [406/302]: training_loss: tensor(0.1273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [407/302]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [408/302]: training_loss: tensor(0.1375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [409/302]: training_loss: tensor(0.1097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [410/302]: training_loss: tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [411/302]: training_loss: tensor(0.1581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [412/302]: training_loss: tensor(0.1237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [413/302]: training_loss: tensor(0.1165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [414/302]: training_loss: tensor(0.1189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [415/302]: training_loss: tensor(0.1085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [416/302]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [417/302]: training_loss: tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [418/302]: training_loss: tensor(0.1213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [419/302]: training_loss: tensor(0.1043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [420/302]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [421/302]: training_loss: tensor(0.1210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [422/302]: training_loss: tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [423/302]: training_loss: tensor(0.1394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [424/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [425/302]: training_loss: tensor(0.1234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [426/302]: training_loss: tensor(0.1103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [427/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [428/302]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [429/302]: training_loss: tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [430/302]: training_loss: tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [431/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [432/302]: training_loss: tensor(0.1165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [433/302]: training_loss: tensor(0.1155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [434/302]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [435/302]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [436/302]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [437/302]: training_loss: tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [438/302]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [439/302]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [440/302]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [441/302]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [442/302]: training_loss: tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [443/302]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [444/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [445/302]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [446/302]: training_loss: tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [447/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [448/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [449/302]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [450/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [451/302]: training_loss: tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [452/302]: training_loss: tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [453/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [454/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [455/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [456/302]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [457/302]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [458/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [459/302]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [460/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [461/302]: training_loss: tensor(0.3180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [462/302]: training_loss: tensor(2.6633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [463/302]: training_loss: tensor(2.5329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [464/302]: training_loss: tensor(2.5406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [465/302]: training_loss: tensor(2.6374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [466/302]: training_loss: tensor(2.6044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [467/302]: training_loss: tensor(2.6611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [468/302]: training_loss: tensor(2.4548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [469/302]: training_loss: tensor(2.6512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [470/302]: training_loss: tensor(2.4258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [471/302]: training_loss: tensor(0.9310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [472/302]: training_loss: tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [473/302]: training_loss: tensor(0.1224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [474/302]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [475/302]: training_loss: tensor(0.1376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [476/302]: training_loss: tensor(0.1335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [477/302]: training_loss: tensor(0.1346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [478/302]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [479/302]: training_loss: tensor(0.1105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [480/302]: training_loss: tensor(0.1375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [481/302]: training_loss: tensor(0.1535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [482/302]: training_loss: tensor(0.1682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [483/302]: training_loss: tensor(0.1572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [484/302]: training_loss: tensor(0.1218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [485/302]: training_loss: tensor(0.1609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [486/302]: training_loss: tensor(0.1289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [487/302]: training_loss: tensor(0.1239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [488/302]: training_loss: tensor(0.1356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [489/302]: training_loss: tensor(0.1469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [490/302]: training_loss: tensor(0.1539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [491/302]: training_loss: tensor(0.1399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [492/302]: training_loss: tensor(0.1270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [493/302]: training_loss: tensor(0.1281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [494/302]: training_loss: tensor(0.1209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [495/302]: training_loss: tensor(0.1357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [496/302]: training_loss: tensor(0.1264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [497/302]: training_loss: tensor(0.1433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [498/302]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [499/302]: training_loss: tensor(0.1193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [500/302]: training_loss: tensor(0.1310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [501/302]: training_loss: tensor(0.1229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [502/302]: training_loss: tensor(0.1282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [503/302]: training_loss: tensor(0.1423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [504/302]: training_loss: tensor(0.1318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [505/302]: training_loss: tensor(0.1349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [506/302]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [507/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [508/302]: training_loss: tensor(0.1064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [509/302]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [510/302]: training_loss: tensor(0.1211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [511/302]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [512/302]: training_loss: tensor(0.1360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [513/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [514/302]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [515/302]: training_loss: tensor(0.1149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [516/302]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [517/302]: training_loss: tensor(0.1078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [518/302]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [519/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [520/302]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [521/302]: training_loss: tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [522/302]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [523/302]: training_loss: tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [524/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [525/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [526/302]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [527/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [528/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [529/302]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [530/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [531/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [532/302]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [533/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [534/302]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [535/302]: training_loss: tensor(0.1164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [536/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [537/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [538/302]: training_loss: tensor(0.1116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [539/302]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [540/302]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [541/302]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [542/302]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [543/302]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [544/302]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [545/302]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [546/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [547/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [548/302]: training_loss: tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [549/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [550/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [551/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [552/302]: training_loss: tensor(1.6498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [553/302]: training_loss: tensor(2.6108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [554/302]: training_loss: tensor(2.6462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [555/302]: training_loss: tensor(2.6710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [556/302]: training_loss: tensor(2.6692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [557/302]: training_loss: tensor(2.7397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [558/302]: training_loss: tensor(2.4590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [559/302]: training_loss: tensor(2.3607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [560/302]: training_loss: tensor(2.6905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [561/302]: training_loss: tensor(0.7062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [562/302]: training_loss: tensor(0.1242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [563/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [564/302]: training_loss: tensor(0.1396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [565/302]: training_loss: tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [566/302]: training_loss: tensor(0.1149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [567/302]: training_loss: tensor(0.1222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [568/302]: training_loss: tensor(0.1154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [569/302]: training_loss: tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [570/302]: training_loss: tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [571/302]: training_loss: tensor(0.1271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [572/302]: training_loss: tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [573/302]: training_loss: tensor(0.1198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [574/302]: training_loss: tensor(0.1469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [575/302]: training_loss: tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [576/302]: training_loss: tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [577/302]: training_loss: tensor(0.1639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [578/302]: training_loss: tensor(0.1517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [579/302]: training_loss: tensor(0.1300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [580/302]: training_loss: tensor(0.1853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [581/302]: training_loss: tensor(0.1125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [582/302]: training_loss: tensor(0.1195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [583/302]: training_loss: tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [584/302]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [585/302]: training_loss: tensor(0.1355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [586/302]: training_loss: tensor(0.1416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [587/302]: training_loss: tensor(0.1178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [588/302]: training_loss: tensor(0.1455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [589/302]: training_loss: tensor(0.1423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [590/302]: training_loss: tensor(0.1214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [591/302]: training_loss: tensor(0.1219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [592/302]: training_loss: tensor(0.1315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [593/302]: training_loss: tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [594/302]: training_loss: tensor(0.1132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [595/302]: training_loss: tensor(0.1370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [596/302]: training_loss: tensor(0.1204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [597/302]: training_loss: tensor(0.1329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [598/302]: training_loss: tensor(0.1216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [599/302]: training_loss: tensor(0.1131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [600/302]: training_loss: tensor(0.1130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [601/302]: training_loss: tensor(0.1108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [602/302]: training_loss: tensor(0.1099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [603/302]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [604/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [605/302]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [606/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [607/302]: training_loss: tensor(0.1078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [608/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [609/302]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [610/302]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [611/302]: training_loss: tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [612/302]: training_loss: tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [613/302]: training_loss: tensor(0.1157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [614/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [615/302]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [616/302]: training_loss: tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [617/302]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [618/302]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [619/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [620/302]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [621/302]: training_loss: tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [622/302]: training_loss: tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [623/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [624/302]: training_loss: tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [625/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [626/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [627/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [628/302]: training_loss: tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [629/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [630/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [631/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [632/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [633/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [634/302]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [635/302]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [636/302]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [637/302]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [638/302]: training_loss: tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [639/302]: training_loss: tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [640/302]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [641/302]: training_loss: tensor(0.0555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [642/302]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [643/302]: training_loss: tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [644/302]: training_loss: tensor(1.1480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [645/302]: training_loss: tensor(2.7405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [646/302]: training_loss: tensor(2.8109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [647/302]: training_loss: tensor(2.8856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [648/302]: training_loss: tensor(2.7508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [649/302]: training_loss: tensor(2.7269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [650/302]: training_loss: tensor(2.7084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [651/302]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [652/302]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [653/302]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [654/302]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [655/302]: training_loss: tensor(0.1288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [656/302]: training_loss: tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [657/302]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [658/302]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [659/302]: training_loss: tensor(0.1099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [660/302]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [661/302]: training_loss: tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [662/302]: training_loss: tensor(0.1384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [663/302]: training_loss: tensor(0.1091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [664/302]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [665/302]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [666/302]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [667/302]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [668/302]: training_loss: tensor(0.1103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [669/302]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [670/302]: training_loss: tensor(0.1147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [671/302]: training_loss: tensor(0.1156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [672/302]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [673/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [674/302]: training_loss: tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [675/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [676/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [677/302]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [678/302]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [679/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [680/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [681/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [682/302]: training_loss: tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [683/302]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [684/302]: training_loss: tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [685/302]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [686/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [687/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [688/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [689/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [690/302]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [691/302]: training_loss: tensor(0.1110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [692/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [693/302]: training_loss: tensor(0.1105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [694/302]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [695/302]: training_loss: tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [696/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [697/302]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [698/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [699/302]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [700/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [701/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [702/302]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [703/302]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [704/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [705/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [706/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [707/302]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [708/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [709/302]: training_loss: tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [710/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [711/302]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [712/302]: training_loss: tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [713/302]: training_loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [714/302]: training_loss: tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [715/302]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [716/302]: training_loss: tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [717/302]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [718/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [719/302]: training_loss: tensor(0.0640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [720/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [721/302]: training_loss: tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [722/302]: training_loss: tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [723/302]: training_loss: tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [724/302]: training_loss: tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [725/302]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [726/302]: training_loss: tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [727/302]: training_loss: tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [728/302]: training_loss: tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [729/302]: training_loss: tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [730/302]: training_loss: tensor(2.2515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [731/302]: training_loss: tensor(2.9198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [732/302]: training_loss: tensor(2.8325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [733/302]: training_loss: tensor(2.7908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [734/302]: training_loss: tensor(2.7937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [735/302]: training_loss: tensor(2.8577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [736/302]: training_loss: tensor(2.7733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [737/302]: training_loss: tensor(2.6090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [738/302]: training_loss: tensor(2.6455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [739/302]: training_loss: tensor(2.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [740/302]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [741/302]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [742/302]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [743/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [744/302]: training_loss: tensor(0.1264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [745/302]: training_loss: tensor(0.1176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [746/302]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [747/302]: training_loss: tensor(0.1120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [748/302]: training_loss: tensor(0.1401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [749/302]: training_loss: tensor(0.1147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [750/302]: training_loss: tensor(0.1436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [751/302]: training_loss: tensor(0.1395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [752/302]: training_loss: tensor(0.1520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [753/302]: training_loss: tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [754/302]: training_loss: tensor(0.1266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [755/302]: training_loss: tensor(0.1358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [756/302]: training_loss: tensor(0.1396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [757/302]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [758/302]: training_loss: tensor(0.1334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [759/302]: training_loss: tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [760/302]: training_loss: tensor(0.1378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [761/302]: training_loss: tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [762/302]: training_loss: tensor(0.1277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [763/302]: training_loss: tensor(0.1229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [764/302]: training_loss: tensor(0.1156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [765/302]: training_loss: tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [766/302]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [767/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [768/302]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [769/302]: training_loss: tensor(0.1218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [770/302]: training_loss: tensor(0.1209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [771/302]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [772/302]: training_loss: tensor(0.1394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [773/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [774/302]: training_loss: tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [775/302]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [776/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [777/302]: training_loss: tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [778/302]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [779/302]: training_loss: tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [780/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [781/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [782/302]: training_loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [783/302]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [784/302]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [785/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [786/302]: training_loss: tensor(0.1019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [787/302]: training_loss: tensor(0.1190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [788/302]: training_loss: tensor(0.1154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [789/302]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [790/302]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [791/302]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [792/302]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [793/302]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [794/302]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [795/302]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [796/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [797/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [798/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [799/302]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [800/302]: training_loss: tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [801/302]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [802/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [803/302]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [804/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [805/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [806/302]: training_loss: tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [807/302]: training_loss: tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [808/302]: training_loss: tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [809/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [810/302]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [811/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [812/302]: training_loss: tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [813/302]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [814/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [815/302]: training_loss: tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [816/302]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [817/302]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [818/302]: training_loss: tensor(2.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [819/302]: training_loss: tensor(2.7287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [820/302]: training_loss: tensor(2.7200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [821/302]: training_loss: tensor(2.7623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [822/302]: training_loss: tensor(2.9345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [823/302]: training_loss: tensor(2.8031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [824/302]: training_loss: tensor(2.7562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [825/302]: training_loss: tensor(0.4797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [826/302]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [827/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [828/302]: training_loss: tensor(0.1224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [829/302]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [830/302]: training_loss: tensor(0.1193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [831/302]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [832/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [833/302]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [834/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [835/302]: training_loss: tensor(0.1285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [836/302]: training_loss: tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [837/302]: training_loss: tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [838/302]: training_loss: tensor(0.1205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [839/302]: training_loss: tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [840/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [841/302]: training_loss: tensor(0.1274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [842/302]: training_loss: tensor(0.1156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [843/302]: training_loss: tensor(0.1250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [844/302]: training_loss: tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [845/302]: training_loss: tensor(0.1418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [846/302]: training_loss: tensor(0.1292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [847/302]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [848/302]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [849/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [850/302]: training_loss: tensor(0.1225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [851/302]: training_loss: tensor(0.1197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [852/302]: training_loss: tensor(0.1091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [853/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [854/302]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [855/302]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [856/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [857/302]: training_loss: tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [858/302]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [859/302]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [860/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [861/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [862/302]: training_loss: tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [863/302]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [864/302]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [865/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [866/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [867/302]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [868/302]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [869/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [870/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [871/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [872/302]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [873/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [874/302]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [875/302]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [876/302]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [877/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [878/302]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [879/302]: training_loss: tensor(0.0629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [880/302]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [881/302]: training_loss: tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [882/302]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [883/302]: training_loss: tensor(0.0594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [884/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [885/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [886/302]: training_loss: tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [887/302]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [888/302]: training_loss: tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [889/302]: training_loss: tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [890/302]: training_loss: tensor(0.0645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [891/302]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [892/302]: training_loss: tensor(0.0676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [893/302]: training_loss: tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [894/302]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [895/302]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [896/302]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [897/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [898/302]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [899/302]: training_loss: tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [900/302]: training_loss: tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [901/302]: training_loss: tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [902/302]: training_loss: tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [903/302]: training_loss: tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [904/302]: training_loss: tensor(0.3579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [905/302]: training_loss: tensor(2.7521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [906/302]: training_loss: tensor(2.8595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [907/302]: training_loss: tensor(3.0381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [908/302]: training_loss: tensor(2.7981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [909/302]: training_loss: tensor(2.7013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [910/302]: training_loss: tensor(2.7775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [911/302]: training_loss: tensor(2.5760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [912/302]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [913/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [914/302]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [915/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [916/302]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [917/302]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [918/302]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [919/302]: training_loss: tensor(0.1156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [920/302]: training_loss: tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [921/302]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [922/302]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [923/302]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [924/302]: training_loss: tensor(0.1263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [925/302]: training_loss: tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [926/302]: training_loss: tensor(0.1237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [927/302]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [928/302]: training_loss: tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [929/302]: training_loss: tensor(0.1326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [930/302]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [931/302]: training_loss: tensor(0.1095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [932/302]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [933/302]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [934/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [935/302]: training_loss: tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [936/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [937/302]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [938/302]: training_loss: tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [939/302]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [940/302]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [941/302]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [942/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [943/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [944/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [945/302]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [946/302]: training_loss: tensor(0.1315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [947/302]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [948/302]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [949/302]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [950/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [951/302]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [952/302]: training_loss: tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [953/302]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [954/302]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [955/302]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [956/302]: training_loss: tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [957/302]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [958/302]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [959/302]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [960/302]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [961/302]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [962/302]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [963/302]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [964/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [965/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [966/302]: training_loss: tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [967/302]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [968/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [969/302]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [970/302]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [971/302]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [972/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [973/302]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [974/302]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [975/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [976/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [977/302]: training_loss: tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [978/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [979/302]: training_loss: tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [980/302]: training_loss: tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [981/302]: training_loss: tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [982/302]: training_loss: tensor(0.0579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [983/302]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [984/302]: training_loss: tensor(2.4893, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [985/302]: training_loss: tensor(2.8819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [986/302]: training_loss: tensor(2.8226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [987/302]: training_loss: tensor(2.8604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [988/302]: training_loss: tensor(2.7653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [989/302]: training_loss: tensor(2.8104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [990/302]: training_loss: tensor(2.6358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [991/302]: training_loss: tensor(2.8285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [992/302]: training_loss: tensor(0.9492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [993/302]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [994/302]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [995/302]: training_loss: tensor(0.1218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [996/302]: training_loss: tensor(0.1316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [997/302]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [998/302]: training_loss: tensor(0.1150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [999/302]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1000/302]: training_loss: tensor(0.1374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1001/302]: training_loss: tensor(0.1446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1002/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1003/302]: training_loss: tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1004/302]: training_loss: tensor(0.1137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1005/302]: training_loss: tensor(0.1422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/302]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/302]: training_loss: tensor(0.1110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/302]: training_loss: tensor(0.1427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/302]: training_loss: tensor(0.1277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/302]: training_loss: tensor(0.1358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/302]: training_loss: tensor(0.1263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/302]: training_loss: tensor(0.1072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/302]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/302]: training_loss: tensor(0.1497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/302]: training_loss: tensor(0.1457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/302]: training_loss: tensor(0.1333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/302]: training_loss: tensor(0.1392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/302]: training_loss: tensor(0.1236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1019/302]: training_loss: tensor(0.1157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/302]: training_loss: tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/302]: training_loss: tensor(0.1382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/302]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1023/302]: training_loss: tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/302]: training_loss: tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/302]: training_loss: tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/302]: training_loss: tensor(0.1139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/302]: training_loss: tensor(0.1218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/302]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/302]: training_loss: tensor(0.1069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/302]: training_loss: tensor(0.1173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/302]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/302]: training_loss: tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/302]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/302]: training_loss: tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/302]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1040/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/302]: training_loss: tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1043/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/302]: training_loss: tensor(0.0621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/302]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/302]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/302]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/302]: training_loss: tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/302]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/302]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/302]: training_loss: tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/302]: training_loss: tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/302]: training_loss: tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/302]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/302]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/302]: training_loss: tensor(2.4296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/302]: training_loss: tensor(2.5955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/302]: training_loss: tensor(2.9223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/302]: training_loss: tensor(2.9156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/302]: training_loss: tensor(2.8930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/302]: training_loss: tensor(2.5850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/302]: training_loss: tensor(1.9346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/302]: training_loss: tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/302]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/302]: training_loss: tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/302]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/302]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1074/302]: training_loss: tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/302]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/302]: training_loss: tensor(0.1352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/302]: training_loss: tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/302]: training_loss: tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/302]: training_loss: tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/302]: training_loss: tensor(0.1181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/302]: training_loss: tensor(0.1333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/302]: training_loss: tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/302]: training_loss: tensor(0.1182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/302]: training_loss: tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/302]: training_loss: tensor(0.1127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/302]: training_loss: tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/302]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/302]: training_loss: tensor(0.1491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/302]: training_loss: tensor(0.1123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/302]: training_loss: tensor(0.1244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/302]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/302]: training_loss: tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/302]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/302]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/302]: training_loss: tensor(0.1113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/302]: training_loss: tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1100/302]: training_loss: tensor(0.1068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/302]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/302]: training_loss: tensor(0.1154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/302]: training_loss: tensor(0.1177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/302]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/302]: training_loss: tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/302]: training_loss: tensor(0.1123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/302]: training_loss: tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/302]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/302]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/302]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/302]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/302]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/302]: training_loss: tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/302]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/302]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/302]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/302]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/302]: training_loss: tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/302]: training_loss: tensor(0.0686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/302]: training_loss: tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1129/302]: training_loss: tensor(0.0611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/302]: training_loss: tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/302]: training_loss: tensor(2.5945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/302]: training_loss: tensor(2.6025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/302]: training_loss: tensor(2.7950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/302]: training_loss: tensor(2.7597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/302]: training_loss: tensor(1.8122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/302]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/302]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/302]: training_loss: tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/302]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/302]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/302]: training_loss: tensor(0.1154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/302]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/302]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/302]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/302]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/302]: training_loss: tensor(0.1129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/302]: training_loss: tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/302]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/302]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/302]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1162/302]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/302]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/302]: training_loss: tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/302]: training_loss: tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/302]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/302]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/302]: training_loss: tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/302]: training_loss: tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/302]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/302]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/302]: training_loss: tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/302]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/302]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/302]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/302]: training_loss: tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/302]: training_loss: tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/302]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/302]: training_loss: tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/302]: training_loss: tensor(2.2049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/302]: training_loss: tensor(2.8044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/302]: training_loss: tensor(2.9187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/302]: training_loss: tensor(2.6623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/302]: training_loss: tensor(2.9196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/302]: training_loss: tensor(2.7525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/302]: training_loss: tensor(0.3973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/302]: training_loss: tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/302]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/302]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/302]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/302]: training_loss: tensor(0.1069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/302]: training_loss: tensor(0.1116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/302]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/302]: training_loss: tensor(0.1176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1213/302]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/302]: training_loss: tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1215/302]: training_loss: tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/302]: training_loss: tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/302]: training_loss: tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/302]: training_loss: tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/302]: training_loss: tensor(0.1347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/302]: training_loss: tensor(0.1236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/302]: training_loss: tensor(0.1286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/302]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/302]: training_loss: tensor(0.1198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/302]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/302]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/302]: training_loss: tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/302]: training_loss: tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/302]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/302]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/302]: training_loss: tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/302]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/302]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/302]: training_loss: tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/302]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/302]: training_loss: tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/302]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/302]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/302]: training_loss: tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/302]: training_loss: tensor(2.3296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/302]: training_loss: tensor(2.6421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/302]: training_loss: tensor(2.7071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/302]: training_loss: tensor(2.5920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/302]: training_loss: tensor(2.4337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/302]: training_loss: tensor(0.6976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/302]: training_loss: tensor(0.1118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/302]: training_loss: tensor(0.1118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/302]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/302]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/302]: training_loss: tensor(0.1131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/302]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/302]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/302]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/302]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/302]: training_loss: tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/302]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/302]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/302]: training_loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/302]: training_loss: tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/302]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/302]: training_loss: tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/302]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/302]: training_loss: tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/302]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/302]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/302]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/302]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/302]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/302]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/302]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/302]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/302]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1299/302]: training_loss: tensor(0.0635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/302]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/302]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/302]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/302]: training_loss: tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/302]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/302]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/302]: training_loss: tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/302]: training_loss: tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/302]: training_loss: tensor(0.0620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/302]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/302]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/302]: training_loss: tensor(2.1053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/302]: training_loss: tensor(2.8118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/302]: training_loss: tensor(2.8496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/302]: training_loss: tensor(2.6310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/302]: training_loss: tensor(2.6770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/302]: training_loss: tensor(1.0582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/302]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/302]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/302]: training_loss: tensor(0.1178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/302]: training_loss: tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/302]: training_loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/302]: training_loss: tensor(0.1106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/302]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/302]: training_loss: tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/302]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/302]: training_loss: tensor(0.1264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/302]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/302]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/302]: training_loss: tensor(0.1145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/302]: training_loss: tensor(0.1133, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1338/302]: training_loss: tensor(0.1019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/302]: training_loss: tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/302]: training_loss: tensor(0.1454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/302]: training_loss: tensor(0.1134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/302]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/302]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/302]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/302]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/302]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/302]: training_loss: tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/302]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/302]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1361/302]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/302]: training_loss: tensor(0.1083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1363/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/302]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/302]: training_loss: tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/302]: training_loss: tensor(2.6677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/302]: training_loss: tensor(2.7451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/302]: training_loss: tensor(2.5216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/302]: training_loss: tensor(1.6979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/302]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/302]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/302]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/302]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/302]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/302]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/302]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/302]: training_loss: tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/302]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1385/302]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1386/302]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1387/302]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1388/302]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1389/302]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1390/302]: training_loss: tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1391/302]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1392/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1393/302]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1394/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1395/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1396/302]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1397/302]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1398/302]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1399/302]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1400/302]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1401/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1402/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1403/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1404/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1405/302]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1406/302]: training_loss: tensor(0.0603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1407/302]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1408/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1409/302]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1410/302]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1411/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1412/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1413/302]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1414/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1415/302]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1416/302]: training_loss: tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1417/302]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1418/302]: training_loss: tensor(2.7524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1419/302]: training_loss: tensor(2.7849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1420/302]: training_loss: tensor(2.8943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1421/302]: training_loss: tensor(2.7384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1422/302]: training_loss: tensor(1.6804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1423/302]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1424/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1425/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1426/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1427/302]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1428/302]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1429/302]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1430/302]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1431/302]: training_loss: tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1432/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1433/302]: training_loss: tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1434/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1435/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1436/302]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1437/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1438/302]: training_loss: tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1439/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1440/302]: training_loss: tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1441/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1442/302]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1443/302]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1444/302]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1445/302]: training_loss: tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1446/302]: training_loss: tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1447/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1448/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1449/302]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1450/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1451/302]: training_loss: tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1452/302]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1453/302]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1454/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1455/302]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1456/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1457/302]: training_loss: tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1458/302]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1459/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1460/302]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1461/302]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1462/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1463/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1464/302]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1465/302]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1466/302]: training_loss: tensor(0.7482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1467/302]: training_loss: tensor(2.7213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1468/302]: training_loss: tensor(2.7144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1469/302]: training_loss: tensor(2.5872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1470/302]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1471/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1472/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1473/302]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1474/302]: training_loss: tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1475/302]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1476/302]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1477/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1478/302]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1479/302]: training_loss: tensor(0.1068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1480/302]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1481/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1482/302]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1483/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1484/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1485/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1486/302]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1487/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1488/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1489/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1490/302]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1491/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1492/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1493/302]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1494/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1495/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1496/302]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1497/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1498/302]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1499/302]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1500/302]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1501/302]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1502/302]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1503/302]: training_loss: tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1504/302]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1505/302]: training_loss: tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1506/302]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1507/302]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1508/302]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1509/302]: training_loss: tensor(0.0650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1510/302]: training_loss: tensor(1.9915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1511/302]: training_loss: tensor(2.7095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1512/302]: training_loss: tensor(1.7548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1513/302]: training_loss: tensor(0.0594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1514/302]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1515/302]: training_loss: tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1516/302]: training_loss: tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1517/302]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1518/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1519/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1520/302]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1521/302]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1522/302]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1523/302]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1524/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1525/302]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1526/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1527/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1528/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1529/302]: training_loss: tensor(0.0596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1530/302]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1531/302]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1532/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1533/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1534/302]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1535/302]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1536/302]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1537/302]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1538/302]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1539/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1540/302]: training_loss: tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1541/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1542/302]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1543/302]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1544/302]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1545/302]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1546/302]: training_loss: tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1547/302]: training_loss: tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1548/302]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1549/302]: training_loss: tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1550/302]: training_loss: tensor(1.8183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1551/302]: training_loss: tensor(2.9234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1552/302]: training_loss: tensor(2.9348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1553/302]: training_loss: tensor(0.0586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1554/302]: training_loss: tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1555/302]: training_loss: tensor(0.0611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1556/302]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1557/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1558/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1559/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1560/302]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1561/302]: training_loss: tensor(0.0592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1562/302]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1563/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1564/302]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1565/302]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1566/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1567/302]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1568/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1569/302]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1570/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1571/302]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1572/302]: training_loss: tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1573/302]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1574/302]: training_loss: tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1575/302]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1576/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1577/302]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1578/302]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1579/302]: training_loss: tensor(0.0635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1580/302]: training_loss: tensor(0.0592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1581/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1582/302]: training_loss: tensor(0.0579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1583/302]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1584/302]: training_loss: tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1585/302]: training_loss: tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1586/302]: training_loss: tensor(0.0594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1587/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1588/302]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1589/302]: training_loss: tensor(0.0645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1590/302]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1591/302]: training_loss: tensor(2.9563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1592/302]: training_loss: tensor(2.8778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1593/302]: training_loss: tensor(2.6679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1594/302]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1595/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1596/302]: training_loss: tensor(0.0570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1597/302]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1598/302]: training_loss: tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1599/302]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1600/302]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1601/302]: training_loss: tensor(0.0676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1602/302]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1603/302]: training_loss: tensor(0.0570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1604/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1605/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1606/302]: training_loss: tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1607/302]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1608/302]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1609/302]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1610/302]: training_loss: tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1611/302]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1612/302]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1613/302]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1614/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1615/302]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1616/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1617/302]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1618/302]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1619/302]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1620/302]: training_loss: tensor(0.0573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1621/302]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1622/302]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1623/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1624/302]: training_loss: tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1625/302]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1626/302]: training_loss: tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1627/302]: training_loss: tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1628/302]: training_loss: tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1629/302]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1630/302]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1631/302]: training_loss: tensor(2.4584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1632/302]: training_loss: tensor(2.6153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1633/302]: training_loss: tensor(2.5599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1634/302]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1635/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1636/302]: training_loss: tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1637/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1638/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1639/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1640/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1641/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1642/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1643/302]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1644/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1645/302]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1646/302]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1647/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1648/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1649/302]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1650/302]: training_loss: tensor(0.0629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1651/302]: training_loss: tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1652/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1653/302]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1654/302]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1655/302]: training_loss: tensor(0.0635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1656/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1657/302]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1658/302]: training_loss: tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1659/302]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1660/302]: training_loss: tensor(0.0615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1661/302]: training_loss: tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1662/302]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1663/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1664/302]: training_loss: tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1665/302]: training_loss: tensor(1.7703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1666/302]: training_loss: tensor(2.8493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1667/302]: training_loss: tensor(2.2764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1668/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1669/302]: training_loss: tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1670/302]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1671/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1672/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1673/302]: training_loss: tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1674/302]: training_loss: tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1675/302]: training_loss: tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1676/302]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1677/302]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1678/302]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1679/302]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1680/302]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1681/302]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1682/302]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1683/302]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1684/302]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1685/302]: training_loss: tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1686/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1687/302]: training_loss: tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1688/302]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1689/302]: training_loss: tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1690/302]: training_loss: tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1691/302]: training_loss: tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1692/302]: training_loss: tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1693/302]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1694/302]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1695/302]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1696/302]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1697/302]: training_loss: tensor(0.7175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1698/302]: training_loss: tensor(2.6921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1699/302]: training_loss: tensor(2.8063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1700/302]: training_loss: tensor(0.8252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1701/302]: training_loss: tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1702/302]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1703/302]: training_loss: tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1704/302]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1705/302]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1706/302]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1707/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1708/302]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1709/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1710/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1711/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1712/302]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1713/302]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1714/302]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1715/302]: training_loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1716/302]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1717/302]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1718/302]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1719/302]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1720/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1721/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1722/302]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1723/302]: training_loss: tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1724/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1725/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1726/302]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1727/302]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1728/302]: training_loss: tensor(0.0594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1729/302]: training_loss: tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1730/302]: training_loss: tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1731/302]: training_loss: tensor(1.3687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1732/302]: training_loss: tensor(2.5701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1733/302]: training_loss: tensor(2.7486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1734/302]: training_loss: tensor(0.7884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1735/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1736/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1737/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1738/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1739/302]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1740/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1741/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1742/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1743/302]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1744/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1745/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1746/302]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1747/302]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1748/302]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1749/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1750/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1751/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1752/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1753/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1754/302]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1755/302]: training_loss: tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1756/302]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1757/302]: training_loss: tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1758/302]: training_loss: tensor(0.6766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1759/302]: training_loss: tensor(2.7289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1760/302]: training_loss: tensor(2.6606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1761/302]: training_loss: tensor(1.4570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1762/302]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1763/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1764/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1765/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1766/302]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1767/302]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1768/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1769/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1770/302]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1771/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1772/302]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1773/302]: training_loss: tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1774/302]: training_loss: tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1775/302]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1776/302]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1777/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1778/302]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1779/302]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1780/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1781/302]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1782/302]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1783/302]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1784/302]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1785/302]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1786/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1787/302]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1788/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1789/302]: training_loss: tensor(1.6778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1790/302]: training_loss: tensor(1.3098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1791/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1792/302]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1793/302]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1794/302]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1795/302]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1796/302]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1797/302]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1798/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1799/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1800/302]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1801/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1802/302]: training_loss: tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1803/302]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1804/302]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1805/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1806/302]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1807/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1808/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1809/302]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1810/302]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1811/302]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1812/302]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1813/302]: training_loss: tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1814/302]: training_loss: tensor(0.3737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1815/302]: training_loss: tensor(2.7716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1816/302]: training_loss: tensor(2.5958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1817/302]: training_loss: tensor(0.4013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1818/302]: training_loss: tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1819/302]: training_loss: tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1820/302]: training_loss: tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1821/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1822/302]: training_loss: tensor(0.0640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1823/302]: training_loss: tensor(0.1071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1824/302]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1825/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1826/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1827/302]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1828/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1829/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1830/302]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1831/302]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1832/302]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1833/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1834/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1835/302]: training_loss: tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1836/302]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1837/302]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1838/302]: training_loss: tensor(0.0576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1839/302]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1840/302]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1841/302]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1842/302]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1843/302]: training_loss: tensor(2.8059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1844/302]: training_loss: tensor(1.9221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1845/302]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1846/302]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1847/302]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1848/302]: training_loss: tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1849/302]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1850/302]: training_loss: tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1851/302]: training_loss: tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1852/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1853/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1854/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1855/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1856/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1857/302]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1858/302]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1859/302]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1860/302]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1861/302]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1862/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1863/302]: training_loss: tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1864/302]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1865/302]: training_loss: tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1866/302]: training_loss: tensor(1.9905, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1867/302]: training_loss: tensor(2.7024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1868/302]: training_loss: tensor(1.3844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1869/302]: training_loss: tensor(0.0650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1870/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1871/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1872/302]: training_loss: tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1873/302]: training_loss: tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1874/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1875/302]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1876/302]: training_loss: tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1877/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1878/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1879/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1880/302]: training_loss: tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1881/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1882/302]: training_loss: tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1883/302]: training_loss: tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1884/302]: training_loss: tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1885/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1886/302]: training_loss: tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1887/302]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1888/302]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1889/302]: training_loss: tensor(0.6445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1890/302]: training_loss: tensor(2.6602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1891/302]: training_loss: tensor(2.7768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1892/302]: training_loss: tensor(1.3856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1893/302]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1894/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1895/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1896/302]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1897/302]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1898/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1899/302]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1900/302]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1901/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1902/302]: training_loss: tensor(0.1269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1903/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1904/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1905/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1906/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1907/302]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1908/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1909/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1910/302]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1911/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1912/302]: training_loss: tensor(0.4251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1913/302]: training_loss: tensor(2.5118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1914/302]: training_loss: tensor(1.9179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1915/302]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1916/302]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1917/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1918/302]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1919/302]: training_loss: tensor(0.1272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1920/302]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1921/302]: training_loss: tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1922/302]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1923/302]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1924/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1925/302]: training_loss: tensor(0.1093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1926/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1927/302]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1928/302]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1929/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1930/302]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1931/302]: training_loss: tensor(0.0640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1932/302]: training_loss: tensor(0.7586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1933/302]: training_loss: tensor(2.4022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1934/302]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1935/302]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1936/302]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1937/302]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1938/302]: training_loss: tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1939/302]: training_loss: tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1940/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1941/302]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1942/302]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1943/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1944/302]: training_loss: tensor(0.1083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1945/302]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1946/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1947/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1948/302]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1949/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1950/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1951/302]: training_loss: tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1952/302]: training_loss: tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1953/302]: training_loss: tensor(1.7420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1954/302]: training_loss: tensor(1.0499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1955/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1956/302]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1957/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1958/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1959/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1960/302]: training_loss: tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1961/302]: training_loss: tensor(0.0683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1962/302]: training_loss: tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1963/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1964/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1965/302]: training_loss: tensor(0.0620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1966/302]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1967/302]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1968/302]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1969/302]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1970/302]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1971/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1972/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1973/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1974/302]: training_loss: tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1975/302]: training_loss: tensor(0.3891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1976/302]: training_loss: tensor(2.7813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1977/302]: training_loss: tensor(1.3882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1978/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1979/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1980/302]: training_loss: tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1981/302]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1982/302]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1983/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1984/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1985/302]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1986/302]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1987/302]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1988/302]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1989/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1990/302]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1991/302]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1992/302]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1993/302]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1994/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1995/302]: training_loss: tensor(0.3526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1996/302]: training_loss: tensor(2.6358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1997/302]: training_loss: tensor(2.4974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1998/302]: training_loss: tensor(0.4113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1999/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2000/302]: training_loss: tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2001/302]: training_loss: tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2002/302]: training_loss: tensor(0.1272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2003/302]: training_loss: tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2004/302]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2005/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2006/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2007/302]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2008/302]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2009/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2010/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2011/302]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2012/302]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2013/302]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2014/302]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2015/302]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2016/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2017/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2018/302]: training_loss: tensor(2.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2019/302]: training_loss: tensor(1.3152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2020/302]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2021/302]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2022/302]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2023/302]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2024/302]: training_loss: tensor(0.1106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2025/302]: training_loss: tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2026/302]: training_loss: tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2027/302]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2028/302]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2029/302]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2030/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2031/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2032/302]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2033/302]: training_loss: tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2034/302]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2035/302]: training_loss: tensor(2.3465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2036/302]: training_loss: tensor(1.1113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2037/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2038/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2039/302]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2040/302]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2041/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2042/302]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2043/302]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2044/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2045/302]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2046/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2047/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2048/302]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2049/302]: training_loss: tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2050/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2051/302]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2052/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2053/302]: training_loss: tensor(1.4867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2054/302]: training_loss: tensor(1.3794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2055/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2056/302]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2057/302]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2058/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2059/302]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2060/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2061/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2062/302]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2063/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2064/302]: training_loss: tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2065/302]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2066/302]: training_loss: tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2067/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2068/302]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2069/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2070/302]: training_loss: tensor(2.2783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2071/302]: training_loss: tensor(2.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2072/302]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2073/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2074/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2075/302]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2076/302]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2077/302]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2078/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2079/302]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2080/302]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2081/302]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2082/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2083/302]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2084/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2085/302]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2086/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2087/302]: training_loss: tensor(1.3006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2088/302]: training_loss: tensor(1.8030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2089/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2090/302]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2091/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2092/302]: training_loss: tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2093/302]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2094/302]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2095/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2096/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2097/302]: training_loss: tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2098/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2099/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2100/302]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2101/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2102/302]: training_loss: tensor(1.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2103/302]: training_loss: tensor(1.6536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2104/302]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2105/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2106/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2107/302]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2108/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2109/302]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2110/302]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2111/302]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2112/302]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2113/302]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2114/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2115/302]: training_loss: tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2116/302]: training_loss: tensor(0.8045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2117/302]: training_loss: tensor(1.6617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2118/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2119/302]: training_loss: tensor(0.0576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2120/302]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2121/302]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2122/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2123/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2124/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2125/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2126/302]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2127/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2128/302]: training_loss: tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2129/302]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2130/302]: training_loss: tensor(0.7028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2131/302]: training_loss: tensor(1.8045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2132/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2133/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2134/302]: training_loss: tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2135/302]: training_loss: tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2136/302]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2137/302]: training_loss: tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2138/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2139/302]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2140/302]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2141/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2142/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2143/302]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2144/302]: training_loss: tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2145/302]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2146/302]: training_loss: tensor(1.0021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2147/302]: training_loss: tensor(1.3846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2148/302]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2149/302]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2150/302]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2151/302]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2152/302]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2153/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2154/302]: training_loss: tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2155/302]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2156/302]: training_loss: tensor(0.0645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2157/302]: training_loss: tensor(0.0648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2158/302]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2159/302]: training_loss: tensor(0.4036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2160/302]: training_loss: tensor(2.8600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2161/302]: training_loss: tensor(0.3982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2162/302]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2163/302]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2164/302]: training_loss: tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2165/302]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2166/302]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2167/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2168/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2169/302]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2170/302]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2171/302]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2172/302]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2173/302]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2174/302]: training_loss: tensor(1.7674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2175/302]: training_loss: tensor(2.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2176/302]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2177/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2178/302]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2179/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2180/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2181/302]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2182/302]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2183/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2184/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2185/302]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2186/302]: training_loss: tensor(1.1471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2187/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2188/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2189/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2190/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2191/302]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2192/302]: training_loss: tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2193/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2194/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2195/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2196/302]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2197/302]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2198/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2199/302]: training_loss: tensor(0.7630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2200/302]: training_loss: tensor(2.4159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2201/302]: training_loss: tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2202/302]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2203/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2204/302]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2205/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2206/302]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2207/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2208/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2209/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2210/302]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2211/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2212/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2213/302]: training_loss: tensor(2.7476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2214/302]: training_loss: tensor(1.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2215/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2216/302]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2217/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2218/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2219/302]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2220/302]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2221/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2222/302]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2223/302]: training_loss: tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2224/302]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2225/302]: training_loss: tensor(0.4549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2226/302]: training_loss: tensor(2.6168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2227/302]: training_loss: tensor(0.7295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2228/302]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2229/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2230/302]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2231/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2232/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2233/302]: training_loss: tensor(0.1059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2234/302]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2235/302]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2236/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2237/302]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2238/302]: training_loss: tensor(1.7600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2239/302]: training_loss: tensor(1.0383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2240/302]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2241/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2242/302]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2243/302]: training_loss: tensor(0.1106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2244/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2245/302]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2246/302]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2247/302]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2248/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2249/302]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2250/302]: training_loss: tensor(2.5680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2251/302]: training_loss: tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2252/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2253/302]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2254/302]: training_loss: tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2255/302]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2256/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2257/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2258/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2259/302]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2260/302]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2261/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2262/302]: training_loss: tensor(1.5853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2263/302]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2264/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2265/302]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2266/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2267/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2268/302]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2269/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2270/302]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2271/302]: training_loss: tensor(0.4566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2272/302]: training_loss: tensor(2.5928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2273/302]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2274/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2275/302]: training_loss: tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2276/302]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2277/302]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2278/302]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2279/302]: training_loss: tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2280/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2281/302]: training_loss: tensor(2.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2282/302]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2283/302]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2284/302]: training_loss: tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2285/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2286/302]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2287/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2288/302]: training_loss: tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2289/302]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2290/302]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2291/302]: training_loss: tensor(0.9623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2292/302]: training_loss: tensor(1.8776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2293/302]: training_loss: tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2294/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2295/302]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2296/302]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2297/302]: training_loss: tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2298/302]: training_loss: tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2299/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2300/302]: training_loss: tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2301/302]: training_loss: tensor(1.2507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2302/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2303/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2304/302]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2305/302]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2306/302]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2307/302]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2308/302]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2309/302]: training_loss: tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2310/302]: training_loss: tensor(0.3773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2311/302]: training_loss: tensor(2.2049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2312/302]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2313/302]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2314/302]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2315/302]: training_loss: tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2316/302]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2317/302]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2318/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2319/302]: training_loss: tensor(0.9639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2320/302]: training_loss: tensor(0.1137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2321/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2322/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2323/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2324/302]: training_loss: tensor(0.1097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2325/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2326/302]: training_loss: tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2327/302]: training_loss: tensor(1.3378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2328/302]: training_loss: tensor(0.1093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2329/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2330/302]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2331/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2332/302]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2333/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2334/302]: training_loss: tensor(0.6405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2335/302]: training_loss: tensor(0.3245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2336/302]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2337/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2338/302]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2339/302]: training_loss: tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2340/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2341/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2342/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2343/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2344/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2345/302]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2346/302]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2347/302]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2348/302]: training_loss: tensor(2.1776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2349/302]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2350/302]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2351/302]: training_loss: tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2352/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2353/302]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2354/302]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2355/302]: training_loss: tensor(1.3089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2356/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2357/302]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2358/302]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2359/302]: training_loss: tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2360/302]: training_loss: tensor(1.3622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2361/302]: training_loss: tensor(0.7525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2362/302]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2363/302]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2364/302]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2365/302]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2366/302]: training_loss: tensor(0.0645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2367/302]: training_loss: tensor(1.6911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2368/302]: training_loss: tensor(0.0603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2369/302]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2370/302]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2371/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2372/302]: training_loss: tensor(1.7123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2373/302]: training_loss: tensor(0.4048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2374/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2375/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2376/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2377/302]: training_loss: tensor(1.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2378/302]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2379/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2380/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2381/302]: training_loss: tensor(0.7617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2382/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2383/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2384/302]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2385/302]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2386/302]: training_loss: tensor(1.2966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2387/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2388/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2389/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2390/302]: training_loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2391/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2392/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2393/302]: training_loss: tensor(0.5490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2394/302]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2395/302]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2396/302]: training_loss: tensor(0.3443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2397/302]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2398/302]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2399/302]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2400/302]: training_loss: tensor(1.3075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2401/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2402/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2403/302]: training_loss: tensor(0.4377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2404/302]: training_loss: tensor(0.3194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2405/302]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2406/302]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2407/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2408/302]: training_loss: tensor(0.3686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2409/302]: training_loss: tensor(0.6773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2410/302]: training_loss: tensor(0.3635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2411/302]: training_loss: tensor(0.3385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2412/302]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2413/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2414/302]: training_loss: tensor(0.4068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2415/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2416/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2417/302]: training_loss: tensor(0.7277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2418/302]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2419/302]: training_loss: tensor(0.6565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2420/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [1/2], global step [2421/4842], PT Loss: 0.3216, Val Loss: 0.2905\n",
      "batch_no [0/302]: training_loss: tensor(2.5818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1/302]: training_loss: tensor(2.5771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2/302]: training_loss: tensor(0.6625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4/302]: training_loss: tensor(0.1226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [5/302]: training_loss: tensor(0.1083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [6/302]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [7/302]: training_loss: tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [8/302]: training_loss: tensor(0.1405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [9/302]: training_loss: tensor(0.1183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [10/302]: training_loss: tensor(0.1180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [11/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [12/302]: training_loss: tensor(0.1171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [13/302]: training_loss: tensor(0.1169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [14/302]: training_loss: tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [15/302]: training_loss: tensor(0.1105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [16/302]: training_loss: tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [17/302]: training_loss: tensor(0.1150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [18/302]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [19/302]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [20/302]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [21/302]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [22/302]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [23/302]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [24/302]: training_loss: tensor(0.1131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [25/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [26/302]: training_loss: tensor(0.7806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [27/302]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [28/302]: training_loss: tensor(2.1651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [29/302]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [30/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [31/302]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [32/302]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [33/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [34/302]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [35/302]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [36/302]: training_loss: tensor(0.1019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [37/302]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [38/302]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [39/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [40/302]: training_loss: tensor(0.7846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [41/302]: training_loss: tensor(2.5862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [42/302]: training_loss: tensor(2.2599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [43/302]: training_loss: tensor(0.0686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [44/302]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [45/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [46/302]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [47/302]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [48/302]: training_loss: tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [49/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [50/302]: training_loss: tensor(0.1328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [51/302]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [52/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [53/302]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [54/302]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [55/302]: training_loss: tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [56/302]: training_loss: tensor(0.1185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [57/302]: training_loss: tensor(0.1173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [58/302]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [59/302]: training_loss: tensor(0.1121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [60/302]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [61/302]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [62/302]: training_loss: tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [63/302]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [64/302]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [65/302]: training_loss: tensor(0.6639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [66/302]: training_loss: tensor(2.5363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [67/302]: training_loss: tensor(1.6714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [68/302]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [69/302]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [70/302]: training_loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [71/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [72/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [73/302]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [74/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [75/302]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [76/302]: training_loss: tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [77/302]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [78/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [79/302]: training_loss: tensor(0.1130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [80/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [81/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [82/302]: training_loss: tensor(0.1338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [83/302]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [84/302]: training_loss: tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [85/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [86/302]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [87/302]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [88/302]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [89/302]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [90/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [91/302]: training_loss: tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [92/302]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [93/302]: training_loss: tensor(0.3624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [94/302]: training_loss: tensor(2.7298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [95/302]: training_loss: tensor(2.5586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [96/302]: training_loss: tensor(2.5449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [97/302]: training_loss: tensor(1.8641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [98/302]: training_loss: tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [99/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [100/302]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [101/302]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [102/302]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [103/302]: training_loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [104/302]: training_loss: tensor(0.1108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [105/302]: training_loss: tensor(0.1165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [106/302]: training_loss: tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [107/302]: training_loss: tensor(0.1083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [108/302]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [109/302]: training_loss: tensor(0.1127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [110/302]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [111/302]: training_loss: tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [112/302]: training_loss: tensor(0.1288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [113/302]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [114/302]: training_loss: tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [115/302]: training_loss: tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [116/302]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [117/302]: training_loss: tensor(0.1244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [118/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [119/302]: training_loss: tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [120/302]: training_loss: tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [121/302]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [122/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [123/302]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [124/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [125/302]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [126/302]: training_loss: tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [127/302]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [128/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [129/302]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [130/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [131/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [132/302]: training_loss: tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [133/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [134/302]: training_loss: tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [135/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [136/302]: training_loss: tensor(1.9531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [137/302]: training_loss: tensor(2.5945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [138/302]: training_loss: tensor(2.5840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [139/302]: training_loss: tensor(2.5314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [140/302]: training_loss: tensor(1.3575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [141/302]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [142/302]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [143/302]: training_loss: tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [144/302]: training_loss: tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [145/302]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [146/302]: training_loss: tensor(0.1188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [147/302]: training_loss: tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [148/302]: training_loss: tensor(0.1155, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [149/302]: training_loss: tensor(0.1176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [150/302]: training_loss: tensor(0.1254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [151/302]: training_loss: tensor(0.1186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [152/302]: training_loss: tensor(0.1252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [153/302]: training_loss: tensor(0.1117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [154/302]: training_loss: tensor(0.1110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [155/302]: training_loss: tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [156/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [157/302]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [158/302]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [159/302]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [160/302]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [161/302]: training_loss: tensor(0.1154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [162/302]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [163/302]: training_loss: tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [164/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [165/302]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [166/302]: training_loss: tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [167/302]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [168/302]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [169/302]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [170/302]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [171/302]: training_loss: tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [172/302]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [173/302]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [174/302]: training_loss: tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [175/302]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [176/302]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [177/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [178/302]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [179/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [180/302]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [181/302]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [182/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [183/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [184/302]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [185/302]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [186/302]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [187/302]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [188/302]: training_loss: tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [189/302]: training_loss: tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [190/302]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [191/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [192/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [193/302]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [194/302]: training_loss: tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [195/302]: training_loss: tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [196/302]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [197/302]: training_loss: tensor(2.7392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [198/302]: training_loss: tensor(2.8748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [199/302]: training_loss: tensor(2.9403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [200/302]: training_loss: tensor(2.7159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [201/302]: training_loss: tensor(2.8921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [202/302]: training_loss: tensor(1.3321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [203/302]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [204/302]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [205/302]: training_loss: tensor(0.0683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [206/302]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [207/302]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [208/302]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [209/302]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [210/302]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [211/302]: training_loss: tensor(0.1059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [212/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [213/302]: training_loss: tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [214/302]: training_loss: tensor(0.1083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [215/302]: training_loss: tensor(0.1271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [216/302]: training_loss: tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [217/302]: training_loss: tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [218/302]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [219/302]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [220/302]: training_loss: tensor(0.1182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [221/302]: training_loss: tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [222/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [223/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [224/302]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [225/302]: training_loss: tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [226/302]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [227/302]: training_loss: tensor(0.1044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [228/302]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [229/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [230/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [231/302]: training_loss: tensor(0.1255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [232/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [233/302]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [234/302]: training_loss: tensor(0.1181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [235/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [236/302]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [237/302]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [238/302]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [239/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [240/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [241/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [242/302]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [243/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [244/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [245/302]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [246/302]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [247/302]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [248/302]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [249/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [250/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [251/302]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [252/302]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [253/302]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [254/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [255/302]: training_loss: tensor(0.0645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [256/302]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [257/302]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [258/302]: training_loss: tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [259/302]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [260/302]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [261/302]: training_loss: tensor(0.0592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [262/302]: training_loss: tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [263/302]: training_loss: tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [264/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [265/302]: training_loss: tensor(0.0566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [266/302]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [267/302]: training_loss: tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [268/302]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [269/302]: training_loss: tensor(0.0507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [270/302]: training_loss: tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [271/302]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [272/302]: training_loss: tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [273/302]: training_loss: tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [274/302]: training_loss: tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [275/302]: training_loss: tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [276/302]: training_loss: tensor(1.3670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [277/302]: training_loss: tensor(3.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [278/302]: training_loss: tensor(2.7235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [279/302]: training_loss: tensor(2.7705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [280/302]: training_loss: tensor(2.9376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [281/302]: training_loss: tensor(2.8234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [282/302]: training_loss: tensor(2.6727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [283/302]: training_loss: tensor(2.1349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [284/302]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [285/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [286/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [287/302]: training_loss: tensor(0.1199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [288/302]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [289/302]: training_loss: tensor(0.1044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [290/302]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [291/302]: training_loss: tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [292/302]: training_loss: tensor(0.1244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [293/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [294/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [295/302]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [296/302]: training_loss: tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [297/302]: training_loss: tensor(0.1132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [298/302]: training_loss: tensor(0.1194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [299/302]: training_loss: tensor(0.1200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [300/302]: training_loss: tensor(0.1066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [301/302]: training_loss: tensor(0.1206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [302/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [303/302]: training_loss: tensor(0.1240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [304/302]: training_loss: tensor(0.1130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [305/302]: training_loss: tensor(0.1236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [306/302]: training_loss: tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [307/302]: training_loss: tensor(0.1211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [308/302]: training_loss: tensor(0.1059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [309/302]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [310/302]: training_loss: tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [311/302]: training_loss: tensor(0.1064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [312/302]: training_loss: tensor(0.1172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [313/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [314/302]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [315/302]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [316/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [317/302]: training_loss: tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [318/302]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [319/302]: training_loss: tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [320/302]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [321/302]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [322/302]: training_loss: tensor(0.1192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [323/302]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [324/302]: training_loss: tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [325/302]: training_loss: tensor(0.1106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [326/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [327/302]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [328/302]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [329/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [330/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [331/302]: training_loss: tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [332/302]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [333/302]: training_loss: tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [334/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [335/302]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [336/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [337/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [338/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [339/302]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [340/302]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [341/302]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [342/302]: training_loss: tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [343/302]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [344/302]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [345/302]: training_loss: tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [346/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [347/302]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [348/302]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [349/302]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [350/302]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [351/302]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [352/302]: training_loss: tensor(0.0492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [353/302]: training_loss: tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [354/302]: training_loss: tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [355/302]: training_loss: tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [356/302]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [357/302]: training_loss: tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [358/302]: training_loss: tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [359/302]: training_loss: tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [360/302]: training_loss: tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [361/302]: training_loss: tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [362/302]: training_loss: tensor(0.0594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [363/302]: training_loss: tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [364/302]: training_loss: tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [365/302]: training_loss: tensor(0.0586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [366/302]: training_loss: tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [367/302]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [368/302]: training_loss: tensor(2.5797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [369/302]: training_loss: tensor(2.8776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [370/302]: training_loss: tensor(2.8417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [371/302]: training_loss: tensor(2.8347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [372/302]: training_loss: tensor(2.8742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [373/302]: training_loss: tensor(2.7710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [374/302]: training_loss: tensor(2.6742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [375/302]: training_loss: tensor(2.6035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [376/302]: training_loss: tensor(2.4562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [377/302]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [378/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [379/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [380/302]: training_loss: tensor(0.1194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [381/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [382/302]: training_loss: tensor(0.1389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [383/302]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [384/302]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [385/302]: training_loss: tensor(0.1212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [386/302]: training_loss: tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [387/302]: training_loss: tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [388/302]: training_loss: tensor(0.1223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [389/302]: training_loss: tensor(0.1315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [390/302]: training_loss: tensor(0.1197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [391/302]: training_loss: tensor(0.1334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [392/302]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [393/302]: training_loss: tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [394/302]: training_loss: tensor(0.1290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [395/302]: training_loss: tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [396/302]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [397/302]: training_loss: tensor(0.1149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [398/302]: training_loss: tensor(0.1077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [399/302]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [400/302]: training_loss: tensor(0.1144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [401/302]: training_loss: tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [402/302]: training_loss: tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [403/302]: training_loss: tensor(0.1205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [404/302]: training_loss: tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [405/302]: training_loss: tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [406/302]: training_loss: tensor(0.1105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [407/302]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [408/302]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [409/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [410/302]: training_loss: tensor(0.1171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [411/302]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [412/302]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [413/302]: training_loss: tensor(0.1186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [414/302]: training_loss: tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [415/302]: training_loss: tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [416/302]: training_loss: tensor(0.1210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [417/302]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [418/302]: training_loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [419/302]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [420/302]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [421/302]: training_loss: tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [422/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [423/302]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [424/302]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [425/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [426/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [427/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [428/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [429/302]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [430/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [431/302]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [432/302]: training_loss: tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [433/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [434/302]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [435/302]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [436/302]: training_loss: tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [437/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [438/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [439/302]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [440/302]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [441/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [442/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [443/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [444/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [445/302]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [446/302]: training_loss: tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [447/302]: training_loss: tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [448/302]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [449/302]: training_loss: tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [450/302]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [451/302]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [452/302]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [453/302]: training_loss: tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [454/302]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [455/302]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [456/302]: training_loss: tensor(0.0586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [457/302]: training_loss: tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [458/302]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [459/302]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [460/302]: training_loss: tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [461/302]: training_loss: tensor(0.4282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [462/302]: training_loss: tensor(2.6835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [463/302]: training_loss: tensor(2.6445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [464/302]: training_loss: tensor(2.7142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [465/302]: training_loss: tensor(3.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [466/302]: training_loss: tensor(2.6788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [467/302]: training_loss: tensor(2.5851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [468/302]: training_loss: tensor(2.4317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [469/302]: training_loss: tensor(2.5031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [470/302]: training_loss: tensor(2.5192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [471/302]: training_loss: tensor(0.9917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [472/302]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [473/302]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [474/302]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [475/302]: training_loss: tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [476/302]: training_loss: tensor(0.1123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [477/302]: training_loss: tensor(0.1211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [478/302]: training_loss: tensor(0.1354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [479/302]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [480/302]: training_loss: tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [481/302]: training_loss: tensor(0.1229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [482/302]: training_loss: tensor(0.1302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [483/302]: training_loss: tensor(0.1401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [484/302]: training_loss: tensor(0.1191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [485/302]: training_loss: tensor(0.1209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [486/302]: training_loss: tensor(0.1019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [487/302]: training_loss: tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [488/302]: training_loss: tensor(0.1108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [489/302]: training_loss: tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [490/302]: training_loss: tensor(0.1451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [491/302]: training_loss: tensor(0.1360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [492/302]: training_loss: tensor(0.1394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [493/302]: training_loss: tensor(0.1095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [494/302]: training_loss: tensor(0.1168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [495/302]: training_loss: tensor(0.1208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [496/302]: training_loss: tensor(0.1230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [497/302]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [498/302]: training_loss: tensor(0.1213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [499/302]: training_loss: tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [500/302]: training_loss: tensor(0.1373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [501/302]: training_loss: tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [502/302]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [503/302]: training_loss: tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [504/302]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [505/302]: training_loss: tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [506/302]: training_loss: tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [507/302]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [508/302]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [509/302]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [510/302]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [511/302]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [512/302]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [513/302]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [514/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [515/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [516/302]: training_loss: tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [517/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [518/302]: training_loss: tensor(0.1108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [519/302]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [520/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [521/302]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [522/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [523/302]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [524/302]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [525/302]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [526/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [527/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [528/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [529/302]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [530/302]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [531/302]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [532/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [533/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [534/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [535/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [536/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [537/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [538/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [539/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [540/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [541/302]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [542/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [543/302]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [544/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [545/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [546/302]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [547/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [548/302]: training_loss: tensor(0.0621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [549/302]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [550/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [551/302]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [552/302]: training_loss: tensor(1.8627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [553/302]: training_loss: tensor(2.7373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [554/302]: training_loss: tensor(2.5915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [555/302]: training_loss: tensor(2.7597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [556/302]: training_loss: tensor(2.5157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [557/302]: training_loss: tensor(2.6212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [558/302]: training_loss: tensor(2.3899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [559/302]: training_loss: tensor(2.2564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [560/302]: training_loss: tensor(2.3997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [561/302]: training_loss: tensor(0.7221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [562/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [563/302]: training_loss: tensor(0.1150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [564/302]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [565/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [566/302]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [567/302]: training_loss: tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [568/302]: training_loss: tensor(0.1237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [569/302]: training_loss: tensor(0.1265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [570/302]: training_loss: tensor(0.1146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [571/302]: training_loss: tensor(0.1514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [572/302]: training_loss: tensor(0.1286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [573/302]: training_loss: tensor(0.1159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [574/302]: training_loss: tensor(0.1182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [575/302]: training_loss: tensor(0.1224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [576/302]: training_loss: tensor(0.1105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [577/302]: training_loss: tensor(0.1288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [578/302]: training_loss: tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [579/302]: training_loss: tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [580/302]: training_loss: tensor(0.1091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [581/302]: training_loss: tensor(0.1093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [582/302]: training_loss: tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [583/302]: training_loss: tensor(0.1410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [584/302]: training_loss: tensor(0.1264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [585/302]: training_loss: tensor(0.1172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [586/302]: training_loss: tensor(0.1123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [587/302]: training_loss: tensor(0.1478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [588/302]: training_loss: tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [589/302]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [590/302]: training_loss: tensor(0.1071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [591/302]: training_loss: tensor(0.1304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [592/302]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [593/302]: training_loss: tensor(0.1180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [594/302]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [595/302]: training_loss: tensor(0.1077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [596/302]: training_loss: tensor(0.1222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [597/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [598/302]: training_loss: tensor(0.1139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [599/302]: training_loss: tensor(0.1219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [600/302]: training_loss: tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [601/302]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [602/302]: training_loss: tensor(0.1186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [603/302]: training_loss: tensor(0.1106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [604/302]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [605/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [606/302]: training_loss: tensor(0.1162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [607/302]: training_loss: tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [608/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [609/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [610/302]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [611/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [612/302]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [613/302]: training_loss: tensor(0.1143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [614/302]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [615/302]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [616/302]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [617/302]: training_loss: tensor(0.1071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [618/302]: training_loss: tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [619/302]: training_loss: tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [620/302]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [621/302]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [622/302]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [623/302]: training_loss: tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [624/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [625/302]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [626/302]: training_loss: tensor(0.1078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [627/302]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [628/302]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [629/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [630/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [631/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [632/302]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [633/302]: training_loss: tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [634/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [635/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [636/302]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [637/302]: training_loss: tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [638/302]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [639/302]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [640/302]: training_loss: tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [641/302]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [642/302]: training_loss: tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [643/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [644/302]: training_loss: tensor(1.0469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [645/302]: training_loss: tensor(2.7741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [646/302]: training_loss: tensor(2.6336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [647/302]: training_loss: tensor(2.7311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [648/302]: training_loss: tensor(2.6015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [649/302]: training_loss: tensor(2.6685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [650/302]: training_loss: tensor(2.5927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [651/302]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [652/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [653/302]: training_loss: tensor(0.1129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [654/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [655/302]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [656/302]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [657/302]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [658/302]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [659/302]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [660/302]: training_loss: tensor(0.1043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [661/302]: training_loss: tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [662/302]: training_loss: tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [663/302]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [664/302]: training_loss: tensor(0.1202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [665/302]: training_loss: tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [666/302]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [667/302]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [668/302]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [669/302]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [670/302]: training_loss: tensor(0.1142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [671/302]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [672/302]: training_loss: tensor(0.1114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [673/302]: training_loss: tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [674/302]: training_loss: tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [675/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [676/302]: training_loss: tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [677/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [678/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [679/302]: training_loss: tensor(0.1177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [680/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [681/302]: training_loss: tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [682/302]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [683/302]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [684/302]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [685/302]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [686/302]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [687/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [688/302]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [689/302]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [690/302]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [691/302]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [692/302]: training_loss: tensor(0.1085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [693/302]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [694/302]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [695/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [696/302]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [697/302]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [698/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [699/302]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [700/302]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [701/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [702/302]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [703/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [704/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [705/302]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [706/302]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [707/302]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [708/302]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [709/302]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [710/302]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [711/302]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [712/302]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [713/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [714/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [715/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [716/302]: training_loss: tensor(0.0666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [717/302]: training_loss: tensor(0.0640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [718/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [719/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [720/302]: training_loss: tensor(0.0611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [721/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [722/302]: training_loss: tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [723/302]: training_loss: tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [724/302]: training_loss: tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [725/302]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [726/302]: training_loss: tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [727/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [728/302]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [729/302]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [730/302]: training_loss: tensor(2.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [731/302]: training_loss: tensor(2.8591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [732/302]: training_loss: tensor(2.6442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [733/302]: training_loss: tensor(2.7974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [734/302]: training_loss: tensor(2.7268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [735/302]: training_loss: tensor(2.6877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [736/302]: training_loss: tensor(2.4796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [737/302]: training_loss: tensor(2.6200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [738/302]: training_loss: tensor(2.6825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [739/302]: training_loss: tensor(1.8515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [740/302]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [741/302]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [742/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [743/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [744/302]: training_loss: tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [745/302]: training_loss: tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [746/302]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [747/302]: training_loss: tensor(0.1130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [748/302]: training_loss: tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [749/302]: training_loss: tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [750/302]: training_loss: tensor(0.1043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [751/302]: training_loss: tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [752/302]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [753/302]: training_loss: tensor(0.1256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [754/302]: training_loss: tensor(0.1130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [755/302]: training_loss: tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [756/302]: training_loss: tensor(0.1380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [757/302]: training_loss: tensor(0.1222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [758/302]: training_loss: tensor(0.1258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [759/302]: training_loss: tensor(0.1194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [760/302]: training_loss: tensor(0.1090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [761/302]: training_loss: tensor(0.1347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [762/302]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [763/302]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [764/302]: training_loss: tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [765/302]: training_loss: tensor(0.1456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [766/302]: training_loss: tensor(0.1201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [767/302]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [768/302]: training_loss: tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [769/302]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [770/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [771/302]: training_loss: tensor(0.1129, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [772/302]: training_loss: tensor(0.1169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [773/302]: training_loss: tensor(0.1180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [774/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [775/302]: training_loss: tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [776/302]: training_loss: tensor(0.1305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [777/302]: training_loss: tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [778/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [779/302]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [780/302]: training_loss: tensor(0.1164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [781/302]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [782/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [783/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [784/302]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [785/302]: training_loss: tensor(0.1127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [786/302]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [787/302]: training_loss: tensor(0.1125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [788/302]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [789/302]: training_loss: tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [790/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [791/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [792/302]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [793/302]: training_loss: tensor(0.1083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [794/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [795/302]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [796/302]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [797/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [798/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [799/302]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [800/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [801/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [802/302]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [803/302]: training_loss: tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [804/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [805/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [806/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [807/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [808/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [809/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [810/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [811/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [812/302]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [813/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [814/302]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [815/302]: training_loss: tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [816/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [817/302]: training_loss: tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [818/302]: training_loss: tensor(1.8256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [819/302]: training_loss: tensor(2.4876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [820/302]: training_loss: tensor(2.6835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [821/302]: training_loss: tensor(2.5259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [822/302]: training_loss: tensor(2.4344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [823/302]: training_loss: tensor(2.3793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [824/302]: training_loss: tensor(2.5395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [825/302]: training_loss: tensor(0.3425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [826/302]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [827/302]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [828/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [829/302]: training_loss: tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [830/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [831/302]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [832/302]: training_loss: tensor(0.1260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [833/302]: training_loss: tensor(0.1107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [834/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [835/302]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [836/302]: training_loss: tensor(0.1212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [837/302]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [838/302]: training_loss: tensor(0.1210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [839/302]: training_loss: tensor(0.1085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [840/302]: training_loss: tensor(0.1260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [841/302]: training_loss: tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [842/302]: training_loss: tensor(0.1143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [843/302]: training_loss: tensor(0.1097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [844/302]: training_loss: tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [845/302]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [846/302]: training_loss: tensor(0.1077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [847/302]: training_loss: tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [848/302]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [849/302]: training_loss: tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [850/302]: training_loss: tensor(0.1044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [851/302]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [852/302]: training_loss: tensor(0.1154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [853/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [854/302]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [855/302]: training_loss: tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [856/302]: training_loss: tensor(0.1169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [857/302]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [858/302]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [859/302]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [860/302]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [861/302]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [862/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [863/302]: training_loss: tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [864/302]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [865/302]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [866/302]: training_loss: tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [867/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [868/302]: training_loss: tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [869/302]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [870/302]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [871/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [872/302]: training_loss: tensor(0.1175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [873/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [874/302]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [875/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [876/302]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [877/302]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [878/302]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [879/302]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [880/302]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [881/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [882/302]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [883/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [884/302]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [885/302]: training_loss: tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [886/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [887/302]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [888/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [889/302]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [890/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [891/302]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [892/302]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [893/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [894/302]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [895/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [896/302]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [897/302]: training_loss: tensor(0.0635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [898/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [899/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [900/302]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [901/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [902/302]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [903/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [904/302]: training_loss: tensor(0.4154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [905/302]: training_loss: tensor(2.5470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [906/302]: training_loss: tensor(2.6847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [907/302]: training_loss: tensor(2.6646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [908/302]: training_loss: tensor(2.5882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [909/302]: training_loss: tensor(2.7232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [910/302]: training_loss: tensor(2.5357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [911/302]: training_loss: tensor(2.5012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [912/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [913/302]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [914/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [915/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [916/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [917/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [918/302]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [919/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [920/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [921/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [922/302]: training_loss: tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [923/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [924/302]: training_loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [925/302]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [926/302]: training_loss: tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [927/302]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [928/302]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [929/302]: training_loss: tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [930/302]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [931/302]: training_loss: tensor(0.1131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [932/302]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [933/302]: training_loss: tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [934/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [935/302]: training_loss: tensor(0.1117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [936/302]: training_loss: tensor(0.1083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [937/302]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [938/302]: training_loss: tensor(0.1072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [939/302]: training_loss: tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [940/302]: training_loss: tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [941/302]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [942/302]: training_loss: tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [943/302]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [944/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [945/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [946/302]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [947/302]: training_loss: tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [948/302]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [949/302]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [950/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [951/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [952/302]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [953/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [954/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [955/302]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [956/302]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [957/302]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [958/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [959/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [960/302]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [961/302]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [962/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [963/302]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [964/302]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [965/302]: training_loss: tensor(0.1116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [966/302]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [967/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [968/302]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [969/302]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [970/302]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [971/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [972/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [973/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [974/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [975/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [976/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [977/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [978/302]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [979/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [980/302]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [981/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [982/302]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [983/302]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [984/302]: training_loss: tensor(2.4257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [985/302]: training_loss: tensor(2.6472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [986/302]: training_loss: tensor(2.6240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [987/302]: training_loss: tensor(2.6373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [988/302]: training_loss: tensor(2.7057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [989/302]: training_loss: tensor(2.5635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [990/302]: training_loss: tensor(2.4888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [991/302]: training_loss: tensor(2.6954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [992/302]: training_loss: tensor(0.9371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [993/302]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [994/302]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [995/302]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [996/302]: training_loss: tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [997/302]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [998/302]: training_loss: tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [999/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1000/302]: training_loss: tensor(0.1137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1001/302]: training_loss: tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1002/302]: training_loss: tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1003/302]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1004/302]: training_loss: tensor(0.1155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1005/302]: training_loss: tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/302]: training_loss: tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/302]: training_loss: tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/302]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/302]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/302]: training_loss: tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/302]: training_loss: tensor(0.1114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/302]: training_loss: tensor(0.1425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/302]: training_loss: tensor(0.1071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/302]: training_loss: tensor(0.1076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/302]: training_loss: tensor(0.1230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/302]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/302]: training_loss: tensor(0.1101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/302]: training_loss: tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1019/302]: training_loss: tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/302]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/302]: training_loss: tensor(0.1103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1023/302]: training_loss: tensor(0.1195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/302]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/302]: training_loss: tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/302]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/302]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/302]: training_loss: tensor(0.1261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/302]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/302]: training_loss: tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/302]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/302]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/302]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/302]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/302]: training_loss: tensor(0.1085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/302]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/302]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1040/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1043/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/302]: training_loss: tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/302]: training_loss: tensor(0.1044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/302]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/302]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/302]: training_loss: tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/302]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/302]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/302]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/302]: training_loss: tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/302]: training_loss: tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/302]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/302]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/302]: training_loss: tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/302]: training_loss: tensor(2.2819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/302]: training_loss: tensor(2.6031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/302]: training_loss: tensor(2.6659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/302]: training_loss: tensor(2.6713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/302]: training_loss: tensor(2.6837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/302]: training_loss: tensor(2.4921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/302]: training_loss: tensor(2.0619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/302]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/302]: training_loss: tensor(0.1173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/302]: training_loss: tensor(0.1107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/302]: training_loss: tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/302]: training_loss: tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/302]: training_loss: tensor(0.1064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/302]: training_loss: tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/302]: training_loss: tensor(0.1120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/302]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/302]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/302]: training_loss: tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/302]: training_loss: tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/302]: training_loss: tensor(0.1019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/302]: training_loss: tensor(0.1173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/302]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/302]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/302]: training_loss: tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/302]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/302]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/302]: training_loss: tensor(0.1184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/302]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/302]: training_loss: tensor(0.1093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/302]: training_loss: tensor(0.1169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/302]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/302]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/302]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1100/302]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/302]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/302]: training_loss: tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/302]: training_loss: tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/302]: training_loss: tensor(0.1143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/302]: training_loss: tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/302]: training_loss: tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/302]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/302]: training_loss: tensor(0.1059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/302]: training_loss: tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/302]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/302]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/302]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/302]: training_loss: tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/302]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/302]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/302]: training_loss: tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/302]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/302]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/302]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/302]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1128/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1129/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/302]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/302]: training_loss: tensor(2.5959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/302]: training_loss: tensor(2.3618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/302]: training_loss: tensor(2.4700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/302]: training_loss: tensor(2.4976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/302]: training_loss: tensor(1.4757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/302]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/302]: training_loss: tensor(0.1076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/302]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/302]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/302]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/302]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/302]: training_loss: tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/302]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/302]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/302]: training_loss: tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/302]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/302]: training_loss: tensor(0.1117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/302]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/302]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/302]: training_loss: tensor(0.1175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/302]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/302]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/302]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/302]: training_loss: tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/302]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/302]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/302]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/302]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/302]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/302]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/302]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/302]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/302]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/302]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/302]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/302]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/302]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/302]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/302]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/302]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/302]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/302]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/302]: training_loss: tensor(1.8939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/302]: training_loss: tensor(2.5942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/302]: training_loss: tensor(2.2959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/302]: training_loss: tensor(2.5866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/302]: training_loss: tensor(2.6313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/302]: training_loss: tensor(2.5300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/302]: training_loss: tensor(0.4216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/302]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/302]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/302]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/302]: training_loss: tensor(0.1236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/302]: training_loss: tensor(0.1066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/302]: training_loss: tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/302]: training_loss: tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1213/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/302]: training_loss: tensor(0.1099, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1215/302]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/302]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/302]: training_loss: tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/302]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/302]: training_loss: tensor(0.1146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/302]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/302]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/302]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/302]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/302]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/302]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/302]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/302]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/302]: training_loss: tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/302]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/302]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/302]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/302]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/302]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/302]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/302]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/302]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/302]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/302]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/302]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/302]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/302]: training_loss: tensor(2.3160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/302]: training_loss: tensor(2.6769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/302]: training_loss: tensor(2.6216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/302]: training_loss: tensor(2.5073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/302]: training_loss: tensor(2.4366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/302]: training_loss: tensor(0.7053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/302]: training_loss: tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/302]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/302]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/302]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/302]: training_loss: tensor(0.1078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/302]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/302]: training_loss: tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/302]: training_loss: tensor(0.1112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/302]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/302]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/302]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/302]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/302]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/302]: training_loss: tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/302]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/302]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/302]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/302]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/302]: training_loss: tensor(0.1105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/302]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/302]: training_loss: tensor(0.1107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/302]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/302]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/302]: training_loss: tensor(0.1160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/302]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/302]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1299/302]: training_loss: tensor(0.1114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/302]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/302]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/302]: training_loss: tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/302]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/302]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/302]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/302]: training_loss: tensor(1.6987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/302]: training_loss: tensor(2.5280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/302]: training_loss: tensor(2.5199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/302]: training_loss: tensor(2.4557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/302]: training_loss: tensor(2.5712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/302]: training_loss: tensor(0.9566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/302]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/302]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/302]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/302]: training_loss: tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/302]: training_loss: tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/302]: training_loss: tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/302]: training_loss: tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/302]: training_loss: tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/302]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/302]: training_loss: tensor(0.1043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/302]: training_loss: tensor(0.1068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/302]: training_loss: tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/302]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/302]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/302]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/302]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/302]: training_loss: tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/302]: training_loss: tensor(0.1044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/302]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/302]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/302]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/302]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/302]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/302]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/302]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/302]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/302]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/302]: training_loss: tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1361/302]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/302]: training_loss: tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1363/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/302]: training_loss: tensor(0.7374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/302]: training_loss: tensor(2.5246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/302]: training_loss: tensor(2.4227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/302]: training_loss: tensor(2.4952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/302]: training_loss: tensor(1.5141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/302]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/302]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/302]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/302]: training_loss: tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/302]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/302]: training_loss: tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1385/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1386/302]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1387/302]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1388/302]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1389/302]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1390/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1391/302]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1392/302]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1393/302]: training_loss: tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1394/302]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1395/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1396/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1397/302]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1398/302]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1399/302]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1400/302]: training_loss: tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1401/302]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1402/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1403/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1404/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1405/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1406/302]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1407/302]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1408/302]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1409/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1410/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1411/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1412/302]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1413/302]: training_loss: tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1414/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1415/302]: training_loss: tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1416/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1417/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1418/302]: training_loss: tensor(2.4909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1419/302]: training_loss: tensor(2.4941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1420/302]: training_loss: tensor(2.6113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1421/302]: training_loss: tensor(2.5537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1422/302]: training_loss: tensor(1.5972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1423/302]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1424/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1425/302]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1426/302]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1427/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1428/302]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1429/302]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1430/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1431/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1432/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1433/302]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1434/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1435/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1436/302]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1437/302]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1438/302]: training_loss: tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1439/302]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1440/302]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1441/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1442/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1443/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1444/302]: training_loss: tensor(0.1043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1445/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1446/302]: training_loss: tensor(0.1185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1447/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1448/302]: training_loss: tensor(0.1059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1449/302]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1450/302]: training_loss: tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1451/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1452/302]: training_loss: tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1453/302]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1454/302]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1455/302]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1456/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1457/302]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1458/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1459/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1460/302]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1461/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1462/302]: training_loss: tensor(0.1131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1463/302]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1464/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1465/302]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1466/302]: training_loss: tensor(0.7297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1467/302]: training_loss: tensor(2.5240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1468/302]: training_loss: tensor(2.5050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1469/302]: training_loss: tensor(2.4450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1470/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1471/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1472/302]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1473/302]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1474/302]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1475/302]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1476/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1477/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1478/302]: training_loss: tensor(0.1125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1479/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1480/302]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1481/302]: training_loss: tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1482/302]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1483/302]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1484/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1485/302]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1486/302]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1487/302]: training_loss: tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1488/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1489/302]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1490/302]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1491/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1492/302]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1493/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1494/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1495/302]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1496/302]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1497/302]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1498/302]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1499/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1500/302]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1501/302]: training_loss: tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1502/302]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1503/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1504/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1505/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1506/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1507/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1508/302]: training_loss: tensor(0.1066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1509/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1510/302]: training_loss: tensor(1.9687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1511/302]: training_loss: tensor(2.4018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1512/302]: training_loss: tensor(1.5270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1513/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1514/302]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1515/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1516/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1517/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1518/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1519/302]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1520/302]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1521/302]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1522/302]: training_loss: tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1523/302]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1524/302]: training_loss: tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1525/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1526/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1527/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1528/302]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1529/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1530/302]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1531/302]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1532/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1533/302]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1534/302]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1535/302]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1536/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1537/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1538/302]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1539/302]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1540/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1541/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1542/302]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1543/302]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1544/302]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1545/302]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1546/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1547/302]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1548/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1549/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1550/302]: training_loss: tensor(1.7430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1551/302]: training_loss: tensor(2.5910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1552/302]: training_loss: tensor(2.5106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1553/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1554/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1555/302]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1556/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1557/302]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1558/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1559/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1560/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1561/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1562/302]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1563/302]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1564/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1565/302]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1566/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1567/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1568/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1569/302]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1570/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1571/302]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1572/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1573/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1574/302]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1575/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1576/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1577/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1578/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1579/302]: training_loss: tensor(0.1076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1580/302]: training_loss: tensor(0.1120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1581/302]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1582/302]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1583/302]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1584/302]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1585/302]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1586/302]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1587/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1588/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1589/302]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1590/302]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1591/302]: training_loss: tensor(2.5322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1592/302]: training_loss: tensor(2.5734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1593/302]: training_loss: tensor(2.5352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1594/302]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1595/302]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1596/302]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1597/302]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1598/302]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1599/302]: training_loss: tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1600/302]: training_loss: tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1601/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1602/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1603/302]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1604/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1605/302]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1606/302]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1607/302]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1608/302]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1609/302]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1610/302]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1611/302]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1612/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1613/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1614/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1615/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1616/302]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1617/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1618/302]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1619/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1620/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1621/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1622/302]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1623/302]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1624/302]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1625/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1626/302]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1627/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1628/302]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1629/302]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1630/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1631/302]: training_loss: tensor(2.3324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1632/302]: training_loss: tensor(2.5381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1633/302]: training_loss: tensor(2.4699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1634/302]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1635/302]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1636/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1637/302]: training_loss: tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1638/302]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1639/302]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1640/302]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1641/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1642/302]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1643/302]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1644/302]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1645/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1646/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1647/302]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1648/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1649/302]: training_loss: tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1650/302]: training_loss: tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1651/302]: training_loss: tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1652/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1653/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1654/302]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1655/302]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1656/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1657/302]: training_loss: tensor(0.1093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1658/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1659/302]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1660/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1661/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1662/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1663/302]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1664/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1665/302]: training_loss: tensor(1.6931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1666/302]: training_loss: tensor(2.7280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1667/302]: training_loss: tensor(1.9232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1668/302]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1669/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1670/302]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1671/302]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1672/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1673/302]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1674/302]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1675/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1676/302]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1677/302]: training_loss: tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1678/302]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1679/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1680/302]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1681/302]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1682/302]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1683/302]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1684/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1685/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1686/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1687/302]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1688/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1689/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1690/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1691/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1692/302]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1693/302]: training_loss: tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1694/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1695/302]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1696/302]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1697/302]: training_loss: tensor(0.7873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1698/302]: training_loss: tensor(2.6319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1699/302]: training_loss: tensor(2.6788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1700/302]: training_loss: tensor(0.8216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1701/302]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1702/302]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1703/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1704/302]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1705/302]: training_loss: tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1706/302]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1707/302]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1708/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1709/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1710/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1711/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1712/302]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1713/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1714/302]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1715/302]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1716/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1717/302]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1718/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1719/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1720/302]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1721/302]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1722/302]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1723/302]: training_loss: tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1724/302]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1725/302]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1726/302]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1727/302]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1728/302]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1729/302]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1730/302]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1731/302]: training_loss: tensor(1.3727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1732/302]: training_loss: tensor(2.6258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1733/302]: training_loss: tensor(2.5152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1734/302]: training_loss: tensor(0.7649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1735/302]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1736/302]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1737/302]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1738/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1739/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1740/302]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1741/302]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1742/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1743/302]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1744/302]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1745/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1746/302]: training_loss: tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1747/302]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1748/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1749/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1750/302]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1751/302]: training_loss: tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1752/302]: training_loss: tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1753/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1754/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1755/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1756/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1757/302]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1758/302]: training_loss: tensor(0.6365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1759/302]: training_loss: tensor(2.6116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1760/302]: training_loss: tensor(2.5823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1761/302]: training_loss: tensor(1.4049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1762/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1763/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1764/302]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1765/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1766/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1767/302]: training_loss: tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1768/302]: training_loss: tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1769/302]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1770/302]: training_loss: tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1771/302]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1772/302]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1773/302]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1774/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1775/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1776/302]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1777/302]: training_loss: tensor(0.1043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1778/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1779/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1780/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1781/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1782/302]: training_loss: tensor(0.1019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1783/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1784/302]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1785/302]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1786/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1787/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1788/302]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1789/302]: training_loss: tensor(1.5619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1790/302]: training_loss: tensor(1.3407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1791/302]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1792/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1793/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1794/302]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1795/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1796/302]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1797/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1798/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1799/302]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1800/302]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1801/302]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1802/302]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1803/302]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1804/302]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1805/302]: training_loss: tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1806/302]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1807/302]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1808/302]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1809/302]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1810/302]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1811/302]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1812/302]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1813/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1814/302]: training_loss: tensor(0.3691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1815/302]: training_loss: tensor(2.6021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1816/302]: training_loss: tensor(2.6522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1817/302]: training_loss: tensor(0.3962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1818/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1819/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1820/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1821/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1822/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1823/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1824/302]: training_loss: tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1825/302]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1826/302]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1827/302]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1828/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1829/302]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1830/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1831/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1832/302]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1833/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1834/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1835/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1836/302]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1837/302]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1838/302]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1839/302]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1840/302]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1841/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1842/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1843/302]: training_loss: tensor(2.6617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1844/302]: training_loss: tensor(1.9816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1845/302]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1846/302]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1847/302]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1848/302]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1849/302]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1850/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1851/302]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1852/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1853/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1854/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1855/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1856/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1857/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1858/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1859/302]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1860/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1861/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1862/302]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1863/302]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1864/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1865/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1866/302]: training_loss: tensor(2.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1867/302]: training_loss: tensor(2.5573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1868/302]: training_loss: tensor(1.3528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1869/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1870/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1871/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1872/302]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1873/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1874/302]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1875/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1876/302]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1877/302]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1878/302]: training_loss: tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1879/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1880/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1881/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1882/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1883/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1884/302]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1885/302]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1886/302]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1887/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1888/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1889/302]: training_loss: tensor(0.6361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1890/302]: training_loss: tensor(2.5432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1891/302]: training_loss: tensor(2.6851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1892/302]: training_loss: tensor(1.3885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1893/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1894/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1895/302]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1896/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1897/302]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1898/302]: training_loss: tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1899/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1900/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1901/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1902/302]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1903/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1904/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1905/302]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1906/302]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1907/302]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1908/302]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1909/302]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1910/302]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1911/302]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1912/302]: training_loss: tensor(0.3829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1913/302]: training_loss: tensor(2.4679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1914/302]: training_loss: tensor(2.0637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1915/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1916/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1917/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1918/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1919/302]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1920/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1921/302]: training_loss: tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1922/302]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1923/302]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1924/302]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1925/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1926/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1927/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1928/302]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1929/302]: training_loss: tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1930/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1931/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1932/302]: training_loss: tensor(0.7248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1933/302]: training_loss: tensor(2.2237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1934/302]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1935/302]: training_loss: tensor(0.0637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1936/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1937/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1938/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1939/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1940/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1941/302]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1942/302]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1943/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1944/302]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1945/302]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1946/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1947/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1948/302]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1949/302]: training_loss: tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1950/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1951/302]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1952/302]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1953/302]: training_loss: tensor(1.5559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1954/302]: training_loss: tensor(0.9601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1955/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1956/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1957/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1958/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1959/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1960/302]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1961/302]: training_loss: tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1962/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1963/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1964/302]: training_loss: tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1965/302]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1966/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1967/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1968/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1969/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1970/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1971/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1972/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1973/302]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1974/302]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1975/302]: training_loss: tensor(0.4085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1976/302]: training_loss: tensor(2.7180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1977/302]: training_loss: tensor(1.3625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1978/302]: training_loss: tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1979/302]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1980/302]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1981/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1982/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1983/302]: training_loss: tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1984/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1985/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1986/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1987/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1988/302]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1989/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1990/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1991/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1992/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1993/302]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1994/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1995/302]: training_loss: tensor(0.4333, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1996/302]: training_loss: tensor(2.6353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1997/302]: training_loss: tensor(2.6466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1998/302]: training_loss: tensor(0.3917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1999/302]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2000/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2001/302]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2002/302]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2003/302]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2004/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2005/302]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2006/302]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2007/302]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2008/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2009/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2010/302]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2011/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2012/302]: training_loss: tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2013/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2014/302]: training_loss: tensor(0.0676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2015/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2016/302]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2017/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2018/302]: training_loss: tensor(2.4422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2019/302]: training_loss: tensor(1.3175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2020/302]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2021/302]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2022/302]: training_loss: tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2023/302]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2024/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2025/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2026/302]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2027/302]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2028/302]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2029/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2030/302]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2031/302]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2032/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2033/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2034/302]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2035/302]: training_loss: tensor(2.1211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2036/302]: training_loss: tensor(1.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2037/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2038/302]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2039/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2040/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2041/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2042/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2043/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2044/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2045/302]: training_loss: tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2046/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2047/302]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2048/302]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2049/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2050/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2051/302]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2052/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2053/302]: training_loss: tensor(1.2595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2054/302]: training_loss: tensor(1.2996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2055/302]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2056/302]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2057/302]: training_loss: tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2058/302]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2059/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2060/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2061/302]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2062/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2063/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2064/302]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2065/302]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2066/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2067/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2068/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2069/302]: training_loss: tensor(0.1093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2070/302]: training_loss: tensor(2.2835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2071/302]: training_loss: tensor(2.1430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2072/302]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2073/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2074/302]: training_loss: tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2075/302]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2076/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2077/302]: training_loss: tensor(0.0683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2078/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2079/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2080/302]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2081/302]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2082/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2083/302]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2084/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2085/302]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2086/302]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2087/302]: training_loss: tensor(1.2578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2088/302]: training_loss: tensor(1.7051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2089/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2090/302]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2091/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2092/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2093/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2094/302]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2095/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2096/302]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2097/302]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2098/302]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2099/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2100/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2101/302]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2102/302]: training_loss: tensor(1.0700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2103/302]: training_loss: tensor(1.6367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2104/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2105/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2106/302]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2107/302]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2108/302]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2109/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2110/302]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2111/302]: training_loss: tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2112/302]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2113/302]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2114/302]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2115/302]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2116/302]: training_loss: tensor(0.7363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2117/302]: training_loss: tensor(1.6186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2118/302]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2119/302]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2120/302]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2121/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2122/302]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2123/302]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2124/302]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2125/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2126/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2127/302]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2128/302]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2129/302]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2130/302]: training_loss: tensor(0.6656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2131/302]: training_loss: tensor(1.5349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2132/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2133/302]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2134/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2135/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2136/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2137/302]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2138/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2139/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2140/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2141/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2142/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2143/302]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2144/302]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2145/302]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2146/302]: training_loss: tensor(1.2057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2147/302]: training_loss: tensor(1.4213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2148/302]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2149/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2150/302]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2151/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2152/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2153/302]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2154/302]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2155/302]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2156/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2157/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2158/302]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2159/302]: training_loss: tensor(0.4001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2160/302]: training_loss: tensor(2.6339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2161/302]: training_loss: tensor(0.4030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2162/302]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2163/302]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2164/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2165/302]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2166/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2167/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2168/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2169/302]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2170/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2171/302]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2172/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2173/302]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2174/302]: training_loss: tensor(1.6011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2175/302]: training_loss: tensor(1.8182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2176/302]: training_loss: tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2177/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2178/302]: training_loss: tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2179/302]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2180/302]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2181/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2182/302]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2183/302]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2184/302]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2185/302]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2186/302]: training_loss: tensor(1.0527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2187/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2188/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2189/302]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2190/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2191/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2192/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2193/302]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2194/302]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2195/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2196/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2197/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2198/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2199/302]: training_loss: tensor(0.6388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2200/302]: training_loss: tensor(2.4408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2201/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2202/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2203/302]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2204/302]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2205/302]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2206/302]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2207/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2208/302]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2209/302]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2210/302]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2211/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2212/302]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2213/302]: training_loss: tensor(2.4790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2214/302]: training_loss: tensor(1.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2215/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2216/302]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2217/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2218/302]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2219/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2220/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2221/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2222/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2223/302]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2224/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2225/302]: training_loss: tensor(0.3903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2226/302]: training_loss: tensor(2.5725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2227/302]: training_loss: tensor(0.7147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2228/302]: training_loss: tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2229/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2230/302]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2231/302]: training_loss: tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2232/302]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2233/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2234/302]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2235/302]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2236/302]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2237/302]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2238/302]: training_loss: tensor(1.6676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2239/302]: training_loss: tensor(1.0597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2240/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2241/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2242/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2243/302]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2244/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2245/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2246/302]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2247/302]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2248/302]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2249/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2250/302]: training_loss: tensor(2.4999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2251/302]: training_loss: tensor(0.6332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2252/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2253/302]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2254/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2255/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2256/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2257/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2258/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2259/302]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2260/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2261/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2262/302]: training_loss: tensor(1.6969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2263/302]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2264/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2265/302]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2266/302]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2267/302]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2268/302]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2269/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2270/302]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2271/302]: training_loss: tensor(0.4181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2272/302]: training_loss: tensor(2.6631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2273/302]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2274/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2275/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2276/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2277/302]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2278/302]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2279/302]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2280/302]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2281/302]: training_loss: tensor(2.1470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2282/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2283/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2284/302]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2285/302]: training_loss: tensor(0.1019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2286/302]: training_loss: tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2287/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2288/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2289/302]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2290/302]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2291/302]: training_loss: tensor(1.0357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2292/302]: training_loss: tensor(1.8823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2293/302]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2294/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2295/302]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2296/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2297/302]: training_loss: tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2298/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2299/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2300/302]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2301/302]: training_loss: tensor(1.3397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2302/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2303/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2304/302]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2305/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2306/302]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2307/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2308/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2309/302]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2310/302]: training_loss: tensor(0.4556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2311/302]: training_loss: tensor(2.4418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2312/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2313/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2314/302]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2315/302]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2316/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2317/302]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2318/302]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2319/302]: training_loss: tensor(0.9219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2320/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2321/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2322/302]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2323/302]: training_loss: tensor(0.1157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2324/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2325/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2326/302]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2327/302]: training_loss: tensor(1.2459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2328/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2329/302]: training_loss: tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2330/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2331/302]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2332/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2333/302]: training_loss: tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2334/302]: training_loss: tensor(0.7241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2335/302]: training_loss: tensor(0.3449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2336/302]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2337/302]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2338/302]: training_loss: tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2339/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2340/302]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2341/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2342/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2343/302]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2344/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2345/302]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2346/302]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2347/302]: training_loss: tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2348/302]: training_loss: tensor(2.3405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2349/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2350/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2351/302]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2352/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2353/302]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2354/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2355/302]: training_loss: tensor(1.3272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2356/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2357/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2358/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2359/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2360/302]: training_loss: tensor(1.2678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2361/302]: training_loss: tensor(0.7891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2362/302]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2363/302]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2364/302]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2365/302]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2366/302]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2367/302]: training_loss: tensor(1.5856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2368/302]: training_loss: tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2369/302]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2370/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2371/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2372/302]: training_loss: tensor(1.5919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2373/302]: training_loss: tensor(0.3807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2374/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2375/302]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2376/302]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2377/302]: training_loss: tensor(0.9629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2378/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2379/302]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2380/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2381/302]: training_loss: tensor(0.7660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2382/302]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2383/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2384/302]: training_loss: tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2385/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2386/302]: training_loss: tensor(1.3340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2387/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2388/302]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2389/302]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2390/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2391/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2392/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2393/302]: training_loss: tensor(0.7724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2394/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2395/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2396/302]: training_loss: tensor(0.3809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2397/302]: training_loss: tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2398/302]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2399/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2400/302]: training_loss: tensor(1.3665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2401/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2402/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2403/302]: training_loss: tensor(0.3604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2404/302]: training_loss: tensor(0.4034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2405/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2406/302]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2407/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2408/302]: training_loss: tensor(0.4027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2409/302]: training_loss: tensor(0.7661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2410/302]: training_loss: tensor(0.3621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2411/302]: training_loss: tensor(0.4241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2412/302]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2413/302]: training_loss: tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2414/302]: training_loss: tensor(0.3751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2415/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2416/302]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2417/302]: training_loss: tensor(0.7774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2418/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2419/302]: training_loss: tensor(0.7367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2420/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [2/2], global step [4842/4842], PT Loss: 0.3028, Val Loss: 0.2902\n",
      "Pre-training done!\n",
      "======================= Start training =================================\n",
      "batch_no [0/302]: training_loss: tensor(2.5370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1/302]: training_loss: tensor(2.4576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2/302]: training_loss: tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [5/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [6/302]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [7/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [8/302]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [9/302]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [10/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [11/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [12/302]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [13/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [14/302]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [15/302]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [16/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [17/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [18/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [19/302]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [20/302]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [21/302]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [22/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [23/302]: training_loss: tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [24/302]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [25/302]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [26/302]: training_loss: tensor(0.7205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [27/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [28/302]: training_loss: tensor(2.3794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [29/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [30/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [31/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [32/302]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [33/302]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [34/302]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [35/302]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [36/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [37/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [38/302]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [39/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [40/302]: training_loss: tensor(0.6177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [41/302]: training_loss: tensor(2.6586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [42/302]: training_loss: tensor(2.2511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [43/302]: training_loss: tensor(0.1110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [44/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [45/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [46/302]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [47/302]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [48/302]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [49/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [50/302]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [51/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [52/302]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [53/302]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [54/302]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [55/302]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [56/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [57/302]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [58/302]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [59/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [60/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [61/302]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [62/302]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [63/302]: training_loss: tensor(0.1105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [64/302]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [65/302]: training_loss: tensor(0.7449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [66/302]: training_loss: tensor(2.6433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [67/302]: training_loss: tensor(1.5359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [68/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [69/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [70/302]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [71/302]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [72/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [73/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [74/302]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [75/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [76/302]: training_loss: tensor(0.1197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [77/302]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [78/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [79/302]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [80/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [81/302]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [82/302]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [83/302]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [84/302]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [85/302]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [86/302]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [87/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [88/302]: training_loss: tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [89/302]: training_loss: tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [90/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [91/302]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [92/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [93/302]: training_loss: tensor(0.4228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [94/302]: training_loss: tensor(2.6271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [95/302]: training_loss: tensor(2.5552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [96/302]: training_loss: tensor(2.4277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [97/302]: training_loss: tensor(2.1278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [98/302]: training_loss: tensor(0.1044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [99/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [100/302]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [101/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [102/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [103/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [104/302]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [105/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [106/302]: training_loss: tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [107/302]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [108/302]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [109/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [110/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [111/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [112/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [113/302]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [114/302]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [115/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [116/302]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [117/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [118/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [119/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [120/302]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [121/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [122/302]: training_loss: tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [123/302]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [124/302]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [125/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [126/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [127/302]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [128/302]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [129/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [130/302]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [131/302]: training_loss: tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [132/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [133/302]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [134/302]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [135/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [136/302]: training_loss: tensor(1.9156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [137/302]: training_loss: tensor(2.6849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [138/302]: training_loss: tensor(2.4034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [139/302]: training_loss: tensor(2.5487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [140/302]: training_loss: tensor(1.3660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [141/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [142/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [143/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [144/302]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [145/302]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [146/302]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [147/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [148/302]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [149/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [150/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [151/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [152/302]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [153/302]: training_loss: tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [154/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [155/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [156/302]: training_loss: tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [157/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [158/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [159/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [160/302]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [161/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [162/302]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [163/302]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [164/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [165/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [166/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [167/302]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [168/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [169/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [170/302]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [171/302]: training_loss: tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [172/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [173/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [174/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [175/302]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [176/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [177/302]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [178/302]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [179/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [180/302]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [181/302]: training_loss: tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [182/302]: training_loss: tensor(0.0650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [183/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [184/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [185/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [186/302]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [187/302]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [188/302]: training_loss: tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [189/302]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [190/302]: training_loss: tensor(0.1072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [191/302]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [192/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [193/302]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [194/302]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [195/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [196/302]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [197/302]: training_loss: tensor(2.7691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [198/302]: training_loss: tensor(2.5152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [199/302]: training_loss: tensor(2.5129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [200/302]: training_loss: tensor(2.5708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [201/302]: training_loss: tensor(2.7040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [202/302]: training_loss: tensor(1.3290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [203/302]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [204/302]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [205/302]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [206/302]: training_loss: tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [207/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [208/302]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [209/302]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [210/302]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [211/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [212/302]: training_loss: tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [213/302]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [214/302]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [215/302]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [216/302]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [217/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [218/302]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [219/302]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [220/302]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [221/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [222/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [223/302]: training_loss: tensor(0.1059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [224/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [225/302]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [226/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [227/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [228/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [229/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [230/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [231/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [232/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [233/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [234/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [235/302]: training_loss: tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [236/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [237/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [238/302]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [239/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [240/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [241/302]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [242/302]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [243/302]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [244/302]: training_loss: tensor(0.1144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [245/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [246/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [247/302]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [248/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [249/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [250/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [251/302]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [252/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [253/302]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [254/302]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [255/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [256/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [257/302]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [258/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [259/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [260/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [261/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [262/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [263/302]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [264/302]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [265/302]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [266/302]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [267/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [268/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [269/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [270/302]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [271/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [272/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [273/302]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [274/302]: training_loss: tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [275/302]: training_loss: tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [276/302]: training_loss: tensor(1.2923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [277/302]: training_loss: tensor(2.4191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [278/302]: training_loss: tensor(2.2621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [279/302]: training_loss: tensor(2.4937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [280/302]: training_loss: tensor(2.5760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [281/302]: training_loss: tensor(2.5259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [282/302]: training_loss: tensor(2.2338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [283/302]: training_loss: tensor(1.9996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [284/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [285/302]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [286/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [287/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [288/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [289/302]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [290/302]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [291/302]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [292/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [293/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [294/302]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [295/302]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [296/302]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [297/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [298/302]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [299/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [300/302]: training_loss: tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [301/302]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [302/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [303/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [304/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [305/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [306/302]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [307/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [308/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [309/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [310/302]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [311/302]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [312/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [313/302]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [314/302]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [315/302]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [316/302]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [317/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [318/302]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [319/302]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [320/302]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [321/302]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [322/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [323/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [324/302]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [325/302]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [326/302]: training_loss: tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [327/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [328/302]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [329/302]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [330/302]: training_loss: tensor(0.0650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [331/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [332/302]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [333/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [334/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [335/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [336/302]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [337/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [338/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [339/302]: training_loss: tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [340/302]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [341/302]: training_loss: tensor(0.0683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [342/302]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [343/302]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [344/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [345/302]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [346/302]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [347/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [348/302]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [349/302]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [350/302]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [351/302]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [352/302]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [353/302]: training_loss: tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [354/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [355/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [356/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [357/302]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [358/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [359/302]: training_loss: tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [360/302]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [361/302]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [362/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [363/302]: training_loss: tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [364/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [365/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [366/302]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [367/302]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [368/302]: training_loss: tensor(2.1679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [369/302]: training_loss: tensor(2.6833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [370/302]: training_loss: tensor(2.4991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [371/302]: training_loss: tensor(2.4237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [372/302]: training_loss: tensor(2.5999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [373/302]: training_loss: tensor(2.5219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [374/302]: training_loss: tensor(2.6012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [375/302]: training_loss: tensor(2.6774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [376/302]: training_loss: tensor(2.4647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [377/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [378/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [379/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [380/302]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [381/302]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [382/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [383/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [384/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [385/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [386/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [387/302]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [388/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [389/302]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [390/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [391/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [392/302]: training_loss: tensor(0.0637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [393/302]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [394/302]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [395/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [396/302]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [397/302]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [398/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [399/302]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [400/302]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [401/302]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [402/302]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [403/302]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [404/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [405/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [406/302]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [407/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [408/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [409/302]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [410/302]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [411/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [412/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [413/302]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [414/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [415/302]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [416/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [417/302]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [418/302]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [419/302]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [420/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [421/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [422/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [423/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [424/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [425/302]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [426/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [427/302]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [428/302]: training_loss: tensor(0.1044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [429/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [430/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [431/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [432/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [433/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [434/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [435/302]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [436/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [437/302]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [438/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [439/302]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [440/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [441/302]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [442/302]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [443/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [444/302]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [445/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [446/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [447/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [448/302]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [449/302]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [450/302]: training_loss: tensor(0.0686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [451/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [452/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [453/302]: training_loss: tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [454/302]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [455/302]: training_loss: tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [456/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [457/302]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [458/302]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [459/302]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [460/302]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [461/302]: training_loss: tensor(0.3881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [462/302]: training_loss: tensor(2.8206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [463/302]: training_loss: tensor(2.4863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [464/302]: training_loss: tensor(2.6926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [465/302]: training_loss: tensor(2.6540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [466/302]: training_loss: tensor(2.4790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [467/302]: training_loss: tensor(2.4543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [468/302]: training_loss: tensor(2.6543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [469/302]: training_loss: tensor(2.5145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [470/302]: training_loss: tensor(2.4898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [471/302]: training_loss: tensor(1.0395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [472/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [473/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [474/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [475/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [476/302]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [477/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [478/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [479/302]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [480/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [481/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [482/302]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [483/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [484/302]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [485/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [486/302]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [487/302]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [488/302]: training_loss: tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [489/302]: training_loss: tensor(0.0635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [490/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [491/302]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [492/302]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [493/302]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [494/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [495/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [496/302]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [497/302]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [498/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [499/302]: training_loss: tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [500/302]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [501/302]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [502/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [503/302]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [504/302]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [505/302]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [506/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [507/302]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [508/302]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [509/302]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [510/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [511/302]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [512/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [513/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [514/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [515/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [516/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [517/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [518/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [519/302]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [520/302]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [521/302]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [522/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [523/302]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [524/302]: training_loss: tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [525/302]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [526/302]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [527/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [528/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [529/302]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [530/302]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [531/302]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [532/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [533/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [534/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [535/302]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [536/302]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [537/302]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [538/302]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [539/302]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [540/302]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [541/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [542/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [543/302]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [544/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [545/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [546/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [547/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [548/302]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [549/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [550/302]: training_loss: tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [551/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [552/302]: training_loss: tensor(1.7411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [553/302]: training_loss: tensor(2.5274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [554/302]: training_loss: tensor(2.5751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [555/302]: training_loss: tensor(2.6040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [556/302]: training_loss: tensor(2.6475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [557/302]: training_loss: tensor(2.5752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [558/302]: training_loss: tensor(2.4089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [559/302]: training_loss: tensor(2.4263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [560/302]: training_loss: tensor(2.6846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [561/302]: training_loss: tensor(0.7378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [562/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [563/302]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [564/302]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [565/302]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [566/302]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [567/302]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [568/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [569/302]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [570/302]: training_loss: tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [571/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [572/302]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [573/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [574/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [575/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [576/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [577/302]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [578/302]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [579/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [580/302]: training_loss: tensor(0.1069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [581/302]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [582/302]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [583/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [584/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [585/302]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [586/302]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [587/302]: training_loss: tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [588/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [589/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [590/302]: training_loss: tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [591/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [592/302]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [593/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [594/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [595/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [596/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [597/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [598/302]: training_loss: tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [599/302]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [600/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [601/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [602/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [603/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [604/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [605/302]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [606/302]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [607/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [608/302]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [609/302]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [610/302]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [611/302]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [612/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [613/302]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [614/302]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [615/302]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [616/302]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [617/302]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [618/302]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [619/302]: training_loss: tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [620/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [621/302]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [622/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [623/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [624/302]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [625/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [626/302]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [627/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [628/302]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [629/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [630/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [631/302]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [632/302]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [633/302]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [634/302]: training_loss: tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [635/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [636/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [637/302]: training_loss: tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [638/302]: training_loss: tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [639/302]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [640/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [641/302]: training_loss: tensor(0.1101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [642/302]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [643/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [644/302]: training_loss: tensor(0.9804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [645/302]: training_loss: tensor(2.5072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [646/302]: training_loss: tensor(2.5210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [647/302]: training_loss: tensor(2.3726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [648/302]: training_loss: tensor(2.5046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [649/302]: training_loss: tensor(2.5995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [650/302]: training_loss: tensor(2.5962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [651/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [652/302]: training_loss: tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [653/302]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [654/302]: training_loss: tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [655/302]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [656/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [657/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [658/302]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [659/302]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [660/302]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [661/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [662/302]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [663/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [664/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [665/302]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [666/302]: training_loss: tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [667/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [668/302]: training_loss: tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [669/302]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [670/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [671/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [672/302]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [673/302]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [674/302]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [675/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [676/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [677/302]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [678/302]: training_loss: tensor(0.1044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [679/302]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [680/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [681/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [682/302]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [683/302]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [684/302]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [685/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [686/302]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [687/302]: training_loss: tensor(0.1097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [688/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [689/302]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [690/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [691/302]: training_loss: tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [692/302]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [693/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [694/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [695/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [696/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [697/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [698/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [699/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [700/302]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [701/302]: training_loss: tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [702/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [703/302]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [704/302]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [705/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [706/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [707/302]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [708/302]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [709/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [710/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [711/302]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [712/302]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [713/302]: training_loss: tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [714/302]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [715/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [716/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [717/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [718/302]: training_loss: tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [719/302]: training_loss: tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [720/302]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [721/302]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [722/302]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [723/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [724/302]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [725/302]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [726/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [727/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [728/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [729/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [730/302]: training_loss: tensor(1.7621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [731/302]: training_loss: tensor(2.5564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [732/302]: training_loss: tensor(2.4643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [733/302]: training_loss: tensor(2.4815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [734/302]: training_loss: tensor(2.4413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [735/302]: training_loss: tensor(2.7545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [736/302]: training_loss: tensor(2.4768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [737/302]: training_loss: tensor(2.7411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [738/302]: training_loss: tensor(2.8154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [739/302]: training_loss: tensor(1.8029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [740/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [741/302]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [742/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [743/302]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [744/302]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [745/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [746/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [747/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [748/302]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [749/302]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [750/302]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [751/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [752/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [753/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [754/302]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [755/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [756/302]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [757/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [758/302]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [759/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [760/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [761/302]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [762/302]: training_loss: tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [763/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [764/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [765/302]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [766/302]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [767/302]: training_loss: tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [768/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [769/302]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [770/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [771/302]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [772/302]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [773/302]: training_loss: tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [774/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [775/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [776/302]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [777/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [778/302]: training_loss: tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [779/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [780/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [781/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [782/302]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [783/302]: training_loss: tensor(0.1231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [784/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [785/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [786/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [787/302]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [788/302]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [789/302]: training_loss: tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [790/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [791/302]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [792/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [793/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [794/302]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [795/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [796/302]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [797/302]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [798/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [799/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [800/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [801/302]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [802/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [803/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [804/302]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [805/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [806/302]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [807/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [808/302]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [809/302]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [810/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [811/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [812/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [813/302]: training_loss: tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [814/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [815/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [816/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [817/302]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [818/302]: training_loss: tensor(1.8209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [819/302]: training_loss: tensor(2.5238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [820/302]: training_loss: tensor(2.5446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [821/302]: training_loss: tensor(2.6078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [822/302]: training_loss: tensor(2.6031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [823/302]: training_loss: tensor(2.5732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [824/302]: training_loss: tensor(2.5335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [825/302]: training_loss: tensor(0.3293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [826/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [827/302]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [828/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [829/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [830/302]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [831/302]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [832/302]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [833/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [834/302]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [835/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [836/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [837/302]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [838/302]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [839/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [840/302]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [841/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [842/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [843/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [844/302]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [845/302]: training_loss: tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [846/302]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [847/302]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [848/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [849/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [850/302]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [851/302]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [852/302]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [853/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [854/302]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [855/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [856/302]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [857/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [858/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [859/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [860/302]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [861/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [862/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [863/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [864/302]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [865/302]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [866/302]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [867/302]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [868/302]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [869/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [870/302]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [871/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [872/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [873/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [874/302]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [875/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [876/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [877/302]: training_loss: tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [878/302]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [879/302]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [880/302]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [881/302]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [882/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [883/302]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [884/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [885/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [886/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [887/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [888/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [889/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [890/302]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [891/302]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [892/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [893/302]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [894/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [895/302]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [896/302]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [897/302]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [898/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [899/302]: training_loss: tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [900/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [901/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [902/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [903/302]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [904/302]: training_loss: tensor(0.3153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [905/302]: training_loss: tensor(2.6289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [906/302]: training_loss: tensor(2.4914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [907/302]: training_loss: tensor(2.5389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [908/302]: training_loss: tensor(2.6417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [909/302]: training_loss: tensor(2.5400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [910/302]: training_loss: tensor(2.5089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [911/302]: training_loss: tensor(2.4921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [912/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [913/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [914/302]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [915/302]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [916/302]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [917/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [918/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [919/302]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [920/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [921/302]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [922/302]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [923/302]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [924/302]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [925/302]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [926/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [927/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [928/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [929/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [930/302]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [931/302]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [932/302]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [933/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [934/302]: training_loss: tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [935/302]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [936/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [937/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [938/302]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [939/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [940/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [941/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [942/302]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [943/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [944/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [945/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [946/302]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [947/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [948/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [949/302]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [950/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [951/302]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [952/302]: training_loss: tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [953/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [954/302]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [955/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [956/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [957/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [958/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [959/302]: training_loss: tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [960/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [961/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [962/302]: training_loss: tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [963/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [964/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [965/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [966/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [967/302]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [968/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [969/302]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [970/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [971/302]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [972/302]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [973/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [974/302]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [975/302]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [976/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [977/302]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [978/302]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [979/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [980/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [981/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [982/302]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [983/302]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [984/302]: training_loss: tensor(2.3908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [985/302]: training_loss: tensor(2.4725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [986/302]: training_loss: tensor(2.5473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [987/302]: training_loss: tensor(2.5553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [988/302]: training_loss: tensor(2.5359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [989/302]: training_loss: tensor(2.6358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [990/302]: training_loss: tensor(2.6462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [991/302]: training_loss: tensor(2.4428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [992/302]: training_loss: tensor(1.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [993/302]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [994/302]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [995/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [996/302]: training_loss: tensor(0.1121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [997/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [998/302]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [999/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1000/302]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1001/302]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1002/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1003/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1004/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1005/302]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/302]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/302]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/302]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/302]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/302]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/302]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/302]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1019/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/302]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/302]: training_loss: tensor(0.1173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/302]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1023/302]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/302]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/302]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/302]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/302]: training_loss: tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/302]: training_loss: tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/302]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/302]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/302]: training_loss: tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/302]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1040/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/302]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/302]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1043/302]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/302]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/302]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/302]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/302]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/302]: training_loss: tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/302]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/302]: training_loss: tensor(2.2485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/302]: training_loss: tensor(2.4544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/302]: training_loss: tensor(2.5501, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1065/302]: training_loss: tensor(2.6366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/302]: training_loss: tensor(2.4131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/302]: training_loss: tensor(2.6492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/302]: training_loss: tensor(1.9822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/302]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/302]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/302]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/302]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/302]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/302]: training_loss: tensor(0.1066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/302]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/302]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/302]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/302]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/302]: training_loss: tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/302]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/302]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/302]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/302]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/302]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/302]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/302]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1100/302]: training_loss: tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/302]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/302]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/302]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/302]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/302]: training_loss: tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/302]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/302]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/302]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/302]: training_loss: tensor(0.1105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/302]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/302]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/302]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/302]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/302]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/302]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/302]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/302]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/302]: training_loss: tensor(0.1059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1129/302]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/302]: training_loss: tensor(2.5728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/302]: training_loss: tensor(2.5392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/302]: training_loss: tensor(2.5406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/302]: training_loss: tensor(2.6804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/302]: training_loss: tensor(1.4909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/302]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/302]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/302]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/302]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/302]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/302]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/302]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1152/302]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/302]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/302]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/302]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/302]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/302]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/302]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/302]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/302]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/302]: training_loss: tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/302]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/302]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/302]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/302]: training_loss: tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/302]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/302]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/302]: training_loss: tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/302]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/302]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/302]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/302]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/302]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/302]: training_loss: tensor(2.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/302]: training_loss: tensor(2.4534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/302]: training_loss: tensor(2.5638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/302]: training_loss: tensor(2.6603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/302]: training_loss: tensor(2.6004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/302]: training_loss: tensor(2.4624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/302]: training_loss: tensor(0.3747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/302]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/302]: training_loss: tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/302]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1213/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/302]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1215/302]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/302]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/302]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/302]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/302]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/302]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/302]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/302]: training_loss: tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/302]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/302]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/302]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/302]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/302]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/302]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/302]: training_loss: tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/302]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/302]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/302]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/302]: training_loss: tensor(2.3164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/302]: training_loss: tensor(2.6174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/302]: training_loss: tensor(2.6288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/302]: training_loss: tensor(2.5978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/302]: training_loss: tensor(2.4073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/302]: training_loss: tensor(0.5713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/302]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/302]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/302]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/302]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/302]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/302]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/302]: training_loss: tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/302]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/302]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/302]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/302]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/302]: training_loss: tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/302]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/302]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/302]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/302]: training_loss: tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/302]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/302]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/302]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1299/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/302]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/302]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/302]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/302]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/302]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/302]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/302]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/302]: training_loss: tensor(1.9385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/302]: training_loss: tensor(2.6829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/302]: training_loss: tensor(2.4515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/302]: training_loss: tensor(2.3509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/302]: training_loss: tensor(2.6387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/302]: training_loss: tensor(0.9324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/302]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/302]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/302]: training_loss: tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/302]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/302]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1325/302]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/302]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/302]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/302]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/302]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/302]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/302]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/302]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/302]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/302]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/302]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/302]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/302]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/302]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/302]: training_loss: tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/302]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/302]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/302]: training_loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/302]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/302]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/302]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1361/302]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/302]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1363/302]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/302]: training_loss: tensor(0.6654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/302]: training_loss: tensor(2.4520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/302]: training_loss: tensor(2.5367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/302]: training_loss: tensor(2.6194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/302]: training_loss: tensor(1.5142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/302]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/302]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/302]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/302]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/302]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/302]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/302]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/302]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/302]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1385/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1386/302]: training_loss: tensor(0.1090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1387/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1388/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1389/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1390/302]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1391/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1392/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1393/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1394/302]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1395/302]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1396/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1397/302]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1398/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1399/302]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1400/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1401/302]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1402/302]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1403/302]: training_loss: tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1404/302]: training_loss: tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1405/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1406/302]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1407/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1408/302]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1409/302]: training_loss: tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1410/302]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1411/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1412/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1413/302]: training_loss: tensor(0.1147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1414/302]: training_loss: tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1415/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1416/302]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1417/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1418/302]: training_loss: tensor(2.6782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1419/302]: training_loss: tensor(2.4199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1420/302]: training_loss: tensor(2.3577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1421/302]: training_loss: tensor(2.6099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1422/302]: training_loss: tensor(1.7066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1423/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1424/302]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1425/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1426/302]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1427/302]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1428/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1429/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1430/302]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1431/302]: training_loss: tensor(0.1043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1432/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1433/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1434/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1435/302]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1436/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1437/302]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1438/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1439/302]: training_loss: tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1440/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1441/302]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1442/302]: training_loss: tensor(0.1044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1443/302]: training_loss: tensor(0.1095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1444/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1445/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1446/302]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1447/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1448/302]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1449/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1450/302]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1451/302]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1452/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1453/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1454/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1455/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1456/302]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1457/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1458/302]: training_loss: tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1459/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1460/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1461/302]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1462/302]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1463/302]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1464/302]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1465/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1466/302]: training_loss: tensor(0.7514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1467/302]: training_loss: tensor(2.3962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1468/302]: training_loss: tensor(2.6366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1469/302]: training_loss: tensor(2.6793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1470/302]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1471/302]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1472/302]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1473/302]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1474/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1475/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1476/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1477/302]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1478/302]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1479/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1480/302]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1481/302]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1482/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1483/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1484/302]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1485/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1486/302]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1487/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1488/302]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1489/302]: training_loss: tensor(0.1076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1490/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1491/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1492/302]: training_loss: tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1493/302]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1494/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1495/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1496/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1497/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1498/302]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1499/302]: training_loss: tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1500/302]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1501/302]: training_loss: tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1502/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1503/302]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1504/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1505/302]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1506/302]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1507/302]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1508/302]: training_loss: tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1509/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1510/302]: training_loss: tensor(1.9173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1511/302]: training_loss: tensor(2.4188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1512/302]: training_loss: tensor(1.6173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1513/302]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1514/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1515/302]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1516/302]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1517/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1518/302]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1519/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1520/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1521/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1522/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1523/302]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1524/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1525/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1526/302]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1527/302]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1528/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1529/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1530/302]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1531/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1532/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1533/302]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1534/302]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1535/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1536/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1537/302]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1538/302]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1539/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1540/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1541/302]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1542/302]: training_loss: tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1543/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1544/302]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1545/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1546/302]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1547/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1548/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1549/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1550/302]: training_loss: tensor(1.6806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1551/302]: training_loss: tensor(2.6801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1552/302]: training_loss: tensor(2.5089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1553/302]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1554/302]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1555/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1556/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1557/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1558/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1559/302]: training_loss: tensor(0.1310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1560/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1561/302]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1562/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1563/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1564/302]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1565/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1566/302]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1567/302]: training_loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1568/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1569/302]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1570/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1571/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1572/302]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1573/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1574/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1575/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1576/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1577/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1578/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1579/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1580/302]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1581/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1582/302]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1583/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1584/302]: training_loss: tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1585/302]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1586/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1587/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1588/302]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1589/302]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1590/302]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1591/302]: training_loss: tensor(2.5553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1592/302]: training_loss: tensor(2.6819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1593/302]: training_loss: tensor(2.5008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1594/302]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1595/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1596/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1597/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1598/302]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1599/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1600/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1601/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1602/302]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1603/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1604/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1605/302]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1606/302]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1607/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1608/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1609/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1610/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1611/302]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1612/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1613/302]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1614/302]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1615/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1616/302]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1617/302]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1618/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1619/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1620/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1621/302]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1622/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1623/302]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1624/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1625/302]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1626/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1627/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1628/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1629/302]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1630/302]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1631/302]: training_loss: tensor(2.2402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1632/302]: training_loss: tensor(2.4662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1633/302]: training_loss: tensor(2.1637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1634/302]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1635/302]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1636/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1637/302]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1638/302]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1639/302]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1640/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1641/302]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1642/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1643/302]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1644/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1645/302]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1646/302]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1647/302]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1648/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1649/302]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1650/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1651/302]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1652/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1653/302]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1654/302]: training_loss: tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1655/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1656/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1657/302]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1658/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1659/302]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1660/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1661/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1662/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1663/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1664/302]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1665/302]: training_loss: tensor(1.6902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1666/302]: training_loss: tensor(2.5030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1667/302]: training_loss: tensor(1.9998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1668/302]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1669/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1670/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1671/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1672/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1673/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1674/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1675/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1676/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1677/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1678/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1679/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1680/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1681/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1682/302]: training_loss: tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1683/302]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1684/302]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1685/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1686/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1687/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1688/302]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1689/302]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1690/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1691/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1692/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1693/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1694/302]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1695/302]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1696/302]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1697/302]: training_loss: tensor(0.7619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1698/302]: training_loss: tensor(2.6052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1699/302]: training_loss: tensor(2.4998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1700/302]: training_loss: tensor(0.7033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1701/302]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1702/302]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1703/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1704/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1705/302]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1706/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1707/302]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1708/302]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1709/302]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1710/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1711/302]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1712/302]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1713/302]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1714/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1715/302]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1716/302]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1717/302]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1718/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1719/302]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1720/302]: training_loss: tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1721/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1722/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1723/302]: training_loss: tensor(0.1120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1724/302]: training_loss: tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1725/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1726/302]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1727/302]: training_loss: tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1728/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1729/302]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1730/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1731/302]: training_loss: tensor(1.3335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1732/302]: training_loss: tensor(2.4633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1733/302]: training_loss: tensor(2.4699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1734/302]: training_loss: tensor(0.6593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1735/302]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1736/302]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1737/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1738/302]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1739/302]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1740/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1741/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1742/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1743/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1744/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1745/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1746/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1747/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1748/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1749/302]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1750/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1751/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1752/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1753/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1754/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1755/302]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1756/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1757/302]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1758/302]: training_loss: tensor(0.7022, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1759/302]: training_loss: tensor(2.6465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1760/302]: training_loss: tensor(2.5321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1761/302]: training_loss: tensor(1.2227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1762/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1763/302]: training_loss: tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1764/302]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1765/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1766/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1767/302]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1768/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1769/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1770/302]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1771/302]: training_loss: tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1772/302]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1773/302]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1774/302]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1775/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1776/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1777/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1778/302]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1779/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1780/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1781/302]: training_loss: tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1782/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1783/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1784/302]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1785/302]: training_loss: tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1786/302]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1787/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1788/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1789/302]: training_loss: tensor(1.6521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1790/302]: training_loss: tensor(1.3193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1791/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1792/302]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1793/302]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1794/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1795/302]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1796/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1797/302]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1798/302]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1799/302]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1800/302]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1801/302]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1802/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1803/302]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1804/302]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1805/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1806/302]: training_loss: tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1807/302]: training_loss: tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1808/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1809/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1810/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1811/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1812/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1813/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1814/302]: training_loss: tensor(0.4406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1815/302]: training_loss: tensor(2.6087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1816/302]: training_loss: tensor(2.5352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1817/302]: training_loss: tensor(0.3757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1818/302]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1819/302]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1820/302]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1821/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1822/302]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1823/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1824/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1825/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1826/302]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1827/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1828/302]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1829/302]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1830/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1831/302]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1832/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1833/302]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1834/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1835/302]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1836/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1837/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1838/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1839/302]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1840/302]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1841/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1842/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1843/302]: training_loss: tensor(2.5513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1844/302]: training_loss: tensor(1.9957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1845/302]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1846/302]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1847/302]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1848/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1849/302]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1850/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1851/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1852/302]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1853/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1854/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1855/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1856/302]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1857/302]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1858/302]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1859/302]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1860/302]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1861/302]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1862/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1863/302]: training_loss: tensor(0.1072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1864/302]: training_loss: tensor(0.0676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1865/302]: training_loss: tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1866/302]: training_loss: tensor(1.9244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1867/302]: training_loss: tensor(2.5192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1868/302]: training_loss: tensor(1.4758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1869/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1870/302]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1871/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1872/302]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1873/302]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1874/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1875/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1876/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1877/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1878/302]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1879/302]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1880/302]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1881/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1882/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1883/302]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1884/302]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1885/302]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1886/302]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1887/302]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1888/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1889/302]: training_loss: tensor(0.6967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1890/302]: training_loss: tensor(2.3799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1891/302]: training_loss: tensor(2.3990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1892/302]: training_loss: tensor(1.3856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1893/302]: training_loss: tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1894/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1895/302]: training_loss: tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1896/302]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1897/302]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1898/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1899/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1900/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1901/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1902/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1903/302]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1904/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1905/302]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1906/302]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1907/302]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1908/302]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1909/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1910/302]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1911/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1912/302]: training_loss: tensor(0.4268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1913/302]: training_loss: tensor(2.6018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1914/302]: training_loss: tensor(1.8551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1915/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1916/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1917/302]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1918/302]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1919/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1920/302]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1921/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1922/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1923/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1924/302]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1925/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1926/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1927/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1928/302]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1929/302]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1930/302]: training_loss: tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1931/302]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1932/302]: training_loss: tensor(0.6252, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1933/302]: training_loss: tensor(2.2195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1934/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1935/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1936/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1937/302]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1938/302]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1939/302]: training_loss: tensor(0.1110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1940/302]: training_loss: tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1941/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1942/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1943/302]: training_loss: tensor(0.1069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1944/302]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1945/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1946/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1947/302]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1948/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1949/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1950/302]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1951/302]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1952/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1953/302]: training_loss: tensor(1.7501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1954/302]: training_loss: tensor(1.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1955/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1956/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1957/302]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1958/302]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1959/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1960/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1961/302]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1962/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1963/302]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1964/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1965/302]: training_loss: tensor(0.0635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1966/302]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1967/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1968/302]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1969/302]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1970/302]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1971/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1972/302]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1973/302]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1974/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1975/302]: training_loss: tensor(0.4135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1976/302]: training_loss: tensor(2.6406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1977/302]: training_loss: tensor(1.3489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1978/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1979/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1980/302]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1981/302]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1982/302]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1983/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1984/302]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1985/302]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1986/302]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1987/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1988/302]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1989/302]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1990/302]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1991/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1992/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1993/302]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1994/302]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1995/302]: training_loss: tensor(0.3939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1996/302]: training_loss: tensor(2.6139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1997/302]: training_loss: tensor(2.6270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1998/302]: training_loss: tensor(0.3794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1999/302]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2000/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2001/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2002/302]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2003/302]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2004/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2005/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2006/302]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2007/302]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2008/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2009/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2010/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2011/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2012/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2013/302]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2014/302]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2015/302]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2016/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2017/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2018/302]: training_loss: tensor(2.1377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2019/302]: training_loss: tensor(1.2357, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2020/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2021/302]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2022/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2023/302]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2024/302]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2025/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2026/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2027/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2028/302]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2029/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2030/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2031/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2032/302]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2033/302]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2034/302]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2035/302]: training_loss: tensor(2.3345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2036/302]: training_loss: tensor(0.9738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2037/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2038/302]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2039/302]: training_loss: tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2040/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2041/302]: training_loss: tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2042/302]: training_loss: tensor(0.1083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2043/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2044/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2045/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2046/302]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2047/302]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2048/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2049/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2050/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2051/302]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2052/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2053/302]: training_loss: tensor(1.3307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2054/302]: training_loss: tensor(1.2321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2055/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2056/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2057/302]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2058/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2059/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2060/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2061/302]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2062/302]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2063/302]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2064/302]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2065/302]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2066/302]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2067/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2068/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2069/302]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2070/302]: training_loss: tensor(2.3878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2071/302]: training_loss: tensor(1.8759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2072/302]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2073/302]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2074/302]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2075/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2076/302]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2077/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2078/302]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2079/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2080/302]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2081/302]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2082/302]: training_loss: tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2083/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2084/302]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2085/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2086/302]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2087/302]: training_loss: tensor(1.3725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2088/302]: training_loss: tensor(1.6998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2089/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2090/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2091/302]: training_loss: tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2092/302]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2093/302]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2094/302]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2095/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2096/302]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2097/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2098/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2099/302]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2100/302]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2101/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2102/302]: training_loss: tensor(0.9583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2103/302]: training_loss: tensor(1.6615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2104/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2105/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2106/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2107/302]: training_loss: tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2108/302]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2109/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2110/302]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2111/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2112/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2113/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2114/302]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2115/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2116/302]: training_loss: tensor(0.6746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2117/302]: training_loss: tensor(1.6170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2118/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2119/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2120/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2121/302]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2122/302]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2123/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2124/302]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2125/302]: training_loss: tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2126/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2127/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2128/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2129/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2130/302]: training_loss: tensor(0.7065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2131/302]: training_loss: tensor(1.5315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2132/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2133/302]: training_loss: tensor(0.0666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2134/302]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2135/302]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2136/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2137/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2138/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2139/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2140/302]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2141/302]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2142/302]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2143/302]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2144/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2145/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2146/302]: training_loss: tensor(1.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2147/302]: training_loss: tensor(1.2349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2148/302]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2149/302]: training_loss: tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2150/302]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2151/302]: training_loss: tensor(0.1068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2152/302]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2153/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2154/302]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2155/302]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2156/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2157/302]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2158/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2159/302]: training_loss: tensor(0.3847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2160/302]: training_loss: tensor(2.5520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2161/302]: training_loss: tensor(0.4299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2162/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2163/302]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2164/302]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2165/302]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2166/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2167/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2168/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2169/302]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2170/302]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2171/302]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2172/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2173/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2174/302]: training_loss: tensor(1.5880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2175/302]: training_loss: tensor(1.8222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2176/302]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2177/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2178/302]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2179/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2180/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2181/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2182/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2183/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2184/302]: training_loss: tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2185/302]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2186/302]: training_loss: tensor(1.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2187/302]: training_loss: tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2188/302]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2189/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2190/302]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2191/302]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2192/302]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2193/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2194/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2195/302]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2196/302]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2197/302]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2198/302]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2199/302]: training_loss: tensor(0.7060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2200/302]: training_loss: tensor(2.2349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2201/302]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2202/302]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2203/302]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2204/302]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2205/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2206/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2207/302]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2208/302]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2209/302]: training_loss: tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2210/302]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2211/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2212/302]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2213/302]: training_loss: tensor(2.5142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2214/302]: training_loss: tensor(1.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2215/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2216/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2217/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2218/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2219/302]: training_loss: tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2220/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2221/302]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2222/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2223/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2224/302]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2225/302]: training_loss: tensor(0.4020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2226/302]: training_loss: tensor(2.7685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2227/302]: training_loss: tensor(0.7437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2228/302]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2229/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2230/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2231/302]: training_loss: tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2232/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2233/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2234/302]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2235/302]: training_loss: tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2236/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2237/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2238/302]: training_loss: tensor(1.6121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2239/302]: training_loss: tensor(0.9534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2240/302]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2241/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2242/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2243/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2244/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2245/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2246/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2247/302]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2248/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2249/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2250/302]: training_loss: tensor(2.4644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2251/302]: training_loss: tensor(0.7300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2252/302]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2253/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2254/302]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2255/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2256/302]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2257/302]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2258/302]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2259/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2260/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2261/302]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2262/302]: training_loss: tensor(1.5775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2263/302]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2264/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2265/302]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2266/302]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2267/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2268/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2269/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2270/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2271/302]: training_loss: tensor(0.3821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2272/302]: training_loss: tensor(2.3188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2273/302]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2274/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2275/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2276/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2277/302]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2278/302]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2279/302]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2280/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2281/302]: training_loss: tensor(2.2859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2282/302]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2283/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2284/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2285/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2286/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2287/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2288/302]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2289/302]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2290/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2291/302]: training_loss: tensor(0.9906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2292/302]: training_loss: tensor(1.8558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2293/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2294/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2295/302]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2296/302]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2297/302]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2298/302]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2299/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2300/302]: training_loss: tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2301/302]: training_loss: tensor(1.3145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2302/302]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2303/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2304/302]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2305/302]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2306/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2307/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2308/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2309/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2310/302]: training_loss: tensor(0.4373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2311/302]: training_loss: tensor(2.2447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2312/302]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2313/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2314/302]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2315/302]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2316/302]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2317/302]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2318/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2319/302]: training_loss: tensor(1.0344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2320/302]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2321/302]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2322/302]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2323/302]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2324/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2325/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2326/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2327/302]: training_loss: tensor(1.3157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2328/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2329/302]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2330/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2331/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2332/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2333/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2334/302]: training_loss: tensor(0.6237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2335/302]: training_loss: tensor(0.3953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2336/302]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2337/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2338/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2339/302]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2340/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2341/302]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2342/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2343/302]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2344/302]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2345/302]: training_loss: tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2346/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2347/302]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2348/302]: training_loss: tensor(2.2493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2349/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2350/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2351/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2352/302]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2353/302]: training_loss: tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2354/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2355/302]: training_loss: tensor(1.3266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2356/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2357/302]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2358/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2359/302]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2360/302]: training_loss: tensor(1.3521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2361/302]: training_loss: tensor(0.6584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2362/302]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2363/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2364/302]: training_loss: tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2365/302]: training_loss: tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2366/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2367/302]: training_loss: tensor(1.5558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2368/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2369/302]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2370/302]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2371/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2372/302]: training_loss: tensor(1.5871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2373/302]: training_loss: tensor(0.4143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2374/302]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2375/302]: training_loss: tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2376/302]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2377/302]: training_loss: tensor(0.9231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2378/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2379/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2380/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2381/302]: training_loss: tensor(0.7256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2382/302]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2383/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2384/302]: training_loss: tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2385/302]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2386/302]: training_loss: tensor(1.3305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2387/302]: training_loss: tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2388/302]: training_loss: tensor(0.1125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2389/302]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2390/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2391/302]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2392/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2393/302]: training_loss: tensor(0.6817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2394/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2395/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2396/302]: training_loss: tensor(0.4253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2397/302]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2398/302]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2399/302]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2400/302]: training_loss: tensor(1.2731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2401/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2402/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2403/302]: training_loss: tensor(0.3817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2404/302]: training_loss: tensor(0.4276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2405/302]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2406/302]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2407/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2408/302]: training_loss: tensor(0.3640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2409/302]: training_loss: tensor(0.7366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2410/302]: training_loss: tensor(0.4215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2411/302]: training_loss: tensor(0.4309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2412/302]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2413/302]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2414/302]: training_loss: tensor(0.3901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2415/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2416/302]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2417/302]: training_loss: tensor(0.6889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2418/302]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2419/302]: training_loss: tensor(0.7166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2420/302]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [1/3], global step [2421/7263], Train Loss: 0.2927, Valid Loss: 0.2903\n",
      "batch_no [0/302]: training_loss: tensor(2.3707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1/302]: training_loss: tensor(2.5227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2/302]: training_loss: tensor(0.6850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3/302]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [5/302]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [6/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [7/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [8/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [9/302]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [10/302]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [11/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [12/302]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [13/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [14/302]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [15/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [16/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [17/302]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [18/302]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [19/302]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [20/302]: training_loss: tensor(0.1077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [21/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [22/302]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [23/302]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [24/302]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [25/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [26/302]: training_loss: tensor(0.6421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [27/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [28/302]: training_loss: tensor(2.1122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [29/302]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [30/302]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [31/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [32/302]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [33/302]: training_loss: tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [34/302]: training_loss: tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [35/302]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [36/302]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [37/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [38/302]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [39/302]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [40/302]: training_loss: tensor(0.7351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [41/302]: training_loss: tensor(2.5581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [42/302]: training_loss: tensor(2.2610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [43/302]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [44/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [45/302]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [46/302]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [47/302]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [48/302]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [49/302]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [50/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [51/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [52/302]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [53/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [54/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [55/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [56/302]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [57/302]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [58/302]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [59/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [60/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [61/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [62/302]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [63/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [64/302]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [65/302]: training_loss: tensor(0.6175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [66/302]: training_loss: tensor(2.6596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [67/302]: training_loss: tensor(1.7186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [68/302]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [69/302]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [70/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [71/302]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [72/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [73/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [74/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [75/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [76/302]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [77/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [78/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [79/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [80/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [81/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [82/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [83/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [84/302]: training_loss: tensor(0.1044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [85/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [86/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [87/302]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [88/302]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [89/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [90/302]: training_loss: tensor(0.0648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [91/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [92/302]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [93/302]: training_loss: tensor(0.3493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [94/302]: training_loss: tensor(2.5338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [95/302]: training_loss: tensor(2.7110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [96/302]: training_loss: tensor(2.5955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [97/302]: training_loss: tensor(1.9851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [98/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [99/302]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [100/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [101/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [102/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [103/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [104/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [105/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [106/302]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [107/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [108/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [109/302]: training_loss: tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [110/302]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [111/302]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [112/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [113/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [114/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [115/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [116/302]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [117/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [118/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [119/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [120/302]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [121/302]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [122/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [123/302]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [124/302]: training_loss: tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [125/302]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [126/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [127/302]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [128/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [129/302]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [130/302]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [131/302]: training_loss: tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [132/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [133/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [134/302]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [135/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [136/302]: training_loss: tensor(1.9943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [137/302]: training_loss: tensor(2.8739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [138/302]: training_loss: tensor(2.4558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [139/302]: training_loss: tensor(2.7989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [140/302]: training_loss: tensor(1.2614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [141/302]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [142/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [143/302]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [144/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [145/302]: training_loss: tensor(0.1231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [146/302]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [147/302]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [148/302]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [149/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [150/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [151/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [152/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [153/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [154/302]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [155/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [156/302]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [157/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [158/302]: training_loss: tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [159/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [160/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [161/302]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [162/302]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [163/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [164/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [165/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [166/302]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [167/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [168/302]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [169/302]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [170/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [171/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [172/302]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [173/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [174/302]: training_loss: tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [175/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [176/302]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [177/302]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [178/302]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [179/302]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [180/302]: training_loss: tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [181/302]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [182/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [183/302]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [184/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [185/302]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [186/302]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [187/302]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [188/302]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [189/302]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [190/302]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [191/302]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [192/302]: training_loss: tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [193/302]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [194/302]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [195/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [196/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [197/302]: training_loss: tensor(2.5273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [198/302]: training_loss: tensor(2.4122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [199/302]: training_loss: tensor(2.4415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [200/302]: training_loss: tensor(2.5017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [201/302]: training_loss: tensor(2.4424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [202/302]: training_loss: tensor(1.4122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [203/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [204/302]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [205/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [206/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [207/302]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [208/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [209/302]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [210/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [211/302]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [212/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [213/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [214/302]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [215/302]: training_loss: tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [216/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [217/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [218/302]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [219/302]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [220/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [221/302]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [222/302]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [223/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [224/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [225/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [226/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [227/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [228/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [229/302]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [230/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [231/302]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [232/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [233/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [234/302]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [235/302]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [236/302]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [237/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [238/302]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [239/302]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [240/302]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [241/302]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [242/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [243/302]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [244/302]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [245/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [246/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [247/302]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [248/302]: training_loss: tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [249/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [250/302]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [251/302]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [252/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [253/302]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [254/302]: training_loss: tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [255/302]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [256/302]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [257/302]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [258/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [259/302]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [260/302]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [261/302]: training_loss: tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [262/302]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [263/302]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [264/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [265/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [266/302]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [267/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [268/302]: training_loss: tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [269/302]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [270/302]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [271/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [272/302]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [273/302]: training_loss: tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [274/302]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [275/302]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [276/302]: training_loss: tensor(1.3670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [277/302]: training_loss: tensor(2.4815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [278/302]: training_loss: tensor(2.6120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [279/302]: training_loss: tensor(2.4882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [280/302]: training_loss: tensor(2.7006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [281/302]: training_loss: tensor(2.6250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [282/302]: training_loss: tensor(2.4505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [283/302]: training_loss: tensor(1.9937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [284/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [285/302]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [286/302]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [287/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [288/302]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [289/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [290/302]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [291/302]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [292/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [293/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [294/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [295/302]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [296/302]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [297/302]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [298/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [299/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [300/302]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [301/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [302/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [303/302]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [304/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [305/302]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [306/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [307/302]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [308/302]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [309/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [310/302]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [311/302]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [312/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [313/302]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [314/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [315/302]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [316/302]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [317/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [318/302]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [319/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [320/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [321/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [322/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [323/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [324/302]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [325/302]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [326/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [327/302]: training_loss: tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [328/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [329/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [330/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [331/302]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [332/302]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [333/302]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [334/302]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [335/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [336/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [337/302]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [338/302]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [339/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [340/302]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [341/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [342/302]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [343/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [344/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [345/302]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [346/302]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [347/302]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [348/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [349/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [350/302]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [351/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [352/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [353/302]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [354/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [355/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [356/302]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [357/302]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [358/302]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [359/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [360/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [361/302]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [362/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [363/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [364/302]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [365/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [366/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [367/302]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [368/302]: training_loss: tensor(2.2992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [369/302]: training_loss: tensor(2.5817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [370/302]: training_loss: tensor(2.5657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [371/302]: training_loss: tensor(2.5118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [372/302]: training_loss: tensor(2.6346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [373/302]: training_loss: tensor(2.6210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [374/302]: training_loss: tensor(2.5310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [375/302]: training_loss: tensor(2.4046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [376/302]: training_loss: tensor(2.6245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [377/302]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [378/302]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [379/302]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [380/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [381/302]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [382/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [383/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [384/302]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [385/302]: training_loss: tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [386/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [387/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [388/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [389/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [390/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [391/302]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [392/302]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [393/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [394/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [395/302]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [396/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [397/302]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [398/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [399/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [400/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [401/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [402/302]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [403/302]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [404/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [405/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [406/302]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [407/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [408/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [409/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [410/302]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [411/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [412/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [413/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [414/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [415/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [416/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [417/302]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [418/302]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [419/302]: training_loss: tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [420/302]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [421/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [422/302]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [423/302]: training_loss: tensor(0.1019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [424/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [425/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [426/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [427/302]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [428/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [429/302]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [430/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [431/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [432/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [433/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [434/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [435/302]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [436/302]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [437/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [438/302]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [439/302]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [440/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [441/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [442/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [443/302]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [444/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [445/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [446/302]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [447/302]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [448/302]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [449/302]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [450/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [451/302]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [452/302]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [453/302]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [454/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [455/302]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [456/302]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [457/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [458/302]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [459/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [460/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [461/302]: training_loss: tensor(0.4281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [462/302]: training_loss: tensor(2.6427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [463/302]: training_loss: tensor(2.6937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [464/302]: training_loss: tensor(2.6806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [465/302]: training_loss: tensor(2.6064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [466/302]: training_loss: tensor(2.7154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [467/302]: training_loss: tensor(2.7672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [468/302]: training_loss: tensor(2.5334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [469/302]: training_loss: tensor(2.5243, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [470/302]: training_loss: tensor(2.5718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [471/302]: training_loss: tensor(1.1480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [472/302]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [473/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [474/302]: training_loss: tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [475/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [476/302]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [477/302]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [478/302]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [479/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [480/302]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [481/302]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [482/302]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [483/302]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [484/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [485/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [486/302]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [487/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [488/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [489/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [490/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [491/302]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [492/302]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [493/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [494/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [495/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [496/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [497/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [498/302]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [499/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [500/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [501/302]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [502/302]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [503/302]: training_loss: tensor(0.0686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [504/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [505/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [506/302]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [507/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [508/302]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [509/302]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [510/302]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [511/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [512/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [513/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [514/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [515/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [516/302]: training_loss: tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [517/302]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [518/302]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [519/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [520/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [521/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [522/302]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [523/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [524/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [525/302]: training_loss: tensor(0.1069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [526/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [527/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [528/302]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [529/302]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [530/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [531/302]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [532/302]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [533/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [534/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [535/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [536/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [537/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [538/302]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [539/302]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [540/302]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [541/302]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [542/302]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [543/302]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [544/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [545/302]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [546/302]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [547/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [548/302]: training_loss: tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [549/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [550/302]: training_loss: tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [551/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [552/302]: training_loss: tensor(1.6870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [553/302]: training_loss: tensor(2.6482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [554/302]: training_loss: tensor(2.6925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [555/302]: training_loss: tensor(2.5097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [556/302]: training_loss: tensor(2.5793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [557/302]: training_loss: tensor(2.4561, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [558/302]: training_loss: tensor(2.5595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [559/302]: training_loss: tensor(2.6452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [560/302]: training_loss: tensor(2.5122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [561/302]: training_loss: tensor(0.6646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [562/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [563/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [564/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [565/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [566/302]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [567/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [568/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [569/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [570/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [571/302]: training_loss: tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [572/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [573/302]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [574/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [575/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [576/302]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [577/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [578/302]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [579/302]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [580/302]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [581/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [582/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [583/302]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [584/302]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [585/302]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [586/302]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [587/302]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [588/302]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [589/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [590/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [591/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [592/302]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [593/302]: training_loss: tensor(0.1044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [594/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [595/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [596/302]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [597/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [598/302]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [599/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [600/302]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [601/302]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [602/302]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [603/302]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [604/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [605/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [606/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [607/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [608/302]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [609/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [610/302]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [611/302]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [612/302]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [613/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [614/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [615/302]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [616/302]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [617/302]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [618/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [619/302]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [620/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [621/302]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [622/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [623/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [624/302]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [625/302]: training_loss: tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [626/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [627/302]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [628/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [629/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [630/302]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [631/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [632/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [633/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [634/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [635/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [636/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [637/302]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [638/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [639/302]: training_loss: tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [640/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [641/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [642/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [643/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [644/302]: training_loss: tensor(0.9499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [645/302]: training_loss: tensor(2.4209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [646/302]: training_loss: tensor(2.6516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [647/302]: training_loss: tensor(2.5736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [648/302]: training_loss: tensor(2.6345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [649/302]: training_loss: tensor(2.3817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [650/302]: training_loss: tensor(2.6059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [651/302]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [652/302]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [653/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [654/302]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [655/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [656/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [657/302]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [658/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [659/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [660/302]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [661/302]: training_loss: tensor(0.1110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [662/302]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [663/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [664/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [665/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [666/302]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [667/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [668/302]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [669/302]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [670/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [671/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [672/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [673/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [674/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [675/302]: training_loss: tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [676/302]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [677/302]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [678/302]: training_loss: tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [679/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [680/302]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [681/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [682/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [683/302]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [684/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [685/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [686/302]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [687/302]: training_loss: tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [688/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [689/302]: training_loss: tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [690/302]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [691/302]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [692/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [693/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [694/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [695/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [696/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [697/302]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [698/302]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [699/302]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [700/302]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [701/302]: training_loss: tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [702/302]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [703/302]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [704/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [705/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [706/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [707/302]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [708/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [709/302]: training_loss: tensor(0.1146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [710/302]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [711/302]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [712/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [713/302]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [714/302]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [715/302]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [716/302]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [717/302]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [718/302]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [719/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [720/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [721/302]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [722/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [723/302]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [724/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [725/302]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [726/302]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [727/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [728/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [729/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [730/302]: training_loss: tensor(1.9544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [731/302]: training_loss: tensor(2.5473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [732/302]: training_loss: tensor(2.6604, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [733/302]: training_loss: tensor(2.6605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [734/302]: training_loss: tensor(2.6133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [735/302]: training_loss: tensor(2.3960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [736/302]: training_loss: tensor(2.4436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [737/302]: training_loss: tensor(2.5684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [738/302]: training_loss: tensor(2.2665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [739/302]: training_loss: tensor(1.8505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [740/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [741/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [742/302]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [743/302]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [744/302]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [745/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [746/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [747/302]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [748/302]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [749/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [750/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [751/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [752/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [753/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [754/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [755/302]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [756/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [757/302]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [758/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [759/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [760/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [761/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [762/302]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [763/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [764/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [765/302]: training_loss: tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [766/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [767/302]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [768/302]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [769/302]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [770/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [771/302]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [772/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [773/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [774/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [775/302]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [776/302]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [777/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [778/302]: training_loss: tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [779/302]: training_loss: tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [780/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [781/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [782/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [783/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [784/302]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [785/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [786/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [787/302]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [788/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [789/302]: training_loss: tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [790/302]: training_loss: tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [791/302]: training_loss: tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [792/302]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [793/302]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [794/302]: training_loss: tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [795/302]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [796/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [797/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [798/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [799/302]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [800/302]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [801/302]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [802/302]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [803/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [804/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [805/302]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [806/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [807/302]: training_loss: tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [808/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [809/302]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [810/302]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [811/302]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [812/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [813/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [814/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [815/302]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [816/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [817/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [818/302]: training_loss: tensor(1.8084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [819/302]: training_loss: tensor(2.5581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [820/302]: training_loss: tensor(2.8076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [821/302]: training_loss: tensor(2.5782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [822/302]: training_loss: tensor(2.4610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [823/302]: training_loss: tensor(2.6177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [824/302]: training_loss: tensor(2.5458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [825/302]: training_loss: tensor(0.3710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [826/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [827/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [828/302]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [829/302]: training_loss: tensor(0.1069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [830/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [831/302]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [832/302]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [833/302]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [834/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [835/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [836/302]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [837/302]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [838/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [839/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [840/302]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [841/302]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [842/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [843/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [844/302]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [845/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [846/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [847/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [848/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [849/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [850/302]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [851/302]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [852/302]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [853/302]: training_loss: tensor(0.1133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [854/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [855/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [856/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [857/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [858/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [859/302]: training_loss: tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [860/302]: training_loss: tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [861/302]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [862/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [863/302]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [864/302]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [865/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [866/302]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [867/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [868/302]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [869/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [870/302]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [871/302]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [872/302]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [873/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [874/302]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [875/302]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [876/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [877/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [878/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [879/302]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [880/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [881/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [882/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [883/302]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [884/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [885/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [886/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [887/302]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [888/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [889/302]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [890/302]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [891/302]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [892/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [893/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [894/302]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [895/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [896/302]: training_loss: tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [897/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [898/302]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [899/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [900/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [901/302]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [902/302]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [903/302]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [904/302]: training_loss: tensor(0.3849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [905/302]: training_loss: tensor(2.5858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [906/302]: training_loss: tensor(2.8258, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [907/302]: training_loss: tensor(2.4967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [908/302]: training_loss: tensor(2.5119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [909/302]: training_loss: tensor(2.6931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [910/302]: training_loss: tensor(2.4240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [911/302]: training_loss: tensor(2.7118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [912/302]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [913/302]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [914/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [915/302]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [916/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [917/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [918/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [919/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [920/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [921/302]: training_loss: tensor(0.1043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [922/302]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [923/302]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [924/302]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [925/302]: training_loss: tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [926/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [927/302]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [928/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [929/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [930/302]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [931/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [932/302]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [933/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [934/302]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [935/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [936/302]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [937/302]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [938/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [939/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [940/302]: training_loss: tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [941/302]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [942/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [943/302]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [944/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [945/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [946/302]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [947/302]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [948/302]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [949/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [950/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [951/302]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [952/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [953/302]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [954/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [955/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [956/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [957/302]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [958/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [959/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [960/302]: training_loss: tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [961/302]: training_loss: tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [962/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [963/302]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [964/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [965/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [966/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [967/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [968/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [969/302]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [970/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [971/302]: training_loss: tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [972/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [973/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [974/302]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [975/302]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [976/302]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [977/302]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [978/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [979/302]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [980/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [981/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [982/302]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [983/302]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [984/302]: training_loss: tensor(2.2543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [985/302]: training_loss: tensor(2.3977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [986/302]: training_loss: tensor(2.4332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [987/302]: training_loss: tensor(2.5856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [988/302]: training_loss: tensor(2.5951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [989/302]: training_loss: tensor(2.4496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [990/302]: training_loss: tensor(2.5645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [991/302]: training_loss: tensor(2.4532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [992/302]: training_loss: tensor(0.9067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [993/302]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [994/302]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [995/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [996/302]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [997/302]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [998/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [999/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1000/302]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1001/302]: training_loss: tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1002/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1003/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1004/302]: training_loss: tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1005/302]: training_loss: tensor(0.1107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/302]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/302]: training_loss: tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/302]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/302]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/302]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1019/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/302]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/302]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1023/302]: training_loss: tensor(0.1127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/302]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/302]: training_loss: tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/302]: training_loss: tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/302]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/302]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/302]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1040/302]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1043/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/302]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/302]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/302]: training_loss: tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/302]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/302]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/302]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/302]: training_loss: tensor(0.1071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/302]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/302]: training_loss: tensor(2.1573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/302]: training_loss: tensor(2.6475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/302]: training_loss: tensor(2.5162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/302]: training_loss: tensor(2.5982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/302]: training_loss: tensor(2.4367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/302]: training_loss: tensor(2.5710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/302]: training_loss: tensor(1.9939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/302]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/302]: training_loss: tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/302]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/302]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/302]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/302]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/302]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/302]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1082/302]: training_loss: tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/302]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/302]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/302]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/302]: training_loss: tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/302]: training_loss: tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/302]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/302]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/302]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1100/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/302]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/302]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/302]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/302]: training_loss: tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/302]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/302]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/302]: training_loss: tensor(0.1132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/302]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/302]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/302]: training_loss: tensor(0.1043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/302]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/302]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/302]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/302]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1129/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/302]: training_loss: tensor(2.4134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/302]: training_loss: tensor(2.4899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/302]: training_loss: tensor(2.5179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/302]: training_loss: tensor(2.5180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/302]: training_loss: tensor(1.4882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/302]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/302]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/302]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/302]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/302]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/302]: training_loss: tensor(0.1153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/302]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/302]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/302]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/302]: training_loss: tensor(0.1105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/302]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/302]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/302]: training_loss: tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/302]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/302]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/302]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/302]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/302]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/302]: training_loss: tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/302]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/302]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/302]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1169/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/302]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/302]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/302]: training_loss: tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/302]: training_loss: tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/302]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/302]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/302]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/302]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/302]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/302]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/302]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/302]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/302]: training_loss: tensor(1.8170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/302]: training_loss: tensor(2.4505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/302]: training_loss: tensor(2.4956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/302]: training_loss: tensor(2.4485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/302]: training_loss: tensor(2.5556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/302]: training_loss: tensor(2.5039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/302]: training_loss: tensor(0.4110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/302]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/302]: training_loss: tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/302]: training_loss: tensor(0.1114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/302]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/302]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/302]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/302]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1213/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/302]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1215/302]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/302]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/302]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/302]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/302]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/302]: training_loss: tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/302]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/302]: training_loss: tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/302]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/302]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/302]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/302]: training_loss: tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/302]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/302]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/302]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/302]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/302]: training_loss: tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/302]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/302]: training_loss: tensor(2.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/302]: training_loss: tensor(2.4677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/302]: training_loss: tensor(2.5397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/302]: training_loss: tensor(2.5017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/302]: training_loss: tensor(2.5177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/302]: training_loss: tensor(0.6354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/302]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/302]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/302]: training_loss: tensor(0.1083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/302]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/302]: training_loss: tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/302]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/302]: training_loss: tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/302]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/302]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/302]: training_loss: tensor(0.1069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/302]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/302]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/302]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/302]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/302]: training_loss: tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/302]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/302]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/302]: training_loss: tensor(0.1168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1299/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/302]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/302]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/302]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/302]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/302]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/302]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/302]: training_loss: tensor(1.9032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/302]: training_loss: tensor(2.5537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/302]: training_loss: tensor(2.5909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/302]: training_loss: tensor(2.4407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/302]: training_loss: tensor(2.5040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/302]: training_loss: tensor(1.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/302]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/302]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/302]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/302]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/302]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/302]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/302]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/302]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/302]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/302]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/302]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/302]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/302]: training_loss: tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/302]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/302]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/302]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/302]: training_loss: tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1342/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/302]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/302]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/302]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/302]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1361/302]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/302]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1363/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/302]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/302]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/302]: training_loss: tensor(0.6982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/302]: training_loss: tensor(2.6281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/302]: training_loss: tensor(2.4253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/302]: training_loss: tensor(2.5156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/302]: training_loss: tensor(1.5824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/302]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/302]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/302]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/302]: training_loss: tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/302]: training_loss: tensor(0.1059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/302]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1385/302]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1386/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1387/302]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1388/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1389/302]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1390/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1391/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1392/302]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1393/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1394/302]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1395/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1396/302]: training_loss: tensor(0.1127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1397/302]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1398/302]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1399/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1400/302]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1401/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1402/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1403/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1404/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1405/302]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1406/302]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1407/302]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1408/302]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1409/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1410/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1411/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1412/302]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1413/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1414/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1415/302]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1416/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1417/302]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1418/302]: training_loss: tensor(2.3189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1419/302]: training_loss: tensor(2.5170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1420/302]: training_loss: tensor(2.4467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1421/302]: training_loss: tensor(2.6714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1422/302]: training_loss: tensor(1.6488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1423/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1424/302]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1425/302]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1426/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1427/302]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1428/302]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1429/302]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1430/302]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1431/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1432/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1433/302]: training_loss: tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1434/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1435/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1436/302]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1437/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1438/302]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1439/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1440/302]: training_loss: tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1441/302]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1442/302]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1443/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1444/302]: training_loss: tensor(0.1076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1445/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1446/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1447/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1448/302]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1449/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1450/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1451/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1452/302]: training_loss: tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1453/302]: training_loss: tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1454/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1455/302]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1456/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1457/302]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1458/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1459/302]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1460/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1461/302]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1462/302]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1463/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1464/302]: training_loss: tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1465/302]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1466/302]: training_loss: tensor(0.6067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1467/302]: training_loss: tensor(2.5624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1468/302]: training_loss: tensor(2.4884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1469/302]: training_loss: tensor(2.4215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1470/302]: training_loss: tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1471/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1472/302]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1473/302]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1474/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1475/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1476/302]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1477/302]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1478/302]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1479/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1480/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1481/302]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1482/302]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1483/302]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1484/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1485/302]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1486/302]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1487/302]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1488/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1489/302]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1490/302]: training_loss: tensor(0.1064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1491/302]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1492/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1493/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1494/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1495/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1496/302]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1497/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1498/302]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1499/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1500/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1501/302]: training_loss: tensor(0.1095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1502/302]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1503/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1504/302]: training_loss: tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1505/302]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1506/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1507/302]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1508/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1509/302]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1510/302]: training_loss: tensor(1.8920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1511/302]: training_loss: tensor(2.4736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1512/302]: training_loss: tensor(1.4571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1513/302]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1514/302]: training_loss: tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1515/302]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1516/302]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1517/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1518/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1519/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1520/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1521/302]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1522/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1523/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1524/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1525/302]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1526/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1527/302]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1528/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1529/302]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1530/302]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1531/302]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1532/302]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1533/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1534/302]: training_loss: tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1535/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1536/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1537/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1538/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1539/302]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1540/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1541/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1542/302]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1543/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1544/302]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1545/302]: training_loss: tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1546/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1547/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1548/302]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1549/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1550/302]: training_loss: tensor(1.6208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1551/302]: training_loss: tensor(2.5039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1552/302]: training_loss: tensor(2.5560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1553/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1554/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1555/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1556/302]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1557/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1558/302]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1559/302]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1560/302]: training_loss: tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1561/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1562/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1563/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1564/302]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1565/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1566/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1567/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1568/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1569/302]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1570/302]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1571/302]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1572/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1573/302]: training_loss: tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1574/302]: training_loss: tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1575/302]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1576/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1577/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1578/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1579/302]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1580/302]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1581/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1582/302]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1583/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1584/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1585/302]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1586/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1587/302]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1588/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1589/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1590/302]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1591/302]: training_loss: tensor(2.5366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1592/302]: training_loss: tensor(2.5225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1593/302]: training_loss: tensor(2.5414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1594/302]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1595/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1596/302]: training_loss: tensor(0.1066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1597/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1598/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1599/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1600/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1601/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1602/302]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1603/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1604/302]: training_loss: tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1605/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1606/302]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1607/302]: training_loss: tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1608/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1609/302]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1610/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1611/302]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1612/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1613/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1614/302]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1615/302]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1616/302]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1617/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1618/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1619/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1620/302]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1621/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1622/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1623/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1624/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1625/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1626/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1627/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1628/302]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1629/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1630/302]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1631/302]: training_loss: tensor(2.2534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1632/302]: training_loss: tensor(2.5129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1633/302]: training_loss: tensor(2.1684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1634/302]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1635/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1636/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1637/302]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1638/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1639/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1640/302]: training_loss: tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1641/302]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1642/302]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1643/302]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1644/302]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1645/302]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1646/302]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1647/302]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1648/302]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1649/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1650/302]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1651/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1652/302]: training_loss: tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1653/302]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1654/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1655/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1656/302]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1657/302]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1658/302]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1659/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1660/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1661/302]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1662/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1663/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1664/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1665/302]: training_loss: tensor(1.5945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1666/302]: training_loss: tensor(2.5293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1667/302]: training_loss: tensor(1.9510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1668/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1669/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1670/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1671/302]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1672/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1673/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1674/302]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1675/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1676/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1677/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1678/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1679/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1680/302]: training_loss: tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1681/302]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1682/302]: training_loss: tensor(0.1121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1683/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1684/302]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1685/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1686/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1687/302]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1688/302]: training_loss: tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1689/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1690/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1691/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1692/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1693/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1694/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1695/302]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1696/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1697/302]: training_loss: tensor(0.6747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1698/302]: training_loss: tensor(2.5766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1699/302]: training_loss: tensor(2.7098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1700/302]: training_loss: tensor(0.7184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1701/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1702/302]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1703/302]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1704/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1705/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1706/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1707/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1708/302]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1709/302]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1710/302]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1711/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1712/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1713/302]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1714/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1715/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1716/302]: training_loss: tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1717/302]: training_loss: tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1718/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1719/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1720/302]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1721/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1722/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1723/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1724/302]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1725/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1726/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1727/302]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1728/302]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1729/302]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1730/302]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1731/302]: training_loss: tensor(1.3714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1732/302]: training_loss: tensor(2.4970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1733/302]: training_loss: tensor(2.7249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1734/302]: training_loss: tensor(0.6873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1735/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1736/302]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1737/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1738/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1739/302]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1740/302]: training_loss: tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1741/302]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1742/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1743/302]: training_loss: tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1744/302]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1745/302]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1746/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1747/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1748/302]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1749/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1750/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1751/302]: training_loss: tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1752/302]: training_loss: tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1753/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1754/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1755/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1756/302]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1757/302]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1758/302]: training_loss: tensor(0.7758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1759/302]: training_loss: tensor(2.5761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1760/302]: training_loss: tensor(2.6036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1761/302]: training_loss: tensor(1.3234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1762/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1763/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1764/302]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1765/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1766/302]: training_loss: tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1767/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1768/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1769/302]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1770/302]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1771/302]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1772/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1773/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1774/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1775/302]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1776/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1777/302]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1778/302]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1779/302]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1780/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1781/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1782/302]: training_loss: tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1783/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1784/302]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1785/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1786/302]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1787/302]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1788/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1789/302]: training_loss: tensor(1.5377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1790/302]: training_loss: tensor(1.2292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1791/302]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1792/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1793/302]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1794/302]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1795/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1796/302]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1797/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1798/302]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1799/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1800/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1801/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1802/302]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1803/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1804/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1805/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1806/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1807/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1808/302]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1809/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1810/302]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1811/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1812/302]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1813/302]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1814/302]: training_loss: tensor(0.4066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1815/302]: training_loss: tensor(2.5800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1816/302]: training_loss: tensor(2.5103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1817/302]: training_loss: tensor(0.3687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1818/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1819/302]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1820/302]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1821/302]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1822/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1823/302]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1824/302]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1825/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1826/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1827/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1828/302]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1829/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1830/302]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1831/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1832/302]: training_loss: tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1833/302]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1834/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1835/302]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1836/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1837/302]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1838/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1839/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1840/302]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1841/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1842/302]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1843/302]: training_loss: tensor(2.6301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1844/302]: training_loss: tensor(2.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1845/302]: training_loss: tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1846/302]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1847/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1848/302]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1849/302]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1850/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1851/302]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1852/302]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1853/302]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1854/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1855/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1856/302]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1857/302]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1858/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1859/302]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1860/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1861/302]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1862/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1863/302]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1864/302]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1865/302]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1866/302]: training_loss: tensor(1.9781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1867/302]: training_loss: tensor(2.7449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1868/302]: training_loss: tensor(1.3190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1869/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1870/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1871/302]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1872/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1873/302]: training_loss: tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1874/302]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1875/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1876/302]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1877/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1878/302]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1879/302]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1880/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1881/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1882/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1883/302]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1884/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1885/302]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1886/302]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1887/302]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1888/302]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1889/302]: training_loss: tensor(0.6830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1890/302]: training_loss: tensor(2.4130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1891/302]: training_loss: tensor(2.8124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1892/302]: training_loss: tensor(1.3441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1893/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1894/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1895/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1896/302]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1897/302]: training_loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1898/302]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1899/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1900/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1901/302]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1902/302]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1903/302]: training_loss: tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1904/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1905/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1906/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1907/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1908/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1909/302]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1910/302]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1911/302]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1912/302]: training_loss: tensor(0.4217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1913/302]: training_loss: tensor(2.6564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1914/302]: training_loss: tensor(1.9218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1915/302]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1916/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1917/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1918/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1919/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1920/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1921/302]: training_loss: tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1922/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1923/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1924/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1925/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1926/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1927/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1928/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1929/302]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1930/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1931/302]: training_loss: tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1932/302]: training_loss: tensor(0.7155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1933/302]: training_loss: tensor(2.3686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1934/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1935/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1936/302]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1937/302]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1938/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1939/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1940/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1941/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1942/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1943/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1944/302]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1945/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1946/302]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1947/302]: training_loss: tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1948/302]: training_loss: tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1949/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1950/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1951/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1952/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1953/302]: training_loss: tensor(1.6138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1954/302]: training_loss: tensor(1.1440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1955/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1956/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1957/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1958/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1959/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1960/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1961/302]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1962/302]: training_loss: tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1963/302]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1964/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1965/302]: training_loss: tensor(0.1044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1966/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1967/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1968/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1969/302]: training_loss: tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1970/302]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1971/302]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1972/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1973/302]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1974/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1975/302]: training_loss: tensor(0.4155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1976/302]: training_loss: tensor(2.6361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1977/302]: training_loss: tensor(1.3935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1978/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1979/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1980/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1981/302]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1982/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1983/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1984/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1985/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1986/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1987/302]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1988/302]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1989/302]: training_loss: tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1990/302]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1991/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1992/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1993/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1994/302]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1995/302]: training_loss: tensor(0.4131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1996/302]: training_loss: tensor(2.5926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1997/302]: training_loss: tensor(2.6190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1998/302]: training_loss: tensor(0.4493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1999/302]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2000/302]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2001/302]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2002/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2003/302]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2004/302]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2005/302]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2006/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2007/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2008/302]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2009/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2010/302]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2011/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2012/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2013/302]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2014/302]: training_loss: tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2015/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2016/302]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2017/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2018/302]: training_loss: tensor(2.1964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2019/302]: training_loss: tensor(1.2993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2020/302]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2021/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2022/302]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2023/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2024/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2025/302]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2026/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2027/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2028/302]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2029/302]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2030/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2031/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2032/302]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2033/302]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2034/302]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2035/302]: training_loss: tensor(2.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2036/302]: training_loss: tensor(1.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2037/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2038/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2039/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2040/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2041/302]: training_loss: tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2042/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2043/302]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2044/302]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2045/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2046/302]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2047/302]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2048/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2049/302]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2050/302]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2051/302]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2052/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2053/302]: training_loss: tensor(1.1702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2054/302]: training_loss: tensor(1.3886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2055/302]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2056/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2057/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2058/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2059/302]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2060/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2061/302]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2062/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2063/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2064/302]: training_loss: tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2065/302]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2066/302]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2067/302]: training_loss: tensor(0.1090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2068/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2069/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2070/302]: training_loss: tensor(2.1140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2071/302]: training_loss: tensor(1.9338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2072/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2073/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2074/302]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2075/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2076/302]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2077/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2078/302]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2079/302]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2080/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2081/302]: training_loss: tensor(0.1043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2082/302]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2083/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2084/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2085/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2086/302]: training_loss: tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2087/302]: training_loss: tensor(1.4355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2088/302]: training_loss: tensor(1.6064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2089/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2090/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2091/302]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2092/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2093/302]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2094/302]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2095/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2096/302]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2097/302]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2098/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2099/302]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2100/302]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2101/302]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2102/302]: training_loss: tensor(0.9367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2103/302]: training_loss: tensor(1.6017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2104/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2105/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2106/302]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2107/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2108/302]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2109/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2110/302]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2111/302]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2112/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2113/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2114/302]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2115/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2116/302]: training_loss: tensor(0.7384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2117/302]: training_loss: tensor(1.6314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2118/302]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2119/302]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2120/302]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2121/302]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2122/302]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2123/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2124/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2125/302]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2126/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2127/302]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2128/302]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2129/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2130/302]: training_loss: tensor(0.7327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2131/302]: training_loss: tensor(1.6660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2132/302]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2133/302]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2134/302]: training_loss: tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2135/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2136/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2137/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2138/302]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2139/302]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2140/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2141/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2142/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2143/302]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2144/302]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2145/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2146/302]: training_loss: tensor(1.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2147/302]: training_loss: tensor(1.3759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2148/302]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2149/302]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2150/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2151/302]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2152/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2153/302]: training_loss: tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2154/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2155/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2156/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2157/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2158/302]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2159/302]: training_loss: tensor(0.2962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2160/302]: training_loss: tensor(2.5778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2161/302]: training_loss: tensor(0.4252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2162/302]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2163/302]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2164/302]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2165/302]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2166/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2167/302]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2168/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2169/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2170/302]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2171/302]: training_loss: tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2172/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2173/302]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2174/302]: training_loss: tensor(1.6096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2175/302]: training_loss: tensor(1.8121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2176/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2177/302]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2178/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2179/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2180/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2181/302]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2182/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2183/302]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2184/302]: training_loss: tensor(0.0666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2185/302]: training_loss: tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2186/302]: training_loss: tensor(0.9761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2187/302]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2188/302]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2189/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2190/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2191/302]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2192/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2193/302]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2194/302]: training_loss: tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2195/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2196/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2197/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2198/302]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2199/302]: training_loss: tensor(0.6782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2200/302]: training_loss: tensor(2.2599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2201/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2202/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2203/302]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2204/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2205/302]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2206/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2207/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2208/302]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2209/302]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2210/302]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2211/302]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2212/302]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2213/302]: training_loss: tensor(2.4312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2214/302]: training_loss: tensor(0.9716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2215/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2216/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2217/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2218/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2219/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2220/302]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2221/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2222/302]: training_loss: tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2223/302]: training_loss: tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2224/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2225/302]: training_loss: tensor(0.4046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2226/302]: training_loss: tensor(2.5309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2227/302]: training_loss: tensor(0.7641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2228/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2229/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2230/302]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2231/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2232/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2233/302]: training_loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2234/302]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2235/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2236/302]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2237/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2238/302]: training_loss: tensor(1.6807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2239/302]: training_loss: tensor(0.9894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2240/302]: training_loss: tensor(0.1044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2241/302]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2242/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2243/302]: training_loss: tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2244/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2245/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2246/302]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2247/302]: training_loss: tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2248/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2249/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2250/302]: training_loss: tensor(2.4875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2251/302]: training_loss: tensor(0.7283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2252/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2253/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2254/302]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2255/302]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2256/302]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2257/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2258/302]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2259/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2260/302]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2261/302]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2262/302]: training_loss: tensor(1.6743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2263/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2264/302]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2265/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2266/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2267/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2268/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2269/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2270/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2271/302]: training_loss: tensor(0.3545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2272/302]: training_loss: tensor(2.5697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2273/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2274/302]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2275/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2276/302]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2277/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2278/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2279/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2280/302]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2281/302]: training_loss: tensor(2.4215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2282/302]: training_loss: tensor(0.0648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2283/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2284/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2285/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2286/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2287/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2288/302]: training_loss: tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2289/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2290/302]: training_loss: tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2291/302]: training_loss: tensor(1.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2292/302]: training_loss: tensor(2.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2293/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2294/302]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2295/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2296/302]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2297/302]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2298/302]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2299/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2300/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2301/302]: training_loss: tensor(1.2728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2302/302]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2303/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2304/302]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2305/302]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2306/302]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2307/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2308/302]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2309/302]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2310/302]: training_loss: tensor(0.2690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2311/302]: training_loss: tensor(2.2090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2312/302]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2313/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2314/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2315/302]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2316/302]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2317/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2318/302]: training_loss: tensor(0.1083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2319/302]: training_loss: tensor(1.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2320/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2321/302]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2322/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2323/302]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2324/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2325/302]: training_loss: tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2326/302]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2327/302]: training_loss: tensor(1.2904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2328/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2329/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2330/302]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2331/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2332/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2333/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2334/302]: training_loss: tensor(0.7145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2335/302]: training_loss: tensor(0.4321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2336/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2337/302]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2338/302]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2339/302]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2340/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2341/302]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2342/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2343/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2344/302]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2345/302]: training_loss: tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2346/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2347/302]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2348/302]: training_loss: tensor(2.2938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2349/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2350/302]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2351/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2352/302]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2353/302]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2354/302]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2355/302]: training_loss: tensor(1.2321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2356/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2357/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2358/302]: training_loss: tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2359/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2360/302]: training_loss: tensor(1.3545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2361/302]: training_loss: tensor(0.5974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2362/302]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2363/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2364/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2365/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2366/302]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2367/302]: training_loss: tensor(1.6656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2368/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2369/302]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2370/302]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2371/302]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2372/302]: training_loss: tensor(1.6136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2373/302]: training_loss: tensor(0.4154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2374/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2375/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2376/302]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2377/302]: training_loss: tensor(1.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2378/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2379/302]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2380/302]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2381/302]: training_loss: tensor(0.6889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2382/302]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2383/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2384/302]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2385/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2386/302]: training_loss: tensor(1.2443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2387/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2388/302]: training_loss: tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2389/302]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2390/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2391/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2392/302]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2393/302]: training_loss: tensor(0.7759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2394/302]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2395/302]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2396/302]: training_loss: tensor(0.4331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2397/302]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2398/302]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2399/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2400/302]: training_loss: tensor(1.1969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2401/302]: training_loss: tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2402/302]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2403/302]: training_loss: tensor(0.4155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2404/302]: training_loss: tensor(0.4058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2405/302]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2406/302]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2407/302]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2408/302]: training_loss: tensor(0.4108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2409/302]: training_loss: tensor(0.6574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2410/302]: training_loss: tensor(0.3962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2411/302]: training_loss: tensor(0.3874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2412/302]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2413/302]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2414/302]: training_loss: tensor(0.4279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2415/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2416/302]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2417/302]: training_loss: tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2418/302]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2419/302]: training_loss: tensor(0.7023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2420/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [2/3], global step [4842/7263], Train Loss: 0.2946, Valid Loss: 0.2900\n",
      "batch_no [0/302]: training_loss: tensor(2.5036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1/302]: training_loss: tensor(2.7800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2/302]: training_loss: tensor(0.6724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3/302]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [5/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [6/302]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [7/302]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [8/302]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [9/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [10/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [11/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [12/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [13/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [14/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [15/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [16/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [17/302]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [18/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [19/302]: training_loss: tensor(0.1066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [20/302]: training_loss: tensor(0.1114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [21/302]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [22/302]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [23/302]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [24/302]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [25/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [26/302]: training_loss: tensor(0.6708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [27/302]: training_loss: tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [28/302]: training_loss: tensor(2.2334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [29/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [30/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [31/302]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [32/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [33/302]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [34/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [35/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [36/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [37/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [38/302]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [39/302]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [40/302]: training_loss: tensor(0.7094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [41/302]: training_loss: tensor(2.5233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [42/302]: training_loss: tensor(2.1932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [43/302]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [44/302]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [45/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [46/302]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [47/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [48/302]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [49/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [50/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [51/302]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [52/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [53/302]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [54/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [55/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [56/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [57/302]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [58/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [59/302]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [60/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [61/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [62/302]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [63/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [64/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [65/302]: training_loss: tensor(0.6832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [66/302]: training_loss: tensor(2.4586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [67/302]: training_loss: tensor(1.7655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [68/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [69/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [70/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [71/302]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [72/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [73/302]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [74/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [75/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [76/302]: training_loss: tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [77/302]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [78/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [79/302]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [80/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [81/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [82/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [83/302]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [84/302]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [85/302]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [86/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [87/302]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [88/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [89/302]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [90/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [91/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [92/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [93/302]: training_loss: tensor(0.3800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [94/302]: training_loss: tensor(2.6995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [95/302]: training_loss: tensor(2.4290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [96/302]: training_loss: tensor(2.4291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [97/302]: training_loss: tensor(1.9106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [98/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [99/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [100/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [101/302]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [102/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [103/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [104/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [105/302]: training_loss: tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [106/302]: training_loss: tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [107/302]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [108/302]: training_loss: tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [109/302]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [110/302]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [111/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [112/302]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [113/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [114/302]: training_loss: tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [115/302]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [116/302]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [117/302]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [118/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [119/302]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [120/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [121/302]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [122/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [123/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [124/302]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [125/302]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [126/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [127/302]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [128/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [129/302]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [130/302]: training_loss: tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [131/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [132/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [133/302]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [134/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [135/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [136/302]: training_loss: tensor(1.9343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [137/302]: training_loss: tensor(2.6870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [138/302]: training_loss: tensor(2.5670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [139/302]: training_loss: tensor(2.3945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [140/302]: training_loss: tensor(1.4026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [141/302]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [142/302]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [143/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [144/302]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [145/302]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [146/302]: training_loss: tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [147/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [148/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [149/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [150/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [151/302]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [152/302]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [153/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [154/302]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [155/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [156/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [157/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [158/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [159/302]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [160/302]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [161/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [162/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [163/302]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [164/302]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [165/302]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [166/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [167/302]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [168/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [169/302]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [170/302]: training_loss: tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [171/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [172/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [173/302]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [174/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [175/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [176/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [177/302]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [178/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [179/302]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [180/302]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [181/302]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [182/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [183/302]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [184/302]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [185/302]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [186/302]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [187/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [188/302]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [189/302]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [190/302]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [191/302]: training_loss: tensor(0.1110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [192/302]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [193/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [194/302]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [195/302]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [196/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [197/302]: training_loss: tensor(2.5089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [198/302]: training_loss: tensor(2.4691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [199/302]: training_loss: tensor(2.5095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [200/302]: training_loss: tensor(2.4135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [201/302]: training_loss: tensor(2.6656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [202/302]: training_loss: tensor(1.3287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [203/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [204/302]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [205/302]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [206/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [207/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [208/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [209/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [210/302]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [211/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [212/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [213/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [214/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [215/302]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [216/302]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [217/302]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [218/302]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [219/302]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [220/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [221/302]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [222/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [223/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [224/302]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [225/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [226/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [227/302]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [228/302]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [229/302]: training_loss: tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [230/302]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [231/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [232/302]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [233/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [234/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [235/302]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [236/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [237/302]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [238/302]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [239/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [240/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [241/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [242/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [243/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [244/302]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [245/302]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [246/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [247/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [248/302]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [249/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [250/302]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [251/302]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [252/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [253/302]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [254/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [255/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [256/302]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [257/302]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [258/302]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [259/302]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [260/302]: training_loss: tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [261/302]: training_loss: tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [262/302]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [263/302]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [264/302]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [265/302]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [266/302]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [267/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [268/302]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [269/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [270/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [271/302]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [272/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [273/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [274/302]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [275/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [276/302]: training_loss: tensor(1.3409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [277/302]: training_loss: tensor(2.5729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [278/302]: training_loss: tensor(2.6214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [279/302]: training_loss: tensor(2.6878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [280/302]: training_loss: tensor(2.6204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [281/302]: training_loss: tensor(2.4588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [282/302]: training_loss: tensor(2.5580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [283/302]: training_loss: tensor(1.9232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [284/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [285/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [286/302]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [287/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [288/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [289/302]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [290/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [291/302]: training_loss: tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [292/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [293/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [294/302]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [295/302]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [296/302]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [297/302]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [298/302]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [299/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [300/302]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [301/302]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [302/302]: training_loss: tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [303/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [304/302]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [305/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [306/302]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [307/302]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [308/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [309/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [310/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [311/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [312/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [313/302]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [314/302]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [315/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [316/302]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [317/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [318/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [319/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [320/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [321/302]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [322/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [323/302]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [324/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [325/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [326/302]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [327/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [328/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [329/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [330/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [331/302]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [332/302]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [333/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [334/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [335/302]: training_loss: tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [336/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [337/302]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [338/302]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [339/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [340/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [341/302]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [342/302]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [343/302]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [344/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [345/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [346/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [347/302]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [348/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [349/302]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [350/302]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [351/302]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [352/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [353/302]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [354/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [355/302]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [356/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [357/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [358/302]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [359/302]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [360/302]: training_loss: tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [361/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [362/302]: training_loss: tensor(0.1077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [363/302]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [364/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [365/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [366/302]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [367/302]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [368/302]: training_loss: tensor(2.2640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [369/302]: training_loss: tensor(2.6002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [370/302]: training_loss: tensor(2.7027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [371/302]: training_loss: tensor(2.6340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [372/302]: training_loss: tensor(2.4877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [373/302]: training_loss: tensor(2.5226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [374/302]: training_loss: tensor(2.5249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [375/302]: training_loss: tensor(2.4929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [376/302]: training_loss: tensor(2.3226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [377/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [378/302]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [379/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [380/302]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [381/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [382/302]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [383/302]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [384/302]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [385/302]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [386/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [387/302]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [388/302]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [389/302]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [390/302]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [391/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [392/302]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [393/302]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [394/302]: training_loss: tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [395/302]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [396/302]: training_loss: tensor(0.1085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [397/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [398/302]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [399/302]: training_loss: tensor(0.1103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [400/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [401/302]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [402/302]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [403/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [404/302]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [405/302]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [406/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [407/302]: training_loss: tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [408/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [409/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [410/302]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [411/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [412/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [413/302]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [414/302]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [415/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [416/302]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [417/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [418/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [419/302]: training_loss: tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [420/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [421/302]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [422/302]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [423/302]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [424/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [425/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [426/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [427/302]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [428/302]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [429/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [430/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [431/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [432/302]: training_loss: tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [433/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [434/302]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [435/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [436/302]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [437/302]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [438/302]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [439/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [440/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [441/302]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [442/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [443/302]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [444/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [445/302]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [446/302]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [447/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [448/302]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [449/302]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [450/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [451/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [452/302]: training_loss: tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [453/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [454/302]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [455/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [456/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [457/302]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [458/302]: training_loss: tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [459/302]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [460/302]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [461/302]: training_loss: tensor(0.3692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [462/302]: training_loss: tensor(2.4538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [463/302]: training_loss: tensor(2.5171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [464/302]: training_loss: tensor(2.5826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [465/302]: training_loss: tensor(2.5529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [466/302]: training_loss: tensor(2.5863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [467/302]: training_loss: tensor(2.6808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [468/302]: training_loss: tensor(2.4574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [469/302]: training_loss: tensor(2.5275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [470/302]: training_loss: tensor(2.5675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [471/302]: training_loss: tensor(1.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [472/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [473/302]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [474/302]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [475/302]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [476/302]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [477/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [478/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [479/302]: training_loss: tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [480/302]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [481/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [482/302]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [483/302]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [484/302]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [485/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [486/302]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [487/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [488/302]: training_loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [489/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [490/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [491/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [492/302]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [493/302]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [494/302]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [495/302]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [496/302]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [497/302]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [498/302]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [499/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [500/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [501/302]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [502/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [503/302]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [504/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [505/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [506/302]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [507/302]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [508/302]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [509/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [510/302]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [511/302]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [512/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [513/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [514/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [515/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [516/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [517/302]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [518/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [519/302]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [520/302]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [521/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [522/302]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [523/302]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [524/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [525/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [526/302]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [527/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [528/302]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [529/302]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [530/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [531/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [532/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [533/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [534/302]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [535/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [536/302]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [537/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [538/302]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [539/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [540/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [541/302]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [542/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [543/302]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [544/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [545/302]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [546/302]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [547/302]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [548/302]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [549/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [550/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [551/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [552/302]: training_loss: tensor(1.4617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [553/302]: training_loss: tensor(2.4288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [554/302]: training_loss: tensor(2.5208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [555/302]: training_loss: tensor(2.7989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [556/302]: training_loss: tensor(2.5553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [557/302]: training_loss: tensor(2.5014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [558/302]: training_loss: tensor(2.4484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [559/302]: training_loss: tensor(2.4944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [560/302]: training_loss: tensor(2.4684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [561/302]: training_loss: tensor(0.7002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [562/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [563/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [564/302]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [565/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [566/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [567/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [568/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [569/302]: training_loss: tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [570/302]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [571/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [572/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [573/302]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [574/302]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [575/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [576/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [577/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [578/302]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [579/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [580/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [581/302]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [582/302]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [583/302]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [584/302]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [585/302]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [586/302]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [587/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [588/302]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [589/302]: training_loss: tensor(0.1078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [590/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [591/302]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [592/302]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [593/302]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [594/302]: training_loss: tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [595/302]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [596/302]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [597/302]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [598/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [599/302]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [600/302]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [601/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [602/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [603/302]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [604/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [605/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [606/302]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [607/302]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [608/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [609/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [610/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [611/302]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [612/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [613/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [614/302]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [615/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [616/302]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [617/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [618/302]: training_loss: tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [619/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [620/302]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [621/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [622/302]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [623/302]: training_loss: tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [624/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [625/302]: training_loss: tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [626/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [627/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [628/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [629/302]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [630/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [631/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [632/302]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [633/302]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [634/302]: training_loss: tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [635/302]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [636/302]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [637/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [638/302]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [639/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [640/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [641/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [642/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [643/302]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [644/302]: training_loss: tensor(0.9878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [645/302]: training_loss: tensor(2.5912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [646/302]: training_loss: tensor(2.7726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [647/302]: training_loss: tensor(2.6196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [648/302]: training_loss: tensor(2.5401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [649/302]: training_loss: tensor(2.5484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [650/302]: training_loss: tensor(2.5938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [651/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [652/302]: training_loss: tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [653/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [654/302]: training_loss: tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [655/302]: training_loss: tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [656/302]: training_loss: tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [657/302]: training_loss: tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [658/302]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [659/302]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [660/302]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [661/302]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [662/302]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [663/302]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [664/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [665/302]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [666/302]: training_loss: tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [667/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [668/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [669/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [670/302]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [671/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [672/302]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [673/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [674/302]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [675/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [676/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [677/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [678/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [679/302]: training_loss: tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [680/302]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [681/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [682/302]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [683/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [684/302]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [685/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [686/302]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [687/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [688/302]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [689/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [690/302]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [691/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [692/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [693/302]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [694/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [695/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [696/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [697/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [698/302]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [699/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [700/302]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [701/302]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [702/302]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [703/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [704/302]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [705/302]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [706/302]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [707/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [708/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [709/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [710/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [711/302]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [712/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [713/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [714/302]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [715/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [716/302]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [717/302]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [718/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [719/302]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [720/302]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [721/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [722/302]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [723/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [724/302]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [725/302]: training_loss: tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [726/302]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [727/302]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [728/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [729/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [730/302]: training_loss: tensor(1.8565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [731/302]: training_loss: tensor(2.4518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [732/302]: training_loss: tensor(2.5176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [733/302]: training_loss: tensor(2.5756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [734/302]: training_loss: tensor(2.7502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [735/302]: training_loss: tensor(2.6067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [736/302]: training_loss: tensor(2.5145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [737/302]: training_loss: tensor(2.5039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [738/302]: training_loss: tensor(2.5417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [739/302]: training_loss: tensor(1.9252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [740/302]: training_loss: tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [741/302]: training_loss: tensor(0.1099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [742/302]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [743/302]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [744/302]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [745/302]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [746/302]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [747/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [748/302]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [749/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [750/302]: training_loss: tensor(0.1077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [751/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [752/302]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [753/302]: training_loss: tensor(0.1129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [754/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [755/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [756/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [757/302]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [758/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [759/302]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [760/302]: training_loss: tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [761/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [762/302]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [763/302]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [764/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [765/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [766/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [767/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [768/302]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [769/302]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [770/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [771/302]: training_loss: tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [772/302]: training_loss: tensor(0.1091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [773/302]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [774/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [775/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [776/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [777/302]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [778/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [779/302]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [780/302]: training_loss: tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [781/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [782/302]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [783/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [784/302]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [785/302]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [786/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [787/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [788/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [789/302]: training_loss: tensor(0.1107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [790/302]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [791/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [792/302]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [793/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [794/302]: training_loss: tensor(0.1291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [795/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [796/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [797/302]: training_loss: tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [798/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [799/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [800/302]: training_loss: tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [801/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [802/302]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [803/302]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [804/302]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [805/302]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [806/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [807/302]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [808/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [809/302]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [810/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [811/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [812/302]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [813/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [814/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [815/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [816/302]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [817/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [818/302]: training_loss: tensor(1.9302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [819/302]: training_loss: tensor(2.3910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [820/302]: training_loss: tensor(2.5072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [821/302]: training_loss: tensor(2.3630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [822/302]: training_loss: tensor(2.5041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [823/302]: training_loss: tensor(2.6757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [824/302]: training_loss: tensor(2.5073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [825/302]: training_loss: tensor(0.3563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [826/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [827/302]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [828/302]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [829/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [830/302]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [831/302]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [832/302]: training_loss: tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [833/302]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [834/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [835/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [836/302]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [837/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [838/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [839/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [840/302]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [841/302]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [842/302]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [843/302]: training_loss: tensor(0.1184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [844/302]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [845/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [846/302]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [847/302]: training_loss: tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [848/302]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [849/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [850/302]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [851/302]: training_loss: tensor(0.1019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [852/302]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [853/302]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [854/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [855/302]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [856/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [857/302]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [858/302]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [859/302]: training_loss: tensor(0.1113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [860/302]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [861/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [862/302]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [863/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [864/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [865/302]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [866/302]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [867/302]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [868/302]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [869/302]: training_loss: tensor(0.1043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [870/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [871/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [872/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [873/302]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [874/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [875/302]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [876/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [877/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [878/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [879/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [880/302]: training_loss: tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [881/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [882/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [883/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [884/302]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [885/302]: training_loss: tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [886/302]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [887/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [888/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [889/302]: training_loss: tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [890/302]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [891/302]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [892/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [893/302]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [894/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [895/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [896/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [897/302]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [898/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [899/302]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [900/302]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [901/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [902/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [903/302]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [904/302]: training_loss: tensor(0.3480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [905/302]: training_loss: tensor(2.3873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [906/302]: training_loss: tensor(2.4419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [907/302]: training_loss: tensor(2.5266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [908/302]: training_loss: tensor(2.5860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [909/302]: training_loss: tensor(2.4858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [910/302]: training_loss: tensor(2.5024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [911/302]: training_loss: tensor(2.4256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [912/302]: training_loss: tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [913/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [914/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [915/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [916/302]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [917/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [918/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [919/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [920/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [921/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [922/302]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [923/302]: training_loss: tensor(0.1019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [924/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [925/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [926/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [927/302]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [928/302]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [929/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [930/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [931/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [932/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [933/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [934/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [935/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [936/302]: training_loss: tensor(0.1236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [937/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [938/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [939/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [940/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [941/302]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [942/302]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [943/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [944/302]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [945/302]: training_loss: tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [946/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [947/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [948/302]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [949/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [950/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [951/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [952/302]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [953/302]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [954/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [955/302]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [956/302]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [957/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [958/302]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [959/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [960/302]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [961/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [962/302]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [963/302]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [964/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [965/302]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [966/302]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [967/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [968/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [969/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [970/302]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [971/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [972/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [973/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [974/302]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [975/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [976/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [977/302]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [978/302]: training_loss: tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [979/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [980/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [981/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [982/302]: training_loss: tensor(0.1197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [983/302]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [984/302]: training_loss: tensor(2.2303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [985/302]: training_loss: tensor(2.5319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [986/302]: training_loss: tensor(2.6303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [987/302]: training_loss: tensor(2.5864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [988/302]: training_loss: tensor(2.5435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [989/302]: training_loss: tensor(2.6242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [990/302]: training_loss: tensor(2.3643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [991/302]: training_loss: tensor(2.4244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [992/302]: training_loss: tensor(0.9378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [993/302]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [994/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [995/302]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [996/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [997/302]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [998/302]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [999/302]: training_loss: tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1000/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1001/302]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1002/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1003/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1004/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1005/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/302]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/302]: training_loss: tensor(0.1077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/302]: training_loss: tensor(0.1059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/302]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/302]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/302]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/302]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/302]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/302]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1019/302]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/302]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/302]: training_loss: tensor(0.1090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1023/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/302]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/302]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/302]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/302]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/302]: training_loss: tensor(0.1171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/302]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/302]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/302]: training_loss: tensor(0.1099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/302]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/302]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/302]: training_loss: tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1040/302]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/302]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/302]: training_loss: tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1043/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/302]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/302]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/302]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/302]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/302]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/302]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/302]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/302]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/302]: training_loss: tensor(1.9849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/302]: training_loss: tensor(2.4463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/302]: training_loss: tensor(2.4841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/302]: training_loss: tensor(2.4974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/302]: training_loss: tensor(2.3950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/302]: training_loss: tensor(2.4853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/302]: training_loss: tensor(1.7188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/302]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/302]: training_loss: tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/302]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/302]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/302]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/302]: training_loss: tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/302]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/302]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1100/302]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/302]: training_loss: tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/302]: training_loss: tensor(0.1099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/302]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/302]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/302]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/302]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/302]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/302]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/302]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/302]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/302]: training_loss: tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/302]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/302]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/302]: training_loss: tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/302]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/302]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/302]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/302]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/302]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/302]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1129/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/302]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/302]: training_loss: tensor(2.6597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/302]: training_loss: tensor(2.4208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/302]: training_loss: tensor(2.4785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/302]: training_loss: tensor(2.5755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/302]: training_loss: tensor(1.4671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/302]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/302]: training_loss: tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/302]: training_loss: tensor(0.1150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/302]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/302]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/302]: training_loss: tensor(0.1076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/302]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/302]: training_loss: tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/302]: training_loss: tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/302]: training_loss: tensor(0.1130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/302]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/302]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/302]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/302]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/302]: training_loss: tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/302]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/302]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/302]: training_loss: tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/302]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/302]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/302]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/302]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/302]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/302]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1187/302]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/302]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/302]: training_loss: tensor(1.8410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/302]: training_loss: tensor(2.4881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/302]: training_loss: tensor(2.4375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/302]: training_loss: tensor(2.3716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/302]: training_loss: tensor(2.4623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/302]: training_loss: tensor(2.5899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/302]: training_loss: tensor(0.3359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/302]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/302]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/302]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/302]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/302]: training_loss: tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/302]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/302]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/302]: training_loss: tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/302]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1213/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/302]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1215/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/302]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/302]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/302]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/302]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/302]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/302]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/302]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/302]: training_loss: tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/302]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/302]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/302]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/302]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/302]: training_loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/302]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/302]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/302]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/302]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/302]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/302]: training_loss: tensor(2.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/302]: training_loss: tensor(2.3947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/302]: training_loss: tensor(2.4340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/302]: training_loss: tensor(2.5737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/302]: training_loss: tensor(2.5257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/302]: training_loss: tensor(0.7387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/302]: training_loss: tensor(0.1101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/302]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/302]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/302]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/302]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/302]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1274/302]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/302]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/302]: training_loss: tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/302]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/302]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/302]: training_loss: tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/302]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/302]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/302]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/302]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/302]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1299/302]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/302]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/302]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/302]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/302]: training_loss: tensor(0.1154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/302]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/302]: training_loss: tensor(1.8538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/302]: training_loss: tensor(2.4529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/302]: training_loss: tensor(2.4496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/302]: training_loss: tensor(2.3740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/302]: training_loss: tensor(2.5555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/302]: training_loss: tensor(1.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/302]: training_loss: tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/302]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/302]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/302]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/302]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/302]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/302]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/302]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/302]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/302]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/302]: training_loss: tensor(0.1134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/302]: training_loss: tensor(0.1078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/302]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/302]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/302]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/302]: training_loss: tensor(0.1145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/302]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/302]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/302]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/302]: training_loss: tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/302]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/302]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/302]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/302]: training_loss: tensor(0.1068, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1361/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1363/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/302]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/302]: training_loss: tensor(0.7421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/302]: training_loss: tensor(2.4149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/302]: training_loss: tensor(2.3057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/302]: training_loss: tensor(2.5112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/302]: training_loss: tensor(1.7066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/302]: training_loss: tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/302]: training_loss: tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/302]: training_loss: tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/302]: training_loss: tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/302]: training_loss: tensor(0.1099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/302]: training_loss: tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1385/302]: training_loss: tensor(0.1085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1386/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1387/302]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1388/302]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1389/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1390/302]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1391/302]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1392/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1393/302]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1394/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1395/302]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1396/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1397/302]: training_loss: tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1398/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1399/302]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1400/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1401/302]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1402/302]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1403/302]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1404/302]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1405/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1406/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1407/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1408/302]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1409/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1410/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1411/302]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1412/302]: training_loss: tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1413/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1414/302]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1415/302]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1416/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1417/302]: training_loss: tensor(0.1107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1418/302]: training_loss: tensor(2.3982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1419/302]: training_loss: tensor(2.5342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1420/302]: training_loss: tensor(2.7663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1421/302]: training_loss: tensor(2.4524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1422/302]: training_loss: tensor(1.5191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1423/302]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1424/302]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1425/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1426/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1427/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1428/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1429/302]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1430/302]: training_loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1431/302]: training_loss: tensor(0.1077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1432/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1433/302]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1434/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1435/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1436/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1437/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1438/302]: training_loss: tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1439/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1440/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1441/302]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1442/302]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1443/302]: training_loss: tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1444/302]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1445/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1446/302]: training_loss: tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1447/302]: training_loss: tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1448/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1449/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1450/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1451/302]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1452/302]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1453/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1454/302]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1455/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1456/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1457/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1458/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1459/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1460/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1461/302]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1462/302]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1463/302]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1464/302]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1465/302]: training_loss: tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1466/302]: training_loss: tensor(0.6526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1467/302]: training_loss: tensor(2.4387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1468/302]: training_loss: tensor(2.4671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1469/302]: training_loss: tensor(2.4921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1470/302]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1471/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1472/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1473/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1474/302]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1475/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1476/302]: training_loss: tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1477/302]: training_loss: tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1478/302]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1479/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1480/302]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1481/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1482/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1483/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1484/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1485/302]: training_loss: tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1486/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1487/302]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1488/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1489/302]: training_loss: tensor(0.1103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1490/302]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1491/302]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1492/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1493/302]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1494/302]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1495/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1496/302]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1497/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1498/302]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1499/302]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1500/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1501/302]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1502/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1503/302]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1504/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1505/302]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1506/302]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1507/302]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1508/302]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1509/302]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1510/302]: training_loss: tensor(1.9616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1511/302]: training_loss: tensor(2.3569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1512/302]: training_loss: tensor(1.4597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1513/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1514/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1515/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1516/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1517/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1518/302]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1519/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1520/302]: training_loss: tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1521/302]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1522/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1523/302]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1524/302]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1525/302]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1526/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1527/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1528/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1529/302]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1530/302]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1531/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1532/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1533/302]: training_loss: tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1534/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1535/302]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1536/302]: training_loss: tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1537/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1538/302]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1539/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1540/302]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1541/302]: training_loss: tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1542/302]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1543/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1544/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1545/302]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1546/302]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1547/302]: training_loss: tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1548/302]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1549/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1550/302]: training_loss: tensor(1.7107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1551/302]: training_loss: tensor(2.4921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1552/302]: training_loss: tensor(2.3735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1553/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1554/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1555/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1556/302]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1557/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1558/302]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1559/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1560/302]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1561/302]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1562/302]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1563/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1564/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1565/302]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1566/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1567/302]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1568/302]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1569/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1570/302]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1571/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1572/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1573/302]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1574/302]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1575/302]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1576/302]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1577/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1578/302]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1579/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1580/302]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1581/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1582/302]: training_loss: tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1583/302]: training_loss: tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1584/302]: training_loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1585/302]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1586/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1587/302]: training_loss: tensor(0.1139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1588/302]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1589/302]: training_loss: tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1590/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1591/302]: training_loss: tensor(2.4792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1592/302]: training_loss: tensor(2.4486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1593/302]: training_loss: tensor(2.4446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1594/302]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1595/302]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1596/302]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1597/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1598/302]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1599/302]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1600/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1601/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1602/302]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1603/302]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1604/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1605/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1606/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1607/302]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1608/302]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1609/302]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1610/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1611/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1612/302]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1613/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1614/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1615/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1616/302]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1617/302]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1618/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1619/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1620/302]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1621/302]: training_loss: tensor(0.1164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1622/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1623/302]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1624/302]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1625/302]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1626/302]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1627/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1628/302]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1629/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1630/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1631/302]: training_loss: tensor(2.1665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1632/302]: training_loss: tensor(2.5280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1633/302]: training_loss: tensor(2.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1634/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1635/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1636/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1637/302]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1638/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1639/302]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1640/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1641/302]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1642/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1643/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1644/302]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1645/302]: training_loss: tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1646/302]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1647/302]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1648/302]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1649/302]: training_loss: tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1650/302]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1651/302]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1652/302]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1653/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1654/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1655/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1656/302]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1657/302]: training_loss: tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1658/302]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1659/302]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1660/302]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1661/302]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1662/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1663/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1664/302]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1665/302]: training_loss: tensor(1.5390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1666/302]: training_loss: tensor(2.6097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1667/302]: training_loss: tensor(1.8598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1668/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1669/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1670/302]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1671/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1672/302]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1673/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1674/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1675/302]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1676/302]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1677/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1678/302]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1679/302]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1680/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1681/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1682/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1683/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1684/302]: training_loss: tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1685/302]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1686/302]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1687/302]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1688/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1689/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1690/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1691/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1692/302]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1693/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1694/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1695/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1696/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1697/302]: training_loss: tensor(0.7149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1698/302]: training_loss: tensor(2.5524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1699/302]: training_loss: tensor(2.5218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1700/302]: training_loss: tensor(0.7000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1701/302]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1702/302]: training_loss: tensor(0.1145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1703/302]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1704/302]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1705/302]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1706/302]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1707/302]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1708/302]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1709/302]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1710/302]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1711/302]: training_loss: tensor(0.1110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1712/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1713/302]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1714/302]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1715/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1716/302]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1717/302]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1718/302]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1719/302]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1720/302]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1721/302]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1722/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1723/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1724/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1725/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1726/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1727/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1728/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1729/302]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1730/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1731/302]: training_loss: tensor(1.2562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1732/302]: training_loss: tensor(2.3361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1733/302]: training_loss: tensor(2.5583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1734/302]: training_loss: tensor(0.6929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1735/302]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1736/302]: training_loss: tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1737/302]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1738/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1739/302]: training_loss: tensor(0.1308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1740/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1741/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1742/302]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1743/302]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1744/302]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1745/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1746/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1747/302]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1748/302]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1749/302]: training_loss: tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1750/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1751/302]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1752/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1753/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1754/302]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1755/302]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1756/302]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1757/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1758/302]: training_loss: tensor(0.7054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1759/302]: training_loss: tensor(2.4124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1760/302]: training_loss: tensor(2.5344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1761/302]: training_loss: tensor(1.3032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1762/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1763/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1764/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1765/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1766/302]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1767/302]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1768/302]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1769/302]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1770/302]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1771/302]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1772/302]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1773/302]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1774/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1775/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1776/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1777/302]: training_loss: tensor(0.1068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1778/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1779/302]: training_loss: tensor(0.1147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1780/302]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1781/302]: training_loss: tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1782/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1783/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1784/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1785/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1786/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1787/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1788/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1789/302]: training_loss: tensor(1.5555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1790/302]: training_loss: tensor(1.1980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1791/302]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1792/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1793/302]: training_loss: tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1794/302]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1795/302]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1796/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1797/302]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1798/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1799/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1800/302]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1801/302]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1802/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1803/302]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1804/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1805/302]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1806/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1807/302]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1808/302]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1809/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1810/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1811/302]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1812/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1813/302]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1814/302]: training_loss: tensor(0.4169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1815/302]: training_loss: tensor(2.5119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1816/302]: training_loss: tensor(2.3797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1817/302]: training_loss: tensor(0.3332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1818/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1819/302]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1820/302]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1821/302]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1822/302]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1823/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1824/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1825/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1826/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1827/302]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1828/302]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1829/302]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1830/302]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1831/302]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1832/302]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1833/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1834/302]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1835/302]: training_loss: tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1836/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1837/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1838/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1839/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1840/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1841/302]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1842/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1843/302]: training_loss: tensor(2.4783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1844/302]: training_loss: tensor(1.8185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1845/302]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1846/302]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1847/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1848/302]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1849/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1850/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1851/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1852/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1853/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1854/302]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1855/302]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1856/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1857/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1858/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1859/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1860/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1861/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1862/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1863/302]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1864/302]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1865/302]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1866/302]: training_loss: tensor(1.9317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1867/302]: training_loss: tensor(2.2390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1868/302]: training_loss: tensor(1.2158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1869/302]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1870/302]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1871/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1872/302]: training_loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1873/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1874/302]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1875/302]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1876/302]: training_loss: tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1877/302]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1878/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1879/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1880/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1881/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1882/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1883/302]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1884/302]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1885/302]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1886/302]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1887/302]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1888/302]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1889/302]: training_loss: tensor(0.6447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1890/302]: training_loss: tensor(2.5233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1891/302]: training_loss: tensor(2.5183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1892/302]: training_loss: tensor(1.3935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1893/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1894/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1895/302]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1896/302]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1897/302]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1898/302]: training_loss: tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1899/302]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1900/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1901/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1902/302]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1903/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1904/302]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1905/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1906/302]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1907/302]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1908/302]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1909/302]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1910/302]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1911/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1912/302]: training_loss: tensor(0.3877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1913/302]: training_loss: tensor(2.4650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1914/302]: training_loss: tensor(1.8752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1915/302]: training_loss: tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1916/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1917/302]: training_loss: tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1918/302]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1919/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1920/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1921/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1922/302]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1923/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1924/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1925/302]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1926/302]: training_loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1927/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1928/302]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1929/302]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1930/302]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1931/302]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1932/302]: training_loss: tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1933/302]: training_loss: tensor(2.2235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1934/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1935/302]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1936/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1937/302]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1938/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1939/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1940/302]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1941/302]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1942/302]: training_loss: tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1943/302]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1944/302]: training_loss: tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1945/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1946/302]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1947/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1948/302]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1949/302]: training_loss: tensor(0.1128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1950/302]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1951/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1952/302]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1953/302]: training_loss: tensor(1.4579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1954/302]: training_loss: tensor(0.8642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1955/302]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1956/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1957/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1958/302]: training_loss: tensor(0.1044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1959/302]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1960/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1961/302]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1962/302]: training_loss: tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1963/302]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1964/302]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1965/302]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1966/302]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1967/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1968/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1969/302]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1970/302]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1971/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1972/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1973/302]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1974/302]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1975/302]: training_loss: tensor(0.4534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1976/302]: training_loss: tensor(2.5543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1977/302]: training_loss: tensor(1.2837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1978/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1979/302]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1980/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1981/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1982/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1983/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1984/302]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1985/302]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1986/302]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1987/302]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1988/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1989/302]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1990/302]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1991/302]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1992/302]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1993/302]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1994/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1995/302]: training_loss: tensor(0.3850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1996/302]: training_loss: tensor(2.4146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1997/302]: training_loss: tensor(2.5913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1998/302]: training_loss: tensor(0.3994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1999/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2000/302]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2001/302]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2002/302]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2003/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2004/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2005/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2006/302]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2007/302]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2008/302]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2009/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2010/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2011/302]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2012/302]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2013/302]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2014/302]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2015/302]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2016/302]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2017/302]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2018/302]: training_loss: tensor(2.1517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2019/302]: training_loss: tensor(1.2947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2020/302]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2021/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2022/302]: training_loss: tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2023/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2024/302]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2025/302]: training_loss: tensor(0.1066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2026/302]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2027/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2028/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2029/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2030/302]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2031/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2032/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2033/302]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2034/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2035/302]: training_loss: tensor(2.1944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2036/302]: training_loss: tensor(1.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2037/302]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2038/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2039/302]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2040/302]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2041/302]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2042/302]: training_loss: tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2043/302]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2044/302]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2045/302]: training_loss: tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2046/302]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2047/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2048/302]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2049/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2050/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2051/302]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2052/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2053/302]: training_loss: tensor(1.2801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2054/302]: training_loss: tensor(1.3741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2055/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2056/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2057/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2058/302]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2059/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2060/302]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2061/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2062/302]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2063/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2064/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2065/302]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2066/302]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2067/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2068/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2069/302]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2070/302]: training_loss: tensor(2.3259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2071/302]: training_loss: tensor(1.9278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2072/302]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2073/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2074/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2075/302]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2076/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2077/302]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2078/302]: training_loss: tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2079/302]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2080/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2081/302]: training_loss: tensor(0.1189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2082/302]: training_loss: tensor(0.1099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2083/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2084/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2085/302]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2086/302]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2087/302]: training_loss: tensor(1.2046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2088/302]: training_loss: tensor(1.4946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2089/302]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2090/302]: training_loss: tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2091/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2092/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2093/302]: training_loss: tensor(0.1144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2094/302]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2095/302]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2096/302]: training_loss: tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2097/302]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2098/302]: training_loss: tensor(0.1101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2099/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2100/302]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2101/302]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2102/302]: training_loss: tensor(1.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2103/302]: training_loss: tensor(1.7359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2104/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2105/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2106/302]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2107/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2108/302]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2109/302]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2110/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2111/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2112/302]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2113/302]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2114/302]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2115/302]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2116/302]: training_loss: tensor(0.6883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2117/302]: training_loss: tensor(1.5561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2118/302]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2119/302]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2120/302]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2121/302]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2122/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2123/302]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2124/302]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2125/302]: training_loss: tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2126/302]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2127/302]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2128/302]: training_loss: tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2129/302]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2130/302]: training_loss: tensor(0.5700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2131/302]: training_loss: tensor(1.6389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2132/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2133/302]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2134/302]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2135/302]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2136/302]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2137/302]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2138/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2139/302]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2140/302]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2141/302]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2142/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2143/302]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2144/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2145/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2146/302]: training_loss: tensor(0.9222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2147/302]: training_loss: tensor(1.1925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2148/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2149/302]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2150/302]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2151/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2152/302]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2153/302]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2154/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2155/302]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2156/302]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2157/302]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2158/302]: training_loss: tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2159/302]: training_loss: tensor(0.3655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2160/302]: training_loss: tensor(2.4586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2161/302]: training_loss: tensor(0.3741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2162/302]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2163/302]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2164/302]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2165/302]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2166/302]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2167/302]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2168/302]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2169/302]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2170/302]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2171/302]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2172/302]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2173/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2174/302]: training_loss: tensor(1.6795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2175/302]: training_loss: tensor(1.9942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2176/302]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2177/302]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2178/302]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2179/302]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2180/302]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2181/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2182/302]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2183/302]: training_loss: tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2184/302]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2185/302]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2186/302]: training_loss: tensor(0.8821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2187/302]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2188/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2189/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2190/302]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2191/302]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2192/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2193/302]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2194/302]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2195/302]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2196/302]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2197/302]: training_loss: tensor(0.1085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2198/302]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2199/302]: training_loss: tensor(0.7097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2200/302]: training_loss: tensor(2.2607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2201/302]: training_loss: tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2202/302]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2203/302]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2204/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2205/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2206/302]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2207/302]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2208/302]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2209/302]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2210/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2211/302]: training_loss: tensor(0.1127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2212/302]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2213/302]: training_loss: tensor(2.4645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2214/302]: training_loss: tensor(1.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2215/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2216/302]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2217/302]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2218/302]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2219/302]: training_loss: tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2220/302]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2221/302]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2222/302]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2223/302]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2224/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2225/302]: training_loss: tensor(0.3679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2226/302]: training_loss: tensor(2.5953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2227/302]: training_loss: tensor(0.6433, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2228/302]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2229/302]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2230/302]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2231/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2232/302]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2233/302]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2234/302]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2235/302]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2236/302]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2237/302]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2238/302]: training_loss: tensor(1.6752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2239/302]: training_loss: tensor(0.9190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2240/302]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2241/302]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2242/302]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2243/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2244/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2245/302]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2246/302]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2247/302]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2248/302]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2249/302]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2250/302]: training_loss: tensor(2.3687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2251/302]: training_loss: tensor(0.6866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2252/302]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2253/302]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2254/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2255/302]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2256/302]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2257/302]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2258/302]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2259/302]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2260/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2261/302]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2262/302]: training_loss: tensor(1.5477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2263/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2264/302]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2265/302]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2266/302]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2267/302]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2268/302]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2269/302]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2270/302]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2271/302]: training_loss: tensor(0.3759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2272/302]: training_loss: tensor(2.3995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2273/302]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2274/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2275/302]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2276/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2277/302]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2278/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2279/302]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2280/302]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2281/302]: training_loss: tensor(2.1934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2282/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2283/302]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2284/302]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2285/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2286/302]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2287/302]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2288/302]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2289/302]: training_loss: tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2290/302]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2291/302]: training_loss: tensor(1.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2292/302]: training_loss: tensor(2.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2293/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2294/302]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2295/302]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2296/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2297/302]: training_loss: tensor(0.1120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2298/302]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2299/302]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2300/302]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2301/302]: training_loss: tensor(1.3756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2302/302]: training_loss: tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2303/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2304/302]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2305/302]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2306/302]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2307/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2308/302]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2309/302]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2310/302]: training_loss: tensor(0.4101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2311/302]: training_loss: tensor(2.1141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2312/302]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2313/302]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2314/302]: training_loss: tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2315/302]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2316/302]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2317/302]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2318/302]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2319/302]: training_loss: tensor(0.9246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2320/302]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2321/302]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2322/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2323/302]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2324/302]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2325/302]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2326/302]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2327/302]: training_loss: tensor(1.3876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2328/302]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2329/302]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2330/302]: training_loss: tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2331/302]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2332/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2333/302]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2334/302]: training_loss: tensor(0.7949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2335/302]: training_loss: tensor(0.4407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2336/302]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2337/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2338/302]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2339/302]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2340/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2341/302]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2342/302]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2343/302]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2344/302]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2345/302]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2346/302]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2347/302]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2348/302]: training_loss: tensor(2.2312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2349/302]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2350/302]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2351/302]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2352/302]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2353/302]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2354/302]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2355/302]: training_loss: tensor(1.1753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2356/302]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2357/302]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2358/302]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2359/302]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2360/302]: training_loss: tensor(1.3107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2361/302]: training_loss: tensor(0.6864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2362/302]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2363/302]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2364/302]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2365/302]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2366/302]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2367/302]: training_loss: tensor(1.5884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2368/302]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2369/302]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2370/302]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2371/302]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2372/302]: training_loss: tensor(1.5031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2373/302]: training_loss: tensor(0.3607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2374/302]: training_loss: tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2375/302]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2376/302]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2377/302]: training_loss: tensor(0.8972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2378/302]: training_loss: tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2379/302]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2380/302]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2381/302]: training_loss: tensor(0.7078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2382/302]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2383/302]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2384/302]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2385/302]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2386/302]: training_loss: tensor(1.2345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2387/302]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2388/302]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2389/302]: training_loss: tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2390/302]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2391/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2392/302]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2393/302]: training_loss: tensor(0.7484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2394/302]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2395/302]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2396/302]: training_loss: tensor(0.3553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2397/302]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2398/302]: training_loss: tensor(0.1201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2399/302]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2400/302]: training_loss: tensor(1.2712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2401/302]: training_loss: tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2402/302]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2403/302]: training_loss: tensor(0.3513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2404/302]: training_loss: tensor(0.4173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2405/302]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2406/302]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2407/302]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2408/302]: training_loss: tensor(0.3665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2409/302]: training_loss: tensor(0.8021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2410/302]: training_loss: tensor(0.3666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2411/302]: training_loss: tensor(0.4914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2412/302]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2413/302]: training_loss: tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2414/302]: training_loss: tensor(0.3723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2415/302]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2416/302]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2417/302]: training_loss: tensor(0.6595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2418/302]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2419/302]: training_loss: tensor(0.6499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2420/302]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [3/3], global step [7263/7263], Train Loss: 0.2931, Valid Loss: 0.2898\n",
      "Training done!\n"
     ]
    }
   ],
   "source": [
    "# Main training loop\n",
    "NUM_EPOCHS = 2\n",
    "steps_per_epoch = len(train_iter)\n",
    "\n",
    "model = ROBERTAClassifier(0.3)\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"======================= Start pretraining ==============================\")\n",
    "\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=steps_per_epoch*1, \n",
    "                                            num_training_steps=steps_per_epoch*NUM_EPOCHS)\n",
    "pretrain(model=model,\n",
    "         train_iter=train_iter,\n",
    "         valid_iter=valid_iter,\n",
    "         optimizer=optimizer,\n",
    "         scheduler=scheduler,\n",
    "         num_epochs=NUM_EPOCHS)\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "print(\"======================= Start training =================================\")\n",
    "optimizer = AdamW(model.parameters(), lr=2e-6)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=steps_per_epoch*2, \n",
    "                                            num_training_steps=steps_per_epoch*NUM_EPOCHS)\n",
    "\n",
    "train(model=model, \n",
    "      train_iter=train_iter, \n",
    "      valid_iter=valid_iter, \n",
    "      optimizer=optimizer, \n",
    "      scheduler=scheduler, \n",
    "      num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def generate(model, test_loader):\n",
    "    y_pred = []\n",
    "    y_prob = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (source, target), _ in tqdm(test_loader):\n",
    "                mask = (source != PAD_INDEX).type(torch.uint8)\n",
    "                output = model(source, attention_mask=mask)\n",
    "                y_prob.extend(output.tolist())\n",
    "    return y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2421/2421 [01:45<00:00, 23.01it/s]\n"
     ]
    }
   ],
   "source": [
    "prediction_list_GAB = generate(model , train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_prediction_list_GAB = [sum(i) for i in prediction_list_GAB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_folder = 'Data'\n",
    "destination_folder = 'Model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameter\n",
    "MAX_SEQ_LEN = 128\n",
    "BATCH_SIZE = 8\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
    "# Fields\n",
    "\n",
    "label_field = Field(sequential=False, use_vocab=False, batch_first=True)\n",
    "text_field = Field(use_vocab=False, \n",
    "                   tokenize=tokenizer.encode, \n",
    "                   include_lengths=False, \n",
    "                   batch_first=True,\n",
    "                   fix_length=MAX_SEQ_LEN, \n",
    "                   pad_token=PAD_INDEX, \n",
    "                   unk_token=UNK_INDEX)\n",
    "fields = [('comment', text_field),('toxicity', label_field)]\n",
    "\n",
    "# TabularDataset\n",
    "\n",
    "train, valid, test = TabularDataset.splits(path=source_folder, train='test5.csv', validation='test5.csv',\n",
    "                                           test='test5.csv', format='CSV', fields=fields, skip_header=True)\n",
    "\n",
    "\n",
    "train_iter_2400 = BucketIterator(train, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.comment),\n",
    "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
    "valid_iter_2400 = BucketIterator(valid, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.comment),\n",
    "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
    "test_iter_2400 = Iterator(train, batch_size=BATCH_SIZE, device=device, train=False, shuffle=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 290/290 [00:44<00:00,  6.57it/s]\n"
     ]
    }
   ],
   "source": [
    "prediction_list_2400 = generate(model , test_iter_2400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_prediction_list_2400 = [sum(i) for i in prediction_list_2400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "AC_train = pd.read_csv('Data/Train_comment.csv', encoding='utf-8',sep=',')\n",
    "AC_val = pd.read_csv('Data/Val_comment.csv', encoding='utf-8',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comment</th>\n",
       "      <th>attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>this is not creative  those are the dictionar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the term standard model is itself less npov t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>true or false the situation as of march 2002 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>this page will need disambiguation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>important note for all sysops there is a bug...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            comment  attack\n",
       "0           0   this is not creative  those are the dictionar...       0\n",
       "1           1   the term standard model is itself less npov t...       0\n",
       "2           2   true or false the situation as of march 2002 ...       0\n",
       "3           3                this page will need disambiguation        0\n",
       "4           4    important note for all sysops there is a bug...       0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AC_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "AC_train = AC_train.drop(columns=['Unnamed: 0'])\n",
    "AC_val = AC_val.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "AC_train.to_csv('Data/Train_comment.csv',index=False,header=True)\n",
    "AC_val.to_csv('Data/Val_comment.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameter\n",
    "MAX_SEQ_LEN = 128\n",
    "BATCH_SIZE = 8\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
    "# Fields\n",
    "\n",
    "label_field = Field(sequential=False, use_vocab=False, batch_first=True)\n",
    "text_field = Field(use_vocab=False, \n",
    "                   tokenize=tokenizer.encode, \n",
    "                   include_lengths=False, \n",
    "                   batch_first=True,\n",
    "                   fix_length=MAX_SEQ_LEN, \n",
    "                   pad_token=PAD_INDEX, \n",
    "                   unk_token=UNK_INDEX)\n",
    "fields = [('comment', text_field),('attack', label_field)]\n",
    "\n",
    "# TabularDataset\n",
    "\n",
    "train, valid, test = TabularDataset.splits(path=source_folder, train='Train_comment.csv', validation='Val_comment.csv',\n",
    "                                           test='Train_comment.csv', format='CSV', fields=fields, skip_header=True)\n",
    "\n",
    "\n",
    "train_iter_AC = BucketIterator(train, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.comment),\n",
    "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
    "valid_iter_AC = BucketIterator(valid, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.comment),\n",
    "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
    "test_iter_AC = Iterator(train, batch_size=BATCH_SIZE, device=device, train=False, shuffle=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 8691/8691 [06:57<00:00, 20.82it/s]\n"
     ]
    }
   ],
   "source": [
    "prediction_list_AC = generate(model , test_iter_AC)\n",
    "Final_prediction_list_AC = [sum(i) for i in prediction_list_AC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameter\n",
    "MAX_SEQ_LEN = 128\n",
    "BATCH_SIZE = 8\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
    "# Fields\n",
    "\n",
    "label_field = Field(sequential=False, use_vocab=False, batch_first=True)\n",
    "text_field = Field(use_vocab=False, \n",
    "                   tokenize=tokenizer.encode, \n",
    "                   include_lengths=False, \n",
    "                   batch_first=True,\n",
    "                   fix_length=MAX_SEQ_LEN, \n",
    "                   pad_token=PAD_INDEX, \n",
    "                   unk_token=UNK_INDEX)\n",
    "fields = [('comment', text_field),('toxicity', label_field)]\n",
    "\n",
    "# TabularDataset\n",
    "\n",
    "train, valid, test = TabularDataset.splits(path=source_folder, train='Train_comment_toxity.csv', validation='Val_comment_toxity.csv',\n",
    "                                           test='Test_comment_toxity.csv', format='CSV', fields=fields, skip_header=True)\n",
    "\n",
    "\n",
    "train_iter_toxity = BucketIterator(train, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.comment),\n",
    "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
    "valid_iter_toxity = BucketIterator(valid, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.comment),\n",
    "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
    "test_iter_toxity = Iterator(train, batch_size=BATCH_SIZE, device=device, train=False, shuffle=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 11962/11962 [09:44<00:00, 20.47it/s]\n"
     ]
    }
   ],
   "source": [
    "prediction_list_toxity = generate(model , test_iter_toxity)\n",
    "Final_prediction_list_toxity = [sum(i) for i in prediction_list_toxity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.hist(Final_prediction_list_GAB, label= 'GAB', bins=1000,alpha = 0.3)\n",
    "plt.hist(Final_prediction_list_toxity, label = 'toxity', bins = 1000,alpha = 0.3)\n",
    "plt.hist(Final_prediction_list_AC,label = 'AC', bins = 1000,alpha = 0.3)\n",
    "plt.hist(Final_prediction_list_2400, label = '2400', bins = 1000,color='r', alpha = 0.3)\n",
    "plt.title('Data significance comparision')\n",
    "plt.grid()\n",
    "# plt.ylim(0,8)\n",
    "plt.ylabel('Number')\n",
    "plt.xlabel('Roberta score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## graph after normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.hist(Final_prediction_list_GAB, label= 'GAB', bins=1000,alpha = 0.3,density = True)\n",
    "plt.hist(Final_prediction_list_toxity, label = 'toxity', bins = 1000,alpha = 0.3,density = True)\n",
    "plt.hist(Final_prediction_list_AC,label = 'AC', bins = 1000,alpha = 0.3,density = True)\n",
    "plt.hist(Final_prediction_list_2400, label = '2400', bins = 1000,color='r', alpha = 0.3,density = True)\n",
    "plt.title('Data significance comparision')\n",
    "plt.grid()\n",
    "# plt.ylim(0,8)\n",
    "plt.ylabel('Number')\n",
    "plt.xlabel('Roberta score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
