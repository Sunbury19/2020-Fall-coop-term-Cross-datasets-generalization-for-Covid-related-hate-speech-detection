{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERTa model construction with 'EA' data\n",
    "\n",
    "2020/11/10\n",
    "\n",
    "Student: Xuanyu Su                                                                 \n",
    "Supervisor: Isar Nejadgholi\n",
    "\n",
    "In this notebook, we use the parameters set from paper: Detecting East Asian Prejudice on Social Media. and following the operation the author did in the paper. Then we constructed a model by using RoBERTa base model and the same parameter used in the paper.\n",
    "\n",
    "We also test the model in 2400 Covid dataset, and fine-tuning the model twice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EA_data = pd.read_csv('hs_AsianPrejudice_20kdataset_cleaned_anonymized.tsv',sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>annot1</th>\n",
       "      <th>annot2</th>\n",
       "      <th>expert</th>\n",
       "      <th>text</th>\n",
       "      <th>text.clean</th>\n",
       "      <th>annot1.name</th>\n",
       "      <th>annot2.name</th>\n",
       "      <th>expert.name</th>\n",
       "      <th>target.annot1.clean</th>\n",
       "      <th>...</th>\n",
       "      <th>hashtags.annotator1</th>\n",
       "      <th>hashtags.annotator2</th>\n",
       "      <th>hashtags.decision</th>\n",
       "      <th>East Asia</th>\n",
       "      <th>China</th>\n",
       "      <th>Hong Kong</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Korea</th>\n",
       "      <th>Singapore</th>\n",
       "      <th>Taiwan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>idstr_1212372663416639488</td>\n",
       "      <td>none_of_the_above</td>\n",
       "      <td>none_of_the_above</td>\n",
       "      <td>none_of_the_above</td>\n",
       "      <td>@CNN No doubt a ChiNa female. #shameonchina #B...</td>\n",
       "      <td>@cnn no doubt a china female. HASHTAG_EASTASIA...</td>\n",
       "      <td>annotator_GNZuCtwed3</td>\n",
       "      <td>annotator_gbEGjSAk6r</td>\n",
       "      <td>expert_GNZuCtwed3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>hashtags_not_used_at_all_to_identify_themes</td>\n",
       "      <td>hashtags_not_used_at_all_to_identify_themes</td>\n",
       "      <td>agree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>idstr_1212374922993053696</td>\n",
       "      <td>none_of_the_above</td>\n",
       "      <td>none_of_the_above</td>\n",
       "      <td>none_of_the_above</td>\n",
       "      <td>@KongTsungGan The #HongKongPoliceBrutality is ...</td>\n",
       "      <td>@kongtsunggan the HASHTAG_EASTASIA is happenin...</td>\n",
       "      <td>annotator_CAgNlUizNm</td>\n",
       "      <td>annotator_cYKSVBW4HD</td>\n",
       "      <td>expert_GNZuCtwed3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>hashtags_not_used_at_all_to_identify_themes</td>\n",
       "      <td>hashtags_only_used_to_identify_covid_relevance</td>\n",
       "      <td>disagree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>idstr_1213452156251987973</td>\n",
       "      <td>none_of_the_above</td>\n",
       "      <td>none_of_the_above</td>\n",
       "      <td>none_of_the_above</td>\n",
       "      <td>Afraid.  #ChinaPneumonia</td>\n",
       "      <td>afraid.  #HASHTAG</td>\n",
       "      <td>annotator_vDe7GN0NrL</td>\n",
       "      <td>annotator_HtRmsP3KiK</td>\n",
       "      <td>expert_CAgNlUizNm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>hashtags_not_used_at_all_to_identify_themes</td>\n",
       "      <td>hashtags_not_used_at_all_to_identify_themes</td>\n",
       "      <td>agree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>idstr_1213471445294075909</td>\n",
       "      <td>none_of_the_above</td>\n",
       "      <td>none_of_the_above</td>\n",
       "      <td>none_of_the_above</td>\n",
       "      <td>RT @shapponeko @Mugisalty @CatBus2D everybody ...</td>\n",
       "      <td>rt @shapponeko @mugisalty @catbus2d everybody ...</td>\n",
       "      <td>annotator_TbUBpfn6iP</td>\n",
       "      <td>annotator_dqrONtdjbt</td>\n",
       "      <td>expert_GNZuCtwed3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>hashtags_not_used_at_all_to_identify_themes</td>\n",
       "      <td>hashtags_not_used_at_all_to_identify_themes</td>\n",
       "      <td>agree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>idstr_1213494381073711105</td>\n",
       "      <td>entity_directed_hostility</td>\n",
       "      <td>entity_directed_hostility</td>\n",
       "      <td>entity_directed_hostility</td>\n",
       "      <td>RT @JAbbott45624607 @annie_sparrow This makes ...</td>\n",
       "      <td>rt @jabbott45624607 @annie_sparrow this makes ...</td>\n",
       "      <td>annotator_oemYWm1Tjg</td>\n",
       "      <td>annotator_IBsVsBliwX</td>\n",
       "      <td>expert_GNZuCtwed3</td>\n",
       "      <td>China</td>\n",
       "      <td>...</td>\n",
       "      <td>hashtags_not_used_at_all_to_identify_themes</td>\n",
       "      <td>hashtags_only_used_to_identify_covid_relevance</td>\n",
       "      <td>disagree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id                     annot1  \\\n",
       "0  idstr_1212372663416639488          none_of_the_above   \n",
       "1  idstr_1212374922993053696          none_of_the_above   \n",
       "2  idstr_1213452156251987973          none_of_the_above   \n",
       "3  idstr_1213471445294075909          none_of_the_above   \n",
       "4  idstr_1213494381073711105  entity_directed_hostility   \n",
       "\n",
       "                      annot2                     expert  \\\n",
       "0          none_of_the_above          none_of_the_above   \n",
       "1          none_of_the_above          none_of_the_above   \n",
       "2          none_of_the_above          none_of_the_above   \n",
       "3          none_of_the_above          none_of_the_above   \n",
       "4  entity_directed_hostility  entity_directed_hostility   \n",
       "\n",
       "                                                text  \\\n",
       "0  @CNN No doubt a ChiNa female. #shameonchina #B...   \n",
       "1  @KongTsungGan The #HongKongPoliceBrutality is ...   \n",
       "2                           Afraid.  #ChinaPneumonia   \n",
       "3  RT @shapponeko @Mugisalty @CatBus2D everybody ...   \n",
       "4  RT @JAbbott45624607 @annie_sparrow This makes ...   \n",
       "\n",
       "                                          text.clean           annot1.name  \\\n",
       "0  @cnn no doubt a china female. HASHTAG_EASTASIA...  annotator_GNZuCtwed3   \n",
       "1  @kongtsunggan the HASHTAG_EASTASIA is happenin...  annotator_CAgNlUizNm   \n",
       "2                                  afraid.  #HASHTAG  annotator_vDe7GN0NrL   \n",
       "3  rt @shapponeko @mugisalty @catbus2d everybody ...  annotator_TbUBpfn6iP   \n",
       "4  rt @jabbott45624607 @annie_sparrow this makes ...  annotator_oemYWm1Tjg   \n",
       "\n",
       "            annot2.name        expert.name target.annot1.clean  ...  \\\n",
       "0  annotator_gbEGjSAk6r  expert_GNZuCtwed3                 NaN  ...   \n",
       "1  annotator_cYKSVBW4HD  expert_GNZuCtwed3                 NaN  ...   \n",
       "2  annotator_HtRmsP3KiK  expert_CAgNlUizNm                 NaN  ...   \n",
       "3  annotator_dqrONtdjbt  expert_GNZuCtwed3                 NaN  ...   \n",
       "4  annotator_IBsVsBliwX  expert_GNZuCtwed3               China  ...   \n",
       "\n",
       "                           hashtags.annotator1  \\\n",
       "0  hashtags_not_used_at_all_to_identify_themes   \n",
       "1  hashtags_not_used_at_all_to_identify_themes   \n",
       "2  hashtags_not_used_at_all_to_identify_themes   \n",
       "3  hashtags_not_used_at_all_to_identify_themes   \n",
       "4  hashtags_not_used_at_all_to_identify_themes   \n",
       "\n",
       "                              hashtags.annotator2 hashtags.decision East Asia  \\\n",
       "0     hashtags_not_used_at_all_to_identify_themes             agree       NaN   \n",
       "1  hashtags_only_used_to_identify_covid_relevance          disagree       NaN   \n",
       "2     hashtags_not_used_at_all_to_identify_themes             agree       NaN   \n",
       "3     hashtags_not_used_at_all_to_identify_themes             agree       NaN   \n",
       "4  hashtags_only_used_to_identify_covid_relevance          disagree       NaN   \n",
       "\n",
       "  China Hong Kong Japan Korea Singapore  Taiwan  \n",
       "0   NaN       NaN   NaN   NaN       NaN     NaN  \n",
       "1   NaN       NaN   NaN   NaN       NaN     NaN  \n",
       "2   NaN       NaN   NaN   NaN       NaN     NaN  \n",
       "3   NaN       NaN   NaN   NaN       NaN     NaN  \n",
       "4   1.0       NaN   NaN   NaN       NaN     NaN  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EA_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 26 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   id                      20000 non-null  object \n",
      " 1   annot1                  19998 non-null  object \n",
      " 2   annot2                  20000 non-null  object \n",
      " 3   expert                  20000 non-null  object \n",
      " 4   text                    20000 non-null  object \n",
      " 5   text.clean              20000 non-null  object \n",
      " 6   annot1.name             20000 non-null  object \n",
      " 7   annot2.name             20000 non-null  object \n",
      " 8   expert.name             20000 non-null  object \n",
      " 9   target.annot1.clean     5443 non-null   object \n",
      " 10  target.annot2.clean     5129 non-null   object \n",
      " 11  hostile.threatening     3908 non-null   object \n",
      " 12  hostile.dehumanization  3908 non-null   object \n",
      " 13  hostile.interpersonal   3908 non-null   object \n",
      " 14  COVID relevant          20000 non-null  object \n",
      " 15  EA relevant             20000 non-null  object \n",
      " 16  hashtags.annotator1     20000 non-null  object \n",
      " 17  hashtags.annotator2     20000 non-null  object \n",
      " 18  hashtags.decision       20000 non-null  object \n",
      " 19  East Asia               936 non-null    float64\n",
      " 20  China                   5959 non-null   float64\n",
      " 21  Hong Kong               200 non-null    float64\n",
      " 22  Japan                   8 non-null      float64\n",
      " 23  Korea                   22 non-null     float64\n",
      " 24  Singapore               1 non-null      float64\n",
      " 25  Taiwan                  7 non-null      float64\n",
      "dtypes: float64(7), object(19)\n",
      "memory usage: 4.0+ MB\n"
     ]
    }
   ],
   "source": [
    "EA_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@CNN No doubt a ChiNa female. #shameonchina #BoycottBeijing2022 ------\n",
      "@KongTsungGan The #HongKongPoliceBrutality is happening behind the live stream. They are cooperating with the CCP. #Shameonchina #BoycottBeijing2022 ------\n",
      "Afraid.  #ChinaPneumonia ------\n",
      "RT @shapponeko @Mugisalty @CatBus2D everybody should wear masks!  #ChinaPneumonia ------\n",
      "RT @JAbbott45624607 @annie_sparrow This makes me remember the sad days in 2003, china covered up the SARS situation and allowed it to spread to HK. Causing 299 dead and many suffered.   Stop the #chinesepneumonia from spreading! ------\n"
     ]
    }
   ],
   "source": [
    "[print(EA_data['text'][i],'------') for i in range(5)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashtags_not_used_at_all_to_identify_themes ------\n",
      "hashtags_not_used_at_all_to_identify_themes ------\n",
      "hashtags_not_used_at_all_to_identify_themes ------\n",
      "hashtags_not_used_at_all_to_identify_themes ------\n",
      "hashtags_not_used_at_all_to_identify_themes ------\n"
     ]
    }
   ],
   "source": [
    "[print(EA_data['hashtags.annotator1'][i],'------') for i in range(5)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashtags_not_used_at_all_to_identify_themes ------\n",
      "hashtags_only_used_to_identify_covid_relevance ------\n",
      "hashtags_not_used_at_all_to_identify_themes ------\n",
      "hashtags_not_used_at_all_to_identify_themes ------\n",
      "hashtags_only_used_to_identify_covid_relevance ------\n"
     ]
    }
   ],
   "source": [
    "[print(EA_data['hashtags.annotator2'][i],'------') for i in range(5)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agree ------\n",
      "disagree ------\n",
      "agree ------\n",
      "agree ------\n",
      "disagree ------\n"
     ]
    }
   ],
   "source": [
    "[print(EA_data['hashtags.decision'][i],'------') for i in range(5)][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## types and numbers of hashtag themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hashtags.annotator1\n",
       "hashtags_needed_to_identify_both_themes                 4835\n",
       "hashtags_not_used_at_all_to_identify_themes            10769\n",
       "hashtags_only_used_to_identify_covid_relevance          4239\n",
       "hashtags_only_used_to_identify_east_asian_relevance      157\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap = EA_data.groupby(by=['hashtags.annotator1'])\n",
    "ap.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hashtags.annotator2\n",
       "hashtags_needed_to_identify_both_themes                 1407\n",
       "hashtags_not_used_at_all_to_identify_themes             7119\n",
       "hashtags_only_used_to_identify_covid_relevance         10310\n",
       "hashtags_only_used_to_identify_east_asian_relevance     1164\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap = EA_data.groupby(by=['hashtags.annotator2'])\n",
    "ap.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hashtags.decision\n",
       "agree       11151\n",
       "disagree     8849\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap = EA_data.groupby(by=['hashtags.decision'])\n",
    "ap.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## types and numbers of data labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expert\n",
       "counter_speech                         116\n",
       "discussion_of_eastasian_prejudice     1029\n",
       "entity_directed_criticism             1433\n",
       "entity_directed_hostility             3898\n",
       "none_of_the_above                    13524\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap = EA_data.groupby(by=['expert'])\n",
    "ap.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Singapore\n",
       "1.0    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap = EA_data.groupby(by=['Singapore'])\n",
    "ap.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013888888888888888"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for i in EA_data[EA_data['East Asia'] == 1]['expert']:\n",
    "    if i == 'discussion_of_eastasian_prejudice':\n",
    "        count+=1\n",
    "count/len(EA_data[EA_data['East Asia'] == 1]['expert'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine hashtag decision (combine counter_speech with discussion together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 1546743.37it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 2227931.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "relevant1 = []\n",
    "relevant2 = []\n",
    "for i in tqdm(EA_data['hashtags.annotator1']):\n",
    "    if i == 'hashtags_not_used_at_all_to_identify_themes':\n",
    "        relevant1.append(0)\n",
    "    else:\n",
    "        relevant1.append(1)\n",
    "for i in tqdm(EA_data['hashtags.annotator2']):\n",
    "    if i == 'hashtags_not_used_at_all_to_identify_themes':\n",
    "        relevant2.append(0)\n",
    "    else:\n",
    "        relevant2.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_decision = []\n",
    "for i in range(len(relevant1)):\n",
    "    sum_temp = relevant1[i] + relevant2[i]\n",
    "    if sum_temp > 1:\n",
    "        final_decision.append(1)\n",
    "    else:\n",
    "        final_decision.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag keep ratio is: 0.3807\n"
     ]
    }
   ],
   "source": [
    "print(\"Tag keep ratio is:\",final_decision.count(1)/len(final_decision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import emoji\n",
    "\n",
    "class Word_Preprocessing():\n",
    "    def eliminate_url(self,df,target):\n",
    "        print('Start eliminate url: : )')\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        text = df_temp[target_column_name]\n",
    "        for i in tqdm(text):\n",
    "            urls = re.findall(r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})', i)\n",
    "            for i in urls:\n",
    "                df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(i, \"\"))\n",
    "        return df_temp\n",
    "    \n",
    "    def eliminate_username(self,df,target):\n",
    "        print('Start eliminate username: : )')\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        for i in tqdm(df_temp[target_column_name]):\n",
    "            user_name = re.findall(r'@\\w*', i)\n",
    "            for i in user_name:\n",
    "                df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(i, \"\"))\n",
    "        return df_temp\n",
    "     \n",
    "    \n",
    "    def convert_abbreviation(self, df, target):\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        am = \"'m\"\n",
    "        are = \"'re\"\n",
    "        have = \"'ve\"\n",
    "        not_ = \"n't\"\n",
    "        df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(am, \" am\"))\n",
    "        df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(are, \"  are\"))\n",
    "        df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(have, \" have\"))\n",
    "        df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(not_, \" not\"))\n",
    "        return df_temp\n",
    "    \n",
    "    \n",
    "    def final_check(self,df,target):\n",
    "        print('Start Final check: ')\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x:re.sub(r'[^A-Za-z0-9 ]+', ' ', x).lower())\n",
    "        return df_temp\n",
    "            \n",
    "    def eliminate_symbol(self,df,target):\n",
    "        print('Start eliminate symbol: : )')\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        symbol_list = [',',\"'\",'!','@','$','%','^','&','*','(',')','-','+','?','>','<','=','.',':',';','  ','  ','   ','    ','      ','      ','  ']\n",
    "        for i in tqdm(symbol_list):\n",
    "            df_temp[target_column_name] = df_temp[target_column_name].apply(lambda x: x.replace(i, ' '))\n",
    "        return df_temp\n",
    "    \n",
    "    def process_all(self, df,target):\n",
    "        df_temp = df\n",
    "        target_column_name = target\n",
    "        df_fresh = self.convert_abbreviation(df_temp,target_column_name)\n",
    "        df_remove_url = self.eliminate_url(df_fresh,target_column_name)\n",
    "        df_remove_username = self.eliminate_username(df_remove_url, target_column_name)\n",
    "        df_remove_symbol = self.eliminate_symbol(df_remove_username, target_column_name)\n",
    "        df_final_check = self.final_check(df_remove_symbol, target_column_name)\n",
    "        print(\"finished!!\")\n",
    "        return df_final_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Word_Preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                             | 36/20000 [00:00<00:59, 337.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start eliminate url: : )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 20000/20000 [01:14<00:00, 268.81it/s]\n",
      "  0%|                                                                              | 16/20000 [00:00<02:09, 154.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start eliminate username: : )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 20000/20000 [00:03<00:00, 5005.79it/s]\n",
      " 59%|████████████████████████████████████████████████                                 | 16/27 [00:00<00:00, 157.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start eliminate symbol: : )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 127.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Final check: \n",
      "finished!!\n"
     ]
    }
   ],
   "source": [
    "new_EA = processor.process_all(EA_data, 'text.clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " no doubt a china female hashtag eastasia  hashtag ------\n",
      " the hashtag eastasia is happening behind the live stream they are cooperating with the ccp hashtag eastasia  hashtag ------\n",
      "afraid  hashtag ------\n",
      "rt everybody should wear masks  hashtag ------\n",
      "rt this makes me remember the sad days in 2003 china covered up the sars situation and allowed it to spread to hk causing 299 dead and many suffered stop the hashtag eastasia virus from spreading  ------\n",
      " thats how sars started in hong kong in 2003 mainland chinese government tried to cover things up until its broke out in hong kong hundreds of hong kong citizens and medics were dead due to no cure at early stage of eruption  hashtag ------\n",
      " before you wear n95 masks you should look into getting a fit test because unlike surgical masks one size does not fit all for n95 masks having best fit n95 for your face will ensure a good face seal for protection  hashtag hashtag eastasia virus  ------\n",
      "please wear a mask when you come to hong kong   hashtag  hashtag  hashtag  ------\n",
      "hongkongers pls protect ourselves  hashtag ------\n",
      " we call it  chinese pneumonia  before we have more information hashtag eastasia virus  ------\n"
     ]
    }
   ],
   "source": [
    "[print(EA_data['text.clean'][i],'------') for i in range(10)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_EA['text.clean'] = new_EA['text.clean'].apply(lambda x: x.replace('hashtag', \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_EA['text.clean'] = new_EA['text.clean'].apply(lambda x: x.replace('rt', \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/27 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start eliminate symbol: : )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 27/27 [00:00<00:00, 161.14it/s]\n"
     ]
    }
   ],
   "source": [
    "new_EA = processor.eliminate_symbol(new_EA, 'text.clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " no doubt a china female eastasia  ------\n",
      " the eastasia is happening behind the live stream they are cooperating with the ccp eastasia  ------\n",
      "afraid  ------\n",
      " everybody should wear masks  ------\n",
      " this makes me remember the sad days in 2003 china covered up the sars situation and allowed it to spread to hk causing 299 dead and many suffered stop the eastasia virus from spreading  ------\n",
      " thats how sars staed in hong kong in 2003 mainland chinese government tried to cover things up until its broke out in hong kong hundreds of hong kong citizens and medics were dead due to no cure at early stage of eruption  ------\n",
      " before you wear n95 masks you should look into getting a fit test because unlike surgical masks one size does not fit all for n95 masks having best fit n95 for your face will ensure a good face seal for protection eastasia virus  ------\n",
      "please wear a mask when you come to hong kong  ------\n",
      "hongkongers pls protect ourselves  ------\n",
      " we call it chinese pneumonia before we have more information eastasia virus  ------\n"
     ]
    }
   ],
   "source": [
    "[print(new_EA['text.clean'][i],'------') for i in range(10)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_EA = new_EA[['text.clean', 'expert']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-2181c76f21a9>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_EA['length'] = new_EA['text.clean'].apply(lambda x: len(x.split()))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'length')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Plot histogram with the length. Truncate max length to 5000 tokens.\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "new_EA['length'] = new_EA['text.clean'].apply(lambda x: len(x.split()))\n",
    "sns.distplot(new_EA[new_EA['length'] < 150]['length'])\n",
    "plt.title('Frequence of documents of a given length', fontsize=14)\n",
    "plt.xlabel('length', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_EA = new_EA.drop(columns=['length'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the distribution before combine labels ( the number of counter_speech is too few to be recognized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expert\n",
       "counter_speech                         116\n",
       "discussion_of_eastasian_prejudice     1029\n",
       "entity_directed_criticism             1433\n",
       "entity_directed_hostility             3898\n",
       "none_of_the_above                    13524\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap = new_EA.groupby(by=['expert'])\n",
    "ap.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 1542533.93it/s]\n"
     ]
    }
   ],
   "source": [
    "mixed_label = []\n",
    "for i in tqdm(new_EA['expert']):\n",
    "    if i == 'counter_speech':\n",
    "        mixed_label.append('counter_speech+discussion_of_eastasian_prejudice')\n",
    "    elif i == 'discussion_of_eastasian_prejudice':\n",
    "        mixed_label.append('counter_speech+discussion_of_eastasian_prejudice')\n",
    "    else:\n",
    "        mixed_label.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_EA['expert'] = mixed_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "label_final = list(np.unique(new_EA['expert']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 770211.82it/s]\n"
     ]
    }
   ],
   "source": [
    "Final_label = []\n",
    "for i in tqdm(new_EA['expert']):\n",
    "    temp_list = [0 for i in range(len(label_final))]\n",
    "    index = label_final.index(i)\n",
    "    temp_list[index] = 1\n",
    "    Final_label.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_EA['expert'] = Final_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_EA.rename(columns= {'text.clean':'comment'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_EA.rename(columns= {'expert':'label'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no doubt a china female eastasia</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the eastasia is happening behind the live str...</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>afraid</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>everybody should wear masks</td>\n",
       "      <td>[0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this makes me remember the sad days in 2003 c...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment         label\n",
       "0                  no doubt a china female eastasia   [0, 0, 0, 1]\n",
       "1   the eastasia is happening behind the live str...  [0, 0, 0, 1]\n",
       "2                                            afraid   [0, 0, 0, 1]\n",
       "3                       everybody should wear masks   [0, 0, 0, 1]\n",
       "4   this makes me remember the sad days in 2003 c...  [0, 0, 1, 0]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_EA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_EA.to_csv('EA_final.csv',index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)\n",
    "\n",
    "# Set random seed and set device to GPU.\n",
    "torch.manual_seed(17)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)\n",
    "# Initialize tokenizer.\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sections of config\n",
    "\n",
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 80\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.comment_text = dataframe.comment\n",
    "        self.targets = self.data.label\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comment_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        comment_text = str(self.comment_text[index])\n",
    "        comment_text = \" \".join(comment_text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            comment_text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=False\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (20000, 2)\n",
      "TRAIN Dataset: (16000, 2)\n",
      "Val Dataset: (1200, 2)\n",
      "TEST Dataset: (2800, 2)\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "\n",
    "train_size = 0.8\n",
    "val_size = 0.3\n",
    "train_dataset=new_EA.sample(frac=train_size,random_state=200)\n",
    "test_dataset=new_EA.drop(train_dataset.index).reset_index(drop=True)\n",
    "\n",
    "val_dataset=test_dataset.sample(frac=val_size,random_state=200)\n",
    "test_dataset=test_dataset.drop(val_dataset.index).reset_index(drop=True)\n",
    "\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "val_dataset = val_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(new_EA.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"Val Dataset: {}\".format(val_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)\n",
    "val_set = CustomDataset(val_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "val_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "val_loader = DataLoader(val_set, **val_params)\n",
    "test_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with extra layers on top of RoBERTa\n",
    "class ROBERTAClassifier(torch.nn.Module):\n",
    "    def __init__(self, dropout_rate=0.2):\n",
    "        super(ROBERTAClassifier, self).__init__()\n",
    "        \n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.d1 = torch.nn.Dropout(dropout_rate)\n",
    "        self.l1 = torch.nn.Linear(768, 64)\n",
    "        self.bn1 = torch.nn.LayerNorm(64)\n",
    "        self.d2 = torch.nn.Dropout(dropout_rate)\n",
    "        self.l2 = torch.nn.Linear(64, 4)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, x = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        x1 = self.d1(x)\n",
    "        x2 = self.l1(x1)\n",
    "        x3 = self.bn1(x2)\n",
    "        x4 = torch.nn.Tanh()(x3)\n",
    "        x5 = self.d2(x4)\n",
    "        x6 = self.l2(x5)\n",
    "        return x6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ROBERTAClassifier(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (d1): Dropout(p=0.2, inplace=False)\n",
       "  (l1): Linear(in_features=768, out_features=64, bias=True)\n",
       "  (bn1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (d2): Dropout(p=0.2, inplace=False)\n",
       "  (l2): Linear(in_features=64, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ROBERTAClassifier()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train(model,\n",
    "          optimizer,\n",
    "          train_iter,\n",
    "          valid_iter,\n",
    "          scheduler = None,\n",
    "          num_epochs = 5,\n",
    "          valid_period = int(len(train_dataset)/TRAIN_BATCH_SIZE)):\n",
    "    \n",
    "    # Initialize losses and loss histories\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    best_valid_loss = float('Inf')\n",
    "    \n",
    "    global_step = 0\n",
    "    global_steps_list = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Train loop\n",
    "    for epoch in range(num_epochs):\n",
    "        count = 0\n",
    "        for _,data in enumerate(train_iter, 0):\n",
    "            source = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            target = data['targets'].to(device, dtype = torch.float)\n",
    "            y_pred = model(input_ids=source,  \n",
    "                           attention_mask=mask)\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()(y_pred, target)\n",
    "            print('batch_no [{}/{}]:'.format(count, int(len(train_dataset)/TRAIN_BATCH_SIZE)),'training_loss:',loss)\n",
    "            count+=1\n",
    "            loss.backward()\n",
    "            \n",
    "            \n",
    "            # Optimizer and scheduler step\n",
    "            optimizer.step()    \n",
    "            scheduler.step()\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Update train loss and global step\n",
    "            train_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            # Validation loop. Save progress and evaluate model performance.\n",
    "            if global_step % valid_period == 0:\n",
    "                model.eval()\n",
    "                \n",
    "                with torch.no_grad():                    \n",
    "                    for _,data in enumerate(valid_iter, 0):\n",
    "                        source = data['ids'].to(device, dtype = torch.long)\n",
    "                        mask = data['mask'].to(device, dtype = torch.long)\n",
    "                        target = data['targets'].to(device, dtype = torch.float)\n",
    "                        y_pred = model(input_ids=source, \n",
    "                                       attention_mask=mask)\n",
    "\n",
    "                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss()(y_pred, target)\n",
    "\n",
    "                        \n",
    "                        valid_loss += loss.item()\n",
    "\n",
    "                # Store train and validation loss history\n",
    "                train_loss = train_loss / valid_period\n",
    "                valid_loss = valid_loss / len(valid_iter)\n",
    "                train_loss_list.append(train_loss)\n",
    "                valid_loss_list.append(valid_loss)\n",
    "                global_steps_list.append(global_step)\n",
    "\n",
    "                # print summary\n",
    "                print('Epoch [{}/{}], global step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_iter),\n",
    "                              train_loss, valid_loss))\n",
    "                \n",
    "                # checkpoint\n",
    "                if best_valid_loss > valid_loss:\n",
    "                    best_valid_loss = valid_loss\n",
    "                        \n",
    "                train_loss = 0.0                \n",
    "                valid_loss = 0.0\n",
    "                model.train()\n",
    "\n",
    "    print('Training done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================= Start training =================================\n",
      "batch_no [0/1000]: training_loss: tensor(0.7477, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [1/1000]: training_loss: tensor(0.7598, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [2/1000]: training_loss: tensor(0.7481, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [3/1000]: training_loss: tensor(0.7622, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [4/1000]: training_loss: tensor(0.7137, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [5/1000]: training_loss: tensor(0.7569, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [6/1000]: training_loss: tensor(0.7868, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [7/1000]: training_loss: tensor(0.7697, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [8/1000]: training_loss: tensor(0.7228, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [9/1000]: training_loss: tensor(0.7120, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [10/1000]: training_loss: tensor(0.7613, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [11/1000]: training_loss: tensor(0.7467, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [12/1000]: training_loss: tensor(0.7835, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [13/1000]: training_loss: tensor(0.7418, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [14/1000]: training_loss: tensor(0.7436, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [15/1000]: training_loss: tensor(0.7825, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [16/1000]: training_loss: tensor(0.7869, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [17/1000]: training_loss: tensor(0.7630, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [18/1000]: training_loss: tensor(0.7694, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [19/1000]: training_loss: tensor(0.7493, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [20/1000]: training_loss: tensor(0.7515, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [21/1000]: training_loss: tensor(0.7748, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [22/1000]: training_loss: tensor(0.7527, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [23/1000]: training_loss: tensor(0.7262, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [24/1000]: training_loss: tensor(0.7366, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [25/1000]: training_loss: tensor(0.7764, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [26/1000]: training_loss: tensor(0.7750, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [27/1000]: training_loss: tensor(0.7446, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [28/1000]: training_loss: tensor(0.7523, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [29/1000]: training_loss: tensor(0.7554, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [30/1000]: training_loss: tensor(0.7311, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [31/1000]: training_loss: tensor(0.7872, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [32/1000]: training_loss: tensor(0.7905, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [33/1000]: training_loss: tensor(0.7464, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [34/1000]: training_loss: tensor(0.7653, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [35/1000]: training_loss: tensor(0.7458, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [36/1000]: training_loss: tensor(0.7593, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [37/1000]: training_loss: tensor(0.7401, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [38/1000]: training_loss: tensor(0.7554, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [39/1000]: training_loss: tensor(0.7440, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [40/1000]: training_loss: tensor(0.7421, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [41/1000]: training_loss: tensor(0.7617, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [42/1000]: training_loss: tensor(0.7296, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [43/1000]: training_loss: tensor(0.7874, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [44/1000]: training_loss: tensor(0.7437, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [45/1000]: training_loss: tensor(0.7407, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [46/1000]: training_loss: tensor(0.7629, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [47/1000]: training_loss: tensor(0.7572, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [48/1000]: training_loss: tensor(0.7507, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [49/1000]: training_loss: tensor(0.7986, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [50/1000]: training_loss: tensor(0.7446, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [51/1000]: training_loss: tensor(0.7261, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [52/1000]: training_loss: tensor(0.7756, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [53/1000]: training_loss: tensor(0.7912, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [54/1000]: training_loss: tensor(0.7663, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [55/1000]: training_loss: tensor(0.7326, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [56/1000]: training_loss: tensor(0.7586, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [57/1000]: training_loss: tensor(0.7875, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [58/1000]: training_loss: tensor(0.7514, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [59/1000]: training_loss: tensor(0.7525, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [60/1000]: training_loss: tensor(0.7638, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [61/1000]: training_loss: tensor(0.7279, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [62/1000]: training_loss: tensor(0.7351, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [63/1000]: training_loss: tensor(0.7730, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [64/1000]: training_loss: tensor(0.7554, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [65/1000]: training_loss: tensor(0.7435, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [66/1000]: training_loss: tensor(0.7462, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [67/1000]: training_loss: tensor(0.7269, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [68/1000]: training_loss: tensor(0.7511, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [69/1000]: training_loss: tensor(0.7406, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [70/1000]: training_loss: tensor(0.6967, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [71/1000]: training_loss: tensor(0.7848, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [72/1000]: training_loss: tensor(0.7202, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [73/1000]: training_loss: tensor(0.7789, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [74/1000]: training_loss: tensor(0.7578, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [75/1000]: training_loss: tensor(0.7874, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [76/1000]: training_loss: tensor(0.7375, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [77/1000]: training_loss: tensor(0.7632, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [78/1000]: training_loss: tensor(0.7498, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [79/1000]: training_loss: tensor(0.7419, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [80/1000]: training_loss: tensor(0.7816, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [81/1000]: training_loss: tensor(0.7508, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [82/1000]: training_loss: tensor(0.7902, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [83/1000]: training_loss: tensor(0.7890, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [84/1000]: training_loss: tensor(0.7723, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [85/1000]: training_loss: tensor(0.7303, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [86/1000]: training_loss: tensor(0.7415, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [87/1000]: training_loss: tensor(0.7841, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [88/1000]: training_loss: tensor(0.7582, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [89/1000]: training_loss: tensor(0.7688, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [90/1000]: training_loss: tensor(0.7191, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [91/1000]: training_loss: tensor(0.7290, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [92/1000]: training_loss: tensor(0.7557, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [93/1000]: training_loss: tensor(0.7832, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [94/1000]: training_loss: tensor(0.7656, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [95/1000]: training_loss: tensor(0.7592, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [96/1000]: training_loss: tensor(0.7119, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [97/1000]: training_loss: tensor(0.7858, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [98/1000]: training_loss: tensor(0.7278, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [99/1000]: training_loss: tensor(0.8074, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [100/1000]: training_loss: tensor(0.7725, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [101/1000]: training_loss: tensor(0.7515, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [102/1000]: training_loss: tensor(0.7932, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [103/1000]: training_loss: tensor(0.7637, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [104/1000]: training_loss: tensor(0.7399, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [105/1000]: training_loss: tensor(0.7324, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [106/1000]: training_loss: tensor(0.7685, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [107/1000]: training_loss: tensor(0.7461, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [108/1000]: training_loss: tensor(0.7279, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [109/1000]: training_loss: tensor(0.7309, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [110/1000]: training_loss: tensor(0.7703, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [111/1000]: training_loss: tensor(0.7311, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [112/1000]: training_loss: tensor(0.7283, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [113/1000]: training_loss: tensor(0.7840, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [114/1000]: training_loss: tensor(0.7372, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [115/1000]: training_loss: tensor(0.7601, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [116/1000]: training_loss: tensor(0.7843, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [117/1000]: training_loss: tensor(0.7131, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [118/1000]: training_loss: tensor(0.7353, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [119/1000]: training_loss: tensor(0.7374, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [120/1000]: training_loss: tensor(0.7058, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [121/1000]: training_loss: tensor(0.7552, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [122/1000]: training_loss: tensor(0.7649, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [123/1000]: training_loss: tensor(0.7748, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [124/1000]: training_loss: tensor(0.7417, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [125/1000]: training_loss: tensor(0.7208, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [126/1000]: training_loss: tensor(0.7588, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [127/1000]: training_loss: tensor(0.7875, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [128/1000]: training_loss: tensor(0.7367, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [129/1000]: training_loss: tensor(0.7482, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [130/1000]: training_loss: tensor(0.7192, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [131/1000]: training_loss: tensor(0.7602, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [132/1000]: training_loss: tensor(0.7328, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [133/1000]: training_loss: tensor(0.7660, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [134/1000]: training_loss: tensor(0.7175, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [135/1000]: training_loss: tensor(0.7645, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [136/1000]: training_loss: tensor(0.7710, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [137/1000]: training_loss: tensor(0.7408, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [138/1000]: training_loss: tensor(0.7599, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [139/1000]: training_loss: tensor(0.7613, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [140/1000]: training_loss: tensor(0.7171, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [141/1000]: training_loss: tensor(0.7755, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [142/1000]: training_loss: tensor(0.7679, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [143/1000]: training_loss: tensor(0.7762, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [144/1000]: training_loss: tensor(0.7458, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [145/1000]: training_loss: tensor(0.7674, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [146/1000]: training_loss: tensor(0.7275, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [147/1000]: training_loss: tensor(0.7293, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [148/1000]: training_loss: tensor(0.7531, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [149/1000]: training_loss: tensor(0.7169, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [150/1000]: training_loss: tensor(0.7423, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [151/1000]: training_loss: tensor(0.7714, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [152/1000]: training_loss: tensor(0.7524, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [153/1000]: training_loss: tensor(0.7355, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [154/1000]: training_loss: tensor(0.7514, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [155/1000]: training_loss: tensor(0.7326, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [156/1000]: training_loss: tensor(0.7690, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [157/1000]: training_loss: tensor(0.7442, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [158/1000]: training_loss: tensor(0.7377, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [159/1000]: training_loss: tensor(0.6979, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [160/1000]: training_loss: tensor(0.7889, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [161/1000]: training_loss: tensor(0.7696, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [162/1000]: training_loss: tensor(0.7390, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [163/1000]: training_loss: tensor(0.7412, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [164/1000]: training_loss: tensor(0.7692, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [165/1000]: training_loss: tensor(0.7591, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [166/1000]: training_loss: tensor(0.7552, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [167/1000]: training_loss: tensor(0.7658, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [168/1000]: training_loss: tensor(0.7690, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [169/1000]: training_loss: tensor(0.7393, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [170/1000]: training_loss: tensor(0.7621, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [171/1000]: training_loss: tensor(0.7475, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [172/1000]: training_loss: tensor(0.7242, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [173/1000]: training_loss: tensor(0.7448, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [174/1000]: training_loss: tensor(0.7505, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [175/1000]: training_loss: tensor(0.7302, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [176/1000]: training_loss: tensor(0.7622, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [177/1000]: training_loss: tensor(0.7273, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [178/1000]: training_loss: tensor(0.7648, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [179/1000]: training_loss: tensor(0.8017, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [180/1000]: training_loss: tensor(0.7633, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [181/1000]: training_loss: tensor(0.7143, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [182/1000]: training_loss: tensor(0.7458, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [183/1000]: training_loss: tensor(0.7567, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [184/1000]: training_loss: tensor(0.7340, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [185/1000]: training_loss: tensor(0.7619, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [186/1000]: training_loss: tensor(0.7511, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [187/1000]: training_loss: tensor(0.7936, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [188/1000]: training_loss: tensor(0.7435, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [189/1000]: training_loss: tensor(0.7496, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [190/1000]: training_loss: tensor(0.7408, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [191/1000]: training_loss: tensor(0.7484, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [192/1000]: training_loss: tensor(0.7092, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [193/1000]: training_loss: tensor(0.7324, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [194/1000]: training_loss: tensor(0.7560, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [195/1000]: training_loss: tensor(0.7995, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [196/1000]: training_loss: tensor(0.7444, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [197/1000]: training_loss: tensor(0.7440, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [198/1000]: training_loss: tensor(0.7263, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [199/1000]: training_loss: tensor(0.7224, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [200/1000]: training_loss: tensor(0.7039, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [201/1000]: training_loss: tensor(0.7696, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [202/1000]: training_loss: tensor(0.7596, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [203/1000]: training_loss: tensor(0.7681, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [204/1000]: training_loss: tensor(0.7452, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [205/1000]: training_loss: tensor(0.7363, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [206/1000]: training_loss: tensor(0.7429, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [207/1000]: training_loss: tensor(0.7686, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [208/1000]: training_loss: tensor(0.7592, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [209/1000]: training_loss: tensor(0.7380, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [210/1000]: training_loss: tensor(0.7455, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [211/1000]: training_loss: tensor(0.7701, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [212/1000]: training_loss: tensor(0.7252, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [213/1000]: training_loss: tensor(0.7541, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [214/1000]: training_loss: tensor(0.7544, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [215/1000]: training_loss: tensor(0.7247, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [216/1000]: training_loss: tensor(0.7531, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [217/1000]: training_loss: tensor(0.7363, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [218/1000]: training_loss: tensor(0.7418, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [219/1000]: training_loss: tensor(0.7364, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [220/1000]: training_loss: tensor(0.6990, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [221/1000]: training_loss: tensor(0.7415, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [222/1000]: training_loss: tensor(0.6917, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [223/1000]: training_loss: tensor(0.7512, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [224/1000]: training_loss: tensor(0.7037, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [225/1000]: training_loss: tensor(0.7554, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [226/1000]: training_loss: tensor(0.7839, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [227/1000]: training_loss: tensor(0.7467, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [228/1000]: training_loss: tensor(0.7537, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [229/1000]: training_loss: tensor(0.7288, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [230/1000]: training_loss: tensor(0.7356, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [231/1000]: training_loss: tensor(0.7359, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [232/1000]: training_loss: tensor(0.7562, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [233/1000]: training_loss: tensor(0.7468, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [234/1000]: training_loss: tensor(0.7395, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [235/1000]: training_loss: tensor(0.7601, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [236/1000]: training_loss: tensor(0.7554, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [237/1000]: training_loss: tensor(0.7556, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [238/1000]: training_loss: tensor(0.7894, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [239/1000]: training_loss: tensor(0.7603, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [240/1000]: training_loss: tensor(0.7589, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [241/1000]: training_loss: tensor(0.7719, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [242/1000]: training_loss: tensor(0.7053, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [243/1000]: training_loss: tensor(0.7395, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [244/1000]: training_loss: tensor(0.7298, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [245/1000]: training_loss: tensor(0.7420, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [246/1000]: training_loss: tensor(0.7485, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [247/1000]: training_loss: tensor(0.7356, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [248/1000]: training_loss: tensor(0.7364, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [249/1000]: training_loss: tensor(0.7378, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [250/1000]: training_loss: tensor(0.7881, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [251/1000]: training_loss: tensor(0.7680, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [252/1000]: training_loss: tensor(0.7601, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [253/1000]: training_loss: tensor(0.7190, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [254/1000]: training_loss: tensor(0.7348, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [255/1000]: training_loss: tensor(0.7449, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [256/1000]: training_loss: tensor(0.7567, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [257/1000]: training_loss: tensor(0.7253, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [258/1000]: training_loss: tensor(0.7492, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [259/1000]: training_loss: tensor(0.7232, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [260/1000]: training_loss: tensor(0.7602, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [261/1000]: training_loss: tensor(0.7397, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [262/1000]: training_loss: tensor(0.7340, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [263/1000]: training_loss: tensor(0.7530, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [264/1000]: training_loss: tensor(0.7478, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [265/1000]: training_loss: tensor(0.7507, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [266/1000]: training_loss: tensor(0.7410, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [267/1000]: training_loss: tensor(0.7281, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [268/1000]: training_loss: tensor(0.6963, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [269/1000]: training_loss: tensor(0.7585, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [270/1000]: training_loss: tensor(0.7398, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [271/1000]: training_loss: tensor(0.7139, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [272/1000]: training_loss: tensor(0.7531, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [273/1000]: training_loss: tensor(0.7366, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [274/1000]: training_loss: tensor(0.7105, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [275/1000]: training_loss: tensor(0.7171, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [276/1000]: training_loss: tensor(0.7523, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [277/1000]: training_loss: tensor(0.7338, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [278/1000]: training_loss: tensor(0.7574, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [279/1000]: training_loss: tensor(0.7396, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [280/1000]: training_loss: tensor(0.7652, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [281/1000]: training_loss: tensor(0.7233, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [282/1000]: training_loss: tensor(0.7242, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [283/1000]: training_loss: tensor(0.7257, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [284/1000]: training_loss: tensor(0.7320, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [285/1000]: training_loss: tensor(0.7368, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [286/1000]: training_loss: tensor(0.7326, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [287/1000]: training_loss: tensor(0.7425, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [288/1000]: training_loss: tensor(0.6965, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [289/1000]: training_loss: tensor(0.7585, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [290/1000]: training_loss: tensor(0.7264, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [291/1000]: training_loss: tensor(0.7274, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [292/1000]: training_loss: tensor(0.7480, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [293/1000]: training_loss: tensor(0.7160, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [294/1000]: training_loss: tensor(0.7530, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [295/1000]: training_loss: tensor(0.7452, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [296/1000]: training_loss: tensor(0.7393, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [297/1000]: training_loss: tensor(0.7525, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [298/1000]: training_loss: tensor(0.7621, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [299/1000]: training_loss: tensor(0.7124, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [300/1000]: training_loss: tensor(0.7344, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [301/1000]: training_loss: tensor(0.7230, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [302/1000]: training_loss: tensor(0.7199, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [303/1000]: training_loss: tensor(0.7449, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [304/1000]: training_loss: tensor(0.7311, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [305/1000]: training_loss: tensor(0.7317, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [306/1000]: training_loss: tensor(0.7448, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [307/1000]: training_loss: tensor(0.7346, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [308/1000]: training_loss: tensor(0.7585, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [309/1000]: training_loss: tensor(0.7410, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [310/1000]: training_loss: tensor(0.7198, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [311/1000]: training_loss: tensor(0.7201, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [312/1000]: training_loss: tensor(0.6818, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [313/1000]: training_loss: tensor(0.7301, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [314/1000]: training_loss: tensor(0.7209, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [315/1000]: training_loss: tensor(0.7646, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [316/1000]: training_loss: tensor(0.7682, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [317/1000]: training_loss: tensor(0.7232, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [318/1000]: training_loss: tensor(0.7156, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [319/1000]: training_loss: tensor(0.7303, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [320/1000]: training_loss: tensor(0.7247, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [321/1000]: training_loss: tensor(0.7008, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [322/1000]: training_loss: tensor(0.7594, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [323/1000]: training_loss: tensor(0.7512, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [324/1000]: training_loss: tensor(0.7233, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [325/1000]: training_loss: tensor(0.6847, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [326/1000]: training_loss: tensor(0.7157, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [327/1000]: training_loss: tensor(0.7306, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [328/1000]: training_loss: tensor(0.6984, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [329/1000]: training_loss: tensor(0.7404, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [330/1000]: training_loss: tensor(0.7383, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [331/1000]: training_loss: tensor(0.7292, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [332/1000]: training_loss: tensor(0.7323, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [333/1000]: training_loss: tensor(0.7131, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [334/1000]: training_loss: tensor(0.7079, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [335/1000]: training_loss: tensor(0.7457, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [336/1000]: training_loss: tensor(0.7392, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [337/1000]: training_loss: tensor(0.7107, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [338/1000]: training_loss: tensor(0.7065, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [339/1000]: training_loss: tensor(0.6926, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [340/1000]: training_loss: tensor(0.7113, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [341/1000]: training_loss: tensor(0.7321, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [342/1000]: training_loss: tensor(0.7332, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [343/1000]: training_loss: tensor(0.7625, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [344/1000]: training_loss: tensor(0.7491, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [345/1000]: training_loss: tensor(0.7000, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [346/1000]: training_loss: tensor(0.7082, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [347/1000]: training_loss: tensor(0.7107, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [348/1000]: training_loss: tensor(0.7388, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [349/1000]: training_loss: tensor(0.6997, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [350/1000]: training_loss: tensor(0.7348, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [351/1000]: training_loss: tensor(0.7323, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [352/1000]: training_loss: tensor(0.6954, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [353/1000]: training_loss: tensor(0.7337, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [354/1000]: training_loss: tensor(0.7448, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [355/1000]: training_loss: tensor(0.7767, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [356/1000]: training_loss: tensor(0.7346, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [357/1000]: training_loss: tensor(0.7559, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [358/1000]: training_loss: tensor(0.7526, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [359/1000]: training_loss: tensor(0.7541, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [360/1000]: training_loss: tensor(0.7419, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [361/1000]: training_loss: tensor(0.7203, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [362/1000]: training_loss: tensor(0.7248, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [363/1000]: training_loss: tensor(0.7208, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [364/1000]: training_loss: tensor(0.7296, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [365/1000]: training_loss: tensor(0.7505, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [366/1000]: training_loss: tensor(0.7394, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [367/1000]: training_loss: tensor(0.7419, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [368/1000]: training_loss: tensor(0.7127, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [369/1000]: training_loss: tensor(0.7147, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [370/1000]: training_loss: tensor(0.7079, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [371/1000]: training_loss: tensor(0.7240, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [372/1000]: training_loss: tensor(0.7365, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [373/1000]: training_loss: tensor(0.7205, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [374/1000]: training_loss: tensor(0.7304, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [375/1000]: training_loss: tensor(0.7230, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [376/1000]: training_loss: tensor(0.7388, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [377/1000]: training_loss: tensor(0.7171, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [378/1000]: training_loss: tensor(0.7065, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [379/1000]: training_loss: tensor(0.7170, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [380/1000]: training_loss: tensor(0.7365, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [381/1000]: training_loss: tensor(0.7145, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [382/1000]: training_loss: tensor(0.6900, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [383/1000]: training_loss: tensor(0.7279, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [384/1000]: training_loss: tensor(0.7079, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [385/1000]: training_loss: tensor(0.7098, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [386/1000]: training_loss: tensor(0.7400, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [387/1000]: training_loss: tensor(0.6996, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [388/1000]: training_loss: tensor(0.7215, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [389/1000]: training_loss: tensor(0.6999, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [390/1000]: training_loss: tensor(0.7232, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [391/1000]: training_loss: tensor(0.7171, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [392/1000]: training_loss: tensor(0.7092, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [393/1000]: training_loss: tensor(0.7290, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [394/1000]: training_loss: tensor(0.7275, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [395/1000]: training_loss: tensor(0.7288, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [396/1000]: training_loss: tensor(0.7364, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [397/1000]: training_loss: tensor(0.6718, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [398/1000]: training_loss: tensor(0.7243, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [399/1000]: training_loss: tensor(0.7227, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [400/1000]: training_loss: tensor(0.7208, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [401/1000]: training_loss: tensor(0.6844, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [402/1000]: training_loss: tensor(0.7404, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [403/1000]: training_loss: tensor(0.7004, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [404/1000]: training_loss: tensor(0.6943, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [405/1000]: training_loss: tensor(0.7053, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [406/1000]: training_loss: tensor(0.6912, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [407/1000]: training_loss: tensor(0.7408, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [408/1000]: training_loss: tensor(0.7014, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [409/1000]: training_loss: tensor(0.7158, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [410/1000]: training_loss: tensor(0.7070, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [411/1000]: training_loss: tensor(0.6826, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [412/1000]: training_loss: tensor(0.7111, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [413/1000]: training_loss: tensor(0.6837, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [414/1000]: training_loss: tensor(0.6839, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [415/1000]: training_loss: tensor(0.6794, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [416/1000]: training_loss: tensor(0.6811, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [417/1000]: training_loss: tensor(0.6939, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [418/1000]: training_loss: tensor(0.7177, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [419/1000]: training_loss: tensor(0.7040, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [420/1000]: training_loss: tensor(0.7081, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [421/1000]: training_loss: tensor(0.7206, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [422/1000]: training_loss: tensor(0.7135, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [423/1000]: training_loss: tensor(0.7084, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [424/1000]: training_loss: tensor(0.7271, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [425/1000]: training_loss: tensor(0.6936, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [426/1000]: training_loss: tensor(0.7396, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [427/1000]: training_loss: tensor(0.7095, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [428/1000]: training_loss: tensor(0.7154, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [429/1000]: training_loss: tensor(0.7483, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [430/1000]: training_loss: tensor(0.7039, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [431/1000]: training_loss: tensor(0.7136, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [432/1000]: training_loss: tensor(0.7257, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [433/1000]: training_loss: tensor(0.6748, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [434/1000]: training_loss: tensor(0.6939, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [435/1000]: training_loss: tensor(0.6814, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [436/1000]: training_loss: tensor(0.6995, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [437/1000]: training_loss: tensor(0.6942, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [438/1000]: training_loss: tensor(0.7356, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [439/1000]: training_loss: tensor(0.7087, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [440/1000]: training_loss: tensor(0.6980, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [441/1000]: training_loss: tensor(0.7111, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [442/1000]: training_loss: tensor(0.7238, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [443/1000]: training_loss: tensor(0.6872, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [444/1000]: training_loss: tensor(0.7562, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [445/1000]: training_loss: tensor(0.7018, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [446/1000]: training_loss: tensor(0.7147, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [447/1000]: training_loss: tensor(0.7111, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [448/1000]: training_loss: tensor(0.6592, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [449/1000]: training_loss: tensor(0.7324, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [450/1000]: training_loss: tensor(0.7144, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [451/1000]: training_loss: tensor(0.7117, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [452/1000]: training_loss: tensor(0.7172, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [453/1000]: training_loss: tensor(0.7047, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [454/1000]: training_loss: tensor(0.6938, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [455/1000]: training_loss: tensor(0.6770, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [456/1000]: training_loss: tensor(0.7010, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [457/1000]: training_loss: tensor(0.6891, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [458/1000]: training_loss: tensor(0.6804, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [459/1000]: training_loss: tensor(0.6941, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [460/1000]: training_loss: tensor(0.6833, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [461/1000]: training_loss: tensor(0.6803, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [462/1000]: training_loss: tensor(0.7024, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [463/1000]: training_loss: tensor(0.7096, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [464/1000]: training_loss: tensor(0.6790, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [465/1000]: training_loss: tensor(0.6732, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [466/1000]: training_loss: tensor(0.6800, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [467/1000]: training_loss: tensor(0.7137, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [468/1000]: training_loss: tensor(0.6679, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [469/1000]: training_loss: tensor(0.7007, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [470/1000]: training_loss: tensor(0.6690, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [471/1000]: training_loss: tensor(0.6921, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [472/1000]: training_loss: tensor(0.6727, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [473/1000]: training_loss: tensor(0.7231, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [474/1000]: training_loss: tensor(0.7233, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [475/1000]: training_loss: tensor(0.7071, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [476/1000]: training_loss: tensor(0.6735, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [477/1000]: training_loss: tensor(0.7143, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [478/1000]: training_loss: tensor(0.6376, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [479/1000]: training_loss: tensor(0.6827, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [480/1000]: training_loss: tensor(0.6885, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [481/1000]: training_loss: tensor(0.6898, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [482/1000]: training_loss: tensor(0.6840, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [483/1000]: training_loss: tensor(0.7092, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [484/1000]: training_loss: tensor(0.6786, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [485/1000]: training_loss: tensor(0.6798, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [486/1000]: training_loss: tensor(0.7119, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [487/1000]: training_loss: tensor(0.6733, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [488/1000]: training_loss: tensor(0.6976, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [489/1000]: training_loss: tensor(0.6996, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [490/1000]: training_loss: tensor(0.7071, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [491/1000]: training_loss: tensor(0.6703, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [492/1000]: training_loss: tensor(0.6852, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [493/1000]: training_loss: tensor(0.6675, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [494/1000]: training_loss: tensor(0.6644, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [495/1000]: training_loss: tensor(0.6888, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [496/1000]: training_loss: tensor(0.6910, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [497/1000]: training_loss: tensor(0.6661, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [498/1000]: training_loss: tensor(0.6819, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [499/1000]: training_loss: tensor(0.6762, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [500/1000]: training_loss: tensor(0.6782, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [501/1000]: training_loss: tensor(0.7085, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [502/1000]: training_loss: tensor(0.6725, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [503/1000]: training_loss: tensor(0.6797, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [504/1000]: training_loss: tensor(0.7064, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [505/1000]: training_loss: tensor(0.6812, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [506/1000]: training_loss: tensor(0.6987, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [507/1000]: training_loss: tensor(0.6587, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [508/1000]: training_loss: tensor(0.6854, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [509/1000]: training_loss: tensor(0.7122, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [510/1000]: training_loss: tensor(0.6940, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [511/1000]: training_loss: tensor(0.6527, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [512/1000]: training_loss: tensor(0.6851, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [513/1000]: training_loss: tensor(0.6653, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [514/1000]: training_loss: tensor(0.6935, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [515/1000]: training_loss: tensor(0.6716, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [516/1000]: training_loss: tensor(0.6448, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [517/1000]: training_loss: tensor(0.6967, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [518/1000]: training_loss: tensor(0.6824, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [519/1000]: training_loss: tensor(0.7115, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [520/1000]: training_loss: tensor(0.6714, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [521/1000]: training_loss: tensor(0.6804, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [522/1000]: training_loss: tensor(0.6544, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [523/1000]: training_loss: tensor(0.6739, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [524/1000]: training_loss: tensor(0.6817, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [525/1000]: training_loss: tensor(0.6955, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [526/1000]: training_loss: tensor(0.6717, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [527/1000]: training_loss: tensor(0.6840, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [528/1000]: training_loss: tensor(0.6793, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [529/1000]: training_loss: tensor(0.6700, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [530/1000]: training_loss: tensor(0.6620, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [531/1000]: training_loss: tensor(0.6891, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [532/1000]: training_loss: tensor(0.6702, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [533/1000]: training_loss: tensor(0.6736, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [534/1000]: training_loss: tensor(0.6556, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [535/1000]: training_loss: tensor(0.6645, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [536/1000]: training_loss: tensor(0.6527, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [537/1000]: training_loss: tensor(0.6462, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [538/1000]: training_loss: tensor(0.6726, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [539/1000]: training_loss: tensor(0.6827, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [540/1000]: training_loss: tensor(0.6914, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [541/1000]: training_loss: tensor(0.6676, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [542/1000]: training_loss: tensor(0.6966, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [543/1000]: training_loss: tensor(0.6855, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [544/1000]: training_loss: tensor(0.6764, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [545/1000]: training_loss: tensor(0.6475, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [546/1000]: training_loss: tensor(0.6419, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [547/1000]: training_loss: tensor(0.6379, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [548/1000]: training_loss: tensor(0.6706, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [549/1000]: training_loss: tensor(0.6626, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [550/1000]: training_loss: tensor(0.6575, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [551/1000]: training_loss: tensor(0.6665, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [552/1000]: training_loss: tensor(0.6713, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [553/1000]: training_loss: tensor(0.6651, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [554/1000]: training_loss: tensor(0.6455, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [555/1000]: training_loss: tensor(0.6353, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [556/1000]: training_loss: tensor(0.6510, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [557/1000]: training_loss: tensor(0.6109, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [558/1000]: training_loss: tensor(0.6793, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [559/1000]: training_loss: tensor(0.6563, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [560/1000]: training_loss: tensor(0.6522, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [561/1000]: training_loss: tensor(0.6673, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [562/1000]: training_loss: tensor(0.6636, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [563/1000]: training_loss: tensor(0.6355, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [564/1000]: training_loss: tensor(0.6758, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [565/1000]: training_loss: tensor(0.6682, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [566/1000]: training_loss: tensor(0.6431, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [567/1000]: training_loss: tensor(0.6404, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [568/1000]: training_loss: tensor(0.6681, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [569/1000]: training_loss: tensor(0.6436, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [570/1000]: training_loss: tensor(0.6695, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [571/1000]: training_loss: tensor(0.6261, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [572/1000]: training_loss: tensor(0.6489, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [573/1000]: training_loss: tensor(0.6572, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [574/1000]: training_loss: tensor(0.6289, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [575/1000]: training_loss: tensor(0.6554, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [576/1000]: training_loss: tensor(0.6707, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [577/1000]: training_loss: tensor(0.6370, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [578/1000]: training_loss: tensor(0.6277, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [579/1000]: training_loss: tensor(0.6176, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [580/1000]: training_loss: tensor(0.6481, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [581/1000]: training_loss: tensor(0.6740, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [582/1000]: training_loss: tensor(0.6153, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [583/1000]: training_loss: tensor(0.6381, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [584/1000]: training_loss: tensor(0.6415, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [585/1000]: training_loss: tensor(0.6391, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [586/1000]: training_loss: tensor(0.6376, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [587/1000]: training_loss: tensor(0.6539, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [588/1000]: training_loss: tensor(0.6205, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [589/1000]: training_loss: tensor(0.6267, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [590/1000]: training_loss: tensor(0.6343, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [591/1000]: training_loss: tensor(0.6027, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [592/1000]: training_loss: tensor(0.6425, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [593/1000]: training_loss: tensor(0.6331, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [594/1000]: training_loss: tensor(0.6297, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [595/1000]: training_loss: tensor(0.6246, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [596/1000]: training_loss: tensor(0.6352, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [597/1000]: training_loss: tensor(0.6445, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [598/1000]: training_loss: tensor(0.6348, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [599/1000]: training_loss: tensor(0.6110, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [600/1000]: training_loss: tensor(0.6105, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [601/1000]: training_loss: tensor(0.6216, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [602/1000]: training_loss: tensor(0.6102, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [603/1000]: training_loss: tensor(0.6192, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [604/1000]: training_loss: tensor(0.6217, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [605/1000]: training_loss: tensor(0.6412, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [606/1000]: training_loss: tensor(0.6160, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [607/1000]: training_loss: tensor(0.6268, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [608/1000]: training_loss: tensor(0.6425, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [609/1000]: training_loss: tensor(0.6054, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [610/1000]: training_loss: tensor(0.6086, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [611/1000]: training_loss: tensor(0.6198, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [612/1000]: training_loss: tensor(0.6095, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [613/1000]: training_loss: tensor(0.6221, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [614/1000]: training_loss: tensor(0.6173, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [615/1000]: training_loss: tensor(0.6320, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [616/1000]: training_loss: tensor(0.6109, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [617/1000]: training_loss: tensor(0.6006, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [618/1000]: training_loss: tensor(0.5898, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [619/1000]: training_loss: tensor(0.6288, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [620/1000]: training_loss: tensor(0.6119, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [621/1000]: training_loss: tensor(0.6047, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [622/1000]: training_loss: tensor(0.5753, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [623/1000]: training_loss: tensor(0.5878, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [624/1000]: training_loss: tensor(0.5777, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [625/1000]: training_loss: tensor(0.5991, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [626/1000]: training_loss: tensor(0.5831, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [627/1000]: training_loss: tensor(0.6122, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [628/1000]: training_loss: tensor(0.6070, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [629/1000]: training_loss: tensor(0.5927, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [630/1000]: training_loss: tensor(0.5750, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [631/1000]: training_loss: tensor(0.5965, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [632/1000]: training_loss: tensor(0.6016, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [633/1000]: training_loss: tensor(0.5808, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [634/1000]: training_loss: tensor(0.5748, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [635/1000]: training_loss: tensor(0.6064, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [636/1000]: training_loss: tensor(0.5636, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [637/1000]: training_loss: tensor(0.5461, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [638/1000]: training_loss: tensor(0.5872, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [639/1000]: training_loss: tensor(0.5777, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [640/1000]: training_loss: tensor(0.5808, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [641/1000]: training_loss: tensor(0.5818, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [642/1000]: training_loss: tensor(0.6029, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [643/1000]: training_loss: tensor(0.5543, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [644/1000]: training_loss: tensor(0.5752, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [645/1000]: training_loss: tensor(0.5796, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [646/1000]: training_loss: tensor(0.5740, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [647/1000]: training_loss: tensor(0.5840, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [648/1000]: training_loss: tensor(0.5960, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [649/1000]: training_loss: tensor(0.5790, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [650/1000]: training_loss: tensor(0.5833, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [651/1000]: training_loss: tensor(0.5572, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [652/1000]: training_loss: tensor(0.5794, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [653/1000]: training_loss: tensor(0.5735, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [654/1000]: training_loss: tensor(0.5521, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [655/1000]: training_loss: tensor(0.5214, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [656/1000]: training_loss: tensor(0.5691, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [657/1000]: training_loss: tensor(0.5929, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [658/1000]: training_loss: tensor(0.5551, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [659/1000]: training_loss: tensor(0.5602, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [660/1000]: training_loss: tensor(0.5539, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [661/1000]: training_loss: tensor(0.5502, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [662/1000]: training_loss: tensor(0.5375, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [663/1000]: training_loss: tensor(0.5675, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [664/1000]: training_loss: tensor(0.5545, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [665/1000]: training_loss: tensor(0.5700, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [666/1000]: training_loss: tensor(0.5261, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [667/1000]: training_loss: tensor(0.5546, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [668/1000]: training_loss: tensor(0.5418, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [669/1000]: training_loss: tensor(0.5526, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [670/1000]: training_loss: tensor(0.5701, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [671/1000]: training_loss: tensor(0.5321, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [672/1000]: training_loss: tensor(0.5345, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [673/1000]: training_loss: tensor(0.5422, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [674/1000]: training_loss: tensor(0.5412, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [675/1000]: training_loss: tensor(0.5577, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [676/1000]: training_loss: tensor(0.5479, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [677/1000]: training_loss: tensor(0.5817, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [678/1000]: training_loss: tensor(0.5670, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [679/1000]: training_loss: tensor(0.5850, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [680/1000]: training_loss: tensor(0.5724, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [681/1000]: training_loss: tensor(0.5256, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [682/1000]: training_loss: tensor(0.5603, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [683/1000]: training_loss: tensor(0.5464, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [684/1000]: training_loss: tensor(0.5584, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [685/1000]: training_loss: tensor(0.5646, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [686/1000]: training_loss: tensor(0.5686, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [687/1000]: training_loss: tensor(0.5294, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [688/1000]: training_loss: tensor(0.5176, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [689/1000]: training_loss: tensor(0.5808, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [690/1000]: training_loss: tensor(0.5152, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [691/1000]: training_loss: tensor(0.5251, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [692/1000]: training_loss: tensor(0.5504, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [693/1000]: training_loss: tensor(0.5337, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [694/1000]: training_loss: tensor(0.5354, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [695/1000]: training_loss: tensor(0.5284, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [696/1000]: training_loss: tensor(0.6031, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [697/1000]: training_loss: tensor(0.5381, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [698/1000]: training_loss: tensor(0.5162, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [699/1000]: training_loss: tensor(0.5494, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [700/1000]: training_loss: tensor(0.5528, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [701/1000]: training_loss: tensor(0.5410, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [702/1000]: training_loss: tensor(0.5099, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [703/1000]: training_loss: tensor(0.5439, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [704/1000]: training_loss: tensor(0.5475, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [705/1000]: training_loss: tensor(0.5289, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [706/1000]: training_loss: tensor(0.5629, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [707/1000]: training_loss: tensor(0.5024, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [708/1000]: training_loss: tensor(0.5546, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [709/1000]: training_loss: tensor(0.4830, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [710/1000]: training_loss: tensor(0.5067, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [711/1000]: training_loss: tensor(0.4820, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [712/1000]: training_loss: tensor(0.5435, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [713/1000]: training_loss: tensor(0.5273, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [714/1000]: training_loss: tensor(0.5571, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [715/1000]: training_loss: tensor(0.5309, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [716/1000]: training_loss: tensor(0.5179, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [717/1000]: training_loss: tensor(0.5358, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [718/1000]: training_loss: tensor(0.5359, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [719/1000]: training_loss: tensor(0.4972, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [720/1000]: training_loss: tensor(0.5289, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [721/1000]: training_loss: tensor(0.4923, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [722/1000]: training_loss: tensor(0.4910, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [723/1000]: training_loss: tensor(0.4972, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [724/1000]: training_loss: tensor(0.5054, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [725/1000]: training_loss: tensor(0.4944, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [726/1000]: training_loss: tensor(0.4853, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [727/1000]: training_loss: tensor(0.5187, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [728/1000]: training_loss: tensor(0.4721, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [729/1000]: training_loss: tensor(0.5325, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [730/1000]: training_loss: tensor(0.4683, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [731/1000]: training_loss: tensor(0.4868, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [732/1000]: training_loss: tensor(0.4647, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [733/1000]: training_loss: tensor(0.4912, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [734/1000]: training_loss: tensor(0.4791, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [735/1000]: training_loss: tensor(0.4966, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [736/1000]: training_loss: tensor(0.5670, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [737/1000]: training_loss: tensor(0.5547, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [738/1000]: training_loss: tensor(0.5446, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [739/1000]: training_loss: tensor(0.4875, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [740/1000]: training_loss: tensor(0.4823, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [741/1000]: training_loss: tensor(0.5053, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [742/1000]: training_loss: tensor(0.4957, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [743/1000]: training_loss: tensor(0.5007, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [744/1000]: training_loss: tensor(0.4556, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [745/1000]: training_loss: tensor(0.5048, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [746/1000]: training_loss: tensor(0.4407, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [747/1000]: training_loss: tensor(0.4971, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [748/1000]: training_loss: tensor(0.5239, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [749/1000]: training_loss: tensor(0.4590, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [750/1000]: training_loss: tensor(0.5165, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [751/1000]: training_loss: tensor(0.4889, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [752/1000]: training_loss: tensor(0.4986, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [753/1000]: training_loss: tensor(0.4689, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [754/1000]: training_loss: tensor(0.4577, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [755/1000]: training_loss: tensor(0.4740, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [756/1000]: training_loss: tensor(0.4920, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [757/1000]: training_loss: tensor(0.4679, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [758/1000]: training_loss: tensor(0.5341, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [759/1000]: training_loss: tensor(0.4398, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [760/1000]: training_loss: tensor(0.5698, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [761/1000]: training_loss: tensor(0.4766, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [762/1000]: training_loss: tensor(0.4784, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [763/1000]: training_loss: tensor(0.4538, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [764/1000]: training_loss: tensor(0.5077, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [765/1000]: training_loss: tensor(0.4801, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [766/1000]: training_loss: tensor(0.4902, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [767/1000]: training_loss: tensor(0.4980, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [768/1000]: training_loss: tensor(0.5832, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [769/1000]: training_loss: tensor(0.4845, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [770/1000]: training_loss: tensor(0.4899, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [771/1000]: training_loss: tensor(0.4427, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [772/1000]: training_loss: tensor(0.5259, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [773/1000]: training_loss: tensor(0.4587, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [774/1000]: training_loss: tensor(0.4757, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [775/1000]: training_loss: tensor(0.5361, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [776/1000]: training_loss: tensor(0.5171, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [777/1000]: training_loss: tensor(0.4626, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [778/1000]: training_loss: tensor(0.4428, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [779/1000]: training_loss: tensor(0.4349, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [780/1000]: training_loss: tensor(0.4197, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [781/1000]: training_loss: tensor(0.4514, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [782/1000]: training_loss: tensor(0.5123, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [783/1000]: training_loss: tensor(0.4705, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [784/1000]: training_loss: tensor(0.4778, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [785/1000]: training_loss: tensor(0.5052, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [786/1000]: training_loss: tensor(0.5369, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [787/1000]: training_loss: tensor(0.3803, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [788/1000]: training_loss: tensor(0.4019, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [789/1000]: training_loss: tensor(0.4721, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [790/1000]: training_loss: tensor(0.4738, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [791/1000]: training_loss: tensor(0.4903, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [792/1000]: training_loss: tensor(0.4808, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [793/1000]: training_loss: tensor(0.4949, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [794/1000]: training_loss: tensor(0.4490, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [795/1000]: training_loss: tensor(0.4517, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [796/1000]: training_loss: tensor(0.4562, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [797/1000]: training_loss: tensor(0.5385, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [798/1000]: training_loss: tensor(0.4867, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [799/1000]: training_loss: tensor(0.4624, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [800/1000]: training_loss: tensor(0.4735, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [801/1000]: training_loss: tensor(0.5104, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [802/1000]: training_loss: tensor(0.5862, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [803/1000]: training_loss: tensor(0.4999, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [804/1000]: training_loss: tensor(0.4046, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [805/1000]: training_loss: tensor(0.4558, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [806/1000]: training_loss: tensor(0.4152, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [807/1000]: training_loss: tensor(0.4874, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [808/1000]: training_loss: tensor(0.4601, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [809/1000]: training_loss: tensor(0.4882, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [810/1000]: training_loss: tensor(0.4221, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [811/1000]: training_loss: tensor(0.5400, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [812/1000]: training_loss: tensor(0.4619, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [813/1000]: training_loss: tensor(0.4244, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [814/1000]: training_loss: tensor(0.3957, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [815/1000]: training_loss: tensor(0.4577, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [816/1000]: training_loss: tensor(0.4325, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [817/1000]: training_loss: tensor(0.4562, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [818/1000]: training_loss: tensor(0.4475, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [819/1000]: training_loss: tensor(0.4272, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [820/1000]: training_loss: tensor(0.4353, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [821/1000]: training_loss: tensor(0.4140, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [822/1000]: training_loss: tensor(0.4708, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [823/1000]: training_loss: tensor(0.4991, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [824/1000]: training_loss: tensor(0.4296, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [825/1000]: training_loss: tensor(0.4602, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [826/1000]: training_loss: tensor(0.4721, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [827/1000]: training_loss: tensor(0.4761, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [828/1000]: training_loss: tensor(0.5254, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [829/1000]: training_loss: tensor(0.5477, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [830/1000]: training_loss: tensor(0.5131, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [831/1000]: training_loss: tensor(0.5157, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [832/1000]: training_loss: tensor(0.5331, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [833/1000]: training_loss: tensor(0.5376, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [834/1000]: training_loss: tensor(0.5560, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [835/1000]: training_loss: tensor(0.4768, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [836/1000]: training_loss: tensor(0.4283, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [837/1000]: training_loss: tensor(0.5137, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [838/1000]: training_loss: tensor(0.4740, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [839/1000]: training_loss: tensor(0.4859, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [840/1000]: training_loss: tensor(0.5210, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [841/1000]: training_loss: tensor(0.4580, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [842/1000]: training_loss: tensor(0.4436, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [843/1000]: training_loss: tensor(0.4144, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [844/1000]: training_loss: tensor(0.4590, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [845/1000]: training_loss: tensor(0.4559, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [846/1000]: training_loss: tensor(0.4360, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [847/1000]: training_loss: tensor(0.4380, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [848/1000]: training_loss: tensor(0.3870, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [849/1000]: training_loss: tensor(0.4820, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [850/1000]: training_loss: tensor(0.4101, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [851/1000]: training_loss: tensor(0.3917, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [852/1000]: training_loss: tensor(0.5201, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [853/1000]: training_loss: tensor(0.3912, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [854/1000]: training_loss: tensor(0.4578, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [855/1000]: training_loss: tensor(0.4661, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [856/1000]: training_loss: tensor(0.4745, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [857/1000]: training_loss: tensor(0.4789, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [858/1000]: training_loss: tensor(0.4224, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [859/1000]: training_loss: tensor(0.4270, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [860/1000]: training_loss: tensor(0.4546, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [861/1000]: training_loss: tensor(0.4303, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [862/1000]: training_loss: tensor(0.4469, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [863/1000]: training_loss: tensor(0.4934, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [864/1000]: training_loss: tensor(0.4341, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [865/1000]: training_loss: tensor(0.3547, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [866/1000]: training_loss: tensor(0.4606, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [867/1000]: training_loss: tensor(0.4482, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [868/1000]: training_loss: tensor(0.4368, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [869/1000]: training_loss: tensor(0.4571, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [870/1000]: training_loss: tensor(0.5684, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [871/1000]: training_loss: tensor(0.4268, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [872/1000]: training_loss: tensor(0.3737, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [873/1000]: training_loss: tensor(0.3920, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [874/1000]: training_loss: tensor(0.4633, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [875/1000]: training_loss: tensor(0.3757, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [876/1000]: training_loss: tensor(0.4681, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [877/1000]: training_loss: tensor(0.4589, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [878/1000]: training_loss: tensor(0.3620, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [879/1000]: training_loss: tensor(0.4486, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [880/1000]: training_loss: tensor(0.4311, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [881/1000]: training_loss: tensor(0.4274, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [882/1000]: training_loss: tensor(0.3958, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [883/1000]: training_loss: tensor(0.5122, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [884/1000]: training_loss: tensor(0.4802, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [885/1000]: training_loss: tensor(0.4089, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [886/1000]: training_loss: tensor(0.4237, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [887/1000]: training_loss: tensor(0.3996, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [888/1000]: training_loss: tensor(0.4819, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [889/1000]: training_loss: tensor(0.4483, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [890/1000]: training_loss: tensor(0.4842, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [891/1000]: training_loss: tensor(0.5200, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [892/1000]: training_loss: tensor(0.4306, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [893/1000]: training_loss: tensor(0.5133, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [894/1000]: training_loss: tensor(0.4921, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [895/1000]: training_loss: tensor(0.3852, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [896/1000]: training_loss: tensor(0.4078, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [897/1000]: training_loss: tensor(0.4589, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [898/1000]: training_loss: tensor(0.5320, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [899/1000]: training_loss: tensor(0.4319, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [900/1000]: training_loss: tensor(0.5040, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [901/1000]: training_loss: tensor(0.4812, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [902/1000]: training_loss: tensor(0.4372, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [903/1000]: training_loss: tensor(0.4261, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [904/1000]: training_loss: tensor(0.5599, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [905/1000]: training_loss: tensor(0.3958, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [906/1000]: training_loss: tensor(0.4407, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [907/1000]: training_loss: tensor(0.4456, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [908/1000]: training_loss: tensor(0.5115, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [909/1000]: training_loss: tensor(0.4641, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [910/1000]: training_loss: tensor(0.4209, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [911/1000]: training_loss: tensor(0.4898, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [912/1000]: training_loss: tensor(0.4288, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [913/1000]: training_loss: tensor(0.4191, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [914/1000]: training_loss: tensor(0.4329, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [915/1000]: training_loss: tensor(0.4236, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [916/1000]: training_loss: tensor(0.4324, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [917/1000]: training_loss: tensor(0.4921, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [918/1000]: training_loss: tensor(0.4684, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [919/1000]: training_loss: tensor(0.4101, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [920/1000]: training_loss: tensor(0.5408, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [921/1000]: training_loss: tensor(0.4270, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [922/1000]: training_loss: tensor(0.4496, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [923/1000]: training_loss: tensor(0.3995, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [924/1000]: training_loss: tensor(0.3775, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [925/1000]: training_loss: tensor(0.4036, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [926/1000]: training_loss: tensor(0.3766, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [927/1000]: training_loss: tensor(0.4453, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [928/1000]: training_loss: tensor(0.5298, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [929/1000]: training_loss: tensor(0.3866, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [930/1000]: training_loss: tensor(0.3868, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [931/1000]: training_loss: tensor(0.3532, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [932/1000]: training_loss: tensor(0.6383, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [933/1000]: training_loss: tensor(0.4490, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [934/1000]: training_loss: tensor(0.4282, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [935/1000]: training_loss: tensor(0.3906, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [936/1000]: training_loss: tensor(0.4812, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [937/1000]: training_loss: tensor(0.4345, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [938/1000]: training_loss: tensor(0.4399, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [939/1000]: training_loss: tensor(0.3947, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [940/1000]: training_loss: tensor(0.5485, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [941/1000]: training_loss: tensor(0.4248, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [942/1000]: training_loss: tensor(0.3693, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [943/1000]: training_loss: tensor(0.4140, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [944/1000]: training_loss: tensor(0.4783, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [945/1000]: training_loss: tensor(0.3700, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [946/1000]: training_loss: tensor(0.3660, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [947/1000]: training_loss: tensor(0.4357, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [948/1000]: training_loss: tensor(0.4797, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [949/1000]: training_loss: tensor(0.4272, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [950/1000]: training_loss: tensor(0.3646, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [951/1000]: training_loss: tensor(0.3920, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [952/1000]: training_loss: tensor(0.4142, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [953/1000]: training_loss: tensor(0.4766, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [954/1000]: training_loss: tensor(0.4586, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [955/1000]: training_loss: tensor(0.3713, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [956/1000]: training_loss: tensor(0.4463, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [957/1000]: training_loss: tensor(0.4388, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [958/1000]: training_loss: tensor(0.4182, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [959/1000]: training_loss: tensor(0.4327, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [960/1000]: training_loss: tensor(0.4435, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [961/1000]: training_loss: tensor(0.4802, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [962/1000]: training_loss: tensor(0.3710, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [963/1000]: training_loss: tensor(0.3973, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [964/1000]: training_loss: tensor(0.4968, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [965/1000]: training_loss: tensor(0.4962, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [966/1000]: training_loss: tensor(0.3756, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [967/1000]: training_loss: tensor(0.4171, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [968/1000]: training_loss: tensor(0.4923, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [969/1000]: training_loss: tensor(0.5390, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [970/1000]: training_loss: tensor(0.4106, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [971/1000]: training_loss: tensor(0.3676, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [972/1000]: training_loss: tensor(0.4348, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [973/1000]: training_loss: tensor(0.3616, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [974/1000]: training_loss: tensor(0.4025, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [975/1000]: training_loss: tensor(0.3534, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [976/1000]: training_loss: tensor(0.4431, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [977/1000]: training_loss: tensor(0.4075, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [978/1000]: training_loss: tensor(0.4510, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [979/1000]: training_loss: tensor(0.3751, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [980/1000]: training_loss: tensor(0.4136, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [981/1000]: training_loss: tensor(0.3589, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [982/1000]: training_loss: tensor(0.5145, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [983/1000]: training_loss: tensor(0.4255, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [984/1000]: training_loss: tensor(0.4712, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [985/1000]: training_loss: tensor(0.4520, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [986/1000]: training_loss: tensor(0.3289, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [987/1000]: training_loss: tensor(0.4675, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [988/1000]: training_loss: tensor(0.3987, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [989/1000]: training_loss: tensor(0.4257, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [990/1000]: training_loss: tensor(0.3786, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [991/1000]: training_loss: tensor(0.4095, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [992/1000]: training_loss: tensor(0.4701, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [993/1000]: training_loss: tensor(0.4197, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [994/1000]: training_loss: tensor(0.4712, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [995/1000]: training_loss: tensor(0.4152, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [996/1000]: training_loss: tensor(0.3448, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [997/1000]: training_loss: tensor(0.4784, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [998/1000]: training_loss: tensor(0.4457, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [999/1000]: training_loss: tensor(0.4504, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Epoch [1/5], global step [1000/5000], Train Loss: 0.6294, Valid Loss: 0.4133\n",
      "batch_no [0/1000]: training_loss: tensor(0.4468, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [1/1000]: training_loss: tensor(0.3990, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [2/1000]: training_loss: tensor(0.4531, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [3/1000]: training_loss: tensor(0.5520, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [4/1000]: training_loss: tensor(0.4663, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [5/1000]: training_loss: tensor(0.4009, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [6/1000]: training_loss: tensor(0.4706, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [7/1000]: training_loss: tensor(0.4861, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [8/1000]: training_loss: tensor(0.4356, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [9/1000]: training_loss: tensor(0.4148, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [10/1000]: training_loss: tensor(0.4065, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [11/1000]: training_loss: tensor(0.4638, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [12/1000]: training_loss: tensor(0.4187, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [13/1000]: training_loss: tensor(0.3998, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [14/1000]: training_loss: tensor(0.4047, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [15/1000]: training_loss: tensor(0.4083, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [16/1000]: training_loss: tensor(0.4964, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [17/1000]: training_loss: tensor(0.4292, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [18/1000]: training_loss: tensor(0.4355, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [19/1000]: training_loss: tensor(0.4327, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [20/1000]: training_loss: tensor(0.3891, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [21/1000]: training_loss: tensor(0.4952, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [22/1000]: training_loss: tensor(0.4026, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [23/1000]: training_loss: tensor(0.4508, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [24/1000]: training_loss: tensor(0.4395, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [25/1000]: training_loss: tensor(0.4142, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [26/1000]: training_loss: tensor(0.3301, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [27/1000]: training_loss: tensor(0.5102, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [28/1000]: training_loss: tensor(0.3775, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [29/1000]: training_loss: tensor(0.4123, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [30/1000]: training_loss: tensor(0.3777, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [31/1000]: training_loss: tensor(0.4844, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [32/1000]: training_loss: tensor(0.4628, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [33/1000]: training_loss: tensor(0.4530, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [34/1000]: training_loss: tensor(0.4676, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [35/1000]: training_loss: tensor(0.3499, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [36/1000]: training_loss: tensor(0.4544, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [37/1000]: training_loss: tensor(0.4343, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [38/1000]: training_loss: tensor(0.4894, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [39/1000]: training_loss: tensor(0.4390, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [40/1000]: training_loss: tensor(0.4864, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [41/1000]: training_loss: tensor(0.3864, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [42/1000]: training_loss: tensor(0.4203, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [43/1000]: training_loss: tensor(0.4695, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [44/1000]: training_loss: tensor(0.4215, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [45/1000]: training_loss: tensor(0.3760, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [46/1000]: training_loss: tensor(0.3912, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [47/1000]: training_loss: tensor(0.5308, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [48/1000]: training_loss: tensor(0.4595, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [49/1000]: training_loss: tensor(0.3895, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [50/1000]: training_loss: tensor(0.4768, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [51/1000]: training_loss: tensor(0.4400, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [52/1000]: training_loss: tensor(0.3454, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [53/1000]: training_loss: tensor(0.4626, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [54/1000]: training_loss: tensor(0.3934, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [55/1000]: training_loss: tensor(0.4865, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [56/1000]: training_loss: tensor(0.4292, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [57/1000]: training_loss: tensor(0.4576, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [58/1000]: training_loss: tensor(0.4282, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [59/1000]: training_loss: tensor(0.4716, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [60/1000]: training_loss: tensor(0.3562, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [61/1000]: training_loss: tensor(0.4689, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [62/1000]: training_loss: tensor(0.4666, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [63/1000]: training_loss: tensor(0.4514, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [64/1000]: training_loss: tensor(0.3722, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [65/1000]: training_loss: tensor(0.3742, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [66/1000]: training_loss: tensor(0.4793, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [67/1000]: training_loss: tensor(0.4203, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [68/1000]: training_loss: tensor(0.4289, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [69/1000]: training_loss: tensor(0.4831, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [70/1000]: training_loss: tensor(0.4203, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [71/1000]: training_loss: tensor(0.4292, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [72/1000]: training_loss: tensor(0.4277, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [73/1000]: training_loss: tensor(0.4522, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [74/1000]: training_loss: tensor(0.4743, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [75/1000]: training_loss: tensor(0.4694, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [76/1000]: training_loss: tensor(0.5147, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [77/1000]: training_loss: tensor(0.4094, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [78/1000]: training_loss: tensor(0.4072, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [79/1000]: training_loss: tensor(0.5519, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [80/1000]: training_loss: tensor(0.3784, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [81/1000]: training_loss: tensor(0.3856, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [82/1000]: training_loss: tensor(0.4587, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [83/1000]: training_loss: tensor(0.3908, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [84/1000]: training_loss: tensor(0.4342, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [85/1000]: training_loss: tensor(0.4967, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [86/1000]: training_loss: tensor(0.4894, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [87/1000]: training_loss: tensor(0.4284, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [88/1000]: training_loss: tensor(0.5242, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [89/1000]: training_loss: tensor(0.3935, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [90/1000]: training_loss: tensor(0.3675, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [91/1000]: training_loss: tensor(0.3851, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [92/1000]: training_loss: tensor(0.4825, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [93/1000]: training_loss: tensor(0.4705, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [94/1000]: training_loss: tensor(0.4055, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [95/1000]: training_loss: tensor(0.4259, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [96/1000]: training_loss: tensor(0.4723, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [97/1000]: training_loss: tensor(0.3658, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [98/1000]: training_loss: tensor(0.4600, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [99/1000]: training_loss: tensor(0.4520, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [100/1000]: training_loss: tensor(0.5522, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [101/1000]: training_loss: tensor(0.5012, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [102/1000]: training_loss: tensor(0.4523, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [103/1000]: training_loss: tensor(0.4831, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [104/1000]: training_loss: tensor(0.4215, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [105/1000]: training_loss: tensor(0.3834, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [106/1000]: training_loss: tensor(0.3709, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [107/1000]: training_loss: tensor(0.4297, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [108/1000]: training_loss: tensor(0.4951, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [109/1000]: training_loss: tensor(0.3640, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [110/1000]: training_loss: tensor(0.4886, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [111/1000]: training_loss: tensor(0.4450, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [112/1000]: training_loss: tensor(0.4326, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [113/1000]: training_loss: tensor(0.4171, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [114/1000]: training_loss: tensor(0.3662, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [115/1000]: training_loss: tensor(0.3264, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [116/1000]: training_loss: tensor(0.3798, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [117/1000]: training_loss: tensor(0.4502, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [118/1000]: training_loss: tensor(0.4262, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [119/1000]: training_loss: tensor(0.4536, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [120/1000]: training_loss: tensor(0.3290, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [121/1000]: training_loss: tensor(0.4535, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [122/1000]: training_loss: tensor(0.4721, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [123/1000]: training_loss: tensor(0.3962, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [124/1000]: training_loss: tensor(0.4074, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [125/1000]: training_loss: tensor(0.4118, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [126/1000]: training_loss: tensor(0.4811, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [127/1000]: training_loss: tensor(0.3637, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [128/1000]: training_loss: tensor(0.4516, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [129/1000]: training_loss: tensor(0.3875, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [130/1000]: training_loss: tensor(0.5113, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [131/1000]: training_loss: tensor(0.4607, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [132/1000]: training_loss: tensor(0.4983, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [133/1000]: training_loss: tensor(0.4925, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [134/1000]: training_loss: tensor(0.4560, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [135/1000]: training_loss: tensor(0.4007, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [136/1000]: training_loss: tensor(0.3756, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [137/1000]: training_loss: tensor(0.4778, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [138/1000]: training_loss: tensor(0.4896, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [139/1000]: training_loss: tensor(0.5212, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [140/1000]: training_loss: tensor(0.4958, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [141/1000]: training_loss: tensor(0.4196, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [142/1000]: training_loss: tensor(0.4175, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [143/1000]: training_loss: tensor(0.4544, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [144/1000]: training_loss: tensor(0.4856, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [145/1000]: training_loss: tensor(0.5125, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [146/1000]: training_loss: tensor(0.4114, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [147/1000]: training_loss: tensor(0.3952, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [148/1000]: training_loss: tensor(0.4113, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [149/1000]: training_loss: tensor(0.4802, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [150/1000]: training_loss: tensor(0.3772, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [151/1000]: training_loss: tensor(0.4082, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [152/1000]: training_loss: tensor(0.3887, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [153/1000]: training_loss: tensor(0.3339, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [154/1000]: training_loss: tensor(0.3849, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [155/1000]: training_loss: tensor(0.3482, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [156/1000]: training_loss: tensor(0.4729, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [157/1000]: training_loss: tensor(0.4076, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [158/1000]: training_loss: tensor(0.5105, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [159/1000]: training_loss: tensor(0.4237, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [160/1000]: training_loss: tensor(0.4262, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [161/1000]: training_loss: tensor(0.4189, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [162/1000]: training_loss: tensor(0.3750, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [163/1000]: training_loss: tensor(0.3862, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [164/1000]: training_loss: tensor(0.4146, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [165/1000]: training_loss: tensor(0.3524, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [166/1000]: training_loss: tensor(0.5261, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [167/1000]: training_loss: tensor(0.4313, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [168/1000]: training_loss: tensor(0.3440, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [169/1000]: training_loss: tensor(0.3493, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [170/1000]: training_loss: tensor(0.4372, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [171/1000]: training_loss: tensor(0.3855, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [172/1000]: training_loss: tensor(0.4984, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [173/1000]: training_loss: tensor(0.3909, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [174/1000]: training_loss: tensor(0.4106, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [175/1000]: training_loss: tensor(0.4634, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [176/1000]: training_loss: tensor(0.4224, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [177/1000]: training_loss: tensor(0.4855, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [178/1000]: training_loss: tensor(0.4521, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [179/1000]: training_loss: tensor(0.4192, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [180/1000]: training_loss: tensor(0.4140, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [181/1000]: training_loss: tensor(0.5820, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [182/1000]: training_loss: tensor(0.3784, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [183/1000]: training_loss: tensor(0.3832, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [184/1000]: training_loss: tensor(0.3626, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [185/1000]: training_loss: tensor(0.3961, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [186/1000]: training_loss: tensor(0.4058, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [187/1000]: training_loss: tensor(0.4029, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [188/1000]: training_loss: tensor(0.4596, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [189/1000]: training_loss: tensor(0.5162, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [190/1000]: training_loss: tensor(0.3957, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [191/1000]: training_loss: tensor(0.4241, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [192/1000]: training_loss: tensor(0.3323, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [193/1000]: training_loss: tensor(0.4140, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [194/1000]: training_loss: tensor(0.5310, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [195/1000]: training_loss: tensor(0.5064, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [196/1000]: training_loss: tensor(0.3776, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [197/1000]: training_loss: tensor(0.4613, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [198/1000]: training_loss: tensor(0.5002, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [199/1000]: training_loss: tensor(0.4430, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [200/1000]: training_loss: tensor(0.4549, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [201/1000]: training_loss: tensor(0.3996, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [202/1000]: training_loss: tensor(0.4017, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [203/1000]: training_loss: tensor(0.4883, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [204/1000]: training_loss: tensor(0.4994, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [205/1000]: training_loss: tensor(0.5197, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [206/1000]: training_loss: tensor(0.4769, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [207/1000]: training_loss: tensor(0.4153, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [208/1000]: training_loss: tensor(0.4483, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [209/1000]: training_loss: tensor(0.4152, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [210/1000]: training_loss: tensor(0.4488, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [211/1000]: training_loss: tensor(0.4945, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [212/1000]: training_loss: tensor(0.4504, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [213/1000]: training_loss: tensor(0.4733, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [214/1000]: training_loss: tensor(0.4717, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [215/1000]: training_loss: tensor(0.4355, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [216/1000]: training_loss: tensor(0.5037, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [217/1000]: training_loss: tensor(0.5180, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [218/1000]: training_loss: tensor(0.4130, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [219/1000]: training_loss: tensor(0.3716, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [220/1000]: training_loss: tensor(0.3987, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [221/1000]: training_loss: tensor(0.4435, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [222/1000]: training_loss: tensor(0.4652, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [223/1000]: training_loss: tensor(0.3895, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [224/1000]: training_loss: tensor(0.4884, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [225/1000]: training_loss: tensor(0.4667, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [226/1000]: training_loss: tensor(0.4822, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [227/1000]: training_loss: tensor(0.5768, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [228/1000]: training_loss: tensor(0.4214, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [229/1000]: training_loss: tensor(0.4495, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [230/1000]: training_loss: tensor(0.4381, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [231/1000]: training_loss: tensor(0.3282, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [232/1000]: training_loss: tensor(0.3344, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [233/1000]: training_loss: tensor(0.3745, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [234/1000]: training_loss: tensor(0.4154, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [235/1000]: training_loss: tensor(0.5299, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [236/1000]: training_loss: tensor(0.3921, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [237/1000]: training_loss: tensor(0.4353, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [238/1000]: training_loss: tensor(0.4431, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [239/1000]: training_loss: tensor(0.3448, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [240/1000]: training_loss: tensor(0.5508, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [241/1000]: training_loss: tensor(0.3618, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [242/1000]: training_loss: tensor(0.4313, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [243/1000]: training_loss: tensor(0.3901, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [244/1000]: training_loss: tensor(0.4146, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [245/1000]: training_loss: tensor(0.4869, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [246/1000]: training_loss: tensor(0.4080, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [247/1000]: training_loss: tensor(0.4163, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [248/1000]: training_loss: tensor(0.3776, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [249/1000]: training_loss: tensor(0.4180, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [250/1000]: training_loss: tensor(0.4702, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [251/1000]: training_loss: tensor(0.4479, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [252/1000]: training_loss: tensor(0.3529, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [253/1000]: training_loss: tensor(0.3987, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [254/1000]: training_loss: tensor(0.4923, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [255/1000]: training_loss: tensor(0.5185, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [256/1000]: training_loss: tensor(0.4734, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [257/1000]: training_loss: tensor(0.4195, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [258/1000]: training_loss: tensor(0.4559, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [259/1000]: training_loss: tensor(0.4052, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [260/1000]: training_loss: tensor(0.3630, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [261/1000]: training_loss: tensor(0.3759, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [262/1000]: training_loss: tensor(0.4456, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [263/1000]: training_loss: tensor(0.3600, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [264/1000]: training_loss: tensor(0.3905, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [265/1000]: training_loss: tensor(0.3815, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [266/1000]: training_loss: tensor(0.3698, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [267/1000]: training_loss: tensor(0.4176, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [268/1000]: training_loss: tensor(0.4797, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [269/1000]: training_loss: tensor(0.4663, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [270/1000]: training_loss: tensor(0.4715, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [271/1000]: training_loss: tensor(0.4135, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [272/1000]: training_loss: tensor(0.4400, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [273/1000]: training_loss: tensor(0.4198, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [274/1000]: training_loss: tensor(0.4371, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [275/1000]: training_loss: tensor(0.4058, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [276/1000]: training_loss: tensor(0.4134, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [277/1000]: training_loss: tensor(0.4093, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [278/1000]: training_loss: tensor(0.4096, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [279/1000]: training_loss: tensor(0.4439, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [280/1000]: training_loss: tensor(0.3992, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [281/1000]: training_loss: tensor(0.3913, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [282/1000]: training_loss: tensor(0.4445, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [283/1000]: training_loss: tensor(0.3157, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [284/1000]: training_loss: tensor(0.4193, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [285/1000]: training_loss: tensor(0.4999, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [286/1000]: training_loss: tensor(0.3623, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [287/1000]: training_loss: tensor(0.3611, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [288/1000]: training_loss: tensor(0.4189, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [289/1000]: training_loss: tensor(0.4704, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [290/1000]: training_loss: tensor(0.4485, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [291/1000]: training_loss: tensor(0.4348, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [292/1000]: training_loss: tensor(0.4171, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [293/1000]: training_loss: tensor(0.3962, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [294/1000]: training_loss: tensor(0.3967, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [295/1000]: training_loss: tensor(0.4525, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [296/1000]: training_loss: tensor(0.4121, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [297/1000]: training_loss: tensor(0.5320, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [298/1000]: training_loss: tensor(0.4074, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [299/1000]: training_loss: tensor(0.4632, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [300/1000]: training_loss: tensor(0.3757, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [301/1000]: training_loss: tensor(0.3476, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [302/1000]: training_loss: tensor(0.4362, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [303/1000]: training_loss: tensor(0.5175, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [304/1000]: training_loss: tensor(0.4078, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [305/1000]: training_loss: tensor(0.4436, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [306/1000]: training_loss: tensor(0.3308, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [307/1000]: training_loss: tensor(0.4818, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [308/1000]: training_loss: tensor(0.3872, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [309/1000]: training_loss: tensor(0.4754, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [310/1000]: training_loss: tensor(0.3659, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [311/1000]: training_loss: tensor(0.3750, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [312/1000]: training_loss: tensor(0.4340, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [313/1000]: training_loss: tensor(0.4935, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [314/1000]: training_loss: tensor(0.4075, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [315/1000]: training_loss: tensor(0.4338, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [316/1000]: training_loss: tensor(0.4199, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [317/1000]: training_loss: tensor(0.4264, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [318/1000]: training_loss: tensor(0.3530, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [319/1000]: training_loss: tensor(0.3715, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [320/1000]: training_loss: tensor(0.3590, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [321/1000]: training_loss: tensor(0.4012, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [322/1000]: training_loss: tensor(0.4058, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [323/1000]: training_loss: tensor(0.3398, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [324/1000]: training_loss: tensor(0.3639, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [325/1000]: training_loss: tensor(0.4178, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [326/1000]: training_loss: tensor(0.4767, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [327/1000]: training_loss: tensor(0.4330, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [328/1000]: training_loss: tensor(0.3114, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [329/1000]: training_loss: tensor(0.3553, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [330/1000]: training_loss: tensor(0.4324, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [331/1000]: training_loss: tensor(0.4012, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [332/1000]: training_loss: tensor(0.4242, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [333/1000]: training_loss: tensor(0.4641, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [334/1000]: training_loss: tensor(0.3876, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [335/1000]: training_loss: tensor(0.4648, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [336/1000]: training_loss: tensor(0.4630, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [337/1000]: training_loss: tensor(0.4176, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [338/1000]: training_loss: tensor(0.4034, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [339/1000]: training_loss: tensor(0.5668, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [340/1000]: training_loss: tensor(0.4925, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [341/1000]: training_loss: tensor(0.3506, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [342/1000]: training_loss: tensor(0.4118, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [343/1000]: training_loss: tensor(0.5308, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [344/1000]: training_loss: tensor(0.4715, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [345/1000]: training_loss: tensor(0.3855, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [346/1000]: training_loss: tensor(0.4715, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [347/1000]: training_loss: tensor(0.5112, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [348/1000]: training_loss: tensor(0.4360, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [349/1000]: training_loss: tensor(0.4194, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [350/1000]: training_loss: tensor(0.4149, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [351/1000]: training_loss: tensor(0.3791, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [352/1000]: training_loss: tensor(0.4526, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [353/1000]: training_loss: tensor(0.3730, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [354/1000]: training_loss: tensor(0.3844, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [355/1000]: training_loss: tensor(0.5673, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [356/1000]: training_loss: tensor(0.3610, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [357/1000]: training_loss: tensor(0.4888, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [358/1000]: training_loss: tensor(0.3982, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [359/1000]: training_loss: tensor(0.4186, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [360/1000]: training_loss: tensor(0.4213, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [361/1000]: training_loss: tensor(0.4104, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [362/1000]: training_loss: tensor(0.4574, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [363/1000]: training_loss: tensor(0.4890, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [364/1000]: training_loss: tensor(0.4304, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [365/1000]: training_loss: tensor(0.4640, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [366/1000]: training_loss: tensor(0.4053, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [367/1000]: training_loss: tensor(0.4018, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [368/1000]: training_loss: tensor(0.3402, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [369/1000]: training_loss: tensor(0.4003, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [370/1000]: training_loss: tensor(0.4068, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [371/1000]: training_loss: tensor(0.4064, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [372/1000]: training_loss: tensor(0.4373, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [373/1000]: training_loss: tensor(0.3972, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [374/1000]: training_loss: tensor(0.3921, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [375/1000]: training_loss: tensor(0.4328, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [376/1000]: training_loss: tensor(0.3980, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [377/1000]: training_loss: tensor(0.3235, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [378/1000]: training_loss: tensor(0.4484, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [379/1000]: training_loss: tensor(0.3203, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [380/1000]: training_loss: tensor(0.4053, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [381/1000]: training_loss: tensor(0.4524, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [382/1000]: training_loss: tensor(0.4062, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [383/1000]: training_loss: tensor(0.4433, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [384/1000]: training_loss: tensor(0.5101, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [385/1000]: training_loss: tensor(0.5019, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [386/1000]: training_loss: tensor(0.3769, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [387/1000]: training_loss: tensor(0.4578, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [388/1000]: training_loss: tensor(0.4311, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [389/1000]: training_loss: tensor(0.3869, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [390/1000]: training_loss: tensor(0.4385, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [391/1000]: training_loss: tensor(0.3734, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [392/1000]: training_loss: tensor(0.3681, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [393/1000]: training_loss: tensor(0.3795, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [394/1000]: training_loss: tensor(0.5214, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [395/1000]: training_loss: tensor(0.3785, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [396/1000]: training_loss: tensor(0.3953, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [397/1000]: training_loss: tensor(0.4586, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [398/1000]: training_loss: tensor(0.4360, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [399/1000]: training_loss: tensor(0.3436, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [400/1000]: training_loss: tensor(0.4715, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [401/1000]: training_loss: tensor(0.4720, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [402/1000]: training_loss: tensor(0.3972, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [403/1000]: training_loss: tensor(0.4130, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [404/1000]: training_loss: tensor(0.4652, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [405/1000]: training_loss: tensor(0.3389, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [406/1000]: training_loss: tensor(0.4326, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [407/1000]: training_loss: tensor(0.4301, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [408/1000]: training_loss: tensor(0.3676, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [409/1000]: training_loss: tensor(0.3943, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [410/1000]: training_loss: tensor(0.4201, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [411/1000]: training_loss: tensor(0.3868, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [412/1000]: training_loss: tensor(0.4075, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [413/1000]: training_loss: tensor(0.4196, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [414/1000]: training_loss: tensor(0.4570, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [415/1000]: training_loss: tensor(0.4681, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [416/1000]: training_loss: tensor(0.4879, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [417/1000]: training_loss: tensor(0.3610, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [418/1000]: training_loss: tensor(0.3682, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [419/1000]: training_loss: tensor(0.3495, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [420/1000]: training_loss: tensor(0.3805, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [421/1000]: training_loss: tensor(0.5117, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [422/1000]: training_loss: tensor(0.4031, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [423/1000]: training_loss: tensor(0.3913, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [424/1000]: training_loss: tensor(0.3702, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [425/1000]: training_loss: tensor(0.3905, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [426/1000]: training_loss: tensor(0.3839, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [427/1000]: training_loss: tensor(0.3404, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [428/1000]: training_loss: tensor(0.4420, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [429/1000]: training_loss: tensor(0.4041, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [430/1000]: training_loss: tensor(0.4731, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [431/1000]: training_loss: tensor(0.4172, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [432/1000]: training_loss: tensor(0.3938, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [433/1000]: training_loss: tensor(0.4710, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [434/1000]: training_loss: tensor(0.4282, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [435/1000]: training_loss: tensor(0.5105, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [436/1000]: training_loss: tensor(0.4426, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [437/1000]: training_loss: tensor(0.4678, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [438/1000]: training_loss: tensor(0.4199, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [439/1000]: training_loss: tensor(0.4539, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [440/1000]: training_loss: tensor(0.3966, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [441/1000]: training_loss: tensor(0.3920, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [442/1000]: training_loss: tensor(0.3747, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [443/1000]: training_loss: tensor(0.4496, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [444/1000]: training_loss: tensor(0.4200, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [445/1000]: training_loss: tensor(0.5116, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [446/1000]: training_loss: tensor(0.3731, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [447/1000]: training_loss: tensor(0.4026, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [448/1000]: training_loss: tensor(0.3482, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [449/1000]: training_loss: tensor(0.4534, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [450/1000]: training_loss: tensor(0.4934, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [451/1000]: training_loss: tensor(0.4368, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [452/1000]: training_loss: tensor(0.4013, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [453/1000]: training_loss: tensor(0.3314, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [454/1000]: training_loss: tensor(0.4022, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [455/1000]: training_loss: tensor(0.3920, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [456/1000]: training_loss: tensor(0.4006, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [457/1000]: training_loss: tensor(0.3184, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [458/1000]: training_loss: tensor(0.3447, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [459/1000]: training_loss: tensor(0.4068, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [460/1000]: training_loss: tensor(0.4676, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [461/1000]: training_loss: tensor(0.4469, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [462/1000]: training_loss: tensor(0.4372, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [463/1000]: training_loss: tensor(0.4689, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [464/1000]: training_loss: tensor(0.4541, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [465/1000]: training_loss: tensor(0.4922, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [466/1000]: training_loss: tensor(0.3668, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [467/1000]: training_loss: tensor(0.4357, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [468/1000]: training_loss: tensor(0.4424, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [469/1000]: training_loss: tensor(0.3485, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [470/1000]: training_loss: tensor(0.4697, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [471/1000]: training_loss: tensor(0.4399, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [472/1000]: training_loss: tensor(0.4310, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [473/1000]: training_loss: tensor(0.3375, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [474/1000]: training_loss: tensor(0.4512, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [475/1000]: training_loss: tensor(0.3875, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [476/1000]: training_loss: tensor(0.3459, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [477/1000]: training_loss: tensor(0.3112, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [478/1000]: training_loss: tensor(0.4837, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [479/1000]: training_loss: tensor(0.4349, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [480/1000]: training_loss: tensor(0.4062, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [481/1000]: training_loss: tensor(0.4010, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [482/1000]: training_loss: tensor(0.4247, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [483/1000]: training_loss: tensor(0.3990, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [484/1000]: training_loss: tensor(0.4364, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [485/1000]: training_loss: tensor(0.3504, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [486/1000]: training_loss: tensor(0.4877, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [487/1000]: training_loss: tensor(0.4805, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [488/1000]: training_loss: tensor(0.3767, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [489/1000]: training_loss: tensor(0.5022, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [490/1000]: training_loss: tensor(0.3667, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [491/1000]: training_loss: tensor(0.3628, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [492/1000]: training_loss: tensor(0.3958, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [493/1000]: training_loss: tensor(0.4472, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [494/1000]: training_loss: tensor(0.3827, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [495/1000]: training_loss: tensor(0.4044, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [496/1000]: training_loss: tensor(0.4272, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [497/1000]: training_loss: tensor(0.4971, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [498/1000]: training_loss: tensor(0.4512, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [499/1000]: training_loss: tensor(0.3945, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [500/1000]: training_loss: tensor(0.3660, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [501/1000]: training_loss: tensor(0.4760, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [502/1000]: training_loss: tensor(0.3760, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [503/1000]: training_loss: tensor(0.3342, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [504/1000]: training_loss: tensor(0.4354, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [505/1000]: training_loss: tensor(0.4361, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [506/1000]: training_loss: tensor(0.3793, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [507/1000]: training_loss: tensor(0.3936, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [508/1000]: training_loss: tensor(0.3943, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [509/1000]: training_loss: tensor(0.4964, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [510/1000]: training_loss: tensor(0.4115, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [511/1000]: training_loss: tensor(0.3532, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [512/1000]: training_loss: tensor(0.3891, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [513/1000]: training_loss: tensor(0.4092, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [514/1000]: training_loss: tensor(0.4346, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [515/1000]: training_loss: tensor(0.3916, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [516/1000]: training_loss: tensor(0.4204, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [517/1000]: training_loss: tensor(0.3866, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [518/1000]: training_loss: tensor(0.4187, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [519/1000]: training_loss: tensor(0.4113, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [520/1000]: training_loss: tensor(0.4434, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [521/1000]: training_loss: tensor(0.3655, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [522/1000]: training_loss: tensor(0.4223, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [523/1000]: training_loss: tensor(0.4056, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [524/1000]: training_loss: tensor(0.3878, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [525/1000]: training_loss: tensor(0.4533, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [526/1000]: training_loss: tensor(0.4228, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [527/1000]: training_loss: tensor(0.3753, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [528/1000]: training_loss: tensor(0.4147, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [529/1000]: training_loss: tensor(0.4780, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [530/1000]: training_loss: tensor(0.3729, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [531/1000]: training_loss: tensor(0.4274, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [532/1000]: training_loss: tensor(0.3380, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [533/1000]: training_loss: tensor(0.4266, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [534/1000]: training_loss: tensor(0.4471, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [535/1000]: training_loss: tensor(0.3589, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [536/1000]: training_loss: tensor(0.4663, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [537/1000]: training_loss: tensor(0.4116, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [538/1000]: training_loss: tensor(0.4585, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [539/1000]: training_loss: tensor(0.4835, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [540/1000]: training_loss: tensor(0.4849, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [541/1000]: training_loss: tensor(0.4084, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [542/1000]: training_loss: tensor(0.4888, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [543/1000]: training_loss: tensor(0.4728, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [544/1000]: training_loss: tensor(0.4228, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [545/1000]: training_loss: tensor(0.4295, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [546/1000]: training_loss: tensor(0.4155, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [547/1000]: training_loss: tensor(0.4284, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [548/1000]: training_loss: tensor(0.4388, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [549/1000]: training_loss: tensor(0.3927, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [550/1000]: training_loss: tensor(0.4344, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [551/1000]: training_loss: tensor(0.4302, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [552/1000]: training_loss: tensor(0.3872, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [553/1000]: training_loss: tensor(0.3623, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [554/1000]: training_loss: tensor(0.4820, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [555/1000]: training_loss: tensor(0.5044, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [556/1000]: training_loss: tensor(0.4017, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [557/1000]: training_loss: tensor(0.3397, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [558/1000]: training_loss: tensor(0.4002, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [559/1000]: training_loss: tensor(0.4162, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [560/1000]: training_loss: tensor(0.3667, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [561/1000]: training_loss: tensor(0.5061, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [562/1000]: training_loss: tensor(0.3370, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [563/1000]: training_loss: tensor(0.4725, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [564/1000]: training_loss: tensor(0.4185, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [565/1000]: training_loss: tensor(0.3834, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [566/1000]: training_loss: tensor(0.4245, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [567/1000]: training_loss: tensor(0.4147, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [568/1000]: training_loss: tensor(0.3673, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [569/1000]: training_loss: tensor(0.3888, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [570/1000]: training_loss: tensor(0.3987, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [571/1000]: training_loss: tensor(0.3885, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [572/1000]: training_loss: tensor(0.4489, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [573/1000]: training_loss: tensor(0.4019, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [574/1000]: training_loss: tensor(0.3397, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [575/1000]: training_loss: tensor(0.4029, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [576/1000]: training_loss: tensor(0.3975, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [577/1000]: training_loss: tensor(0.3921, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [578/1000]: training_loss: tensor(0.4294, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [579/1000]: training_loss: tensor(0.4167, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [580/1000]: training_loss: tensor(0.3869, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [581/1000]: training_loss: tensor(0.4773, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [582/1000]: training_loss: tensor(0.4428, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [583/1000]: training_loss: tensor(0.3829, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [584/1000]: training_loss: tensor(0.3733, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [585/1000]: training_loss: tensor(0.3743, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [586/1000]: training_loss: tensor(0.4123, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [587/1000]: training_loss: tensor(0.4039, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [588/1000]: training_loss: tensor(0.3844, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [589/1000]: training_loss: tensor(0.4007, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [590/1000]: training_loss: tensor(0.4979, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [591/1000]: training_loss: tensor(0.4049, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [592/1000]: training_loss: tensor(0.4034, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [593/1000]: training_loss: tensor(0.4180, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [594/1000]: training_loss: tensor(0.3993, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [595/1000]: training_loss: tensor(0.3967, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [596/1000]: training_loss: tensor(0.3592, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [597/1000]: training_loss: tensor(0.3993, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [598/1000]: training_loss: tensor(0.5144, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [599/1000]: training_loss: tensor(0.3290, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [600/1000]: training_loss: tensor(0.3829, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [601/1000]: training_loss: tensor(0.4036, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [602/1000]: training_loss: tensor(0.4450, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [603/1000]: training_loss: tensor(0.4181, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [604/1000]: training_loss: tensor(0.4287, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [605/1000]: training_loss: tensor(0.3742, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [606/1000]: training_loss: tensor(0.3677, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [607/1000]: training_loss: tensor(0.3976, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [608/1000]: training_loss: tensor(0.5073, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [609/1000]: training_loss: tensor(0.4475, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [610/1000]: training_loss: tensor(0.4387, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [611/1000]: training_loss: tensor(0.3925, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [612/1000]: training_loss: tensor(0.4939, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [613/1000]: training_loss: tensor(0.3902, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [614/1000]: training_loss: tensor(0.3118, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [615/1000]: training_loss: tensor(0.3787, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [616/1000]: training_loss: tensor(0.3724, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [617/1000]: training_loss: tensor(0.3825, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [618/1000]: training_loss: tensor(0.4112, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [619/1000]: training_loss: tensor(0.3990, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [620/1000]: training_loss: tensor(0.4316, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [621/1000]: training_loss: tensor(0.3631, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [622/1000]: training_loss: tensor(0.4027, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [623/1000]: training_loss: tensor(0.5362, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [624/1000]: training_loss: tensor(0.3739, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [625/1000]: training_loss: tensor(0.4061, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [626/1000]: training_loss: tensor(0.3756, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [627/1000]: training_loss: tensor(0.3111, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [628/1000]: training_loss: tensor(0.3319, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [629/1000]: training_loss: tensor(0.5663, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [630/1000]: training_loss: tensor(0.3834, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [631/1000]: training_loss: tensor(0.4527, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [632/1000]: training_loss: tensor(0.4512, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [633/1000]: training_loss: tensor(0.5395, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [634/1000]: training_loss: tensor(0.4253, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [635/1000]: training_loss: tensor(0.3810, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [636/1000]: training_loss: tensor(0.3698, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [637/1000]: training_loss: tensor(0.4248, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [638/1000]: training_loss: tensor(0.4780, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [639/1000]: training_loss: tensor(0.3982, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [640/1000]: training_loss: tensor(0.2984, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [641/1000]: training_loss: tensor(0.4239, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [642/1000]: training_loss: tensor(0.4406, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [643/1000]: training_loss: tensor(0.4575, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [644/1000]: training_loss: tensor(0.4228, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [645/1000]: training_loss: tensor(0.3620, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [646/1000]: training_loss: tensor(0.3223, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [647/1000]: training_loss: tensor(0.3984, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [648/1000]: training_loss: tensor(0.3539, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [649/1000]: training_loss: tensor(0.3814, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [650/1000]: training_loss: tensor(0.4239, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [651/1000]: training_loss: tensor(0.4326, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [652/1000]: training_loss: tensor(0.4263, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [653/1000]: training_loss: tensor(0.3705, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [654/1000]: training_loss: tensor(0.4381, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [655/1000]: training_loss: tensor(0.4312, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [656/1000]: training_loss: tensor(0.3926, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [657/1000]: training_loss: tensor(0.4537, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [658/1000]: training_loss: tensor(0.4631, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [659/1000]: training_loss: tensor(0.3636, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [660/1000]: training_loss: tensor(0.4754, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [661/1000]: training_loss: tensor(0.4557, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [662/1000]: training_loss: tensor(0.4670, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [663/1000]: training_loss: tensor(0.3534, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [664/1000]: training_loss: tensor(0.3807, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [665/1000]: training_loss: tensor(0.4914, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [666/1000]: training_loss: tensor(0.3541, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [667/1000]: training_loss: tensor(0.4218, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [668/1000]: training_loss: tensor(0.3569, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [669/1000]: training_loss: tensor(0.4079, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [670/1000]: training_loss: tensor(0.3575, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [671/1000]: training_loss: tensor(0.4408, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [672/1000]: training_loss: tensor(0.3150, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [673/1000]: training_loss: tensor(0.3641, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [674/1000]: training_loss: tensor(0.3380, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [675/1000]: training_loss: tensor(0.3785, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [676/1000]: training_loss: tensor(0.4736, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [677/1000]: training_loss: tensor(0.3550, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [678/1000]: training_loss: tensor(0.3765, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [679/1000]: training_loss: tensor(0.4216, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [680/1000]: training_loss: tensor(0.4334, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [681/1000]: training_loss: tensor(0.3499, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [682/1000]: training_loss: tensor(0.4871, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [683/1000]: training_loss: tensor(0.3869, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [684/1000]: training_loss: tensor(0.3942, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [685/1000]: training_loss: tensor(0.3980, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [686/1000]: training_loss: tensor(0.4567, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [687/1000]: training_loss: tensor(0.4164, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [688/1000]: training_loss: tensor(0.3491, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [689/1000]: training_loss: tensor(0.4589, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [690/1000]: training_loss: tensor(0.4166, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [691/1000]: training_loss: tensor(0.4145, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [692/1000]: training_loss: tensor(0.4319, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [693/1000]: training_loss: tensor(0.3731, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [694/1000]: training_loss: tensor(0.3950, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [695/1000]: training_loss: tensor(0.3841, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [696/1000]: training_loss: tensor(0.4827, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [697/1000]: training_loss: tensor(0.5761, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [698/1000]: training_loss: tensor(0.4229, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [699/1000]: training_loss: tensor(0.4137, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [700/1000]: training_loss: tensor(0.4189, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [701/1000]: training_loss: tensor(0.3099, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [702/1000]: training_loss: tensor(0.3388, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [703/1000]: training_loss: tensor(0.4054, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [704/1000]: training_loss: tensor(0.4607, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [705/1000]: training_loss: tensor(0.5515, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [706/1000]: training_loss: tensor(0.5195, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [707/1000]: training_loss: tensor(0.3785, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [708/1000]: training_loss: tensor(0.5038, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [709/1000]: training_loss: tensor(0.3854, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [710/1000]: training_loss: tensor(0.3107, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [711/1000]: training_loss: tensor(0.4922, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [712/1000]: training_loss: tensor(0.4207, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [713/1000]: training_loss: tensor(0.4433, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [714/1000]: training_loss: tensor(0.3571, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [715/1000]: training_loss: tensor(0.5220, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [716/1000]: training_loss: tensor(0.4053, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [717/1000]: training_loss: tensor(0.4279, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [718/1000]: training_loss: tensor(0.3808, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [719/1000]: training_loss: tensor(0.3744, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [720/1000]: training_loss: tensor(0.4350, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [721/1000]: training_loss: tensor(0.4159, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [722/1000]: training_loss: tensor(0.4942, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [723/1000]: training_loss: tensor(0.4131, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [724/1000]: training_loss: tensor(0.5536, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [725/1000]: training_loss: tensor(0.5315, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [726/1000]: training_loss: tensor(0.4237, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [727/1000]: training_loss: tensor(0.3680, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [728/1000]: training_loss: tensor(0.3608, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [729/1000]: training_loss: tensor(0.3798, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [730/1000]: training_loss: tensor(0.3422, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [731/1000]: training_loss: tensor(0.4388, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [732/1000]: training_loss: tensor(0.3414, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [733/1000]: training_loss: tensor(0.4356, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [734/1000]: training_loss: tensor(0.3132, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [735/1000]: training_loss: tensor(0.3624, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [736/1000]: training_loss: tensor(0.3212, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [737/1000]: training_loss: tensor(0.4135, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [738/1000]: training_loss: tensor(0.3423, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [739/1000]: training_loss: tensor(0.3865, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [740/1000]: training_loss: tensor(0.3839, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [741/1000]: training_loss: tensor(0.3746, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [742/1000]: training_loss: tensor(0.3741, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [743/1000]: training_loss: tensor(0.3102, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [744/1000]: training_loss: tensor(0.4122, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [745/1000]: training_loss: tensor(0.4002, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [746/1000]: training_loss: tensor(0.5267, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [747/1000]: training_loss: tensor(0.4171, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [748/1000]: training_loss: tensor(0.3022, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [749/1000]: training_loss: tensor(0.3838, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [750/1000]: training_loss: tensor(0.3957, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [751/1000]: training_loss: tensor(0.4458, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [752/1000]: training_loss: tensor(0.3753, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [753/1000]: training_loss: tensor(0.3924, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [754/1000]: training_loss: tensor(0.3426, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [755/1000]: training_loss: tensor(0.4364, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [756/1000]: training_loss: tensor(0.3944, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [757/1000]: training_loss: tensor(0.3749, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [758/1000]: training_loss: tensor(0.3677, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [759/1000]: training_loss: tensor(0.4495, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [760/1000]: training_loss: tensor(0.3197, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [761/1000]: training_loss: tensor(0.3141, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [762/1000]: training_loss: tensor(0.4447, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [763/1000]: training_loss: tensor(0.3949, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [764/1000]: training_loss: tensor(0.3442, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [765/1000]: training_loss: tensor(0.4894, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [766/1000]: training_loss: tensor(0.3236, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [767/1000]: training_loss: tensor(0.3583, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [768/1000]: training_loss: tensor(0.4170, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [769/1000]: training_loss: tensor(0.4011, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [770/1000]: training_loss: tensor(0.3783, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [771/1000]: training_loss: tensor(0.4593, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [772/1000]: training_loss: tensor(0.3926, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [773/1000]: training_loss: tensor(0.4281, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [774/1000]: training_loss: tensor(0.4162, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [775/1000]: training_loss: tensor(0.4685, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [776/1000]: training_loss: tensor(0.4182, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [777/1000]: training_loss: tensor(0.3571, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [778/1000]: training_loss: tensor(0.3954, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [779/1000]: training_loss: tensor(0.4128, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [780/1000]: training_loss: tensor(0.4215, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [781/1000]: training_loss: tensor(0.4371, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [782/1000]: training_loss: tensor(0.4068, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [783/1000]: training_loss: tensor(0.3941, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [784/1000]: training_loss: tensor(0.3987, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [785/1000]: training_loss: tensor(0.5001, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [786/1000]: training_loss: tensor(0.4398, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [787/1000]: training_loss: tensor(0.3296, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [788/1000]: training_loss: tensor(0.3724, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [789/1000]: training_loss: tensor(0.4250, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [790/1000]: training_loss: tensor(0.3775, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [791/1000]: training_loss: tensor(0.4541, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [792/1000]: training_loss: tensor(0.5486, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [793/1000]: training_loss: tensor(0.4741, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [794/1000]: training_loss: tensor(0.4320, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [795/1000]: training_loss: tensor(0.4198, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [796/1000]: training_loss: tensor(0.3996, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [797/1000]: training_loss: tensor(0.4185, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [798/1000]: training_loss: tensor(0.4238, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [799/1000]: training_loss: tensor(0.4771, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [800/1000]: training_loss: tensor(0.4001, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [801/1000]: training_loss: tensor(0.4265, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [802/1000]: training_loss: tensor(0.3649, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [803/1000]: training_loss: tensor(0.3618, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [804/1000]: training_loss: tensor(0.3647, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [805/1000]: training_loss: tensor(0.4534, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [806/1000]: training_loss: tensor(0.3668, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [807/1000]: training_loss: tensor(0.3731, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [808/1000]: training_loss: tensor(0.3864, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [809/1000]: training_loss: tensor(0.3569, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [810/1000]: training_loss: tensor(0.3828, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [811/1000]: training_loss: tensor(0.4756, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [812/1000]: training_loss: tensor(0.3175, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [813/1000]: training_loss: tensor(0.4595, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [814/1000]: training_loss: tensor(0.4927, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [815/1000]: training_loss: tensor(0.4563, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [816/1000]: training_loss: tensor(0.3573, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [817/1000]: training_loss: tensor(0.3882, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [818/1000]: training_loss: tensor(0.3651, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [819/1000]: training_loss: tensor(0.3954, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [820/1000]: training_loss: tensor(0.3860, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [821/1000]: training_loss: tensor(0.3393, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [822/1000]: training_loss: tensor(0.4981, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [823/1000]: training_loss: tensor(0.4128, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [824/1000]: training_loss: tensor(0.3288, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [825/1000]: training_loss: tensor(0.4819, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [826/1000]: training_loss: tensor(0.4237, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [827/1000]: training_loss: tensor(0.4254, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [828/1000]: training_loss: tensor(0.4647, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [829/1000]: training_loss: tensor(0.4176, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [830/1000]: training_loss: tensor(0.3141, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [831/1000]: training_loss: tensor(0.4119, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [832/1000]: training_loss: tensor(0.3623, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [833/1000]: training_loss: tensor(0.3469, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [834/1000]: training_loss: tensor(0.4729, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [835/1000]: training_loss: tensor(0.4407, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [836/1000]: training_loss: tensor(0.4588, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [837/1000]: training_loss: tensor(0.4444, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [838/1000]: training_loss: tensor(0.4611, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [839/1000]: training_loss: tensor(0.3268, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [840/1000]: training_loss: tensor(0.3825, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [841/1000]: training_loss: tensor(0.4421, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [842/1000]: training_loss: tensor(0.4424, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [843/1000]: training_loss: tensor(0.2768, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [844/1000]: training_loss: tensor(0.4321, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [845/1000]: training_loss: tensor(0.5004, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [846/1000]: training_loss: tensor(0.3285, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [847/1000]: training_loss: tensor(0.3527, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [848/1000]: training_loss: tensor(0.3802, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [849/1000]: training_loss: tensor(0.3908, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [850/1000]: training_loss: tensor(0.3584, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [851/1000]: training_loss: tensor(0.4587, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [852/1000]: training_loss: tensor(0.4429, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [853/1000]: training_loss: tensor(0.4320, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [854/1000]: training_loss: tensor(0.3076, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [855/1000]: training_loss: tensor(0.3758, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [856/1000]: training_loss: tensor(0.4418, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [857/1000]: training_loss: tensor(0.4386, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [858/1000]: training_loss: tensor(0.3884, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [859/1000]: training_loss: tensor(0.3114, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [860/1000]: training_loss: tensor(0.3525, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [861/1000]: training_loss: tensor(0.3328, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [862/1000]: training_loss: tensor(0.4127, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [863/1000]: training_loss: tensor(0.4345, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [864/1000]: training_loss: tensor(0.3109, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [865/1000]: training_loss: tensor(0.4628, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [866/1000]: training_loss: tensor(0.3506, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [867/1000]: training_loss: tensor(0.4236, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [868/1000]: training_loss: tensor(0.3423, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [869/1000]: training_loss: tensor(0.4482, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [870/1000]: training_loss: tensor(0.3929, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [871/1000]: training_loss: tensor(0.3911, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [872/1000]: training_loss: tensor(0.3035, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [873/1000]: training_loss: tensor(0.4331, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [874/1000]: training_loss: tensor(0.3282, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [875/1000]: training_loss: tensor(0.3820, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [876/1000]: training_loss: tensor(0.4136, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [877/1000]: training_loss: tensor(0.3666, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [878/1000]: training_loss: tensor(0.3104, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [879/1000]: training_loss: tensor(0.3792, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [880/1000]: training_loss: tensor(0.3982, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [881/1000]: training_loss: tensor(0.4488, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [882/1000]: training_loss: tensor(0.4544, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [883/1000]: training_loss: tensor(0.4100, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [884/1000]: training_loss: tensor(0.3790, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [885/1000]: training_loss: tensor(0.3981, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [886/1000]: training_loss: tensor(0.3829, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [887/1000]: training_loss: tensor(0.3835, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [888/1000]: training_loss: tensor(0.4751, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [889/1000]: training_loss: tensor(0.3465, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [890/1000]: training_loss: tensor(0.3870, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [891/1000]: training_loss: tensor(0.3602, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [892/1000]: training_loss: tensor(0.3977, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [893/1000]: training_loss: tensor(0.3673, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [894/1000]: training_loss: tensor(0.3892, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [895/1000]: training_loss: tensor(0.3738, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [896/1000]: training_loss: tensor(0.4251, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [897/1000]: training_loss: tensor(0.4835, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [898/1000]: training_loss: tensor(0.4636, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [899/1000]: training_loss: tensor(0.3666, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [900/1000]: training_loss: tensor(0.5224, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [901/1000]: training_loss: tensor(0.4045, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [902/1000]: training_loss: tensor(0.4203, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [903/1000]: training_loss: tensor(0.3075, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [904/1000]: training_loss: tensor(0.3634, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [905/1000]: training_loss: tensor(0.3809, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [906/1000]: training_loss: tensor(0.5338, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [907/1000]: training_loss: tensor(0.4065, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [908/1000]: training_loss: tensor(0.3338, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [909/1000]: training_loss: tensor(0.3659, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [910/1000]: training_loss: tensor(0.3430, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [911/1000]: training_loss: tensor(0.4579, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [912/1000]: training_loss: tensor(0.3939, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [913/1000]: training_loss: tensor(0.3624, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [914/1000]: training_loss: tensor(0.4328, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [915/1000]: training_loss: tensor(0.3522, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [916/1000]: training_loss: tensor(0.4569, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [917/1000]: training_loss: tensor(0.4224, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [918/1000]: training_loss: tensor(0.3509, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [919/1000]: training_loss: tensor(0.3474, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [920/1000]: training_loss: tensor(0.3703, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [921/1000]: training_loss: tensor(0.4914, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [922/1000]: training_loss: tensor(0.3347, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [923/1000]: training_loss: tensor(0.3513, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [924/1000]: training_loss: tensor(0.4031, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [925/1000]: training_loss: tensor(0.4848, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [926/1000]: training_loss: tensor(0.4505, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [927/1000]: training_loss: tensor(0.4302, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [928/1000]: training_loss: tensor(0.3927, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [929/1000]: training_loss: tensor(0.4211, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [930/1000]: training_loss: tensor(0.4248, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [931/1000]: training_loss: tensor(0.3626, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [932/1000]: training_loss: tensor(0.3552, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [933/1000]: training_loss: tensor(0.4351, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [934/1000]: training_loss: tensor(0.3040, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [935/1000]: training_loss: tensor(0.4913, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [936/1000]: training_loss: tensor(0.3277, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [937/1000]: training_loss: tensor(0.4351, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [938/1000]: training_loss: tensor(0.4247, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [939/1000]: training_loss: tensor(0.3763, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [940/1000]: training_loss: tensor(0.3972, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [941/1000]: training_loss: tensor(0.3900, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [942/1000]: training_loss: tensor(0.3724, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [943/1000]: training_loss: tensor(0.4049, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [944/1000]: training_loss: tensor(0.3202, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [945/1000]: training_loss: tensor(0.3276, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [946/1000]: training_loss: tensor(0.4445, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [947/1000]: training_loss: tensor(0.3875, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [948/1000]: training_loss: tensor(0.3712, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [949/1000]: training_loss: tensor(0.3566, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [950/1000]: training_loss: tensor(0.3513, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [951/1000]: training_loss: tensor(0.4108, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [952/1000]: training_loss: tensor(0.4225, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [953/1000]: training_loss: tensor(0.3641, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [954/1000]: training_loss: tensor(0.3229, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [955/1000]: training_loss: tensor(0.4620, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [956/1000]: training_loss: tensor(0.5404, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [957/1000]: training_loss: tensor(0.4474, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [958/1000]: training_loss: tensor(0.3600, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [959/1000]: training_loss: tensor(0.3319, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [960/1000]: training_loss: tensor(0.3889, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [961/1000]: training_loss: tensor(0.3466, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [962/1000]: training_loss: tensor(0.4404, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [963/1000]: training_loss: tensor(0.4842, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [964/1000]: training_loss: tensor(0.3863, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [965/1000]: training_loss: tensor(0.3645, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [966/1000]: training_loss: tensor(0.3860, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [967/1000]: training_loss: tensor(0.3253, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [968/1000]: training_loss: tensor(0.3814, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [969/1000]: training_loss: tensor(0.4101, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [970/1000]: training_loss: tensor(0.3971, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [971/1000]: training_loss: tensor(0.3978, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [972/1000]: training_loss: tensor(0.3117, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [973/1000]: training_loss: tensor(0.4071, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [974/1000]: training_loss: tensor(0.3454, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [975/1000]: training_loss: tensor(0.4037, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [976/1000]: training_loss: tensor(0.3867, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [977/1000]: training_loss: tensor(0.4609, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [978/1000]: training_loss: tensor(0.3319, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [979/1000]: training_loss: tensor(0.3615, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [980/1000]: training_loss: tensor(0.3445, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [981/1000]: training_loss: tensor(0.4124, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [982/1000]: training_loss: tensor(0.3062, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [983/1000]: training_loss: tensor(0.4015, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [984/1000]: training_loss: tensor(0.4091, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [985/1000]: training_loss: tensor(0.3247, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [986/1000]: training_loss: tensor(0.4109, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [987/1000]: training_loss: tensor(0.3676, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [988/1000]: training_loss: tensor(0.4451, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [989/1000]: training_loss: tensor(0.5171, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [990/1000]: training_loss: tensor(0.3915, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [991/1000]: training_loss: tensor(0.3745, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [992/1000]: training_loss: tensor(0.4287, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [993/1000]: training_loss: tensor(0.3890, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [994/1000]: training_loss: tensor(0.3870, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [995/1000]: training_loss: tensor(0.3866, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [996/1000]: training_loss: tensor(0.3883, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [997/1000]: training_loss: tensor(0.4242, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [998/1000]: training_loss: tensor(0.4012, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [999/1000]: training_loss: tensor(0.4046, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Epoch [2/5], global step [2000/5000], Train Loss: 0.4159, Valid Loss: 0.3731\n",
      "batch_no [0/1000]: training_loss: tensor(0.3690, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [1/1000]: training_loss: tensor(0.3982, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [2/1000]: training_loss: tensor(0.4050, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [3/1000]: training_loss: tensor(0.3226, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [4/1000]: training_loss: tensor(0.4035, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [5/1000]: training_loss: tensor(0.4192, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [6/1000]: training_loss: tensor(0.3376, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [7/1000]: training_loss: tensor(0.3694, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [8/1000]: training_loss: tensor(0.3852, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [9/1000]: training_loss: tensor(0.3713, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [10/1000]: training_loss: tensor(0.4188, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [11/1000]: training_loss: tensor(0.3195, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [12/1000]: training_loss: tensor(0.3877, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [13/1000]: training_loss: tensor(0.3493, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [14/1000]: training_loss: tensor(0.3726, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [15/1000]: training_loss: tensor(0.4832, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [16/1000]: training_loss: tensor(0.3585, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [17/1000]: training_loss: tensor(0.3576, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [18/1000]: training_loss: tensor(0.4011, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [19/1000]: training_loss: tensor(0.3830, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [20/1000]: training_loss: tensor(0.4145, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [21/1000]: training_loss: tensor(0.3999, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [22/1000]: training_loss: tensor(0.4414, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [23/1000]: training_loss: tensor(0.3608, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [24/1000]: training_loss: tensor(0.3207, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [25/1000]: training_loss: tensor(0.3948, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [26/1000]: training_loss: tensor(0.3818, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [27/1000]: training_loss: tensor(0.3409, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [28/1000]: training_loss: tensor(0.3745, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [29/1000]: training_loss: tensor(0.4294, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [30/1000]: training_loss: tensor(0.3963, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [31/1000]: training_loss: tensor(0.3532, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [32/1000]: training_loss: tensor(0.3960, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [33/1000]: training_loss: tensor(0.3655, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [34/1000]: training_loss: tensor(0.4635, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [35/1000]: training_loss: tensor(0.4024, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [36/1000]: training_loss: tensor(0.4084, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [37/1000]: training_loss: tensor(0.4252, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [38/1000]: training_loss: tensor(0.3419, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [39/1000]: training_loss: tensor(0.3818, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [40/1000]: training_loss: tensor(0.3263, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [41/1000]: training_loss: tensor(0.4826, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [42/1000]: training_loss: tensor(0.3633, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [43/1000]: training_loss: tensor(0.3135, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [44/1000]: training_loss: tensor(0.4153, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [45/1000]: training_loss: tensor(0.3819, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [46/1000]: training_loss: tensor(0.3599, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [47/1000]: training_loss: tensor(0.4686, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [48/1000]: training_loss: tensor(0.3311, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [49/1000]: training_loss: tensor(0.4380, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [50/1000]: training_loss: tensor(0.3702, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [51/1000]: training_loss: tensor(0.4267, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [52/1000]: training_loss: tensor(0.3713, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [53/1000]: training_loss: tensor(0.4262, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [54/1000]: training_loss: tensor(0.3944, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [55/1000]: training_loss: tensor(0.4056, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [56/1000]: training_loss: tensor(0.4664, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [57/1000]: training_loss: tensor(0.4538, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [58/1000]: training_loss: tensor(0.3267, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [59/1000]: training_loss: tensor(0.3854, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [60/1000]: training_loss: tensor(0.4905, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [61/1000]: training_loss: tensor(0.3183, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [62/1000]: training_loss: tensor(0.3337, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [63/1000]: training_loss: tensor(0.4190, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [64/1000]: training_loss: tensor(0.4658, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [65/1000]: training_loss: tensor(0.3313, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [66/1000]: training_loss: tensor(0.4441, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [67/1000]: training_loss: tensor(0.4067, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [68/1000]: training_loss: tensor(0.3705, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [69/1000]: training_loss: tensor(0.3790, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [70/1000]: training_loss: tensor(0.3865, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [71/1000]: training_loss: tensor(0.3785, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [72/1000]: training_loss: tensor(0.4438, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [73/1000]: training_loss: tensor(0.3563, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [74/1000]: training_loss: tensor(0.4452, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [75/1000]: training_loss: tensor(0.4850, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [76/1000]: training_loss: tensor(0.3254, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [77/1000]: training_loss: tensor(0.3442, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [78/1000]: training_loss: tensor(0.4009, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [79/1000]: training_loss: tensor(0.4175, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [80/1000]: training_loss: tensor(0.3075, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [81/1000]: training_loss: tensor(0.3734, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [82/1000]: training_loss: tensor(0.4486, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [83/1000]: training_loss: tensor(0.2883, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [84/1000]: training_loss: tensor(0.3574, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [85/1000]: training_loss: tensor(0.4264, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [86/1000]: training_loss: tensor(0.3867, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [87/1000]: training_loss: tensor(0.4033, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [88/1000]: training_loss: tensor(0.3594, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [89/1000]: training_loss: tensor(0.4043, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [90/1000]: training_loss: tensor(0.4794, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [91/1000]: training_loss: tensor(0.4394, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [92/1000]: training_loss: tensor(0.3245, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [93/1000]: training_loss: tensor(0.3594, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [94/1000]: training_loss: tensor(0.3559, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [95/1000]: training_loss: tensor(0.3373, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [96/1000]: training_loss: tensor(0.3728, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [97/1000]: training_loss: tensor(0.3982, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [98/1000]: training_loss: tensor(0.3551, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [99/1000]: training_loss: tensor(0.3859, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [100/1000]: training_loss: tensor(0.3761, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [101/1000]: training_loss: tensor(0.4037, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [102/1000]: training_loss: tensor(0.3866, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [103/1000]: training_loss: tensor(0.4105, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [104/1000]: training_loss: tensor(0.4002, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [105/1000]: training_loss: tensor(0.3727, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [106/1000]: training_loss: tensor(0.4520, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [107/1000]: training_loss: tensor(0.4033, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [108/1000]: training_loss: tensor(0.3995, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [109/1000]: training_loss: tensor(0.4184, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [110/1000]: training_loss: tensor(0.4444, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [111/1000]: training_loss: tensor(0.3815, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [112/1000]: training_loss: tensor(0.4044, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [113/1000]: training_loss: tensor(0.5074, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [114/1000]: training_loss: tensor(0.4034, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [115/1000]: training_loss: tensor(0.3496, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [116/1000]: training_loss: tensor(0.3836, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [117/1000]: training_loss: tensor(0.4862, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [118/1000]: training_loss: tensor(0.3649, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [119/1000]: training_loss: tensor(0.3886, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [120/1000]: training_loss: tensor(0.3946, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [121/1000]: training_loss: tensor(0.4359, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [122/1000]: training_loss: tensor(0.2654, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [123/1000]: training_loss: tensor(0.4385, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [124/1000]: training_loss: tensor(0.3318, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [125/1000]: training_loss: tensor(0.3856, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [126/1000]: training_loss: tensor(0.4108, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [127/1000]: training_loss: tensor(0.3535, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [128/1000]: training_loss: tensor(0.3524, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [129/1000]: training_loss: tensor(0.4013, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [130/1000]: training_loss: tensor(0.3419, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [131/1000]: training_loss: tensor(0.4226, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [132/1000]: training_loss: tensor(0.3769, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [133/1000]: training_loss: tensor(0.4199, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [134/1000]: training_loss: tensor(0.3506, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [135/1000]: training_loss: tensor(0.3355, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [136/1000]: training_loss: tensor(0.4130, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [137/1000]: training_loss: tensor(0.3305, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [138/1000]: training_loss: tensor(0.3986, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [139/1000]: training_loss: tensor(0.3352, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [140/1000]: training_loss: tensor(0.3477, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [141/1000]: training_loss: tensor(0.3353, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [142/1000]: training_loss: tensor(0.4361, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [143/1000]: training_loss: tensor(0.4616, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [144/1000]: training_loss: tensor(0.3822, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [145/1000]: training_loss: tensor(0.2752, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [146/1000]: training_loss: tensor(0.3687, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [147/1000]: training_loss: tensor(0.3539, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [148/1000]: training_loss: tensor(0.3530, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [149/1000]: training_loss: tensor(0.3518, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [150/1000]: training_loss: tensor(0.3919, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [151/1000]: training_loss: tensor(0.3662, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [152/1000]: training_loss: tensor(0.3439, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [153/1000]: training_loss: tensor(0.3431, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [154/1000]: training_loss: tensor(0.3968, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [155/1000]: training_loss: tensor(0.3992, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [156/1000]: training_loss: tensor(0.4385, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [157/1000]: training_loss: tensor(0.3566, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [158/1000]: training_loss: tensor(0.4177, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [159/1000]: training_loss: tensor(0.4508, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [160/1000]: training_loss: tensor(0.4047, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [161/1000]: training_loss: tensor(0.3969, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [162/1000]: training_loss: tensor(0.4269, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [163/1000]: training_loss: tensor(0.3875, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [164/1000]: training_loss: tensor(0.3713, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [165/1000]: training_loss: tensor(0.3880, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [166/1000]: training_loss: tensor(0.4306, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [167/1000]: training_loss: tensor(0.3135, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [168/1000]: training_loss: tensor(0.3760, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [169/1000]: training_loss: tensor(0.3449, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [170/1000]: training_loss: tensor(0.3503, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [171/1000]: training_loss: tensor(0.3276, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [172/1000]: training_loss: tensor(0.3696, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [173/1000]: training_loss: tensor(0.3964, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [174/1000]: training_loss: tensor(0.3605, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [175/1000]: training_loss: tensor(0.3201, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [176/1000]: training_loss: tensor(0.4121, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [177/1000]: training_loss: tensor(0.2680, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [178/1000]: training_loss: tensor(0.2727, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [179/1000]: training_loss: tensor(0.4196, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [180/1000]: training_loss: tensor(0.4374, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [181/1000]: training_loss: tensor(0.4033, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [182/1000]: training_loss: tensor(0.4997, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [183/1000]: training_loss: tensor(0.3813, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [184/1000]: training_loss: tensor(0.3249, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [185/1000]: training_loss: tensor(0.3514, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [186/1000]: training_loss: tensor(0.3953, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [187/1000]: training_loss: tensor(0.3490, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [188/1000]: training_loss: tensor(0.3998, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [189/1000]: training_loss: tensor(0.4483, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [190/1000]: training_loss: tensor(0.4416, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [191/1000]: training_loss: tensor(0.3972, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [192/1000]: training_loss: tensor(0.3733, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [193/1000]: training_loss: tensor(0.3816, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [194/1000]: training_loss: tensor(0.4072, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [195/1000]: training_loss: tensor(0.4104, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [196/1000]: training_loss: tensor(0.2838, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [197/1000]: training_loss: tensor(0.3153, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [198/1000]: training_loss: tensor(0.4223, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [199/1000]: training_loss: tensor(0.3798, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [200/1000]: training_loss: tensor(0.4608, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [201/1000]: training_loss: tensor(0.3572, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [202/1000]: training_loss: tensor(0.3407, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [203/1000]: training_loss: tensor(0.4361, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [204/1000]: training_loss: tensor(0.3914, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [205/1000]: training_loss: tensor(0.3427, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [206/1000]: training_loss: tensor(0.3732, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [207/1000]: training_loss: tensor(0.3878, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [208/1000]: training_loss: tensor(0.3734, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [209/1000]: training_loss: tensor(0.3593, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [210/1000]: training_loss: tensor(0.3457, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [211/1000]: training_loss: tensor(0.4092, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [212/1000]: training_loss: tensor(0.3738, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [213/1000]: training_loss: tensor(0.3513, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [214/1000]: training_loss: tensor(0.4097, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [215/1000]: training_loss: tensor(0.4028, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [216/1000]: training_loss: tensor(0.3833, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [217/1000]: training_loss: tensor(0.3447, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [218/1000]: training_loss: tensor(0.4504, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [219/1000]: training_loss: tensor(0.4791, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [220/1000]: training_loss: tensor(0.3348, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [221/1000]: training_loss: tensor(0.3449, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [222/1000]: training_loss: tensor(0.3726, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [223/1000]: training_loss: tensor(0.4112, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [224/1000]: training_loss: tensor(0.4949, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [225/1000]: training_loss: tensor(0.3728, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [226/1000]: training_loss: tensor(0.3631, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [227/1000]: training_loss: tensor(0.4452, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [228/1000]: training_loss: tensor(0.4387, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [229/1000]: training_loss: tensor(0.4243, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [230/1000]: training_loss: tensor(0.4038, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [231/1000]: training_loss: tensor(0.2999, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [232/1000]: training_loss: tensor(0.3853, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [233/1000]: training_loss: tensor(0.3281, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [234/1000]: training_loss: tensor(0.4609, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [235/1000]: training_loss: tensor(0.4123, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [236/1000]: training_loss: tensor(0.3763, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [237/1000]: training_loss: tensor(0.2957, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [238/1000]: training_loss: tensor(0.4143, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [239/1000]: training_loss: tensor(0.3459, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [240/1000]: training_loss: tensor(0.3649, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [241/1000]: training_loss: tensor(0.4591, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [242/1000]: training_loss: tensor(0.3717, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [243/1000]: training_loss: tensor(0.4108, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [244/1000]: training_loss: tensor(0.3057, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [245/1000]: training_loss: tensor(0.4358, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [246/1000]: training_loss: tensor(0.4088, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [247/1000]: training_loss: tensor(0.2938, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [248/1000]: training_loss: tensor(0.3380, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [249/1000]: training_loss: tensor(0.4019, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [250/1000]: training_loss: tensor(0.4136, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [251/1000]: training_loss: tensor(0.4693, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [252/1000]: training_loss: tensor(0.4365, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [253/1000]: training_loss: tensor(0.3756, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [254/1000]: training_loss: tensor(0.3859, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [255/1000]: training_loss: tensor(0.3735, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [256/1000]: training_loss: tensor(0.3311, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [257/1000]: training_loss: tensor(0.3655, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [258/1000]: training_loss: tensor(0.4328, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [259/1000]: training_loss: tensor(0.3583, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [260/1000]: training_loss: tensor(0.3825, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [261/1000]: training_loss: tensor(0.3780, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [262/1000]: training_loss: tensor(0.3641, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [263/1000]: training_loss: tensor(0.3567, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [264/1000]: training_loss: tensor(0.4608, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [265/1000]: training_loss: tensor(0.3588, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [266/1000]: training_loss: tensor(0.3876, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [267/1000]: training_loss: tensor(0.3669, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [268/1000]: training_loss: tensor(0.3586, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [269/1000]: training_loss: tensor(0.3869, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [270/1000]: training_loss: tensor(0.3239, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [271/1000]: training_loss: tensor(0.3661, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [272/1000]: training_loss: tensor(0.4173, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [273/1000]: training_loss: tensor(0.4177, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [274/1000]: training_loss: tensor(0.3104, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [275/1000]: training_loss: tensor(0.3137, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [276/1000]: training_loss: tensor(0.4151, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [277/1000]: training_loss: tensor(0.3047, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [278/1000]: training_loss: tensor(0.4141, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [279/1000]: training_loss: tensor(0.3639, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [280/1000]: training_loss: tensor(0.4349, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [281/1000]: training_loss: tensor(0.3980, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [282/1000]: training_loss: tensor(0.3968, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [283/1000]: training_loss: tensor(0.3259, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [284/1000]: training_loss: tensor(0.3647, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [285/1000]: training_loss: tensor(0.4121, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [286/1000]: training_loss: tensor(0.4908, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [287/1000]: training_loss: tensor(0.3840, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [288/1000]: training_loss: tensor(0.3854, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [289/1000]: training_loss: tensor(0.3719, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [290/1000]: training_loss: tensor(0.3310, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [291/1000]: training_loss: tensor(0.3164, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [292/1000]: training_loss: tensor(0.3091, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [293/1000]: training_loss: tensor(0.4396, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [294/1000]: training_loss: tensor(0.3447, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [295/1000]: training_loss: tensor(0.3358, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [296/1000]: training_loss: tensor(0.4417, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [297/1000]: training_loss: tensor(0.3998, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [298/1000]: training_loss: tensor(0.3111, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [299/1000]: training_loss: tensor(0.3894, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [300/1000]: training_loss: tensor(0.3922, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [301/1000]: training_loss: tensor(0.3886, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [302/1000]: training_loss: tensor(0.4030, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [303/1000]: training_loss: tensor(0.4717, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [304/1000]: training_loss: tensor(0.3947, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [305/1000]: training_loss: tensor(0.3693, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [306/1000]: training_loss: tensor(0.3316, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [307/1000]: training_loss: tensor(0.4722, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [308/1000]: training_loss: tensor(0.3380, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [309/1000]: training_loss: tensor(0.4374, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [310/1000]: training_loss: tensor(0.3336, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [311/1000]: training_loss: tensor(0.4213, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [312/1000]: training_loss: tensor(0.3820, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [313/1000]: training_loss: tensor(0.3596, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [314/1000]: training_loss: tensor(0.4771, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [315/1000]: training_loss: tensor(0.3151, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [316/1000]: training_loss: tensor(0.3983, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [317/1000]: training_loss: tensor(0.4029, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [318/1000]: training_loss: tensor(0.3509, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [319/1000]: training_loss: tensor(0.2786, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [320/1000]: training_loss: tensor(0.4207, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [321/1000]: training_loss: tensor(0.3805, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [322/1000]: training_loss: tensor(0.4056, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [323/1000]: training_loss: tensor(0.4473, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [324/1000]: training_loss: tensor(0.4475, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [325/1000]: training_loss: tensor(0.3230, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [326/1000]: training_loss: tensor(0.3363, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [327/1000]: training_loss: tensor(0.4172, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [328/1000]: training_loss: tensor(0.3918, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [329/1000]: training_loss: tensor(0.4201, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [330/1000]: training_loss: tensor(0.4675, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [331/1000]: training_loss: tensor(0.4032, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [332/1000]: training_loss: tensor(0.3646, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [333/1000]: training_loss: tensor(0.3448, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [334/1000]: training_loss: tensor(0.4072, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [335/1000]: training_loss: tensor(0.4153, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [336/1000]: training_loss: tensor(0.3592, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [337/1000]: training_loss: tensor(0.4059, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [338/1000]: training_loss: tensor(0.4167, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [339/1000]: training_loss: tensor(0.3631, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [340/1000]: training_loss: tensor(0.4609, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [341/1000]: training_loss: tensor(0.3799, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [342/1000]: training_loss: tensor(0.4527, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [343/1000]: training_loss: tensor(0.3080, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [344/1000]: training_loss: tensor(0.4130, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [345/1000]: training_loss: tensor(0.3791, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [346/1000]: training_loss: tensor(0.3007, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [347/1000]: training_loss: tensor(0.5598, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [348/1000]: training_loss: tensor(0.3664, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [349/1000]: training_loss: tensor(0.3719, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [350/1000]: training_loss: tensor(0.3524, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [351/1000]: training_loss: tensor(0.3438, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [352/1000]: training_loss: tensor(0.4228, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [353/1000]: training_loss: tensor(0.3996, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [354/1000]: training_loss: tensor(0.3389, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [355/1000]: training_loss: tensor(0.3509, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [356/1000]: training_loss: tensor(0.3829, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [357/1000]: training_loss: tensor(0.3773, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [358/1000]: training_loss: tensor(0.3810, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [359/1000]: training_loss: tensor(0.3592, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [360/1000]: training_loss: tensor(0.4101, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [361/1000]: training_loss: tensor(0.2784, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [362/1000]: training_loss: tensor(0.4844, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [363/1000]: training_loss: tensor(0.3390, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [364/1000]: training_loss: tensor(0.2940, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [365/1000]: training_loss: tensor(0.3934, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [366/1000]: training_loss: tensor(0.3737, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [367/1000]: training_loss: tensor(0.3741, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [368/1000]: training_loss: tensor(0.4357, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [369/1000]: training_loss: tensor(0.3861, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [370/1000]: training_loss: tensor(0.3437, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [371/1000]: training_loss: tensor(0.3420, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [372/1000]: training_loss: tensor(0.3379, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [373/1000]: training_loss: tensor(0.4057, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [374/1000]: training_loss: tensor(0.3661, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [375/1000]: training_loss: tensor(0.3848, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [376/1000]: training_loss: tensor(0.3305, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [377/1000]: training_loss: tensor(0.4667, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [378/1000]: training_loss: tensor(0.4155, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [379/1000]: training_loss: tensor(0.4158, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [380/1000]: training_loss: tensor(0.4161, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [381/1000]: training_loss: tensor(0.3316, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [382/1000]: training_loss: tensor(0.3952, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [383/1000]: training_loss: tensor(0.4945, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [384/1000]: training_loss: tensor(0.3539, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [385/1000]: training_loss: tensor(0.4870, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [386/1000]: training_loss: tensor(0.3412, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [387/1000]: training_loss: tensor(0.3628, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [388/1000]: training_loss: tensor(0.3807, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [389/1000]: training_loss: tensor(0.2971, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [390/1000]: training_loss: tensor(0.3728, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [391/1000]: training_loss: tensor(0.4025, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [392/1000]: training_loss: tensor(0.3854, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [393/1000]: training_loss: tensor(0.4001, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [394/1000]: training_loss: tensor(0.4269, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [395/1000]: training_loss: tensor(0.4831, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [396/1000]: training_loss: tensor(0.3530, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [397/1000]: training_loss: tensor(0.3966, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [398/1000]: training_loss: tensor(0.3445, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [399/1000]: training_loss: tensor(0.3672, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [400/1000]: training_loss: tensor(0.5256, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [401/1000]: training_loss: tensor(0.4160, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [402/1000]: training_loss: tensor(0.3741, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [403/1000]: training_loss: tensor(0.4014, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [404/1000]: training_loss: tensor(0.3548, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [405/1000]: training_loss: tensor(0.3351, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [406/1000]: training_loss: tensor(0.3709, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [407/1000]: training_loss: tensor(0.3490, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [408/1000]: training_loss: tensor(0.3982, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [409/1000]: training_loss: tensor(0.4559, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [410/1000]: training_loss: tensor(0.4005, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [411/1000]: training_loss: tensor(0.3377, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [412/1000]: training_loss: tensor(0.3497, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [413/1000]: training_loss: tensor(0.3388, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [414/1000]: training_loss: tensor(0.4605, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [415/1000]: training_loss: tensor(0.2976, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [416/1000]: training_loss: tensor(0.4957, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [417/1000]: training_loss: tensor(0.3594, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [418/1000]: training_loss: tensor(0.3392, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [419/1000]: training_loss: tensor(0.4682, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [420/1000]: training_loss: tensor(0.3720, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [421/1000]: training_loss: tensor(0.3551, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [422/1000]: training_loss: tensor(0.3743, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [423/1000]: training_loss: tensor(0.3633, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [424/1000]: training_loss: tensor(0.3395, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [425/1000]: training_loss: tensor(0.3319, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [426/1000]: training_loss: tensor(0.3856, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [427/1000]: training_loss: tensor(0.3161, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [428/1000]: training_loss: tensor(0.3327, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [429/1000]: training_loss: tensor(0.3582, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [430/1000]: training_loss: tensor(0.3273, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [431/1000]: training_loss: tensor(0.4053, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [432/1000]: training_loss: tensor(0.3562, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [433/1000]: training_loss: tensor(0.4657, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [434/1000]: training_loss: tensor(0.4120, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [435/1000]: training_loss: tensor(0.3519, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [436/1000]: training_loss: tensor(0.4100, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [437/1000]: training_loss: tensor(0.3679, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [438/1000]: training_loss: tensor(0.4324, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [439/1000]: training_loss: tensor(0.4346, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [440/1000]: training_loss: tensor(0.3831, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [441/1000]: training_loss: tensor(0.4232, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [442/1000]: training_loss: tensor(0.3723, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [443/1000]: training_loss: tensor(0.3642, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [444/1000]: training_loss: tensor(0.3887, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [445/1000]: training_loss: tensor(0.4389, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [446/1000]: training_loss: tensor(0.4316, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [447/1000]: training_loss: tensor(0.3637, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [448/1000]: training_loss: tensor(0.4127, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [449/1000]: training_loss: tensor(0.3107, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [450/1000]: training_loss: tensor(0.3962, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [451/1000]: training_loss: tensor(0.3999, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [452/1000]: training_loss: tensor(0.4273, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [453/1000]: training_loss: tensor(0.3528, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [454/1000]: training_loss: tensor(0.2791, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [455/1000]: training_loss: tensor(0.4038, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [456/1000]: training_loss: tensor(0.3805, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [457/1000]: training_loss: tensor(0.4567, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [458/1000]: training_loss: tensor(0.3976, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [459/1000]: training_loss: tensor(0.3840, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [460/1000]: training_loss: tensor(0.2985, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [461/1000]: training_loss: tensor(0.4152, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [462/1000]: training_loss: tensor(0.3995, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [463/1000]: training_loss: tensor(0.4237, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [464/1000]: training_loss: tensor(0.3516, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [465/1000]: training_loss: tensor(0.3256, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [466/1000]: training_loss: tensor(0.3960, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [467/1000]: training_loss: tensor(0.3302, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [468/1000]: training_loss: tensor(0.3301, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [469/1000]: training_loss: tensor(0.3705, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [470/1000]: training_loss: tensor(0.3359, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [471/1000]: training_loss: tensor(0.4685, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [472/1000]: training_loss: tensor(0.4577, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [473/1000]: training_loss: tensor(0.3595, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [474/1000]: training_loss: tensor(0.3368, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [475/1000]: training_loss: tensor(0.3517, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [476/1000]: training_loss: tensor(0.3820, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [477/1000]: training_loss: tensor(0.3266, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [478/1000]: training_loss: tensor(0.4507, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [479/1000]: training_loss: tensor(0.3614, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [480/1000]: training_loss: tensor(0.4617, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [481/1000]: training_loss: tensor(0.3908, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [482/1000]: training_loss: tensor(0.4129, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [483/1000]: training_loss: tensor(0.3142, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [484/1000]: training_loss: tensor(0.3791, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [485/1000]: training_loss: tensor(0.4619, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [486/1000]: training_loss: tensor(0.4138, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [487/1000]: training_loss: tensor(0.3439, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [488/1000]: training_loss: tensor(0.3420, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [489/1000]: training_loss: tensor(0.3917, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [490/1000]: training_loss: tensor(0.3790, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [491/1000]: training_loss: tensor(0.3466, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [492/1000]: training_loss: tensor(0.2868, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [493/1000]: training_loss: tensor(0.3110, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [494/1000]: training_loss: tensor(0.3367, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [495/1000]: training_loss: tensor(0.4351, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [496/1000]: training_loss: tensor(0.3134, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [497/1000]: training_loss: tensor(0.3658, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [498/1000]: training_loss: tensor(0.3157, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [499/1000]: training_loss: tensor(0.4317, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [500/1000]: training_loss: tensor(0.3206, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [501/1000]: training_loss: tensor(0.4349, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [502/1000]: training_loss: tensor(0.2987, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [503/1000]: training_loss: tensor(0.3888, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [504/1000]: training_loss: tensor(0.4125, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [505/1000]: training_loss: tensor(0.4031, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [506/1000]: training_loss: tensor(0.3703, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [507/1000]: training_loss: tensor(0.4457, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [508/1000]: training_loss: tensor(0.4820, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [509/1000]: training_loss: tensor(0.3668, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [510/1000]: training_loss: tensor(0.3815, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [511/1000]: training_loss: tensor(0.3799, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [512/1000]: training_loss: tensor(0.3994, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [513/1000]: training_loss: tensor(0.3423, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [514/1000]: training_loss: tensor(0.3735, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [515/1000]: training_loss: tensor(0.2567, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [516/1000]: training_loss: tensor(0.4500, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [517/1000]: training_loss: tensor(0.4443, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [518/1000]: training_loss: tensor(0.4143, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [519/1000]: training_loss: tensor(0.4167, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [520/1000]: training_loss: tensor(0.3144, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [521/1000]: training_loss: tensor(0.3305, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [522/1000]: training_loss: tensor(0.3742, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [523/1000]: training_loss: tensor(0.4564, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [524/1000]: training_loss: tensor(0.3517, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [525/1000]: training_loss: tensor(0.4352, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [526/1000]: training_loss: tensor(0.2783, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [527/1000]: training_loss: tensor(0.3935, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [528/1000]: training_loss: tensor(0.3769, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [529/1000]: training_loss: tensor(0.3978, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [530/1000]: training_loss: tensor(0.4410, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [531/1000]: training_loss: tensor(0.3947, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [532/1000]: training_loss: tensor(0.3960, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [533/1000]: training_loss: tensor(0.4125, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [534/1000]: training_loss: tensor(0.3696, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [535/1000]: training_loss: tensor(0.4788, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [536/1000]: training_loss: tensor(0.4174, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [537/1000]: training_loss: tensor(0.3557, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [538/1000]: training_loss: tensor(0.4372, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [539/1000]: training_loss: tensor(0.4211, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [540/1000]: training_loss: tensor(0.3127, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [541/1000]: training_loss: tensor(0.5279, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [542/1000]: training_loss: tensor(0.4178, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [543/1000]: training_loss: tensor(0.3459, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [544/1000]: training_loss: tensor(0.3032, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [545/1000]: training_loss: tensor(0.4075, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [546/1000]: training_loss: tensor(0.4046, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [547/1000]: training_loss: tensor(0.3157, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [548/1000]: training_loss: tensor(0.3198, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [549/1000]: training_loss: tensor(0.3962, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [550/1000]: training_loss: tensor(0.3270, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [551/1000]: training_loss: tensor(0.3331, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [552/1000]: training_loss: tensor(0.4822, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [553/1000]: training_loss: tensor(0.4953, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [554/1000]: training_loss: tensor(0.2717, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [555/1000]: training_loss: tensor(0.3359, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [556/1000]: training_loss: tensor(0.3600, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [557/1000]: training_loss: tensor(0.3628, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [558/1000]: training_loss: tensor(0.3710, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [559/1000]: training_loss: tensor(0.3390, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [560/1000]: training_loss: tensor(0.3559, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [561/1000]: training_loss: tensor(0.3734, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [562/1000]: training_loss: tensor(0.3968, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [563/1000]: training_loss: tensor(0.3247, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [564/1000]: training_loss: tensor(0.3421, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [565/1000]: training_loss: tensor(0.4418, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [566/1000]: training_loss: tensor(0.3793, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [567/1000]: training_loss: tensor(0.3999, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [568/1000]: training_loss: tensor(0.3423, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [569/1000]: training_loss: tensor(0.4564, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [570/1000]: training_loss: tensor(0.4321, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [571/1000]: training_loss: tensor(0.3613, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [572/1000]: training_loss: tensor(0.4362, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [573/1000]: training_loss: tensor(0.3143, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [574/1000]: training_loss: tensor(0.3252, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [575/1000]: training_loss: tensor(0.3410, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [576/1000]: training_loss: tensor(0.3929, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [577/1000]: training_loss: tensor(0.3614, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [578/1000]: training_loss: tensor(0.3205, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [579/1000]: training_loss: tensor(0.3986, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [580/1000]: training_loss: tensor(0.3769, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [581/1000]: training_loss: tensor(0.4149, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [582/1000]: training_loss: tensor(0.4451, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [583/1000]: training_loss: tensor(0.3195, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [584/1000]: training_loss: tensor(0.4489, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [585/1000]: training_loss: tensor(0.4417, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [586/1000]: training_loss: tensor(0.3142, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [587/1000]: training_loss: tensor(0.4632, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [588/1000]: training_loss: tensor(0.3679, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [589/1000]: training_loss: tensor(0.3224, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [590/1000]: training_loss: tensor(0.3733, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [591/1000]: training_loss: tensor(0.3282, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [592/1000]: training_loss: tensor(0.4043, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [593/1000]: training_loss: tensor(0.3571, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [594/1000]: training_loss: tensor(0.2893, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [595/1000]: training_loss: tensor(0.3584, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [596/1000]: training_loss: tensor(0.4250, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [597/1000]: training_loss: tensor(0.3225, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [598/1000]: training_loss: tensor(0.2965, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [599/1000]: training_loss: tensor(0.3337, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [600/1000]: training_loss: tensor(0.2995, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [601/1000]: training_loss: tensor(0.3371, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [602/1000]: training_loss: tensor(0.3945, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [603/1000]: training_loss: tensor(0.4770, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [604/1000]: training_loss: tensor(0.4065, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [605/1000]: training_loss: tensor(0.4402, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [606/1000]: training_loss: tensor(0.4333, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [607/1000]: training_loss: tensor(0.3405, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [608/1000]: training_loss: tensor(0.3367, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [609/1000]: training_loss: tensor(0.3677, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [610/1000]: training_loss: tensor(0.2903, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [611/1000]: training_loss: tensor(0.3652, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [612/1000]: training_loss: tensor(0.3211, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [613/1000]: training_loss: tensor(0.3734, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [614/1000]: training_loss: tensor(0.3910, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [615/1000]: training_loss: tensor(0.4435, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [616/1000]: training_loss: tensor(0.4148, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [617/1000]: training_loss: tensor(0.3503, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [618/1000]: training_loss: tensor(0.3282, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [619/1000]: training_loss: tensor(0.3714, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [620/1000]: training_loss: tensor(0.2891, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [621/1000]: training_loss: tensor(0.3816, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [622/1000]: training_loss: tensor(0.3885, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [623/1000]: training_loss: tensor(0.3478, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [624/1000]: training_loss: tensor(0.4682, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [625/1000]: training_loss: tensor(0.3970, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [626/1000]: training_loss: tensor(0.3980, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [627/1000]: training_loss: tensor(0.3282, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [628/1000]: training_loss: tensor(0.4328, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [629/1000]: training_loss: tensor(0.3422, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [630/1000]: training_loss: tensor(0.3860, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [631/1000]: training_loss: tensor(0.4174, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [632/1000]: training_loss: tensor(0.3785, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [633/1000]: training_loss: tensor(0.4301, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [634/1000]: training_loss: tensor(0.4050, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [635/1000]: training_loss: tensor(0.4135, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [636/1000]: training_loss: tensor(0.3144, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [637/1000]: training_loss: tensor(0.3025, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [638/1000]: training_loss: tensor(0.4183, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [639/1000]: training_loss: tensor(0.4496, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [640/1000]: training_loss: tensor(0.4130, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [641/1000]: training_loss: tensor(0.3321, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [642/1000]: training_loss: tensor(0.3612, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [643/1000]: training_loss: tensor(0.3780, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [644/1000]: training_loss: tensor(0.2732, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [645/1000]: training_loss: tensor(0.3761, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [646/1000]: training_loss: tensor(0.3877, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [647/1000]: training_loss: tensor(0.3676, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [648/1000]: training_loss: tensor(0.3709, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [649/1000]: training_loss: tensor(0.3481, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [650/1000]: training_loss: tensor(0.3408, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [651/1000]: training_loss: tensor(0.3939, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [652/1000]: training_loss: tensor(0.3367, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [653/1000]: training_loss: tensor(0.3294, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [654/1000]: training_loss: tensor(0.3947, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [655/1000]: training_loss: tensor(0.3714, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [656/1000]: training_loss: tensor(0.3453, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [657/1000]: training_loss: tensor(0.3617, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [658/1000]: training_loss: tensor(0.4733, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [659/1000]: training_loss: tensor(0.3690, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [660/1000]: training_loss: tensor(0.4945, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [661/1000]: training_loss: tensor(0.3692, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [662/1000]: training_loss: tensor(0.3828, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [663/1000]: training_loss: tensor(0.3130, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [664/1000]: training_loss: tensor(0.3479, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [665/1000]: training_loss: tensor(0.3625, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [666/1000]: training_loss: tensor(0.4147, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [667/1000]: training_loss: tensor(0.3933, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [668/1000]: training_loss: tensor(0.3949, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [669/1000]: training_loss: tensor(0.3493, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [670/1000]: training_loss: tensor(0.4147, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [671/1000]: training_loss: tensor(0.3640, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [672/1000]: training_loss: tensor(0.3418, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [673/1000]: training_loss: tensor(0.3126, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [674/1000]: training_loss: tensor(0.4108, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [675/1000]: training_loss: tensor(0.3318, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [676/1000]: training_loss: tensor(0.3399, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [677/1000]: training_loss: tensor(0.3295, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [678/1000]: training_loss: tensor(0.3287, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [679/1000]: training_loss: tensor(0.4421, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [680/1000]: training_loss: tensor(0.3409, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [681/1000]: training_loss: tensor(0.3853, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [682/1000]: training_loss: tensor(0.3402, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [683/1000]: training_loss: tensor(0.4334, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [684/1000]: training_loss: tensor(0.4066, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [685/1000]: training_loss: tensor(0.3718, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [686/1000]: training_loss: tensor(0.3614, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [687/1000]: training_loss: tensor(0.3681, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [688/1000]: training_loss: tensor(0.3881, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [689/1000]: training_loss: tensor(0.3735, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [690/1000]: training_loss: tensor(0.4830, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [691/1000]: training_loss: tensor(0.3393, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [692/1000]: training_loss: tensor(0.4862, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [693/1000]: training_loss: tensor(0.3157, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [694/1000]: training_loss: tensor(0.3951, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [695/1000]: training_loss: tensor(0.4838, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [696/1000]: training_loss: tensor(0.3768, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [697/1000]: training_loss: tensor(0.4130, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [698/1000]: training_loss: tensor(0.3981, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [699/1000]: training_loss: tensor(0.3816, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [700/1000]: training_loss: tensor(0.3821, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [701/1000]: training_loss: tensor(0.3814, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [702/1000]: training_loss: tensor(0.3431, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [703/1000]: training_loss: tensor(0.3794, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [704/1000]: training_loss: tensor(0.3344, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [705/1000]: training_loss: tensor(0.4860, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [706/1000]: training_loss: tensor(0.3835, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [707/1000]: training_loss: tensor(0.2483, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [708/1000]: training_loss: tensor(0.4866, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [709/1000]: training_loss: tensor(0.4297, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [710/1000]: training_loss: tensor(0.4041, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [711/1000]: training_loss: tensor(0.3905, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [712/1000]: training_loss: tensor(0.3518, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [713/1000]: training_loss: tensor(0.4178, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [714/1000]: training_loss: tensor(0.3284, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [715/1000]: training_loss: tensor(0.4549, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [716/1000]: training_loss: tensor(0.4530, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [717/1000]: training_loss: tensor(0.3315, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [718/1000]: training_loss: tensor(0.3179, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [719/1000]: training_loss: tensor(0.3440, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [720/1000]: training_loss: tensor(0.3545, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [721/1000]: training_loss: tensor(0.2944, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [722/1000]: training_loss: tensor(0.3383, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [723/1000]: training_loss: tensor(0.4007, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [724/1000]: training_loss: tensor(0.3617, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [725/1000]: training_loss: tensor(0.3789, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [726/1000]: training_loss: tensor(0.3406, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [727/1000]: training_loss: tensor(0.3254, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [728/1000]: training_loss: tensor(0.3768, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [729/1000]: training_loss: tensor(0.4177, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [730/1000]: training_loss: tensor(0.4556, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [731/1000]: training_loss: tensor(0.3228, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [732/1000]: training_loss: tensor(0.3116, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [733/1000]: training_loss: tensor(0.3572, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [734/1000]: training_loss: tensor(0.4274, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [735/1000]: training_loss: tensor(0.4293, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [736/1000]: training_loss: tensor(0.2964, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [737/1000]: training_loss: tensor(0.3614, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [738/1000]: training_loss: tensor(0.3921, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [739/1000]: training_loss: tensor(0.3748, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [740/1000]: training_loss: tensor(0.4151, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [741/1000]: training_loss: tensor(0.2870, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [742/1000]: training_loss: tensor(0.3458, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [743/1000]: training_loss: tensor(0.3513, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [744/1000]: training_loss: tensor(0.3256, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [745/1000]: training_loss: tensor(0.3054, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [746/1000]: training_loss: tensor(0.3647, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [747/1000]: training_loss: tensor(0.3676, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [748/1000]: training_loss: tensor(0.3608, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [749/1000]: training_loss: tensor(0.3107, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [750/1000]: training_loss: tensor(0.3915, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [751/1000]: training_loss: tensor(0.3682, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [752/1000]: training_loss: tensor(0.3569, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [753/1000]: training_loss: tensor(0.2761, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [754/1000]: training_loss: tensor(0.3415, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [755/1000]: training_loss: tensor(0.3138, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [756/1000]: training_loss: tensor(0.4401, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [757/1000]: training_loss: tensor(0.5793, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [758/1000]: training_loss: tensor(0.4396, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [759/1000]: training_loss: tensor(0.3938, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [760/1000]: training_loss: tensor(0.3528, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [761/1000]: training_loss: tensor(0.4120, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [762/1000]: training_loss: tensor(0.3311, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [763/1000]: training_loss: tensor(0.4090, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [764/1000]: training_loss: tensor(0.4566, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [765/1000]: training_loss: tensor(0.3878, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [766/1000]: training_loss: tensor(0.3754, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [767/1000]: training_loss: tensor(0.3915, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [768/1000]: training_loss: tensor(0.3340, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [769/1000]: training_loss: tensor(0.3494, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [770/1000]: training_loss: tensor(0.3709, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [771/1000]: training_loss: tensor(0.3315, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [772/1000]: training_loss: tensor(0.4579, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [773/1000]: training_loss: tensor(0.5131, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [774/1000]: training_loss: tensor(0.2875, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [775/1000]: training_loss: tensor(0.3193, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [776/1000]: training_loss: tensor(0.3246, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [777/1000]: training_loss: tensor(0.3690, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [778/1000]: training_loss: tensor(0.3947, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [779/1000]: training_loss: tensor(0.2754, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [780/1000]: training_loss: tensor(0.4118, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [781/1000]: training_loss: tensor(0.3259, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [782/1000]: training_loss: tensor(0.4208, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [783/1000]: training_loss: tensor(0.4196, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [784/1000]: training_loss: tensor(0.3881, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [785/1000]: training_loss: tensor(0.3160, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [786/1000]: training_loss: tensor(0.4131, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [787/1000]: training_loss: tensor(0.3312, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [788/1000]: training_loss: tensor(0.3812, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [789/1000]: training_loss: tensor(0.4202, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [790/1000]: training_loss: tensor(0.3847, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [791/1000]: training_loss: tensor(0.3121, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [792/1000]: training_loss: tensor(0.3930, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [793/1000]: training_loss: tensor(0.3341, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [794/1000]: training_loss: tensor(0.3086, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [795/1000]: training_loss: tensor(0.2931, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [796/1000]: training_loss: tensor(0.3465, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [797/1000]: training_loss: tensor(0.3835, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [798/1000]: training_loss: tensor(0.4067, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [799/1000]: training_loss: tensor(0.4176, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [800/1000]: training_loss: tensor(0.3920, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [801/1000]: training_loss: tensor(0.3530, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [802/1000]: training_loss: tensor(0.3667, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [803/1000]: training_loss: tensor(0.2895, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [804/1000]: training_loss: tensor(0.4002, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [805/1000]: training_loss: tensor(0.3422, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [806/1000]: training_loss: tensor(0.3292, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [807/1000]: training_loss: tensor(0.3357, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [808/1000]: training_loss: tensor(0.3357, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [809/1000]: training_loss: tensor(0.4433, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [810/1000]: training_loss: tensor(0.3701, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [811/1000]: training_loss: tensor(0.4127, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [812/1000]: training_loss: tensor(0.4035, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [813/1000]: training_loss: tensor(0.3999, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [814/1000]: training_loss: tensor(0.3998, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [815/1000]: training_loss: tensor(0.4420, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [816/1000]: training_loss: tensor(0.3523, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [817/1000]: training_loss: tensor(0.3952, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [818/1000]: training_loss: tensor(0.3153, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [819/1000]: training_loss: tensor(0.3541, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [820/1000]: training_loss: tensor(0.3458, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [821/1000]: training_loss: tensor(0.3815, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [822/1000]: training_loss: tensor(0.3313, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [823/1000]: training_loss: tensor(0.4259, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [824/1000]: training_loss: tensor(0.3866, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [825/1000]: training_loss: tensor(0.3616, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [826/1000]: training_loss: tensor(0.4393, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [827/1000]: training_loss: tensor(0.3486, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [828/1000]: training_loss: tensor(0.4369, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [829/1000]: training_loss: tensor(0.4136, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [830/1000]: training_loss: tensor(0.4095, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [831/1000]: training_loss: tensor(0.3502, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [832/1000]: training_loss: tensor(0.3630, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [833/1000]: training_loss: tensor(0.4082, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [834/1000]: training_loss: tensor(0.4216, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [835/1000]: training_loss: tensor(0.3797, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [836/1000]: training_loss: tensor(0.4620, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [837/1000]: training_loss: tensor(0.3665, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [838/1000]: training_loss: tensor(0.3729, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [839/1000]: training_loss: tensor(0.3669, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [840/1000]: training_loss: tensor(0.4281, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [841/1000]: training_loss: tensor(0.3429, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [842/1000]: training_loss: tensor(0.3180, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [843/1000]: training_loss: tensor(0.3533, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [844/1000]: training_loss: tensor(0.3192, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [845/1000]: training_loss: tensor(0.3093, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [846/1000]: training_loss: tensor(0.3336, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [847/1000]: training_loss: tensor(0.3519, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [848/1000]: training_loss: tensor(0.4585, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [849/1000]: training_loss: tensor(0.3887, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [850/1000]: training_loss: tensor(0.4157, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [851/1000]: training_loss: tensor(0.3175, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [852/1000]: training_loss: tensor(0.3678, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [853/1000]: training_loss: tensor(0.3697, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [854/1000]: training_loss: tensor(0.3380, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [855/1000]: training_loss: tensor(0.4775, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [856/1000]: training_loss: tensor(0.3738, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [857/1000]: training_loss: tensor(0.3970, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [858/1000]: training_loss: tensor(0.4050, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [859/1000]: training_loss: tensor(0.2967, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [860/1000]: training_loss: tensor(0.3672, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [861/1000]: training_loss: tensor(0.4589, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [862/1000]: training_loss: tensor(0.4255, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [863/1000]: training_loss: tensor(0.3567, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [864/1000]: training_loss: tensor(0.4126, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [865/1000]: training_loss: tensor(0.3789, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [866/1000]: training_loss: tensor(0.3431, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [867/1000]: training_loss: tensor(0.4070, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [868/1000]: training_loss: tensor(0.3467, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [869/1000]: training_loss: tensor(0.3856, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [870/1000]: training_loss: tensor(0.4292, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [871/1000]: training_loss: tensor(0.2539, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [872/1000]: training_loss: tensor(0.3716, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [873/1000]: training_loss: tensor(0.3499, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [874/1000]: training_loss: tensor(0.4065, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [875/1000]: training_loss: tensor(0.4765, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [876/1000]: training_loss: tensor(0.3543, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [877/1000]: training_loss: tensor(0.3654, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [878/1000]: training_loss: tensor(0.3469, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [879/1000]: training_loss: tensor(0.4298, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [880/1000]: training_loss: tensor(0.3568, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [881/1000]: training_loss: tensor(0.3421, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [882/1000]: training_loss: tensor(0.3276, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [883/1000]: training_loss: tensor(0.3712, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [884/1000]: training_loss: tensor(0.3572, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [885/1000]: training_loss: tensor(0.4216, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [886/1000]: training_loss: tensor(0.3984, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [887/1000]: training_loss: tensor(0.3587, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [888/1000]: training_loss: tensor(0.3473, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [889/1000]: training_loss: tensor(0.3711, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [890/1000]: training_loss: tensor(0.2845, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [891/1000]: training_loss: tensor(0.3967, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [892/1000]: training_loss: tensor(0.3012, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [893/1000]: training_loss: tensor(0.3294, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [894/1000]: training_loss: tensor(0.3908, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [895/1000]: training_loss: tensor(0.3650, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [896/1000]: training_loss: tensor(0.3556, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [897/1000]: training_loss: tensor(0.3380, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [898/1000]: training_loss: tensor(0.3353, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [899/1000]: training_loss: tensor(0.4420, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [900/1000]: training_loss: tensor(0.3900, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [901/1000]: training_loss: tensor(0.3517, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [902/1000]: training_loss: tensor(0.3536, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [903/1000]: training_loss: tensor(0.3596, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [904/1000]: training_loss: tensor(0.4098, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [905/1000]: training_loss: tensor(0.3989, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [906/1000]: training_loss: tensor(0.3508, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [907/1000]: training_loss: tensor(0.3587, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [908/1000]: training_loss: tensor(0.3600, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [909/1000]: training_loss: tensor(0.4088, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [910/1000]: training_loss: tensor(0.3180, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [911/1000]: training_loss: tensor(0.3468, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [912/1000]: training_loss: tensor(0.4099, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [913/1000]: training_loss: tensor(0.3641, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [914/1000]: training_loss: tensor(0.3776, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [915/1000]: training_loss: tensor(0.2783, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [916/1000]: training_loss: tensor(0.3557, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [917/1000]: training_loss: tensor(0.4197, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [918/1000]: training_loss: tensor(0.3223, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [919/1000]: training_loss: tensor(0.4070, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [920/1000]: training_loss: tensor(0.4250, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [921/1000]: training_loss: tensor(0.4355, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [922/1000]: training_loss: tensor(0.5119, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [923/1000]: training_loss: tensor(0.3809, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [924/1000]: training_loss: tensor(0.3873, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [925/1000]: training_loss: tensor(0.3715, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [926/1000]: training_loss: tensor(0.3118, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [927/1000]: training_loss: tensor(0.4066, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [928/1000]: training_loss: tensor(0.4256, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [929/1000]: training_loss: tensor(0.3332, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [930/1000]: training_loss: tensor(0.2980, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [931/1000]: training_loss: tensor(0.4358, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [932/1000]: training_loss: tensor(0.2971, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [933/1000]: training_loss: tensor(0.3861, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [934/1000]: training_loss: tensor(0.3460, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [935/1000]: training_loss: tensor(0.4107, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [936/1000]: training_loss: tensor(0.3459, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [937/1000]: training_loss: tensor(0.3976, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [938/1000]: training_loss: tensor(0.4087, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [939/1000]: training_loss: tensor(0.4356, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [940/1000]: training_loss: tensor(0.3741, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [941/1000]: training_loss: tensor(0.3229, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [942/1000]: training_loss: tensor(0.3600, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [943/1000]: training_loss: tensor(0.5426, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [944/1000]: training_loss: tensor(0.3104, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [945/1000]: training_loss: tensor(0.3870, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [946/1000]: training_loss: tensor(0.3888, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [947/1000]: training_loss: tensor(0.4775, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [948/1000]: training_loss: tensor(0.4769, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [949/1000]: training_loss: tensor(0.3295, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [950/1000]: training_loss: tensor(0.3255, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [951/1000]: training_loss: tensor(0.3898, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [952/1000]: training_loss: tensor(0.3545, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [953/1000]: training_loss: tensor(0.3895, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [954/1000]: training_loss: tensor(0.3834, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [955/1000]: training_loss: tensor(0.3522, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [956/1000]: training_loss: tensor(0.3537, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [957/1000]: training_loss: tensor(0.3377, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [958/1000]: training_loss: tensor(0.3119, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [959/1000]: training_loss: tensor(0.4017, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [960/1000]: training_loss: tensor(0.4712, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [961/1000]: training_loss: tensor(0.4078, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [962/1000]: training_loss: tensor(0.3842, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [963/1000]: training_loss: tensor(0.3487, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [964/1000]: training_loss: tensor(0.2663, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [965/1000]: training_loss: tensor(0.4200, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [966/1000]: training_loss: tensor(0.2734, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [967/1000]: training_loss: tensor(0.2783, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [968/1000]: training_loss: tensor(0.3196, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [969/1000]: training_loss: tensor(0.4896, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [970/1000]: training_loss: tensor(0.3849, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [971/1000]: training_loss: tensor(0.3420, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [972/1000]: training_loss: tensor(0.4103, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [973/1000]: training_loss: tensor(0.3275, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [974/1000]: training_loss: tensor(0.4189, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [975/1000]: training_loss: tensor(0.3507, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [976/1000]: training_loss: tensor(0.3362, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [977/1000]: training_loss: tensor(0.2895, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [978/1000]: training_loss: tensor(0.3577, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [979/1000]: training_loss: tensor(0.3283, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [980/1000]: training_loss: tensor(0.4645, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [981/1000]: training_loss: tensor(0.2906, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [982/1000]: training_loss: tensor(0.3503, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [983/1000]: training_loss: tensor(0.3253, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [984/1000]: training_loss: tensor(0.3796, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [985/1000]: training_loss: tensor(0.3004, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [986/1000]: training_loss: tensor(0.3469, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [987/1000]: training_loss: tensor(0.3924, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [988/1000]: training_loss: tensor(0.3733, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [989/1000]: training_loss: tensor(0.4101, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [990/1000]: training_loss: tensor(0.3683, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [991/1000]: training_loss: tensor(0.3554, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [992/1000]: training_loss: tensor(0.4317, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [993/1000]: training_loss: tensor(0.3863, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [994/1000]: training_loss: tensor(0.3879, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [995/1000]: training_loss: tensor(0.3433, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [996/1000]: training_loss: tensor(0.4268, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [997/1000]: training_loss: tensor(0.4067, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [998/1000]: training_loss: tensor(0.3661, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [999/1000]: training_loss: tensor(0.4283, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Epoch [3/5], global step [3000/5000], Train Loss: 0.3798, Valid Loss: 0.3662\n",
      "batch_no [0/1000]: training_loss: tensor(0.2953, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [1/1000]: training_loss: tensor(0.5183, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [2/1000]: training_loss: tensor(0.3442, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [3/1000]: training_loss: tensor(0.3432, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [4/1000]: training_loss: tensor(0.4469, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [5/1000]: training_loss: tensor(0.3915, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [6/1000]: training_loss: tensor(0.3814, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [7/1000]: training_loss: tensor(0.3227, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [8/1000]: training_loss: tensor(0.2821, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [9/1000]: training_loss: tensor(0.3837, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [10/1000]: training_loss: tensor(0.2864, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [11/1000]: training_loss: tensor(0.4454, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [12/1000]: training_loss: tensor(0.3633, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [13/1000]: training_loss: tensor(0.3001, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [14/1000]: training_loss: tensor(0.4050, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [15/1000]: training_loss: tensor(0.3273, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [16/1000]: training_loss: tensor(0.3824, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [17/1000]: training_loss: tensor(0.3108, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [18/1000]: training_loss: tensor(0.2884, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [19/1000]: training_loss: tensor(0.3156, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [20/1000]: training_loss: tensor(0.3166, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [21/1000]: training_loss: tensor(0.4158, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [22/1000]: training_loss: tensor(0.4049, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [23/1000]: training_loss: tensor(0.4033, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [24/1000]: training_loss: tensor(0.3297, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [25/1000]: training_loss: tensor(0.3649, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [26/1000]: training_loss: tensor(0.3437, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [27/1000]: training_loss: tensor(0.3277, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [28/1000]: training_loss: tensor(0.4211, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [29/1000]: training_loss: tensor(0.4840, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [30/1000]: training_loss: tensor(0.3752, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [31/1000]: training_loss: tensor(0.4232, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [32/1000]: training_loss: tensor(0.3418, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [33/1000]: training_loss: tensor(0.4183, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [34/1000]: training_loss: tensor(0.3449, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [35/1000]: training_loss: tensor(0.2917, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [36/1000]: training_loss: tensor(0.3882, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [37/1000]: training_loss: tensor(0.3027, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [38/1000]: training_loss: tensor(0.3903, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [39/1000]: training_loss: tensor(0.3430, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [40/1000]: training_loss: tensor(0.3679, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [41/1000]: training_loss: tensor(0.3520, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [42/1000]: training_loss: tensor(0.2838, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [43/1000]: training_loss: tensor(0.3863, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [44/1000]: training_loss: tensor(0.3467, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [45/1000]: training_loss: tensor(0.2946, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [46/1000]: training_loss: tensor(0.3019, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [47/1000]: training_loss: tensor(0.4292, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [48/1000]: training_loss: tensor(0.3285, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [49/1000]: training_loss: tensor(0.3202, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [50/1000]: training_loss: tensor(0.4840, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [51/1000]: training_loss: tensor(0.4496, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [52/1000]: training_loss: tensor(0.3520, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [53/1000]: training_loss: tensor(0.3273, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [54/1000]: training_loss: tensor(0.3512, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [55/1000]: training_loss: tensor(0.4408, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [56/1000]: training_loss: tensor(0.4109, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [57/1000]: training_loss: tensor(0.3664, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [58/1000]: training_loss: tensor(0.3176, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [59/1000]: training_loss: tensor(0.2903, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [60/1000]: training_loss: tensor(0.4413, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [61/1000]: training_loss: tensor(0.3381, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [62/1000]: training_loss: tensor(0.3582, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [63/1000]: training_loss: tensor(0.3670, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [64/1000]: training_loss: tensor(0.4285, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [65/1000]: training_loss: tensor(0.3522, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [66/1000]: training_loss: tensor(0.4189, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [67/1000]: training_loss: tensor(0.3261, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [68/1000]: training_loss: tensor(0.3542, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [69/1000]: training_loss: tensor(0.3936, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [70/1000]: training_loss: tensor(0.4121, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [71/1000]: training_loss: tensor(0.3827, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [72/1000]: training_loss: tensor(0.2836, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [73/1000]: training_loss: tensor(0.3931, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [74/1000]: training_loss: tensor(0.3975, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [75/1000]: training_loss: tensor(0.3932, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [76/1000]: training_loss: tensor(0.3023, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [77/1000]: training_loss: tensor(0.2463, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [78/1000]: training_loss: tensor(0.3810, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [79/1000]: training_loss: tensor(0.4727, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [80/1000]: training_loss: tensor(0.3136, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [81/1000]: training_loss: tensor(0.3666, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [82/1000]: training_loss: tensor(0.2902, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [83/1000]: training_loss: tensor(0.3719, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [84/1000]: training_loss: tensor(0.2941, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [85/1000]: training_loss: tensor(0.3387, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [86/1000]: training_loss: tensor(0.3131, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [87/1000]: training_loss: tensor(0.3625, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [88/1000]: training_loss: tensor(0.3321, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [89/1000]: training_loss: tensor(0.4384, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [90/1000]: training_loss: tensor(0.3528, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [91/1000]: training_loss: tensor(0.4363, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [92/1000]: training_loss: tensor(0.3156, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [93/1000]: training_loss: tensor(0.4460, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [94/1000]: training_loss: tensor(0.3569, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [95/1000]: training_loss: tensor(0.3765, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [96/1000]: training_loss: tensor(0.3480, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [97/1000]: training_loss: tensor(0.3422, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [98/1000]: training_loss: tensor(0.2870, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [99/1000]: training_loss: tensor(0.3989, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [100/1000]: training_loss: tensor(0.3864, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [101/1000]: training_loss: tensor(0.3830, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [102/1000]: training_loss: tensor(0.3783, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [103/1000]: training_loss: tensor(0.3730, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [104/1000]: training_loss: tensor(0.3800, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [105/1000]: training_loss: tensor(0.3863, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [106/1000]: training_loss: tensor(0.2575, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [107/1000]: training_loss: tensor(0.2919, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [108/1000]: training_loss: tensor(0.4220, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [109/1000]: training_loss: tensor(0.3080, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [110/1000]: training_loss: tensor(0.3932, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [111/1000]: training_loss: tensor(0.4507, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [112/1000]: training_loss: tensor(0.3003, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [113/1000]: training_loss: tensor(0.3306, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [114/1000]: training_loss: tensor(0.2768, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [115/1000]: training_loss: tensor(0.3210, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [116/1000]: training_loss: tensor(0.3175, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [117/1000]: training_loss: tensor(0.3185, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [118/1000]: training_loss: tensor(0.4399, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [119/1000]: training_loss: tensor(0.3093, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [120/1000]: training_loss: tensor(0.3526, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [121/1000]: training_loss: tensor(0.3260, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [122/1000]: training_loss: tensor(0.2654, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [123/1000]: training_loss: tensor(0.3923, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [124/1000]: training_loss: tensor(0.5111, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [125/1000]: training_loss: tensor(0.3342, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [126/1000]: training_loss: tensor(0.2930, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [127/1000]: training_loss: tensor(0.3385, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [128/1000]: training_loss: tensor(0.3676, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [129/1000]: training_loss: tensor(0.4760, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [130/1000]: training_loss: tensor(0.2892, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [131/1000]: training_loss: tensor(0.4196, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [132/1000]: training_loss: tensor(0.2994, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [133/1000]: training_loss: tensor(0.3481, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [134/1000]: training_loss: tensor(0.3030, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [135/1000]: training_loss: tensor(0.3450, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [136/1000]: training_loss: tensor(0.3048, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [137/1000]: training_loss: tensor(0.3969, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [138/1000]: training_loss: tensor(0.3698, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [139/1000]: training_loss: tensor(0.2831, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [140/1000]: training_loss: tensor(0.4078, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [141/1000]: training_loss: tensor(0.4001, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [142/1000]: training_loss: tensor(0.3867, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [143/1000]: training_loss: tensor(0.3740, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [144/1000]: training_loss: tensor(0.3507, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [145/1000]: training_loss: tensor(0.3079, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [146/1000]: training_loss: tensor(0.3597, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [147/1000]: training_loss: tensor(0.4568, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [148/1000]: training_loss: tensor(0.3569, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [149/1000]: training_loss: tensor(0.3197, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [150/1000]: training_loss: tensor(0.3165, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [151/1000]: training_loss: tensor(0.3456, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [152/1000]: training_loss: tensor(0.3665, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [153/1000]: training_loss: tensor(0.3091, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [154/1000]: training_loss: tensor(0.3286, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [155/1000]: training_loss: tensor(0.3371, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [156/1000]: training_loss: tensor(0.3247, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [157/1000]: training_loss: tensor(0.3620, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [158/1000]: training_loss: tensor(0.4440, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [159/1000]: training_loss: tensor(0.4058, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [160/1000]: training_loss: tensor(0.3290, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [161/1000]: training_loss: tensor(0.3106, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [162/1000]: training_loss: tensor(0.3351, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [163/1000]: training_loss: tensor(0.3911, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [164/1000]: training_loss: tensor(0.4575, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [165/1000]: training_loss: tensor(0.4963, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [166/1000]: training_loss: tensor(0.3233, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [167/1000]: training_loss: tensor(0.3926, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [168/1000]: training_loss: tensor(0.3078, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [169/1000]: training_loss: tensor(0.3193, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [170/1000]: training_loss: tensor(0.3992, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [171/1000]: training_loss: tensor(0.3816, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [172/1000]: training_loss: tensor(0.3110, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [173/1000]: training_loss: tensor(0.4409, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [174/1000]: training_loss: tensor(0.2847, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [175/1000]: training_loss: tensor(0.3903, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [176/1000]: training_loss: tensor(0.2776, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [177/1000]: training_loss: tensor(0.5092, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [178/1000]: training_loss: tensor(0.3614, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [179/1000]: training_loss: tensor(0.3705, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [180/1000]: training_loss: tensor(0.2650, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [181/1000]: training_loss: tensor(0.3323, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [182/1000]: training_loss: tensor(0.3548, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [183/1000]: training_loss: tensor(0.5478, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [184/1000]: training_loss: tensor(0.3770, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [185/1000]: training_loss: tensor(0.3559, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [186/1000]: training_loss: tensor(0.4239, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [187/1000]: training_loss: tensor(0.3224, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [188/1000]: training_loss: tensor(0.4293, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [189/1000]: training_loss: tensor(0.3077, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [190/1000]: training_loss: tensor(0.4952, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [191/1000]: training_loss: tensor(0.4995, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [192/1000]: training_loss: tensor(0.4393, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [193/1000]: training_loss: tensor(0.2874, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [194/1000]: training_loss: tensor(0.3904, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [195/1000]: training_loss: tensor(0.3196, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [196/1000]: training_loss: tensor(0.3899, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [197/1000]: training_loss: tensor(0.3319, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [198/1000]: training_loss: tensor(0.3591, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [199/1000]: training_loss: tensor(0.3435, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [200/1000]: training_loss: tensor(0.3345, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [201/1000]: training_loss: tensor(0.4626, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [202/1000]: training_loss: tensor(0.4733, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [203/1000]: training_loss: tensor(0.4168, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [204/1000]: training_loss: tensor(0.4284, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [205/1000]: training_loss: tensor(0.3747, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [206/1000]: training_loss: tensor(0.3436, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [207/1000]: training_loss: tensor(0.2661, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [208/1000]: training_loss: tensor(0.3704, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [209/1000]: training_loss: tensor(0.3517, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [210/1000]: training_loss: tensor(0.3607, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [211/1000]: training_loss: tensor(0.3847, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [212/1000]: training_loss: tensor(0.3230, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [213/1000]: training_loss: tensor(0.3289, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [214/1000]: training_loss: tensor(0.3573, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [215/1000]: training_loss: tensor(0.3111, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [216/1000]: training_loss: tensor(0.3128, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [217/1000]: training_loss: tensor(0.4721, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [218/1000]: training_loss: tensor(0.3324, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [219/1000]: training_loss: tensor(0.3432, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [220/1000]: training_loss: tensor(0.4336, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [221/1000]: training_loss: tensor(0.3649, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [222/1000]: training_loss: tensor(0.4109, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [223/1000]: training_loss: tensor(0.3797, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [224/1000]: training_loss: tensor(0.3144, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [225/1000]: training_loss: tensor(0.4069, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [226/1000]: training_loss: tensor(0.3662, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [227/1000]: training_loss: tensor(0.3623, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [228/1000]: training_loss: tensor(0.3346, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [229/1000]: training_loss: tensor(0.3453, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [230/1000]: training_loss: tensor(0.3848, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [231/1000]: training_loss: tensor(0.3039, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [232/1000]: training_loss: tensor(0.3400, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [233/1000]: training_loss: tensor(0.4407, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [234/1000]: training_loss: tensor(0.3364, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [235/1000]: training_loss: tensor(0.3953, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [236/1000]: training_loss: tensor(0.4285, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [237/1000]: training_loss: tensor(0.4048, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [238/1000]: training_loss: tensor(0.4383, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [239/1000]: training_loss: tensor(0.2880, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [240/1000]: training_loss: tensor(0.3786, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [241/1000]: training_loss: tensor(0.3801, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [242/1000]: training_loss: tensor(0.3214, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [243/1000]: training_loss: tensor(0.3496, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [244/1000]: training_loss: tensor(0.3415, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [245/1000]: training_loss: tensor(0.3915, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [246/1000]: training_loss: tensor(0.4202, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [247/1000]: training_loss: tensor(0.3092, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [248/1000]: training_loss: tensor(0.2763, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [249/1000]: training_loss: tensor(0.3947, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [250/1000]: training_loss: tensor(0.3586, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [251/1000]: training_loss: tensor(0.3555, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [252/1000]: training_loss: tensor(0.3401, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [253/1000]: training_loss: tensor(0.3671, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [254/1000]: training_loss: tensor(0.2821, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [255/1000]: training_loss: tensor(0.3084, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [256/1000]: training_loss: tensor(0.3280, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [257/1000]: training_loss: tensor(0.3352, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [258/1000]: training_loss: tensor(0.4050, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [259/1000]: training_loss: tensor(0.2828, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [260/1000]: training_loss: tensor(0.4082, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [261/1000]: training_loss: tensor(0.3734, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [262/1000]: training_loss: tensor(0.3890, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [263/1000]: training_loss: tensor(0.5162, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [264/1000]: training_loss: tensor(0.3955, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [265/1000]: training_loss: tensor(0.3302, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [266/1000]: training_loss: tensor(0.3482, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [267/1000]: training_loss: tensor(0.5048, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [268/1000]: training_loss: tensor(0.3234, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [269/1000]: training_loss: tensor(0.2876, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [270/1000]: training_loss: tensor(0.4387, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [271/1000]: training_loss: tensor(0.4340, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [272/1000]: training_loss: tensor(0.4352, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [273/1000]: training_loss: tensor(0.4014, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [274/1000]: training_loss: tensor(0.3015, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [275/1000]: training_loss: tensor(0.3099, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [276/1000]: training_loss: tensor(0.3213, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [277/1000]: training_loss: tensor(0.4663, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [278/1000]: training_loss: tensor(0.3552, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [279/1000]: training_loss: tensor(0.3521, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [280/1000]: training_loss: tensor(0.3309, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [281/1000]: training_loss: tensor(0.3712, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [282/1000]: training_loss: tensor(0.2582, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [283/1000]: training_loss: tensor(0.3410, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [284/1000]: training_loss: tensor(0.3660, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [285/1000]: training_loss: tensor(0.3804, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [286/1000]: training_loss: tensor(0.2975, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [287/1000]: training_loss: tensor(0.3548, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [288/1000]: training_loss: tensor(0.3176, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [289/1000]: training_loss: tensor(0.3397, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [290/1000]: training_loss: tensor(0.4611, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [291/1000]: training_loss: tensor(0.3252, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [292/1000]: training_loss: tensor(0.2914, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [293/1000]: training_loss: tensor(0.3494, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [294/1000]: training_loss: tensor(0.4101, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [295/1000]: training_loss: tensor(0.4004, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [296/1000]: training_loss: tensor(0.4001, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [297/1000]: training_loss: tensor(0.3176, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [298/1000]: training_loss: tensor(0.4219, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [299/1000]: training_loss: tensor(0.3907, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [300/1000]: training_loss: tensor(0.3338, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [301/1000]: training_loss: tensor(0.3500, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [302/1000]: training_loss: tensor(0.3570, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [303/1000]: training_loss: tensor(0.5309, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [304/1000]: training_loss: tensor(0.3604, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [305/1000]: training_loss: tensor(0.3358, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [306/1000]: training_loss: tensor(0.3933, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [307/1000]: training_loss: tensor(0.4899, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [308/1000]: training_loss: tensor(0.3658, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [309/1000]: training_loss: tensor(0.3761, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [310/1000]: training_loss: tensor(0.3961, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [311/1000]: training_loss: tensor(0.4673, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [312/1000]: training_loss: tensor(0.3100, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [313/1000]: training_loss: tensor(0.2753, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [314/1000]: training_loss: tensor(0.4154, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [315/1000]: training_loss: tensor(0.3878, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [316/1000]: training_loss: tensor(0.4531, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [317/1000]: training_loss: tensor(0.3983, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [318/1000]: training_loss: tensor(0.3210, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [319/1000]: training_loss: tensor(0.3407, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [320/1000]: training_loss: tensor(0.3292, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [321/1000]: training_loss: tensor(0.4118, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [322/1000]: training_loss: tensor(0.3293, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [323/1000]: training_loss: tensor(0.3311, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [324/1000]: training_loss: tensor(0.3164, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [325/1000]: training_loss: tensor(0.3505, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [326/1000]: training_loss: tensor(0.3714, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [327/1000]: training_loss: tensor(0.3147, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [328/1000]: training_loss: tensor(0.3340, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [329/1000]: training_loss: tensor(0.3092, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [330/1000]: training_loss: tensor(0.4227, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [331/1000]: training_loss: tensor(0.2979, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [332/1000]: training_loss: tensor(0.3565, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [333/1000]: training_loss: tensor(0.3833, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [334/1000]: training_loss: tensor(0.3658, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [335/1000]: training_loss: tensor(0.3486, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [336/1000]: training_loss: tensor(0.3370, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [337/1000]: training_loss: tensor(0.3118, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [338/1000]: training_loss: tensor(0.2673, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [339/1000]: training_loss: tensor(0.4142, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [340/1000]: training_loss: tensor(0.3336, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [341/1000]: training_loss: tensor(0.4393, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [342/1000]: training_loss: tensor(0.3353, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [343/1000]: training_loss: tensor(0.3082, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [344/1000]: training_loss: tensor(0.4436, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [345/1000]: training_loss: tensor(0.4540, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [346/1000]: training_loss: tensor(0.3375, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [347/1000]: training_loss: tensor(0.2954, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [348/1000]: training_loss: tensor(0.3483, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [349/1000]: training_loss: tensor(0.3268, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [350/1000]: training_loss: tensor(0.3411, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [351/1000]: training_loss: tensor(0.4315, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [352/1000]: training_loss: tensor(0.3752, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [353/1000]: training_loss: tensor(0.3535, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [354/1000]: training_loss: tensor(0.4243, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [355/1000]: training_loss: tensor(0.3354, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [356/1000]: training_loss: tensor(0.3172, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [357/1000]: training_loss: tensor(0.3601, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [358/1000]: training_loss: tensor(0.2840, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [359/1000]: training_loss: tensor(0.3871, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [360/1000]: training_loss: tensor(0.4386, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [361/1000]: training_loss: tensor(0.3984, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [362/1000]: training_loss: tensor(0.4332, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [363/1000]: training_loss: tensor(0.3957, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [364/1000]: training_loss: tensor(0.3685, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [365/1000]: training_loss: tensor(0.4054, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [366/1000]: training_loss: tensor(0.3378, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [367/1000]: training_loss: tensor(0.2998, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [368/1000]: training_loss: tensor(0.3845, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [369/1000]: training_loss: tensor(0.3216, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [370/1000]: training_loss: tensor(0.3628, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [371/1000]: training_loss: tensor(0.3216, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [372/1000]: training_loss: tensor(0.4210, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [373/1000]: training_loss: tensor(0.3507, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [374/1000]: training_loss: tensor(0.3832, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [375/1000]: training_loss: tensor(0.3208, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [376/1000]: training_loss: tensor(0.3439, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [377/1000]: training_loss: tensor(0.4615, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [378/1000]: training_loss: tensor(0.2829, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [379/1000]: training_loss: tensor(0.3738, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [380/1000]: training_loss: tensor(0.3925, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [381/1000]: training_loss: tensor(0.3092, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [382/1000]: training_loss: tensor(0.3210, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [383/1000]: training_loss: tensor(0.4305, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [384/1000]: training_loss: tensor(0.3732, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [385/1000]: training_loss: tensor(0.5072, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [386/1000]: training_loss: tensor(0.3573, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [387/1000]: training_loss: tensor(0.3693, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [388/1000]: training_loss: tensor(0.2996, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [389/1000]: training_loss: tensor(0.3529, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [390/1000]: training_loss: tensor(0.3515, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [391/1000]: training_loss: tensor(0.3574, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [392/1000]: training_loss: tensor(0.3970, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [393/1000]: training_loss: tensor(0.3109, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [394/1000]: training_loss: tensor(0.3602, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [395/1000]: training_loss: tensor(0.4234, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [396/1000]: training_loss: tensor(0.4567, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [397/1000]: training_loss: tensor(0.3390, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [398/1000]: training_loss: tensor(0.3902, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [399/1000]: training_loss: tensor(0.2774, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [400/1000]: training_loss: tensor(0.3177, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [401/1000]: training_loss: tensor(0.3839, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [402/1000]: training_loss: tensor(0.4321, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [403/1000]: training_loss: tensor(0.4005, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [404/1000]: training_loss: tensor(0.3541, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [405/1000]: training_loss: tensor(0.3574, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [406/1000]: training_loss: tensor(0.3769, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [407/1000]: training_loss: tensor(0.3538, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [408/1000]: training_loss: tensor(0.4471, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [409/1000]: training_loss: tensor(0.3583, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [410/1000]: training_loss: tensor(0.3454, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [411/1000]: training_loss: tensor(0.3669, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [412/1000]: training_loss: tensor(0.4804, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [413/1000]: training_loss: tensor(0.4858, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [414/1000]: training_loss: tensor(0.3038, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [415/1000]: training_loss: tensor(0.3516, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [416/1000]: training_loss: tensor(0.3710, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [417/1000]: training_loss: tensor(0.3626, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [418/1000]: training_loss: tensor(0.4018, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [419/1000]: training_loss: tensor(0.3728, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [420/1000]: training_loss: tensor(0.3196, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [421/1000]: training_loss: tensor(0.4017, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [422/1000]: training_loss: tensor(0.3851, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [423/1000]: training_loss: tensor(0.4320, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [424/1000]: training_loss: tensor(0.4152, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [425/1000]: training_loss: tensor(0.4309, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [426/1000]: training_loss: tensor(0.3040, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [427/1000]: training_loss: tensor(0.4487, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [428/1000]: training_loss: tensor(0.3437, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [429/1000]: training_loss: tensor(0.4396, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [430/1000]: training_loss: tensor(0.3296, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [431/1000]: training_loss: tensor(0.3492, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [432/1000]: training_loss: tensor(0.3727, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [433/1000]: training_loss: tensor(0.3723, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [434/1000]: training_loss: tensor(0.3650, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [435/1000]: training_loss: tensor(0.3955, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [436/1000]: training_loss: tensor(0.4024, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [437/1000]: training_loss: tensor(0.3425, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [438/1000]: training_loss: tensor(0.4015, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [439/1000]: training_loss: tensor(0.3423, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [440/1000]: training_loss: tensor(0.4311, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [441/1000]: training_loss: tensor(0.3207, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [442/1000]: training_loss: tensor(0.3129, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [443/1000]: training_loss: tensor(0.2726, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [444/1000]: training_loss: tensor(0.3328, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [445/1000]: training_loss: tensor(0.3356, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [446/1000]: training_loss: tensor(0.2980, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [447/1000]: training_loss: tensor(0.4253, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [448/1000]: training_loss: tensor(0.3942, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [449/1000]: training_loss: tensor(0.3135, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [450/1000]: training_loss: tensor(0.3974, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [451/1000]: training_loss: tensor(0.3464, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [452/1000]: training_loss: tensor(0.3818, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [453/1000]: training_loss: tensor(0.3480, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [454/1000]: training_loss: tensor(0.3662, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [455/1000]: training_loss: tensor(0.3064, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [456/1000]: training_loss: tensor(0.4546, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [457/1000]: training_loss: tensor(0.3323, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [458/1000]: training_loss: tensor(0.3755, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [459/1000]: training_loss: tensor(0.3234, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [460/1000]: training_loss: tensor(0.3787, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [461/1000]: training_loss: tensor(0.4365, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [462/1000]: training_loss: tensor(0.4497, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [463/1000]: training_loss: tensor(0.3580, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [464/1000]: training_loss: tensor(0.3566, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [465/1000]: training_loss: tensor(0.3113, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [466/1000]: training_loss: tensor(0.3673, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [467/1000]: training_loss: tensor(0.3903, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [468/1000]: training_loss: tensor(0.4757, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [469/1000]: training_loss: tensor(0.3059, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [470/1000]: training_loss: tensor(0.3855, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [471/1000]: training_loss: tensor(0.3686, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [472/1000]: training_loss: tensor(0.3482, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [473/1000]: training_loss: tensor(0.3155, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [474/1000]: training_loss: tensor(0.3736, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [475/1000]: training_loss: tensor(0.3226, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [476/1000]: training_loss: tensor(0.3408, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [477/1000]: training_loss: tensor(0.3477, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [478/1000]: training_loss: tensor(0.3194, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [479/1000]: training_loss: tensor(0.4382, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [480/1000]: training_loss: tensor(0.3442, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [481/1000]: training_loss: tensor(0.3927, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [482/1000]: training_loss: tensor(0.4037, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [483/1000]: training_loss: tensor(0.3249, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [484/1000]: training_loss: tensor(0.3773, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [485/1000]: training_loss: tensor(0.4447, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [486/1000]: training_loss: tensor(0.3725, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [487/1000]: training_loss: tensor(0.4323, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [488/1000]: training_loss: tensor(0.3278, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [489/1000]: training_loss: tensor(0.3851, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [490/1000]: training_loss: tensor(0.3192, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [491/1000]: training_loss: tensor(0.4605, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [492/1000]: training_loss: tensor(0.3620, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [493/1000]: training_loss: tensor(0.3309, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [494/1000]: training_loss: tensor(0.3036, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [495/1000]: training_loss: tensor(0.4466, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [496/1000]: training_loss: tensor(0.3636, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [497/1000]: training_loss: tensor(0.3741, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [498/1000]: training_loss: tensor(0.2861, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [499/1000]: training_loss: tensor(0.3926, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [500/1000]: training_loss: tensor(0.2695, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [501/1000]: training_loss: tensor(0.3861, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [502/1000]: training_loss: tensor(0.2858, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [503/1000]: training_loss: tensor(0.2702, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [504/1000]: training_loss: tensor(0.2858, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [505/1000]: training_loss: tensor(0.2951, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [506/1000]: training_loss: tensor(0.4287, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [507/1000]: training_loss: tensor(0.2849, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [508/1000]: training_loss: tensor(0.3407, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [509/1000]: training_loss: tensor(0.3149, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [510/1000]: training_loss: tensor(0.3394, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [511/1000]: training_loss: tensor(0.3696, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [512/1000]: training_loss: tensor(0.3856, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [513/1000]: training_loss: tensor(0.3864, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [514/1000]: training_loss: tensor(0.3733, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [515/1000]: training_loss: tensor(0.3418, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [516/1000]: training_loss: tensor(0.3541, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [517/1000]: training_loss: tensor(0.5121, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [518/1000]: training_loss: tensor(0.3822, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [519/1000]: training_loss: tensor(0.3011, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [520/1000]: training_loss: tensor(0.3536, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [521/1000]: training_loss: tensor(0.3912, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [522/1000]: training_loss: tensor(0.3312, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [523/1000]: training_loss: tensor(0.3746, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [524/1000]: training_loss: tensor(0.2912, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [525/1000]: training_loss: tensor(0.3479, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [526/1000]: training_loss: tensor(0.5169, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [527/1000]: training_loss: tensor(0.4180, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [528/1000]: training_loss: tensor(0.3808, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [529/1000]: training_loss: tensor(0.2950, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [530/1000]: training_loss: tensor(0.3291, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [531/1000]: training_loss: tensor(0.3644, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [532/1000]: training_loss: tensor(0.3418, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [533/1000]: training_loss: tensor(0.3174, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [534/1000]: training_loss: tensor(0.3362, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [535/1000]: training_loss: tensor(0.2798, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [536/1000]: training_loss: tensor(0.3595, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [537/1000]: training_loss: tensor(0.4216, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [538/1000]: training_loss: tensor(0.3606, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [539/1000]: training_loss: tensor(0.4054, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [540/1000]: training_loss: tensor(0.3148, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [541/1000]: training_loss: tensor(0.3183, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [542/1000]: training_loss: tensor(0.3134, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [543/1000]: training_loss: tensor(0.5322, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [544/1000]: training_loss: tensor(0.3280, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [545/1000]: training_loss: tensor(0.2909, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [546/1000]: training_loss: tensor(0.3174, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [547/1000]: training_loss: tensor(0.4008, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [548/1000]: training_loss: tensor(0.2680, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [549/1000]: training_loss: tensor(0.3893, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [550/1000]: training_loss: tensor(0.3348, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [551/1000]: training_loss: tensor(0.3757, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [552/1000]: training_loss: tensor(0.4217, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [553/1000]: training_loss: tensor(0.3578, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [554/1000]: training_loss: tensor(0.4716, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [555/1000]: training_loss: tensor(0.3802, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [556/1000]: training_loss: tensor(0.3984, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [557/1000]: training_loss: tensor(0.3004, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [558/1000]: training_loss: tensor(0.2704, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [559/1000]: training_loss: tensor(0.4451, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [560/1000]: training_loss: tensor(0.4118, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [561/1000]: training_loss: tensor(0.2913, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [562/1000]: training_loss: tensor(0.3647, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [563/1000]: training_loss: tensor(0.2470, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [564/1000]: training_loss: tensor(0.2925, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [565/1000]: training_loss: tensor(0.2937, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [566/1000]: training_loss: tensor(0.3215, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [567/1000]: training_loss: tensor(0.3114, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [568/1000]: training_loss: tensor(0.3798, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [569/1000]: training_loss: tensor(0.3485, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [570/1000]: training_loss: tensor(0.3781, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [571/1000]: training_loss: tensor(0.3386, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [572/1000]: training_loss: tensor(0.3119, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [573/1000]: training_loss: tensor(0.3761, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [574/1000]: training_loss: tensor(0.3230, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [575/1000]: training_loss: tensor(0.3909, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [576/1000]: training_loss: tensor(0.4525, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [577/1000]: training_loss: tensor(0.4226, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [578/1000]: training_loss: tensor(0.3468, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [579/1000]: training_loss: tensor(0.3426, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [580/1000]: training_loss: tensor(0.3815, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [581/1000]: training_loss: tensor(0.3483, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [582/1000]: training_loss: tensor(0.2692, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [583/1000]: training_loss: tensor(0.3516, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [584/1000]: training_loss: tensor(0.3323, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [585/1000]: training_loss: tensor(0.3408, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [586/1000]: training_loss: tensor(0.4258, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [587/1000]: training_loss: tensor(0.3451, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [588/1000]: training_loss: tensor(0.2786, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [589/1000]: training_loss: tensor(0.4069, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [590/1000]: training_loss: tensor(0.2946, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [591/1000]: training_loss: tensor(0.3323, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [592/1000]: training_loss: tensor(0.3955, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [593/1000]: training_loss: tensor(0.3410, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [594/1000]: training_loss: tensor(0.3545, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [595/1000]: training_loss: tensor(0.3881, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [596/1000]: training_loss: tensor(0.2912, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [597/1000]: training_loss: tensor(0.4696, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [598/1000]: training_loss: tensor(0.3822, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [599/1000]: training_loss: tensor(0.3438, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [600/1000]: training_loss: tensor(0.3710, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [601/1000]: training_loss: tensor(0.3475, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [602/1000]: training_loss: tensor(0.2852, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [603/1000]: training_loss: tensor(0.3675, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [604/1000]: training_loss: tensor(0.2479, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [605/1000]: training_loss: tensor(0.3794, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [606/1000]: training_loss: tensor(0.3498, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [607/1000]: training_loss: tensor(0.2932, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [608/1000]: training_loss: tensor(0.4434, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [609/1000]: training_loss: tensor(0.3975, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [610/1000]: training_loss: tensor(0.3039, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [611/1000]: training_loss: tensor(0.4070, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [612/1000]: training_loss: tensor(0.3448, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [613/1000]: training_loss: tensor(0.3349, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [614/1000]: training_loss: tensor(0.3809, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [615/1000]: training_loss: tensor(0.4114, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [616/1000]: training_loss: tensor(0.3542, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [617/1000]: training_loss: tensor(0.3051, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [618/1000]: training_loss: tensor(0.3649, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [619/1000]: training_loss: tensor(0.3365, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [620/1000]: training_loss: tensor(0.3345, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [621/1000]: training_loss: tensor(0.3240, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [622/1000]: training_loss: tensor(0.3536, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [623/1000]: training_loss: tensor(0.3718, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [624/1000]: training_loss: tensor(0.2843, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [625/1000]: training_loss: tensor(0.3386, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [626/1000]: training_loss: tensor(0.4237, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [627/1000]: training_loss: tensor(0.4060, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [628/1000]: training_loss: tensor(0.4158, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [629/1000]: training_loss: tensor(0.3522, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [630/1000]: training_loss: tensor(0.2790, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [631/1000]: training_loss: tensor(0.3557, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [632/1000]: training_loss: tensor(0.4505, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [633/1000]: training_loss: tensor(0.3342, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [634/1000]: training_loss: tensor(0.4364, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [635/1000]: training_loss: tensor(0.3341, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [636/1000]: training_loss: tensor(0.3881, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [637/1000]: training_loss: tensor(0.3732, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [638/1000]: training_loss: tensor(0.3257, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [639/1000]: training_loss: tensor(0.3488, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [640/1000]: training_loss: tensor(0.3504, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [641/1000]: training_loss: tensor(0.3147, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [642/1000]: training_loss: tensor(0.3093, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [643/1000]: training_loss: tensor(0.2871, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [644/1000]: training_loss: tensor(0.3085, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [645/1000]: training_loss: tensor(0.3530, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [646/1000]: training_loss: tensor(0.3199, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [647/1000]: training_loss: tensor(0.2986, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [648/1000]: training_loss: tensor(0.3735, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [649/1000]: training_loss: tensor(0.3906, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [650/1000]: training_loss: tensor(0.3129, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [651/1000]: training_loss: tensor(0.3644, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [652/1000]: training_loss: tensor(0.2724, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [653/1000]: training_loss: tensor(0.3679, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [654/1000]: training_loss: tensor(0.3411, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [655/1000]: training_loss: tensor(0.3263, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [656/1000]: training_loss: tensor(0.3537, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [657/1000]: training_loss: tensor(0.3235, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [658/1000]: training_loss: tensor(0.3968, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [659/1000]: training_loss: tensor(0.3631, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [660/1000]: training_loss: tensor(0.3703, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [661/1000]: training_loss: tensor(0.3205, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [662/1000]: training_loss: tensor(0.3050, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [663/1000]: training_loss: tensor(0.2659, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [664/1000]: training_loss: tensor(0.2640, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [665/1000]: training_loss: tensor(0.3189, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [666/1000]: training_loss: tensor(0.3526, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [667/1000]: training_loss: tensor(0.3026, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [668/1000]: training_loss: tensor(0.3325, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [669/1000]: training_loss: tensor(0.4009, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [670/1000]: training_loss: tensor(0.3695, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [671/1000]: training_loss: tensor(0.3606, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [672/1000]: training_loss: tensor(0.3870, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [673/1000]: training_loss: tensor(0.3168, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [674/1000]: training_loss: tensor(0.3354, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [675/1000]: training_loss: tensor(0.3008, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [676/1000]: training_loss: tensor(0.3384, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [677/1000]: training_loss: tensor(0.2878, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [678/1000]: training_loss: tensor(0.3580, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [679/1000]: training_loss: tensor(0.3840, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [680/1000]: training_loss: tensor(0.4000, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [681/1000]: training_loss: tensor(0.3362, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [682/1000]: training_loss: tensor(0.4579, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [683/1000]: training_loss: tensor(0.3019, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [684/1000]: training_loss: tensor(0.2780, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [685/1000]: training_loss: tensor(0.3427, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [686/1000]: training_loss: tensor(0.3025, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [687/1000]: training_loss: tensor(0.2981, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [688/1000]: training_loss: tensor(0.2535, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [689/1000]: training_loss: tensor(0.3828, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [690/1000]: training_loss: tensor(0.3900, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [691/1000]: training_loss: tensor(0.3897, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [692/1000]: training_loss: tensor(0.3116, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [693/1000]: training_loss: tensor(0.3530, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [694/1000]: training_loss: tensor(0.3472, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [695/1000]: training_loss: tensor(0.3731, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [696/1000]: training_loss: tensor(0.3471, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [697/1000]: training_loss: tensor(0.3444, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [698/1000]: training_loss: tensor(0.4126, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [699/1000]: training_loss: tensor(0.3265, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [700/1000]: training_loss: tensor(0.4462, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [701/1000]: training_loss: tensor(0.3976, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [702/1000]: training_loss: tensor(0.3438, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [703/1000]: training_loss: tensor(0.3448, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [704/1000]: training_loss: tensor(0.3685, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [705/1000]: training_loss: tensor(0.4128, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [706/1000]: training_loss: tensor(0.3374, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [707/1000]: training_loss: tensor(0.3251, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [708/1000]: training_loss: tensor(0.4437, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [709/1000]: training_loss: tensor(0.2803, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [710/1000]: training_loss: tensor(0.4146, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [711/1000]: training_loss: tensor(0.4302, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [712/1000]: training_loss: tensor(0.4387, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [713/1000]: training_loss: tensor(0.3879, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [714/1000]: training_loss: tensor(0.3163, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [715/1000]: training_loss: tensor(0.3540, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [716/1000]: training_loss: tensor(0.4584, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [717/1000]: training_loss: tensor(0.3121, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [718/1000]: training_loss: tensor(0.3147, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [719/1000]: training_loss: tensor(0.3392, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [720/1000]: training_loss: tensor(0.4718, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [721/1000]: training_loss: tensor(0.4896, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [722/1000]: training_loss: tensor(0.3475, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [723/1000]: training_loss: tensor(0.2509, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [724/1000]: training_loss: tensor(0.3218, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [725/1000]: training_loss: tensor(0.2920, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [726/1000]: training_loss: tensor(0.2571, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [727/1000]: training_loss: tensor(0.3483, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [728/1000]: training_loss: tensor(0.3342, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [729/1000]: training_loss: tensor(0.3164, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [730/1000]: training_loss: tensor(0.3508, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [731/1000]: training_loss: tensor(0.3994, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [732/1000]: training_loss: tensor(0.4026, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [733/1000]: training_loss: tensor(0.3932, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [734/1000]: training_loss: tensor(0.3763, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [735/1000]: training_loss: tensor(0.4030, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [736/1000]: training_loss: tensor(0.2947, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [737/1000]: training_loss: tensor(0.4113, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [738/1000]: training_loss: tensor(0.3637, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [739/1000]: training_loss: tensor(0.3324, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [740/1000]: training_loss: tensor(0.3398, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [741/1000]: training_loss: tensor(0.4204, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [742/1000]: training_loss: tensor(0.3948, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [743/1000]: training_loss: tensor(0.2886, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [744/1000]: training_loss: tensor(0.3472, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [745/1000]: training_loss: tensor(0.3450, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [746/1000]: training_loss: tensor(0.3179, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [747/1000]: training_loss: tensor(0.3244, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [748/1000]: training_loss: tensor(0.3478, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [749/1000]: training_loss: tensor(0.3479, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [750/1000]: training_loss: tensor(0.4033, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [751/1000]: training_loss: tensor(0.3532, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [752/1000]: training_loss: tensor(0.3885, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [753/1000]: training_loss: tensor(0.2616, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [754/1000]: training_loss: tensor(0.2800, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [755/1000]: training_loss: tensor(0.3874, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [756/1000]: training_loss: tensor(0.3423, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [757/1000]: training_loss: tensor(0.4172, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [758/1000]: training_loss: tensor(0.3267, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [759/1000]: training_loss: tensor(0.2934, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [760/1000]: training_loss: tensor(0.3329, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [761/1000]: training_loss: tensor(0.3336, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [762/1000]: training_loss: tensor(0.3048, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [763/1000]: training_loss: tensor(0.2698, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [764/1000]: training_loss: tensor(0.3449, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [765/1000]: training_loss: tensor(0.4295, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [766/1000]: training_loss: tensor(0.4531, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [767/1000]: training_loss: tensor(0.4016, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [768/1000]: training_loss: tensor(0.3171, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [769/1000]: training_loss: tensor(0.3258, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [770/1000]: training_loss: tensor(0.3910, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [771/1000]: training_loss: tensor(0.3695, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [772/1000]: training_loss: tensor(0.2777, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [773/1000]: training_loss: tensor(0.3806, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [774/1000]: training_loss: tensor(0.3643, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [775/1000]: training_loss: tensor(0.4031, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [776/1000]: training_loss: tensor(0.3842, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [777/1000]: training_loss: tensor(0.4506, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [778/1000]: training_loss: tensor(0.3287, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [779/1000]: training_loss: tensor(0.3320, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [780/1000]: training_loss: tensor(0.3742, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [781/1000]: training_loss: tensor(0.3102, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [782/1000]: training_loss: tensor(0.2900, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [783/1000]: training_loss: tensor(0.4299, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [784/1000]: training_loss: tensor(0.3172, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [785/1000]: training_loss: tensor(0.3426, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [786/1000]: training_loss: tensor(0.3463, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [787/1000]: training_loss: tensor(0.3375, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [788/1000]: training_loss: tensor(0.3511, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [789/1000]: training_loss: tensor(0.3461, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [790/1000]: training_loss: tensor(0.3262, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [791/1000]: training_loss: tensor(0.2890, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [792/1000]: training_loss: tensor(0.2902, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [793/1000]: training_loss: tensor(0.3119, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [794/1000]: training_loss: tensor(0.3194, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [795/1000]: training_loss: tensor(0.3402, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [796/1000]: training_loss: tensor(0.3911, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [797/1000]: training_loss: tensor(0.3333, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [798/1000]: training_loss: tensor(0.3869, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [799/1000]: training_loss: tensor(0.4031, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [800/1000]: training_loss: tensor(0.4028, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [801/1000]: training_loss: tensor(0.4024, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [802/1000]: training_loss: tensor(0.3431, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [803/1000]: training_loss: tensor(0.3191, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [804/1000]: training_loss: tensor(0.3566, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [805/1000]: training_loss: tensor(0.3496, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [806/1000]: training_loss: tensor(0.4585, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [807/1000]: training_loss: tensor(0.3906, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [808/1000]: training_loss: tensor(0.3875, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [809/1000]: training_loss: tensor(0.3821, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [810/1000]: training_loss: tensor(0.3085, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [811/1000]: training_loss: tensor(0.4359, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [812/1000]: training_loss: tensor(0.3764, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [813/1000]: training_loss: tensor(0.3946, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [814/1000]: training_loss: tensor(0.3520, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [815/1000]: training_loss: tensor(0.3703, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [816/1000]: training_loss: tensor(0.4185, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [817/1000]: training_loss: tensor(0.3896, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [818/1000]: training_loss: tensor(0.3576, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [819/1000]: training_loss: tensor(0.3295, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [820/1000]: training_loss: tensor(0.3620, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [821/1000]: training_loss: tensor(0.3377, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [822/1000]: training_loss: tensor(0.3319, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [823/1000]: training_loss: tensor(0.4049, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [824/1000]: training_loss: tensor(0.2819, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [825/1000]: training_loss: tensor(0.3079, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [826/1000]: training_loss: tensor(0.3694, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [827/1000]: training_loss: tensor(0.3816, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [828/1000]: training_loss: tensor(0.3648, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [829/1000]: training_loss: tensor(0.3178, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [830/1000]: training_loss: tensor(0.3143, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [831/1000]: training_loss: tensor(0.3996, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [832/1000]: training_loss: tensor(0.2859, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [833/1000]: training_loss: tensor(0.3207, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [834/1000]: training_loss: tensor(0.3176, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [835/1000]: training_loss: tensor(0.4332, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [836/1000]: training_loss: tensor(0.3480, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [837/1000]: training_loss: tensor(0.3935, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [838/1000]: training_loss: tensor(0.3166, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [839/1000]: training_loss: tensor(0.4192, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [840/1000]: training_loss: tensor(0.3544, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [841/1000]: training_loss: tensor(0.3718, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [842/1000]: training_loss: tensor(0.4295, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [843/1000]: training_loss: tensor(0.3335, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [844/1000]: training_loss: tensor(0.3640, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [845/1000]: training_loss: tensor(0.3671, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [846/1000]: training_loss: tensor(0.3369, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [847/1000]: training_loss: tensor(0.3725, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [848/1000]: training_loss: tensor(0.3172, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [849/1000]: training_loss: tensor(0.3984, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [850/1000]: training_loss: tensor(0.4520, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [851/1000]: training_loss: tensor(0.3529, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [852/1000]: training_loss: tensor(0.3602, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [853/1000]: training_loss: tensor(0.3489, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [854/1000]: training_loss: tensor(0.2738, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [855/1000]: training_loss: tensor(0.3055, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [856/1000]: training_loss: tensor(0.4011, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [857/1000]: training_loss: tensor(0.3818, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [858/1000]: training_loss: tensor(0.3250, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [859/1000]: training_loss: tensor(0.3472, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [860/1000]: training_loss: tensor(0.3504, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [861/1000]: training_loss: tensor(0.3191, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [862/1000]: training_loss: tensor(0.3125, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [863/1000]: training_loss: tensor(0.3259, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [864/1000]: training_loss: tensor(0.3694, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [865/1000]: training_loss: tensor(0.3390, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [866/1000]: training_loss: tensor(0.3675, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [867/1000]: training_loss: tensor(0.3726, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [868/1000]: training_loss: tensor(0.3131, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [869/1000]: training_loss: tensor(0.4364, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [870/1000]: training_loss: tensor(0.3711, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [871/1000]: training_loss: tensor(0.3963, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [872/1000]: training_loss: tensor(0.3597, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [873/1000]: training_loss: tensor(0.3521, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [874/1000]: training_loss: tensor(0.2976, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [875/1000]: training_loss: tensor(0.2669, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [876/1000]: training_loss: tensor(0.3893, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [877/1000]: training_loss: tensor(0.2803, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [878/1000]: training_loss: tensor(0.3626, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [879/1000]: training_loss: tensor(0.3197, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [880/1000]: training_loss: tensor(0.2624, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [881/1000]: training_loss: tensor(0.3132, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [882/1000]: training_loss: tensor(0.3568, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [883/1000]: training_loss: tensor(0.3018, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [884/1000]: training_loss: tensor(0.3497, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [885/1000]: training_loss: tensor(0.3639, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [886/1000]: training_loss: tensor(0.3897, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [887/1000]: training_loss: tensor(0.3266, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [888/1000]: training_loss: tensor(0.3663, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [889/1000]: training_loss: tensor(0.2739, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [890/1000]: training_loss: tensor(0.3360, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [891/1000]: training_loss: tensor(0.4134, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [892/1000]: training_loss: tensor(0.3131, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [893/1000]: training_loss: tensor(0.3292, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [894/1000]: training_loss: tensor(0.3048, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [895/1000]: training_loss: tensor(0.3068, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [896/1000]: training_loss: tensor(0.3743, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [897/1000]: training_loss: tensor(0.4011, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [898/1000]: training_loss: tensor(0.3654, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [899/1000]: training_loss: tensor(0.3960, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [900/1000]: training_loss: tensor(0.3818, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [901/1000]: training_loss: tensor(0.3414, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [902/1000]: training_loss: tensor(0.3169, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [903/1000]: training_loss: tensor(0.3401, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [904/1000]: training_loss: tensor(0.2851, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [905/1000]: training_loss: tensor(0.3466, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [906/1000]: training_loss: tensor(0.3433, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [907/1000]: training_loss: tensor(0.2483, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [908/1000]: training_loss: tensor(0.3497, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [909/1000]: training_loss: tensor(0.3285, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [910/1000]: training_loss: tensor(0.3844, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [911/1000]: training_loss: tensor(0.2760, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [912/1000]: training_loss: tensor(0.4025, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [913/1000]: training_loss: tensor(0.3772, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [914/1000]: training_loss: tensor(0.4073, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [915/1000]: training_loss: tensor(0.3042, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [916/1000]: training_loss: tensor(0.3306, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [917/1000]: training_loss: tensor(0.4356, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [918/1000]: training_loss: tensor(0.3143, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [919/1000]: training_loss: tensor(0.3124, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [920/1000]: training_loss: tensor(0.3828, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [921/1000]: training_loss: tensor(0.2920, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [922/1000]: training_loss: tensor(0.3913, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [923/1000]: training_loss: tensor(0.3224, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [924/1000]: training_loss: tensor(0.4126, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [925/1000]: training_loss: tensor(0.3286, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [926/1000]: training_loss: tensor(0.3603, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [927/1000]: training_loss: tensor(0.3621, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [928/1000]: training_loss: tensor(0.3219, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [929/1000]: training_loss: tensor(0.3036, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [930/1000]: training_loss: tensor(0.2964, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [931/1000]: training_loss: tensor(0.4196, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [932/1000]: training_loss: tensor(0.2874, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [933/1000]: training_loss: tensor(0.3447, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [934/1000]: training_loss: tensor(0.3818, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [935/1000]: training_loss: tensor(0.3583, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [936/1000]: training_loss: tensor(0.2959, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [937/1000]: training_loss: tensor(0.3773, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [938/1000]: training_loss: tensor(0.3182, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [939/1000]: training_loss: tensor(0.4235, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [940/1000]: training_loss: tensor(0.3608, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [941/1000]: training_loss: tensor(0.2826, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [942/1000]: training_loss: tensor(0.2869, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [943/1000]: training_loss: tensor(0.3318, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [944/1000]: training_loss: tensor(0.3887, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [945/1000]: training_loss: tensor(0.3548, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [946/1000]: training_loss: tensor(0.4126, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [947/1000]: training_loss: tensor(0.3545, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [948/1000]: training_loss: tensor(0.2575, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [949/1000]: training_loss: tensor(0.3297, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [950/1000]: training_loss: tensor(0.3144, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [951/1000]: training_loss: tensor(0.3126, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [952/1000]: training_loss: tensor(0.2999, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [953/1000]: training_loss: tensor(0.3602, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [954/1000]: training_loss: tensor(0.3797, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [955/1000]: training_loss: tensor(0.3298, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [956/1000]: training_loss: tensor(0.2791, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [957/1000]: training_loss: tensor(0.3287, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [958/1000]: training_loss: tensor(0.3879, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [959/1000]: training_loss: tensor(0.3310, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [960/1000]: training_loss: tensor(0.3438, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [961/1000]: training_loss: tensor(0.3073, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [962/1000]: training_loss: tensor(0.4105, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [963/1000]: training_loss: tensor(0.3312, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [964/1000]: training_loss: tensor(0.3161, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [965/1000]: training_loss: tensor(0.2998, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [966/1000]: training_loss: tensor(0.3915, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [967/1000]: training_loss: tensor(0.3732, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [968/1000]: training_loss: tensor(0.3535, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [969/1000]: training_loss: tensor(0.3201, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [970/1000]: training_loss: tensor(0.2748, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [971/1000]: training_loss: tensor(0.3133, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [972/1000]: training_loss: tensor(0.3972, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [973/1000]: training_loss: tensor(0.3546, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [974/1000]: training_loss: tensor(0.4013, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [975/1000]: training_loss: tensor(0.3150, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [976/1000]: training_loss: tensor(0.4010, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [977/1000]: training_loss: tensor(0.3307, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [978/1000]: training_loss: tensor(0.2971, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [979/1000]: training_loss: tensor(0.4388, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [980/1000]: training_loss: tensor(0.3409, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [981/1000]: training_loss: tensor(0.4810, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [982/1000]: training_loss: tensor(0.3616, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [983/1000]: training_loss: tensor(0.3197, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [984/1000]: training_loss: tensor(0.3365, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [985/1000]: training_loss: tensor(0.3249, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [986/1000]: training_loss: tensor(0.2893, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [987/1000]: training_loss: tensor(0.3463, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [988/1000]: training_loss: tensor(0.3705, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [989/1000]: training_loss: tensor(0.2981, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [990/1000]: training_loss: tensor(0.3259, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [991/1000]: training_loss: tensor(0.3402, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [992/1000]: training_loss: tensor(0.3348, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [993/1000]: training_loss: tensor(0.3928, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [994/1000]: training_loss: tensor(0.3669, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [995/1000]: training_loss: tensor(0.3676, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [996/1000]: training_loss: tensor(0.2952, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [997/1000]: training_loss: tensor(0.2570, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [998/1000]: training_loss: tensor(0.4777, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [999/1000]: training_loss: tensor(0.3358, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Epoch [4/5], global step [4000/5000], Train Loss: 0.3583, Valid Loss: 0.3359\n",
      "batch_no [0/1000]: training_loss: tensor(0.3034, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [1/1000]: training_loss: tensor(0.3773, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [2/1000]: training_loss: tensor(0.3495, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [3/1000]: training_loss: tensor(0.2963, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [4/1000]: training_loss: tensor(0.4048, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [5/1000]: training_loss: tensor(0.2896, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [6/1000]: training_loss: tensor(0.3212, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [7/1000]: training_loss: tensor(0.4104, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [8/1000]: training_loss: tensor(0.3628, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [9/1000]: training_loss: tensor(0.4056, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [10/1000]: training_loss: tensor(0.3005, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [11/1000]: training_loss: tensor(0.3354, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [12/1000]: training_loss: tensor(0.4053, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [13/1000]: training_loss: tensor(0.4275, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [14/1000]: training_loss: tensor(0.4724, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [15/1000]: training_loss: tensor(0.3085, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [16/1000]: training_loss: tensor(0.3022, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [17/1000]: training_loss: tensor(0.3708, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [18/1000]: training_loss: tensor(0.3287, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [19/1000]: training_loss: tensor(0.3270, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [20/1000]: training_loss: tensor(0.3827, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [21/1000]: training_loss: tensor(0.2860, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [22/1000]: training_loss: tensor(0.3491, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [23/1000]: training_loss: tensor(0.2458, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [24/1000]: training_loss: tensor(0.3516, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [25/1000]: training_loss: tensor(0.3320, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [26/1000]: training_loss: tensor(0.3008, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [27/1000]: training_loss: tensor(0.4447, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [28/1000]: training_loss: tensor(0.3622, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [29/1000]: training_loss: tensor(0.4841, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [30/1000]: training_loss: tensor(0.3409, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [31/1000]: training_loss: tensor(0.3100, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [32/1000]: training_loss: tensor(0.2656, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [33/1000]: training_loss: tensor(0.3328, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [34/1000]: training_loss: tensor(0.3971, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [35/1000]: training_loss: tensor(0.2835, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [36/1000]: training_loss: tensor(0.3413, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [37/1000]: training_loss: tensor(0.3880, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [38/1000]: training_loss: tensor(0.3750, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [39/1000]: training_loss: tensor(0.3419, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [40/1000]: training_loss: tensor(0.3059, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [41/1000]: training_loss: tensor(0.2510, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [42/1000]: training_loss: tensor(0.3282, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [43/1000]: training_loss: tensor(0.3335, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [44/1000]: training_loss: tensor(0.3215, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [45/1000]: training_loss: tensor(0.5173, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [46/1000]: training_loss: tensor(0.3113, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [47/1000]: training_loss: tensor(0.3651, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [48/1000]: training_loss: tensor(0.3719, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [49/1000]: training_loss: tensor(0.3248, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [50/1000]: training_loss: tensor(0.3421, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [51/1000]: training_loss: tensor(0.4048, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [52/1000]: training_loss: tensor(0.3043, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [53/1000]: training_loss: tensor(0.3808, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [54/1000]: training_loss: tensor(0.3472, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [55/1000]: training_loss: tensor(0.4296, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [56/1000]: training_loss: tensor(0.3895, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [57/1000]: training_loss: tensor(0.3064, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [58/1000]: training_loss: tensor(0.3439, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [59/1000]: training_loss: tensor(0.3281, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [60/1000]: training_loss: tensor(0.3361, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [61/1000]: training_loss: tensor(0.3418, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [62/1000]: training_loss: tensor(0.3366, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [63/1000]: training_loss: tensor(0.4177, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [64/1000]: training_loss: tensor(0.2659, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [65/1000]: training_loss: tensor(0.2933, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [66/1000]: training_loss: tensor(0.3022, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [67/1000]: training_loss: tensor(0.2914, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [68/1000]: training_loss: tensor(0.4058, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [69/1000]: training_loss: tensor(0.3619, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [70/1000]: training_loss: tensor(0.4007, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [71/1000]: training_loss: tensor(0.3751, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [72/1000]: training_loss: tensor(0.2676, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [73/1000]: training_loss: tensor(0.3546, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [74/1000]: training_loss: tensor(0.4128, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [75/1000]: training_loss: tensor(0.3015, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [76/1000]: training_loss: tensor(0.3106, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [77/1000]: training_loss: tensor(0.3712, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [78/1000]: training_loss: tensor(0.3712, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [79/1000]: training_loss: tensor(0.4004, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [80/1000]: training_loss: tensor(0.4124, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [81/1000]: training_loss: tensor(0.3787, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [82/1000]: training_loss: tensor(0.2811, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [83/1000]: training_loss: tensor(0.3472, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [84/1000]: training_loss: tensor(0.2561, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [85/1000]: training_loss: tensor(0.3353, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [86/1000]: training_loss: tensor(0.4417, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [87/1000]: training_loss: tensor(0.3643, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [88/1000]: training_loss: tensor(0.3132, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [89/1000]: training_loss: tensor(0.3748, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [90/1000]: training_loss: tensor(0.3814, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [91/1000]: training_loss: tensor(0.3432, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [92/1000]: training_loss: tensor(0.3997, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [93/1000]: training_loss: tensor(0.2986, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [94/1000]: training_loss: tensor(0.3947, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [95/1000]: training_loss: tensor(0.3284, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [96/1000]: training_loss: tensor(0.2423, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [97/1000]: training_loss: tensor(0.3160, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [98/1000]: training_loss: tensor(0.3296, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [99/1000]: training_loss: tensor(0.3341, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [100/1000]: training_loss: tensor(0.3412, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [101/1000]: training_loss: tensor(0.3157, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [102/1000]: training_loss: tensor(0.3458, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [103/1000]: training_loss: tensor(0.2927, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [104/1000]: training_loss: tensor(0.4031, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [105/1000]: training_loss: tensor(0.2877, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [106/1000]: training_loss: tensor(0.2746, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [107/1000]: training_loss: tensor(0.3390, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [108/1000]: training_loss: tensor(0.3405, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [109/1000]: training_loss: tensor(0.3906, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [110/1000]: training_loss: tensor(0.3683, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [111/1000]: training_loss: tensor(0.3324, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [112/1000]: training_loss: tensor(0.2911, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [113/1000]: training_loss: tensor(0.3302, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [114/1000]: training_loss: tensor(0.2963, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [115/1000]: training_loss: tensor(0.3517, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [116/1000]: training_loss: tensor(0.3380, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [117/1000]: training_loss: tensor(0.3004, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [118/1000]: training_loss: tensor(0.4303, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [119/1000]: training_loss: tensor(0.2714, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [120/1000]: training_loss: tensor(0.3458, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [121/1000]: training_loss: tensor(0.3526, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [122/1000]: training_loss: tensor(0.3953, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [123/1000]: training_loss: tensor(0.2925, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [124/1000]: training_loss: tensor(0.3655, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [125/1000]: training_loss: tensor(0.3569, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [126/1000]: training_loss: tensor(0.3048, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [127/1000]: training_loss: tensor(0.2749, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [128/1000]: training_loss: tensor(0.2903, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [129/1000]: training_loss: tensor(0.3133, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [130/1000]: training_loss: tensor(0.3022, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [131/1000]: training_loss: tensor(0.3544, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [132/1000]: training_loss: tensor(0.4045, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [133/1000]: training_loss: tensor(0.2701, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [134/1000]: training_loss: tensor(0.3283, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [135/1000]: training_loss: tensor(0.3045, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [136/1000]: training_loss: tensor(0.5001, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [137/1000]: training_loss: tensor(0.3777, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [138/1000]: training_loss: tensor(0.3265, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [139/1000]: training_loss: tensor(0.3067, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [140/1000]: training_loss: tensor(0.3706, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [141/1000]: training_loss: tensor(0.3638, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [142/1000]: training_loss: tensor(0.3876, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [143/1000]: training_loss: tensor(0.3423, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [144/1000]: training_loss: tensor(0.3145, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [145/1000]: training_loss: tensor(0.3111, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [146/1000]: training_loss: tensor(0.3237, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [147/1000]: training_loss: tensor(0.2756, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [148/1000]: training_loss: tensor(0.2633, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [149/1000]: training_loss: tensor(0.3721, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [150/1000]: training_loss: tensor(0.3236, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [151/1000]: training_loss: tensor(0.3020, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [152/1000]: training_loss: tensor(0.3961, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [153/1000]: training_loss: tensor(0.3810, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [154/1000]: training_loss: tensor(0.4331, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [155/1000]: training_loss: tensor(0.4212, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [156/1000]: training_loss: tensor(0.3319, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [157/1000]: training_loss: tensor(0.2960, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [158/1000]: training_loss: tensor(0.3344, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [159/1000]: training_loss: tensor(0.4453, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [160/1000]: training_loss: tensor(0.3990, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [161/1000]: training_loss: tensor(0.2966, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [162/1000]: training_loss: tensor(0.2990, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [163/1000]: training_loss: tensor(0.4088, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [164/1000]: training_loss: tensor(0.3663, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [165/1000]: training_loss: tensor(0.3604, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [166/1000]: training_loss: tensor(0.3147, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [167/1000]: training_loss: tensor(0.3836, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [168/1000]: training_loss: tensor(0.3162, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [169/1000]: training_loss: tensor(0.3814, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [170/1000]: training_loss: tensor(0.3285, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [171/1000]: training_loss: tensor(0.3517, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [172/1000]: training_loss: tensor(0.2541, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [173/1000]: training_loss: tensor(0.2949, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [174/1000]: training_loss: tensor(0.3953, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [175/1000]: training_loss: tensor(0.3729, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [176/1000]: training_loss: tensor(0.3360, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [177/1000]: training_loss: tensor(0.4364, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [178/1000]: training_loss: tensor(0.3344, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [179/1000]: training_loss: tensor(0.3727, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [180/1000]: training_loss: tensor(0.3219, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [181/1000]: training_loss: tensor(0.3599, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [182/1000]: training_loss: tensor(0.3190, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [183/1000]: training_loss: tensor(0.3115, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [184/1000]: training_loss: tensor(0.3584, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [185/1000]: training_loss: tensor(0.3201, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [186/1000]: training_loss: tensor(0.3489, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [187/1000]: training_loss: tensor(0.2697, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [188/1000]: training_loss: tensor(0.3481, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [189/1000]: training_loss: tensor(0.3065, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [190/1000]: training_loss: tensor(0.3605, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [191/1000]: training_loss: tensor(0.2692, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [192/1000]: training_loss: tensor(0.3879, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [193/1000]: training_loss: tensor(0.3446, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [194/1000]: training_loss: tensor(0.3730, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [195/1000]: training_loss: tensor(0.3739, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [196/1000]: training_loss: tensor(0.2784, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [197/1000]: training_loss: tensor(0.3357, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [198/1000]: training_loss: tensor(0.3393, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [199/1000]: training_loss: tensor(0.4090, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [200/1000]: training_loss: tensor(0.3402, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [201/1000]: training_loss: tensor(0.3839, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [202/1000]: training_loss: tensor(0.3841, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [203/1000]: training_loss: tensor(0.3524, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [204/1000]: training_loss: tensor(0.3387, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [205/1000]: training_loss: tensor(0.3718, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [206/1000]: training_loss: tensor(0.3601, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [207/1000]: training_loss: tensor(0.2861, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [208/1000]: training_loss: tensor(0.3376, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [209/1000]: training_loss: tensor(0.2697, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [210/1000]: training_loss: tensor(0.3247, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [211/1000]: training_loss: tensor(0.3914, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [212/1000]: training_loss: tensor(0.4301, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [213/1000]: training_loss: tensor(0.2833, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [214/1000]: training_loss: tensor(0.3021, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [215/1000]: training_loss: tensor(0.3495, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [216/1000]: training_loss: tensor(0.2907, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [217/1000]: training_loss: tensor(0.3482, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [218/1000]: training_loss: tensor(0.3963, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [219/1000]: training_loss: tensor(0.3490, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [220/1000]: training_loss: tensor(0.2831, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [221/1000]: training_loss: tensor(0.3820, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [222/1000]: training_loss: tensor(0.3454, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [223/1000]: training_loss: tensor(0.3145, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [224/1000]: training_loss: tensor(0.2880, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [225/1000]: training_loss: tensor(0.3900, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [226/1000]: training_loss: tensor(0.4809, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [227/1000]: training_loss: tensor(0.4142, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [228/1000]: training_loss: tensor(0.3293, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [229/1000]: training_loss: tensor(0.4324, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [230/1000]: training_loss: tensor(0.3327, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [231/1000]: training_loss: tensor(0.3802, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [232/1000]: training_loss: tensor(0.4590, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [233/1000]: training_loss: tensor(0.3535, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [234/1000]: training_loss: tensor(0.3027, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [235/1000]: training_loss: tensor(0.3153, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [236/1000]: training_loss: tensor(0.3507, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [237/1000]: training_loss: tensor(0.3260, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [238/1000]: training_loss: tensor(0.3618, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [239/1000]: training_loss: tensor(0.2756, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [240/1000]: training_loss: tensor(0.3715, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [241/1000]: training_loss: tensor(0.2736, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [242/1000]: training_loss: tensor(0.3350, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [243/1000]: training_loss: tensor(0.3982, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [244/1000]: training_loss: tensor(0.2933, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [245/1000]: training_loss: tensor(0.3424, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [246/1000]: training_loss: tensor(0.3209, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [247/1000]: training_loss: tensor(0.3064, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [248/1000]: training_loss: tensor(0.3236, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [249/1000]: training_loss: tensor(0.3521, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [250/1000]: training_loss: tensor(0.2908, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [251/1000]: training_loss: tensor(0.3638, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [252/1000]: training_loss: tensor(0.3558, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [253/1000]: training_loss: tensor(0.3666, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [254/1000]: training_loss: tensor(0.2949, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [255/1000]: training_loss: tensor(0.3408, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [256/1000]: training_loss: tensor(0.3699, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [257/1000]: training_loss: tensor(0.3843, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [258/1000]: training_loss: tensor(0.3117, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [259/1000]: training_loss: tensor(0.3556, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [260/1000]: training_loss: tensor(0.4207, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [261/1000]: training_loss: tensor(0.3780, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [262/1000]: training_loss: tensor(0.3578, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [263/1000]: training_loss: tensor(0.3435, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [264/1000]: training_loss: tensor(0.3760, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [265/1000]: training_loss: tensor(0.2811, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [266/1000]: training_loss: tensor(0.3352, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [267/1000]: training_loss: tensor(0.2692, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [268/1000]: training_loss: tensor(0.3960, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [269/1000]: training_loss: tensor(0.2826, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [270/1000]: training_loss: tensor(0.3381, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [271/1000]: training_loss: tensor(0.3377, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [272/1000]: training_loss: tensor(0.3693, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [273/1000]: training_loss: tensor(0.2977, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [274/1000]: training_loss: tensor(0.3111, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [275/1000]: training_loss: tensor(0.3513, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [276/1000]: training_loss: tensor(0.3843, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [277/1000]: training_loss: tensor(0.4601, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [278/1000]: training_loss: tensor(0.3079, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [279/1000]: training_loss: tensor(0.3385, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [280/1000]: training_loss: tensor(0.3730, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [281/1000]: training_loss: tensor(0.3217, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [282/1000]: training_loss: tensor(0.3707, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [283/1000]: training_loss: tensor(0.3096, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [284/1000]: training_loss: tensor(0.3620, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [285/1000]: training_loss: tensor(0.2501, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [286/1000]: training_loss: tensor(0.2858, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [287/1000]: training_loss: tensor(0.3789, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [288/1000]: training_loss: tensor(0.4059, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [289/1000]: training_loss: tensor(0.4383, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [290/1000]: training_loss: tensor(0.3949, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [291/1000]: training_loss: tensor(0.4124, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [292/1000]: training_loss: tensor(0.2719, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [293/1000]: training_loss: tensor(0.3842, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [294/1000]: training_loss: tensor(0.3037, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [295/1000]: training_loss: tensor(0.3748, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [296/1000]: training_loss: tensor(0.3672, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [297/1000]: training_loss: tensor(0.3390, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [298/1000]: training_loss: tensor(0.2640, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [299/1000]: training_loss: tensor(0.3444, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [300/1000]: training_loss: tensor(0.4223, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [301/1000]: training_loss: tensor(0.3298, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [302/1000]: training_loss: tensor(0.3358, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [303/1000]: training_loss: tensor(0.3247, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [304/1000]: training_loss: tensor(0.2887, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [305/1000]: training_loss: tensor(0.3197, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [306/1000]: training_loss: tensor(0.4065, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [307/1000]: training_loss: tensor(0.3424, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [308/1000]: training_loss: tensor(0.3104, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [309/1000]: training_loss: tensor(0.2798, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [310/1000]: training_loss: tensor(0.3339, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [311/1000]: training_loss: tensor(0.3432, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [312/1000]: training_loss: tensor(0.3225, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [313/1000]: training_loss: tensor(0.3541, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [314/1000]: training_loss: tensor(0.4098, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [315/1000]: training_loss: tensor(0.3841, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [316/1000]: training_loss: tensor(0.3746, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [317/1000]: training_loss: tensor(0.3046, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [318/1000]: training_loss: tensor(0.3701, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [319/1000]: training_loss: tensor(0.3285, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [320/1000]: training_loss: tensor(0.4025, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [321/1000]: training_loss: tensor(0.4002, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [322/1000]: training_loss: tensor(0.2859, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [323/1000]: training_loss: tensor(0.3483, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [324/1000]: training_loss: tensor(0.3388, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [325/1000]: training_loss: tensor(0.3176, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [326/1000]: training_loss: tensor(0.3487, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [327/1000]: training_loss: tensor(0.4080, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [328/1000]: training_loss: tensor(0.2918, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [329/1000]: training_loss: tensor(0.3509, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [330/1000]: training_loss: tensor(0.3826, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [331/1000]: training_loss: tensor(0.3319, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [332/1000]: training_loss: tensor(0.3879, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [333/1000]: training_loss: tensor(0.3473, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [334/1000]: training_loss: tensor(0.2674, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [335/1000]: training_loss: tensor(0.2692, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [336/1000]: training_loss: tensor(0.3341, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [337/1000]: training_loss: tensor(0.4050, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [338/1000]: training_loss: tensor(0.2825, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [339/1000]: training_loss: tensor(0.3534, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [340/1000]: training_loss: tensor(0.3047, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [341/1000]: training_loss: tensor(0.3030, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [342/1000]: training_loss: tensor(0.3464, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [343/1000]: training_loss: tensor(0.3157, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [344/1000]: training_loss: tensor(0.3546, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [345/1000]: training_loss: tensor(0.3196, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [346/1000]: training_loss: tensor(0.3389, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [347/1000]: training_loss: tensor(0.4288, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [348/1000]: training_loss: tensor(0.3552, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [349/1000]: training_loss: tensor(0.3522, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [350/1000]: training_loss: tensor(0.2946, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [351/1000]: training_loss: tensor(0.3820, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [352/1000]: training_loss: tensor(0.2702, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [353/1000]: training_loss: tensor(0.4313, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [354/1000]: training_loss: tensor(0.3487, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [355/1000]: training_loss: tensor(0.2787, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [356/1000]: training_loss: tensor(0.3369, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [357/1000]: training_loss: tensor(0.3341, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [358/1000]: training_loss: tensor(0.3272, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [359/1000]: training_loss: tensor(0.2620, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [360/1000]: training_loss: tensor(0.4741, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [361/1000]: training_loss: tensor(0.3497, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [362/1000]: training_loss: tensor(0.3465, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [363/1000]: training_loss: tensor(0.3265, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [364/1000]: training_loss: tensor(0.3415, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [365/1000]: training_loss: tensor(0.3040, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [366/1000]: training_loss: tensor(0.3276, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [367/1000]: training_loss: tensor(0.3353, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [368/1000]: training_loss: tensor(0.3793, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [369/1000]: training_loss: tensor(0.3487, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [370/1000]: training_loss: tensor(0.3568, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [371/1000]: training_loss: tensor(0.3177, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [372/1000]: training_loss: tensor(0.2764, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [373/1000]: training_loss: tensor(0.3475, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [374/1000]: training_loss: tensor(0.3793, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [375/1000]: training_loss: tensor(0.2990, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [376/1000]: training_loss: tensor(0.3693, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [377/1000]: training_loss: tensor(0.3747, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [378/1000]: training_loss: tensor(0.3039, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [379/1000]: training_loss: tensor(0.2964, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [380/1000]: training_loss: tensor(0.3813, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [381/1000]: training_loss: tensor(0.2699, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [382/1000]: training_loss: tensor(0.3454, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [383/1000]: training_loss: tensor(0.3349, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [384/1000]: training_loss: tensor(0.2674, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [385/1000]: training_loss: tensor(0.3737, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [386/1000]: training_loss: tensor(0.3376, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [387/1000]: training_loss: tensor(0.3707, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [388/1000]: training_loss: tensor(0.3017, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [389/1000]: training_loss: tensor(0.3856, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [390/1000]: training_loss: tensor(0.3075, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [391/1000]: training_loss: tensor(0.3571, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [392/1000]: training_loss: tensor(0.3195, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [393/1000]: training_loss: tensor(0.2940, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [394/1000]: training_loss: tensor(0.3619, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [395/1000]: training_loss: tensor(0.3577, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [396/1000]: training_loss: tensor(0.2669, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [397/1000]: training_loss: tensor(0.3017, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [398/1000]: training_loss: tensor(0.3198, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [399/1000]: training_loss: tensor(0.3022, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [400/1000]: training_loss: tensor(0.3138, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [401/1000]: training_loss: tensor(0.3140, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [402/1000]: training_loss: tensor(0.3303, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [403/1000]: training_loss: tensor(0.4213, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [404/1000]: training_loss: tensor(0.2901, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [405/1000]: training_loss: tensor(0.2841, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [406/1000]: training_loss: tensor(0.3836, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [407/1000]: training_loss: tensor(0.3678, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [408/1000]: training_loss: tensor(0.2793, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [409/1000]: training_loss: tensor(0.3076, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [410/1000]: training_loss: tensor(0.2811, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [411/1000]: training_loss: tensor(0.3000, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [412/1000]: training_loss: tensor(0.3240, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [413/1000]: training_loss: tensor(0.4218, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [414/1000]: training_loss: tensor(0.3055, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [415/1000]: training_loss: tensor(0.2868, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [416/1000]: training_loss: tensor(0.3116, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [417/1000]: training_loss: tensor(0.2691, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [418/1000]: training_loss: tensor(0.3420, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [419/1000]: training_loss: tensor(0.3794, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [420/1000]: training_loss: tensor(0.3730, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [421/1000]: training_loss: tensor(0.2734, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [422/1000]: training_loss: tensor(0.3474, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [423/1000]: training_loss: tensor(0.2720, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [424/1000]: training_loss: tensor(0.3775, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [425/1000]: training_loss: tensor(0.2809, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [426/1000]: training_loss: tensor(0.2951, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [427/1000]: training_loss: tensor(0.3455, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [428/1000]: training_loss: tensor(0.2973, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [429/1000]: training_loss: tensor(0.2613, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [430/1000]: training_loss: tensor(0.3767, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [431/1000]: training_loss: tensor(0.4399, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [432/1000]: training_loss: tensor(0.2998, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [433/1000]: training_loss: tensor(0.3813, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [434/1000]: training_loss: tensor(0.3005, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [435/1000]: training_loss: tensor(0.4254, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [436/1000]: training_loss: tensor(0.3083, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [437/1000]: training_loss: tensor(0.3769, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [438/1000]: training_loss: tensor(0.4238, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [439/1000]: training_loss: tensor(0.3070, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [440/1000]: training_loss: tensor(0.3166, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [441/1000]: training_loss: tensor(0.2998, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [442/1000]: training_loss: tensor(0.2908, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [443/1000]: training_loss: tensor(0.3357, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [444/1000]: training_loss: tensor(0.4013, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [445/1000]: training_loss: tensor(0.2844, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [446/1000]: training_loss: tensor(0.3096, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [447/1000]: training_loss: tensor(0.4055, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [448/1000]: training_loss: tensor(0.3334, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [449/1000]: training_loss: tensor(0.3918, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [450/1000]: training_loss: tensor(0.2734, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [451/1000]: training_loss: tensor(0.2595, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [452/1000]: training_loss: tensor(0.3055, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [453/1000]: training_loss: tensor(0.2920, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [454/1000]: training_loss: tensor(0.3027, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [455/1000]: training_loss: tensor(0.3206, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [456/1000]: training_loss: tensor(0.2689, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [457/1000]: training_loss: tensor(0.2808, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [458/1000]: training_loss: tensor(0.3056, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [459/1000]: training_loss: tensor(0.3222, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [460/1000]: training_loss: tensor(0.4310, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [461/1000]: training_loss: tensor(0.3646, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [462/1000]: training_loss: tensor(0.4497, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [463/1000]: training_loss: tensor(0.3794, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [464/1000]: training_loss: tensor(0.3300, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [465/1000]: training_loss: tensor(0.3414, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [466/1000]: training_loss: tensor(0.3462, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [467/1000]: training_loss: tensor(0.3191, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [468/1000]: training_loss: tensor(0.3075, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [469/1000]: training_loss: tensor(0.2835, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [470/1000]: training_loss: tensor(0.2827, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [471/1000]: training_loss: tensor(0.3094, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [472/1000]: training_loss: tensor(0.3309, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [473/1000]: training_loss: tensor(0.2805, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [474/1000]: training_loss: tensor(0.3432, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [475/1000]: training_loss: tensor(0.3228, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [476/1000]: training_loss: tensor(0.4248, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [477/1000]: training_loss: tensor(0.3137, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [478/1000]: training_loss: tensor(0.4382, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [479/1000]: training_loss: tensor(0.3407, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [480/1000]: training_loss: tensor(0.3226, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [481/1000]: training_loss: tensor(0.3104, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [482/1000]: training_loss: tensor(0.3140, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [483/1000]: training_loss: tensor(0.3510, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [484/1000]: training_loss: tensor(0.3975, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [485/1000]: training_loss: tensor(0.2742, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [486/1000]: training_loss: tensor(0.2614, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [487/1000]: training_loss: tensor(0.2874, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [488/1000]: training_loss: tensor(0.3510, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [489/1000]: training_loss: tensor(0.3622, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [490/1000]: training_loss: tensor(0.2813, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [491/1000]: training_loss: tensor(0.4741, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [492/1000]: training_loss: tensor(0.4239, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [493/1000]: training_loss: tensor(0.2946, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [494/1000]: training_loss: tensor(0.3434, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [495/1000]: training_loss: tensor(0.2461, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [496/1000]: training_loss: tensor(0.3095, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [497/1000]: training_loss: tensor(0.3381, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [498/1000]: training_loss: tensor(0.3101, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [499/1000]: training_loss: tensor(0.4002, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [500/1000]: training_loss: tensor(0.2707, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [501/1000]: training_loss: tensor(0.2973, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [502/1000]: training_loss: tensor(0.3087, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [503/1000]: training_loss: tensor(0.2848, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [504/1000]: training_loss: tensor(0.3103, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [505/1000]: training_loss: tensor(0.3458, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [506/1000]: training_loss: tensor(0.4477, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [507/1000]: training_loss: tensor(0.3614, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [508/1000]: training_loss: tensor(0.3994, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [509/1000]: training_loss: tensor(0.3322, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [510/1000]: training_loss: tensor(0.2692, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [511/1000]: training_loss: tensor(0.3184, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [512/1000]: training_loss: tensor(0.2950, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [513/1000]: training_loss: tensor(0.4142, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [514/1000]: training_loss: tensor(0.3125, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [515/1000]: training_loss: tensor(0.2982, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [516/1000]: training_loss: tensor(0.2742, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [517/1000]: training_loss: tensor(0.4195, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [518/1000]: training_loss: tensor(0.3959, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [519/1000]: training_loss: tensor(0.4234, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [520/1000]: training_loss: tensor(0.2708, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [521/1000]: training_loss: tensor(0.3527, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [522/1000]: training_loss: tensor(0.3332, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [523/1000]: training_loss: tensor(0.3211, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [524/1000]: training_loss: tensor(0.3438, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [525/1000]: training_loss: tensor(0.3419, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [526/1000]: training_loss: tensor(0.3347, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [527/1000]: training_loss: tensor(0.2773, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [528/1000]: training_loss: tensor(0.3679, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [529/1000]: training_loss: tensor(0.3051, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [530/1000]: training_loss: tensor(0.3368, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [531/1000]: training_loss: tensor(0.4019, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [532/1000]: training_loss: tensor(0.4335, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [533/1000]: training_loss: tensor(0.3881, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [534/1000]: training_loss: tensor(0.3357, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [535/1000]: training_loss: tensor(0.3944, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [536/1000]: training_loss: tensor(0.2992, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [537/1000]: training_loss: tensor(0.3138, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [538/1000]: training_loss: tensor(0.3467, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [539/1000]: training_loss: tensor(0.3054, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [540/1000]: training_loss: tensor(0.3646, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [541/1000]: training_loss: tensor(0.3177, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [542/1000]: training_loss: tensor(0.4330, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [543/1000]: training_loss: tensor(0.3913, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [544/1000]: training_loss: tensor(0.3509, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [545/1000]: training_loss: tensor(0.4490, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [546/1000]: training_loss: tensor(0.4169, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [547/1000]: training_loss: tensor(0.3656, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [548/1000]: training_loss: tensor(0.3287, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [549/1000]: training_loss: tensor(0.3087, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [550/1000]: training_loss: tensor(0.3394, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [551/1000]: training_loss: tensor(0.3264, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [552/1000]: training_loss: tensor(0.3826, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [553/1000]: training_loss: tensor(0.3106, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [554/1000]: training_loss: tensor(0.3437, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [555/1000]: training_loss: tensor(0.4314, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [556/1000]: training_loss: tensor(0.3347, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [557/1000]: training_loss: tensor(0.2576, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [558/1000]: training_loss: tensor(0.3567, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [559/1000]: training_loss: tensor(0.2731, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [560/1000]: training_loss: tensor(0.3398, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [561/1000]: training_loss: tensor(0.4230, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [562/1000]: training_loss: tensor(0.4291, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [563/1000]: training_loss: tensor(0.3932, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [564/1000]: training_loss: tensor(0.2523, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [565/1000]: training_loss: tensor(0.3823, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [566/1000]: training_loss: tensor(0.3529, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [567/1000]: training_loss: tensor(0.3359, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [568/1000]: training_loss: tensor(0.2735, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [569/1000]: training_loss: tensor(0.3161, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [570/1000]: training_loss: tensor(0.3587, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [571/1000]: training_loss: tensor(0.3772, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [572/1000]: training_loss: tensor(0.3082, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [573/1000]: training_loss: tensor(0.3216, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [574/1000]: training_loss: tensor(0.3836, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [575/1000]: training_loss: tensor(0.3531, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [576/1000]: training_loss: tensor(0.2697, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [577/1000]: training_loss: tensor(0.2936, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [578/1000]: training_loss: tensor(0.3134, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [579/1000]: training_loss: tensor(0.3832, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [580/1000]: training_loss: tensor(0.2422, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [581/1000]: training_loss: tensor(0.2596, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [582/1000]: training_loss: tensor(0.3544, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [583/1000]: training_loss: tensor(0.3079, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [584/1000]: training_loss: tensor(0.3475, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [585/1000]: training_loss: tensor(0.2483, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [586/1000]: training_loss: tensor(0.2433, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [587/1000]: training_loss: tensor(0.3530, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [588/1000]: training_loss: tensor(0.2875, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [589/1000]: training_loss: tensor(0.3921, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [590/1000]: training_loss: tensor(0.3173, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [591/1000]: training_loss: tensor(0.3173, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [592/1000]: training_loss: tensor(0.3024, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [593/1000]: training_loss: tensor(0.3672, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [594/1000]: training_loss: tensor(0.4049, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [595/1000]: training_loss: tensor(0.2671, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [596/1000]: training_loss: tensor(0.3364, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [597/1000]: training_loss: tensor(0.3568, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [598/1000]: training_loss: tensor(0.2663, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [599/1000]: training_loss: tensor(0.2709, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [600/1000]: training_loss: tensor(0.3558, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [601/1000]: training_loss: tensor(0.3646, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [602/1000]: training_loss: tensor(0.3376, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [603/1000]: training_loss: tensor(0.3323, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [604/1000]: training_loss: tensor(0.3887, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [605/1000]: training_loss: tensor(0.3455, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [606/1000]: training_loss: tensor(0.3660, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [607/1000]: training_loss: tensor(0.2994, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [608/1000]: training_loss: tensor(0.3913, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [609/1000]: training_loss: tensor(0.3902, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [610/1000]: training_loss: tensor(0.2860, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [611/1000]: training_loss: tensor(0.3174, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [612/1000]: training_loss: tensor(0.4189, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [613/1000]: training_loss: tensor(0.3524, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [614/1000]: training_loss: tensor(0.2628, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [615/1000]: training_loss: tensor(0.3209, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [616/1000]: training_loss: tensor(0.3520, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [617/1000]: training_loss: tensor(0.3178, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [618/1000]: training_loss: tensor(0.3854, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [619/1000]: training_loss: tensor(0.2763, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [620/1000]: training_loss: tensor(0.2862, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [621/1000]: training_loss: tensor(0.2934, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [622/1000]: training_loss: tensor(0.3413, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [623/1000]: training_loss: tensor(0.2738, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [624/1000]: training_loss: tensor(0.3342, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [625/1000]: training_loss: tensor(0.3743, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [626/1000]: training_loss: tensor(0.3658, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [627/1000]: training_loss: tensor(0.3904, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [628/1000]: training_loss: tensor(0.3491, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [629/1000]: training_loss: tensor(0.3095, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [630/1000]: training_loss: tensor(0.2527, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [631/1000]: training_loss: tensor(0.4584, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [632/1000]: training_loss: tensor(0.3245, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [633/1000]: training_loss: tensor(0.3574, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [634/1000]: training_loss: tensor(0.3357, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [635/1000]: training_loss: tensor(0.3057, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [636/1000]: training_loss: tensor(0.2382, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [637/1000]: training_loss: tensor(0.4385, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [638/1000]: training_loss: tensor(0.3168, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [639/1000]: training_loss: tensor(0.3209, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [640/1000]: training_loss: tensor(0.3535, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [641/1000]: training_loss: tensor(0.3067, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [642/1000]: training_loss: tensor(0.3310, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [643/1000]: training_loss: tensor(0.2737, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [644/1000]: training_loss: tensor(0.3168, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [645/1000]: training_loss: tensor(0.3285, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [646/1000]: training_loss: tensor(0.3433, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [647/1000]: training_loss: tensor(0.3883, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [648/1000]: training_loss: tensor(0.2847, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [649/1000]: training_loss: tensor(0.4445, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [650/1000]: training_loss: tensor(0.3609, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [651/1000]: training_loss: tensor(0.3049, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [652/1000]: training_loss: tensor(0.3072, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [653/1000]: training_loss: tensor(0.2397, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [654/1000]: training_loss: tensor(0.3240, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [655/1000]: training_loss: tensor(0.3703, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [656/1000]: training_loss: tensor(0.2409, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [657/1000]: training_loss: tensor(0.2731, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [658/1000]: training_loss: tensor(0.2281, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [659/1000]: training_loss: tensor(0.3486, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [660/1000]: training_loss: tensor(0.3436, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [661/1000]: training_loss: tensor(0.2712, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [662/1000]: training_loss: tensor(0.4168, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [663/1000]: training_loss: tensor(0.4330, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [664/1000]: training_loss: tensor(0.2712, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [665/1000]: training_loss: tensor(0.4503, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [666/1000]: training_loss: tensor(0.3070, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [667/1000]: training_loss: tensor(0.3882, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [668/1000]: training_loss: tensor(0.4022, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [669/1000]: training_loss: tensor(0.3139, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [670/1000]: training_loss: tensor(0.3130, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [671/1000]: training_loss: tensor(0.3479, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [672/1000]: training_loss: tensor(0.3182, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [673/1000]: training_loss: tensor(0.2993, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [674/1000]: training_loss: tensor(0.3389, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [675/1000]: training_loss: tensor(0.3023, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [676/1000]: training_loss: tensor(0.3224, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [677/1000]: training_loss: tensor(0.4506, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [678/1000]: training_loss: tensor(0.3904, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [679/1000]: training_loss: tensor(0.3347, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [680/1000]: training_loss: tensor(0.3018, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [681/1000]: training_loss: tensor(0.3583, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [682/1000]: training_loss: tensor(0.2971, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [683/1000]: training_loss: tensor(0.2833, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [684/1000]: training_loss: tensor(0.3516, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [685/1000]: training_loss: tensor(0.3539, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [686/1000]: training_loss: tensor(0.3857, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [687/1000]: training_loss: tensor(0.3300, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [688/1000]: training_loss: tensor(0.3351, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [689/1000]: training_loss: tensor(0.3195, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [690/1000]: training_loss: tensor(0.3306, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [691/1000]: training_loss: tensor(0.2401, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [692/1000]: training_loss: tensor(0.3849, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [693/1000]: training_loss: tensor(0.3587, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [694/1000]: training_loss: tensor(0.3098, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [695/1000]: training_loss: tensor(0.3333, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [696/1000]: training_loss: tensor(0.3697, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [697/1000]: training_loss: tensor(0.3080, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [698/1000]: training_loss: tensor(0.4013, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [699/1000]: training_loss: tensor(0.3070, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [700/1000]: training_loss: tensor(0.3426, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [701/1000]: training_loss: tensor(0.3207, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [702/1000]: training_loss: tensor(0.2731, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [703/1000]: training_loss: tensor(0.3304, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [704/1000]: training_loss: tensor(0.3655, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [705/1000]: training_loss: tensor(0.3231, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [706/1000]: training_loss: tensor(0.3367, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [707/1000]: training_loss: tensor(0.2587, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [708/1000]: training_loss: tensor(0.3270, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [709/1000]: training_loss: tensor(0.3751, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [710/1000]: training_loss: tensor(0.3078, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [711/1000]: training_loss: tensor(0.3578, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [712/1000]: training_loss: tensor(0.2999, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [713/1000]: training_loss: tensor(0.3600, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [714/1000]: training_loss: tensor(0.4134, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [715/1000]: training_loss: tensor(0.3846, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [716/1000]: training_loss: tensor(0.3958, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [717/1000]: training_loss: tensor(0.4340, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [718/1000]: training_loss: tensor(0.3535, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [719/1000]: training_loss: tensor(0.3094, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [720/1000]: training_loss: tensor(0.3596, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [721/1000]: training_loss: tensor(0.3542, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [722/1000]: training_loss: tensor(0.3813, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [723/1000]: training_loss: tensor(0.3570, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [724/1000]: training_loss: tensor(0.2771, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [725/1000]: training_loss: tensor(0.2908, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [726/1000]: training_loss: tensor(0.3541, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [727/1000]: training_loss: tensor(0.3833, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [728/1000]: training_loss: tensor(0.2678, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [729/1000]: training_loss: tensor(0.3450, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [730/1000]: training_loss: tensor(0.2976, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [731/1000]: training_loss: tensor(0.2387, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [732/1000]: training_loss: tensor(0.3370, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [733/1000]: training_loss: tensor(0.3632, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [734/1000]: training_loss: tensor(0.3094, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [735/1000]: training_loss: tensor(0.2518, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [736/1000]: training_loss: tensor(0.2478, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [737/1000]: training_loss: tensor(0.3357, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [738/1000]: training_loss: tensor(0.2669, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [739/1000]: training_loss: tensor(0.3438, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [740/1000]: training_loss: tensor(0.3480, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [741/1000]: training_loss: tensor(0.4054, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [742/1000]: training_loss: tensor(0.3730, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [743/1000]: training_loss: tensor(0.3039, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [744/1000]: training_loss: tensor(0.2395, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [745/1000]: training_loss: tensor(0.3257, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [746/1000]: training_loss: tensor(0.3440, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [747/1000]: training_loss: tensor(0.4026, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [748/1000]: training_loss: tensor(0.2872, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [749/1000]: training_loss: tensor(0.3465, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [750/1000]: training_loss: tensor(0.2906, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [751/1000]: training_loss: tensor(0.3700, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [752/1000]: training_loss: tensor(0.3744, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [753/1000]: training_loss: tensor(0.3877, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [754/1000]: training_loss: tensor(0.3019, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [755/1000]: training_loss: tensor(0.3575, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [756/1000]: training_loss: tensor(0.2897, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [757/1000]: training_loss: tensor(0.3160, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [758/1000]: training_loss: tensor(0.3268, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [759/1000]: training_loss: tensor(0.3810, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [760/1000]: training_loss: tensor(0.3803, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [761/1000]: training_loss: tensor(0.4486, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [762/1000]: training_loss: tensor(0.2654, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [763/1000]: training_loss: tensor(0.2817, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [764/1000]: training_loss: tensor(0.2710, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [765/1000]: training_loss: tensor(0.2538, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [766/1000]: training_loss: tensor(0.3316, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [767/1000]: training_loss: tensor(0.4040, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [768/1000]: training_loss: tensor(0.3858, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [769/1000]: training_loss: tensor(0.3539, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [770/1000]: training_loss: tensor(0.2763, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [771/1000]: training_loss: tensor(0.3474, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [772/1000]: training_loss: tensor(0.3611, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [773/1000]: training_loss: tensor(0.2924, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [774/1000]: training_loss: tensor(0.3181, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [775/1000]: training_loss: tensor(0.3261, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [776/1000]: training_loss: tensor(0.3255, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [777/1000]: training_loss: tensor(0.3115, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [778/1000]: training_loss: tensor(0.3884, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [779/1000]: training_loss: tensor(0.4122, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [780/1000]: training_loss: tensor(0.2604, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [781/1000]: training_loss: tensor(0.3064, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [782/1000]: training_loss: tensor(0.3097, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [783/1000]: training_loss: tensor(0.4034, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [784/1000]: training_loss: tensor(0.2707, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [785/1000]: training_loss: tensor(0.3131, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [786/1000]: training_loss: tensor(0.3622, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [787/1000]: training_loss: tensor(0.3400, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [788/1000]: training_loss: tensor(0.3217, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [789/1000]: training_loss: tensor(0.3680, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [790/1000]: training_loss: tensor(0.3966, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [791/1000]: training_loss: tensor(0.3283, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [792/1000]: training_loss: tensor(0.3717, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [793/1000]: training_loss: tensor(0.3225, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [794/1000]: training_loss: tensor(0.2796, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [795/1000]: training_loss: tensor(0.2741, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [796/1000]: training_loss: tensor(0.4144, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [797/1000]: training_loss: tensor(0.3217, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [798/1000]: training_loss: tensor(0.2517, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [799/1000]: training_loss: tensor(0.3403, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [800/1000]: training_loss: tensor(0.2658, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [801/1000]: training_loss: tensor(0.3117, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [802/1000]: training_loss: tensor(0.3357, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [803/1000]: training_loss: tensor(0.4761, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [804/1000]: training_loss: tensor(0.2849, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [805/1000]: training_loss: tensor(0.3016, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [806/1000]: training_loss: tensor(0.3244, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [807/1000]: training_loss: tensor(0.3607, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [808/1000]: training_loss: tensor(0.2681, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [809/1000]: training_loss: tensor(0.2836, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [810/1000]: training_loss: tensor(0.3019, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [811/1000]: training_loss: tensor(0.2795, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [812/1000]: training_loss: tensor(0.3020, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [813/1000]: training_loss: tensor(0.3578, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [814/1000]: training_loss: tensor(0.3504, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [815/1000]: training_loss: tensor(0.3165, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [816/1000]: training_loss: tensor(0.3015, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [817/1000]: training_loss: tensor(0.2409, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [818/1000]: training_loss: tensor(0.3289, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [819/1000]: training_loss: tensor(0.2447, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [820/1000]: training_loss: tensor(0.2376, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [821/1000]: training_loss: tensor(0.3160, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [822/1000]: training_loss: tensor(0.3982, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [823/1000]: training_loss: tensor(0.3408, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [824/1000]: training_loss: tensor(0.2981, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [825/1000]: training_loss: tensor(0.3505, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [826/1000]: training_loss: tensor(0.3191, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [827/1000]: training_loss: tensor(0.2997, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [828/1000]: training_loss: tensor(0.3607, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [829/1000]: training_loss: tensor(0.3503, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [830/1000]: training_loss: tensor(0.3148, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [831/1000]: training_loss: tensor(0.3195, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [832/1000]: training_loss: tensor(0.3561, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [833/1000]: training_loss: tensor(0.2866, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [834/1000]: training_loss: tensor(0.3709, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [835/1000]: training_loss: tensor(0.4324, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [836/1000]: training_loss: tensor(0.2648, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [837/1000]: training_loss: tensor(0.2907, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [838/1000]: training_loss: tensor(0.2369, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [839/1000]: training_loss: tensor(0.2964, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [840/1000]: training_loss: tensor(0.4360, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [841/1000]: training_loss: tensor(0.2371, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [842/1000]: training_loss: tensor(0.3130, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [843/1000]: training_loss: tensor(0.3891, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [844/1000]: training_loss: tensor(0.3288, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [845/1000]: training_loss: tensor(0.3600, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [846/1000]: training_loss: tensor(0.2579, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [847/1000]: training_loss: tensor(0.2878, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [848/1000]: training_loss: tensor(0.3370, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [849/1000]: training_loss: tensor(0.2486, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [850/1000]: training_loss: tensor(0.2298, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [851/1000]: training_loss: tensor(0.3699, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [852/1000]: training_loss: tensor(0.3542, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [853/1000]: training_loss: tensor(0.2949, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [854/1000]: training_loss: tensor(0.3565, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [855/1000]: training_loss: tensor(0.2757, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [856/1000]: training_loss: tensor(0.3044, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [857/1000]: training_loss: tensor(0.3006, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [858/1000]: training_loss: tensor(0.3020, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [859/1000]: training_loss: tensor(0.4342, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [860/1000]: training_loss: tensor(0.3457, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [861/1000]: training_loss: tensor(0.3784, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [862/1000]: training_loss: tensor(0.2418, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [863/1000]: training_loss: tensor(0.3536, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [864/1000]: training_loss: tensor(0.2634, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [865/1000]: training_loss: tensor(0.3195, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [866/1000]: training_loss: tensor(0.3293, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [867/1000]: training_loss: tensor(0.3410, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [868/1000]: training_loss: tensor(0.2934, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [869/1000]: training_loss: tensor(0.3752, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [870/1000]: training_loss: tensor(0.4369, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [871/1000]: training_loss: tensor(0.2409, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [872/1000]: training_loss: tensor(0.3358, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [873/1000]: training_loss: tensor(0.5200, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [874/1000]: training_loss: tensor(0.3762, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [875/1000]: training_loss: tensor(0.3914, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [876/1000]: training_loss: tensor(0.2769, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [877/1000]: training_loss: tensor(0.3074, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [878/1000]: training_loss: tensor(0.3520, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [879/1000]: training_loss: tensor(0.3014, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [880/1000]: training_loss: tensor(0.2670, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [881/1000]: training_loss: tensor(0.4218, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [882/1000]: training_loss: tensor(0.4003, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [883/1000]: training_loss: tensor(0.3826, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [884/1000]: training_loss: tensor(0.3544, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [885/1000]: training_loss: tensor(0.3293, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [886/1000]: training_loss: tensor(0.4006, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [887/1000]: training_loss: tensor(0.2937, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [888/1000]: training_loss: tensor(0.4169, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [889/1000]: training_loss: tensor(0.2261, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [890/1000]: training_loss: tensor(0.3589, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [891/1000]: training_loss: tensor(0.3415, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [892/1000]: training_loss: tensor(0.3315, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [893/1000]: training_loss: tensor(0.2873, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [894/1000]: training_loss: tensor(0.2982, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [895/1000]: training_loss: tensor(0.3914, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [896/1000]: training_loss: tensor(0.3185, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [897/1000]: training_loss: tensor(0.3367, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [898/1000]: training_loss: tensor(0.4480, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [899/1000]: training_loss: tensor(0.3779, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [900/1000]: training_loss: tensor(0.3010, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [901/1000]: training_loss: tensor(0.3902, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [902/1000]: training_loss: tensor(0.3528, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [903/1000]: training_loss: tensor(0.3886, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [904/1000]: training_loss: tensor(0.2999, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [905/1000]: training_loss: tensor(0.3482, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [906/1000]: training_loss: tensor(0.3697, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [907/1000]: training_loss: tensor(0.3187, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [908/1000]: training_loss: tensor(0.3230, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [909/1000]: training_loss: tensor(0.3420, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [910/1000]: training_loss: tensor(0.3267, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [911/1000]: training_loss: tensor(0.3265, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [912/1000]: training_loss: tensor(0.3490, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [913/1000]: training_loss: tensor(0.3710, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [914/1000]: training_loss: tensor(0.3440, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [915/1000]: training_loss: tensor(0.3402, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [916/1000]: training_loss: tensor(0.3076, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [917/1000]: training_loss: tensor(0.3655, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [918/1000]: training_loss: tensor(0.2976, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [919/1000]: training_loss: tensor(0.3464, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [920/1000]: training_loss: tensor(0.3017, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [921/1000]: training_loss: tensor(0.3710, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [922/1000]: training_loss: tensor(0.4032, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [923/1000]: training_loss: tensor(0.3848, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [924/1000]: training_loss: tensor(0.3546, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [925/1000]: training_loss: tensor(0.2985, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [926/1000]: training_loss: tensor(0.3151, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [927/1000]: training_loss: tensor(0.2710, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [928/1000]: training_loss: tensor(0.3394, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [929/1000]: training_loss: tensor(0.3278, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [930/1000]: training_loss: tensor(0.3075, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [931/1000]: training_loss: tensor(0.3530, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [932/1000]: training_loss: tensor(0.3053, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [933/1000]: training_loss: tensor(0.2869, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [934/1000]: training_loss: tensor(0.3072, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [935/1000]: training_loss: tensor(0.3077, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [936/1000]: training_loss: tensor(0.2707, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [937/1000]: training_loss: tensor(0.3284, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [938/1000]: training_loss: tensor(0.3816, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [939/1000]: training_loss: tensor(0.2927, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [940/1000]: training_loss: tensor(0.3504, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [941/1000]: training_loss: tensor(0.3853, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [942/1000]: training_loss: tensor(0.3383, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [943/1000]: training_loss: tensor(0.3133, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [944/1000]: training_loss: tensor(0.3505, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [945/1000]: training_loss: tensor(0.4083, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [946/1000]: training_loss: tensor(0.4035, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [947/1000]: training_loss: tensor(0.2997, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [948/1000]: training_loss: tensor(0.2890, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [949/1000]: training_loss: tensor(0.2531, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [950/1000]: training_loss: tensor(0.2793, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [951/1000]: training_loss: tensor(0.3437, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [952/1000]: training_loss: tensor(0.2954, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [953/1000]: training_loss: tensor(0.3196, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [954/1000]: training_loss: tensor(0.3540, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [955/1000]: training_loss: tensor(0.4347, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [956/1000]: training_loss: tensor(0.3841, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [957/1000]: training_loss: tensor(0.2492, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [958/1000]: training_loss: tensor(0.3744, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [959/1000]: training_loss: tensor(0.4197, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [960/1000]: training_loss: tensor(0.3771, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [961/1000]: training_loss: tensor(0.3482, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [962/1000]: training_loss: tensor(0.3770, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [963/1000]: training_loss: tensor(0.2815, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [964/1000]: training_loss: tensor(0.3530, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [965/1000]: training_loss: tensor(0.3596, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [966/1000]: training_loss: tensor(0.3130, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [967/1000]: training_loss: tensor(0.2945, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [968/1000]: training_loss: tensor(0.2880, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [969/1000]: training_loss: tensor(0.2741, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [970/1000]: training_loss: tensor(0.3591, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [971/1000]: training_loss: tensor(0.3397, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [972/1000]: training_loss: tensor(0.2475, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [973/1000]: training_loss: tensor(0.2628, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [974/1000]: training_loss: tensor(0.2751, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [975/1000]: training_loss: tensor(0.3675, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [976/1000]: training_loss: tensor(0.2504, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [977/1000]: training_loss: tensor(0.3065, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [978/1000]: training_loss: tensor(0.3350, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [979/1000]: training_loss: tensor(0.3340, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [980/1000]: training_loss: tensor(0.3295, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [981/1000]: training_loss: tensor(0.2763, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [982/1000]: training_loss: tensor(0.3742, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [983/1000]: training_loss: tensor(0.2835, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [984/1000]: training_loss: tensor(0.2513, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [985/1000]: training_loss: tensor(0.2494, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [986/1000]: training_loss: tensor(0.3358, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [987/1000]: training_loss: tensor(0.3034, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [988/1000]: training_loss: tensor(0.2500, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [989/1000]: training_loss: tensor(0.4080, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [990/1000]: training_loss: tensor(0.2575, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [991/1000]: training_loss: tensor(0.2369, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [992/1000]: training_loss: tensor(0.3955, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [993/1000]: training_loss: tensor(0.4165, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [994/1000]: training_loss: tensor(0.3963, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [995/1000]: training_loss: tensor(0.3144, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [996/1000]: training_loss: tensor(0.3027, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [997/1000]: training_loss: tensor(0.3414, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [998/1000]: training_loss: tensor(0.2911, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [999/1000]: training_loss: tensor(0.2518, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Epoch [5/5], global step [5000/5000], Train Loss: 0.3369, Valid Loss: 0.3290\n",
      "Training done!\n"
     ]
    }
   ],
   "source": [
    "# Main training loop\n",
    "NUM_EPOCHS = 5\n",
    "steps_per_epoch = len(train_dataset)\n",
    "LEARNING_RATE = 1e-05\n",
    "print(\"======================= Start training =================================\")\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=steps_per_epoch*2, \n",
    "                                            num_training_steps=steps_per_epoch*NUM_EPOCHS)\n",
    "\n",
    "train(model=model, \n",
    "      train_iter=training_loader, \n",
    "      valid_iter=val_loader, \n",
    "      optimizer=optimizer, \n",
    "      scheduler=scheduler, \n",
    "      num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "350it [00:14, 24.34it/s]\n"
     ]
    }
   ],
   "source": [
    "fin_targets = []\n",
    "fin_outputs = []\n",
    "with torch.no_grad():                    \n",
    "    for _,data in tqdm(enumerate(test_loader, 0)):\n",
    "        source = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        target = data['targets'].to(device, dtype = torch.float)\n",
    "        y_pred = model(input_ids=source, \n",
    "                       attention_mask=mask)\n",
    "        fin_targets.extend(target.cpu().detach().numpy().tolist())\n",
    "        fin_outputs.extend(torch.sigmoid(y_pred).cpu().detach().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['counter_speech+discussion_of_eastasian_prejudice',\n",
       " 'entity_directed_criticism',\n",
       " 'entity_directed_hostility',\n",
       " 'none_of_the_above']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_final = []\n",
    "for i in fin_outputs:\n",
    "    temp = [0 for i in range(len(label_final))]\n",
    "    index = i.index(max(i))\n",
    "    temp[index] = 1\n",
    "    outputs_final.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-domain test(accuracy and F1 score):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.8067857142857143\n",
      "F1 Score (Micro) = 0.8067857142857143\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "accuracy = metrics.accuracy_score(fin_targets, outputs_final)\n",
    "f1_score_micro = metrics.f1_score(fin_targets, outputs_final, average='micro')\n",
    "print(f\"Accuracy Score = {accuracy}\")\n",
    "print(f\"F1 Score (Micro) = {f1_score_micro}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array(outputs_final)\n",
    "y_true = np.array(fin_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true.argmax(axis=1), y_pred.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtQAAAKzCAYAAAA+3JoSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeVxU1f/H8dcsIItiQOKCWwikKCqKe5kWWZGapam5fM0lNU3z55JLX00z961UsL4uLZoVlXtqZlpqVKJoLrjhlghuYCJuCMzvD2KSALEZwaX38/HgIXPvOfece+YynvnM554xWCwWCyIiIiIiYhPjne6AiIiIiMi9TBNqERERERE7aEItIiIiImIHTahFREREROygCbWIiIiIiB00oRYRERERmxyPT7zTXbgrGLRsnoiIiIjYyjnotUJp58qO2YXSji0UoRYRERERsYP5TndARERERO5hBsVnNQIiIiIiInZQhFpEREREbGcw3Oke3HGKUIuIiIiI2EERahERERGxnXKoFaEWEREREbGHItQiIiIiYjvlUCtCLSIiIiJiD02oRURERETsoJQPEREREbGdbkpUhFpERERExB6KUIuIiIiI7XRToiLUIiIiIiL2UIRaRERERGynHGpFqEVERERE7KEItYiIiIjYTjnUilCLiIiIiNhDEWoRERERsZ1yqBWhFhERERGxhyLUIiIiImI75VArQi0iIiIiYg9NqEVERERE7KCUDxERERGxnW5KVIRaRERERMQeilCLiIiIiO10U6Ii1CIiIiIi9lCEWkRERERspxxqRahFREREROyhCLWIiIiI2E4RakWoRURERETsoQi1iIiIiNjOqFU+FKEWEREREbGDItQiIiIiYru7JIc6PDyc6OhoihcvzrRp06zb16xZw9q1azGZTNSqVYtOnToBsHTpUjZs2IDRaKRr167UrFkTgCNHjhAWFkZqaipBQUF07doVQz5rbWtCLSIiIiL3vCZNmvD0008TFhZm3bZnzx62bdvG1KlTcXBw4MKFCwDExcURGRnJ9OnTOX/+PGPHjuW9997DaDQyd+5cevXqhZ+fHxMmTGDnzp0EBQXdtO274y2FiIiIiNybDIbC+clHQEAARYsWzbZt3bp1PPfcczg4OABQvHhxAKKiomjYsCEODg54eXlRqlQpYmNjOX/+PFeuXMHf3x+DwUDjxo2JiorKt21FqEVERETknjBs2DDr7yEhIYSEhNy0fEJCAvv37+fzzz/HwcGBzp074+vrS1JSEn5+ftZyHh4eJCUlYTKZ8PT0tG739PQkKSkp335pQi0iIiIi94SJEyf+o/IZGRmkpKQwbtw4Dh8+zIwZM5g9ezYWiyXX8nltz48m1CIiIiJiu7vkpsTceHh4UK9ePQwGA76+vhiNRi5evIinpyeJiYnWcklJSXh4eOTYnpiYiIeHR77t3L0jICIiIiJihzp16rBnzx4A4uPjSUtLo1ixYgQHBxMZGcn169c5c+YMCQkJ+Pr64u7ujrOzMwcPHsRisbBp0yaCg4PzbcdgsTW2LSIiIiL/es5PTiqUdq58N/Sm+999911iYmK4ePEixYsXp23btjRu3Jjw8HCOHz+O2Wymc+fOVKtWDYAlS5awceNGjEYjL7/8snUlj8OHDxMeHk5qaio1a9akW7du+S6bpwm1iIiIiNjsbplQ30nKoRYRERER293FOdSFRSMgIiIiImIHRahFRERExHa38KUr9ztFqEVERERE7KAItYiIiIjYTjnUilCLiIiIiNhDEWoRERERsZ1yqBWhFhERERGxhyLUIiIiImI75VArQi0iIiIiYg9NqEVERERE7KCUDxERERGxnW5KVIRaRERERMQeilCLiIiIiO10U6Ii1CIiIiIi9lCEWkRERERspwi1ItQiIiIiIvZQhFpEREREbKdVPhShFhERERGxhyLUIiIiImI75VArQi0iIiIiYg9FqEVERETEdsqhVoRaRERERMQeilCLiIiIiO2UQ60ItYiIiIiIPTShFhERERGxg1I+RERERMR2uilREWoREREREXsoQi0iIiIiNjMoQq0ItYiIiIiIPRShFhERERGbKUKtCLWIiIiIiF0UoRYRERER2ylArQi1iIiIiIg9FKEWEREREZsph1oRahERERERuyhCLSIiIiI2U4RaEWoREREREbsoQi0iIiIiNlOEWhFqERERERG7aEItIiIiImIHpXyIiIiIiM2U8qEItYiI3GPS0tLo1q0bnp6eGAwGfvjhh9ty3IoVK/LOO+/clmPdC44dO4bBYGDLli13uisi9zxFqEVExG6JiYlMmjSJ5cuXc/z4cdzc3KhcuTI9evSgQ4cOmM2377+br7/+msWLF7NhwwZ8fHzw8PC4LceNiorCxcXlthzrTgkJCaFs2bJ89NFH+ZYtV64cCQkJeHp6FnzH5P6mALUm1CIiYp+4uDgaNWqE2Wzm7bffJigoCAcHByIjI5k6dSrVq1enZs2at629Q4cO4e3tTcOGDW/bMQFKlChxW493N0tNTcXR0ZFSpUrd6a6I3BeU8iEiInZ59dVXuXbtGtHR0XTs2JGAgAD8/Pzo0qUL27dvx8/PD4Dr168zbNgwvL29cXR0JCAggMWLF2c7lsFgIDw8nM6dO1OsWDHKlSvH5MmTrfubNGnCyJEjOXLkCAaDgYoVK1q39+jRI9ux3nnnHet+gL179/LUU0/xwAMP4OrqSpUqVVi4cKF1/99TPi5evEivXr0oUaIETk5OBAcHs27dOuv+rJSJiIgIWrRogYuLCz4+PtmOmZuPPvoIs9nMxo0bCQwMxNnZmccee4z4+Hg2bdpEUFAQrq6uhISEcPLkSWu9o0eP8sILL1CmTBlcXFwIDAzM1tbLL7/M999/z8cff4zBYLCmw2T189NPPyU0NBRXV1dGjBiRI+UjIiICR0dHtm7daj3mJ598gpOTEzt27LjpOcm/W9b1VtA/dzNNqEVExGZJSUmsXr2a1157jeLFi+fY7+DggKurKwAjRoxg7ty5vPvuu+zZs4dOnTrRqVMnvv/++2x1xowZQ+PGjdm5cydDhgxh6NChbNy4EYAlS5YwaNAgKlasSEJCAlFRUbfc15deeglPT08iIyPZvXs306dPx93dPc/y3bp149tvv2XRokXs2LGDRo0a0bx5c/bv35+t3LBhw+jcuTO7du2ibdu2dO3alUOHDt20LxkZGYwZM4Z58+bx008/ER8fT7t27Rg1ahRz5sxhy5YtxMXFMXDgQGudlJQUnnjiCdauXcvu3bvp2bMnXbt2tY7Ne++9x6OPPkrbtm1JSEggISEhWxR/6NChdOjQgd27d9O3b98cfWrbti1dunThpZdeIjk5mYMHD9K3b1+mTJlCUFDQLY2xyL+VUj5ERMRmsbGxZGRkEBAQcNNyly9fZubMmcyYMYMXX3wRyJxgR0VFMW7cOJ544glr2Xbt2vHKK68A0L9/f8LDw1m3bh1NmzbFw8ODokWLYjKZ/nG6wvHjxxk4cKC1rz4+Pjc9r6+++opvvvmGp556CsicsG7evJnJkyezYMECa9nXXnuNtm3bAplR8dmzZ7NhwwZrZD43FouFd99915oK07NnT9544w22bdtG7dq1AejVqxfjxo2z1gkMDCQwMND6uF+/fqxfv57FixfTtGlTihcvjqOjI87OzrmOTa9evejUqZP18bFjx3KUmTlzJsHBwfTo0YNDhw7xxBNP0K9fvzzPQwS0ygcoQi0iInawWCxA/v+hxsbGkpqaSuPGjbNtf+yxx9i7d2+2bX/Pt/b29ub06dN293Xw4MH06NGDJk2aMHr0aKKjo/MsGxMTA5Cjv40bN75pf81mMyVLlsy3vwaDIdvkOGsCXL169WzbEhMTSU9PBzLflAwbNoyqVata31isXr2a48eP37StLHXr1s23jLOzM1988QVLlizhzJkz2d44iEjeNKEWERGb+fn5YTQac0wy8/L3ibfFYsmxzdHRMUedjIyMmx7XaDRaJ/dZrl+/nu3xyJEjOXjwIG3btmXPnj3Ur1+f//73v7fU74Lor8lkylYHMlNk/r4t67yGDBnCokWLGDVqFBs3bmTnzp2EhoaSmpp6S33PSr3JT1ZO9R9//MGZM2duqY78uymHWhNqERGxg4eHB8888wyzZ8/mwoULOfZfv36dS5cu4evrS5EiRfjxxx+z7d+0aRNVq1a1ux9eXl7Ex8dn25ZbBNrHx4c+ffrw1Vdf8fbbbzNnzpxcj5fVp02bNmXbvnnz5tvSX1ts2rSJjh070q5dO2rUqIGPjw8HDx7MVsbR0dEa0bbF3r17GThwIB988AHPPPMM7du359q1a/Z2XeS+pwm1iIjYJTw8HAcHB2rXrs3ixYuJiYkhNjaWRYsWERwczKFDh3BxcaF///6MHDmSL7/8kkOHDjF+/HiWL1/OiBEj7O5DSEgI69evJyIigtjYWCZOnMjmzZut+1NSUujbty8bNmzg6NGj7Nixg7Vr1+aZ+12pUiVefPFF+vTpw7fffsv+/ft5/fXX2bNnD0OGDLG7v7Z4+OGHWb58OVu3biUmJoaePXvmeBPx0EMPsX37dg4fPsy5c+dyROlv5urVq7Rv356WLVvSvXt35s6dy/nz5xk8ePDtPhW5zyhCrZsSRUTETuXLlyc6OpqJEycyevRofv/9d9zc3KhSpQpDhgyhWrVqAIwbNw6j0ciAAQM4e/Ysvr6+LFq0KNsNibbq0qULe/bs4bXXXiM1NZWOHTvSv39/PvnkEyAzt/n8+fN0796dhIQE3NzcaNq0KVOnTs3zmPPmzWPIkCF06tSJ5ORkAgMDWbVqFZUrV7a7v7aYMWMGPXr0oGnTpri5udGzZ0/atGnD4cOHrWUGDRrE7t27qVGjBpcuXWLjxo3Zlg68mf/7v//j0qVLfPDBBwC4u7vz6aef0rRpU5588klatmxZEKclcl8wWP6edCYiIiIicos8u3xWKO0kfvxSobRjC6V8iIiIiIjYQSkfIiIiInLPCw8PJzo6muLFizNt2rRs+1asWMGiRYuYN28ebm5uACxdupQNGzZgNBrp2rWrdQnMI0eOEBYWRmpqKkFBQXTt2jXfHG5FqEVERETEZnfLTYlNmjTJ9Sbnc+fOsXv3bh588EHrtri4OCIjI5k+fTpvvvkm8+fPty53OXfuXHr16sXMmTM5deoUO3fuzLdtTahFRERE5J4XEBBA0aJFc2z/+OOP6dixY7ZJeVRUFA0bNsTBwQEvLy9KlSpFbGws58+f58qVK/j7+2MwGGjcuDFRUVH5tq2UDxERERGxWWEuaTds2DDr7yEhIYSEhNy0/LZt2/Dw8Mix2k1SUhJ+fn7Wxx4eHiQlJWEymfD09LRu9/T0JCkpKd9+aUItIneF08m3vl7uneThaiLpku1fnFHYirs45F/oLuFogtR7ZGjTM+6dBbKczHA17U734tbdS4uPOTsYuHL93uhv0SL3R1LCxIkTb7nstWvXWLJkSa7fiJrXdWbr9acJtYiIiIjY7G790pXTp09z5swZ65cxJSYmMnToUCZMmICnpyeJiYnWsklJSXh4eOTYnpiYiIeHR75t3R9vV0REREREblC+fHnmzZtHWFgYYWFheHp6MmnSJB544AGCg4OJjIzk+vXrnDlzhoSEBHx9fXF3d8fZ2ZmDBw9isVjYtGkTwcHB+balCLWIiIiI2O4uCVC/++67xMTEcPHiRXr37k3btm15/PHHcy1brlw5GjRowMCBAzEajXTv3h2jMTPO3KNHD8LDw0lNTaVmzZoEBQXl27a+KVFE7grKoS4YyqEuGMqhLjj30rREOdSZvLpHFNixb3RmfttCaccWilCLiIiIiM3u1hzqwqQcahEREREROyhCLSIiIiI2U4RaEWoREREREbsoQi0iIiIiNlOEWhFqERERERG7aEItIiIiImIHpXyIiIiIiM2U8qEItYiIiIiIXRShFhERERHbKUCtCLWIiIiIiD0UoRYRERERmymHWhFqERERERG7KEItIiIiIjZThFoRahERERERuyhCLSIiIiI2U4RaEWoREREREbsoQi0iIiIitlOAWhFqERERERF7aEItIiIiImIHpXyIiIiIiM10U6Ii1CIiIiIidlGEWkRERERspgi1ItQiIiIiInZRhFpEREREbKYItSLUIiIiIiJ2UYRaRERERGymCLUi1CIiIiIidlGEWkRERERspwC1ItQiIiIiIvbQhFpE/lUcTQY8Xc14FjXj4pj7S6CDyYCHqxlPVzPuLibrdhdHI2ZjZv3izqYc9VwcjZR0cyArndDxz+Nk/TiY7u8wzrpv11K96sNUrezLlMkTc+y3WCwMHNCfqpV9qRNUnR3R0dnqVqmcs25SUhLPPv0k1ar48ezTT3L+/HnrvimTJlC1si/Vqz7Md+u+LdiTu4O++3YtQdUqU72KH9Om5BzXA/v383jjhngUc+K96VOz7Xu1ZzdKlypJnaDAbNuTkpJo8UwzagT40+KZZtZxPX7sGA8Wd6FBnSAa1Amif9/eBXdid4Hv1q0lKLAKNQL8mTZlUo79X3z2KfWDa1I/uCZPNHmE3bt+y1Y3oErlHHXHjx2Dv085GtatRcO6tfh27WoAtkVttW5rUCeIFcuXFvwJFhKDwVAoP3czTahF5F+lmLOJPy6nkZiShpODEdPfXgUNgJvTn2UupfHHlXQAjIbMCXNahoXES2kAODn89QJvNICj2UB6hsW6LcNi4Y/LaSRdSuPClbRcJ+H3i/T0dAb078vylWvYsSuGLz//jH0xMdnKfLt2DYdjD7Fn3yFmz/kf/V97NVvdb1bnrDt18kSaPP4Ee/YdosnjTzD1z8n2vpgYvvzic6J/28uKVWt5vV8f0tPTC/ekC0F6ejoDX3+NJStWs+23vXz5xefs25d9XN09PJgy/T36/9+gHPU7dn6Zb1avybF9+pSJNHn8cX6LOUiTxx9n+g0T9Yd8KvFz1A5+jtrBzLD3b/9J3SXS09MZ9Ho/liz/hqide/gq4nP2/21sK1R8iDXfbeSXbTsZOvxN6xuMrLqrvlmda92+/QYQuTWayK3RPPV0KAABVauxKXIrkVujWbpiNa+/9ippaWmFd8JSoDShFpF/DQdT5oQ3/c8579XrGRQxZ38ZdHIwcjUtg6x5scVCrgxARsZfj4s5mUi5mn1Cl5aB9TjpGfd3mmHU1q1UquTLQz4+ODo68mK79qxauTxbmVUrltOh038wGAzUq1+fCxf+ICEhwVrXJ5e6q1Yup1PnLgB06tyFlSuWWbe/2K49RYoUoeJDD1Gpki9RW7cW7kkXgm1RW/G5YVzbtG3HN38bVy8vL2oH18HBwSFH/UcebYyHh0eO7d+sXEHHTpnj2rFTF1atWJ6jzP0uc2wrWce29YvtWLVyRbYy9Rs0xN3dHYA6detz8mRctro+N6n7dy4uLpjNmbeuXb169a6PuP4TilBrQi0i/yJGQ/ZJcIbFkiNCbTJmlnN3MeHharZGoTMscCk1A7PRQImiZjKA1D9n5kXMBjIyLKRlkKciZgPXM/KYnd8H4uNPUrZsOetjb++ynDx5Mt8y8SdP3rTumdOnKV26NAClS5fm7JkzAJw8mcux4rO3dz+Ijz9J2XJlrY+zxsxeZ86cptSf41qqdGnOnj1j3Xf82FEa1q3FUyFN+GnLZrvbulslxJ/EO9s15E3CTa6hTz5awJPNnr6luv+bE0b94Jq82rN7tjSlqK2/UicokPrBNXh3Vrh1gi33Pk2oRURuYDCA2Wjg/OV0zl9Ow9XRhMmYGV12MhtIy7BwNiUt8/Gfk23XIkZSruU9mzYZoaiTiYtX7r+UhCyWXEL5f48o5VXmVurm0uA/r3MPsmls7FCqdGn2xR4ncms0EydPo1uXjiQnJxdYe3fSPxnbTT9s5JOPFvD2uIn51u3Rsze79h0icms0pUqVZsTQwdYyderWI2rHbn746VemT5nE1atXb8epyF3gvphQf/PNN1y7du1Od8NmERERrFhx84+KbsWZM2cYNCgzh+7w4cMsWLDA7mPa4v333ycuLq7A20lOTmbEiBG88cYb7Nu3r8Da2bt3LwcOHLCpblJSEtOmTbvNPbLPunXr+PHHH/9RndGjR3P48GEAJkyYwKVLlwqiawUuwwLGG171jAYD6X+bB6dn/BV5tljgeroFs9HwZ370X+WupWXgYDJgNoLJYMCzqJkHi5oxGsDTNfPfzDbgAWczyVfSrakm9yNv77LExZ2wPj55Mo4yZcrkW6Z0mTI3retVsiQJCQkAJCQkUMLLK/NYZXM5Vuns7d0PvL3LEnfir9fTrDGzl5dXSU79Oa6nEhIoUSJzXIsUKYKnpycAQbVq85BPJWIPHbS7vbtRGe+ynMx2DZ2kVC7X0J7du3jt1Z58/tVS69jcrK5XyZKYTCaMRiMvd+vB9m1ROY5ZuXIVXFxcidm753af1h1hMBTOz93svphQr169+h9PqDMybvLZ7F1o7969hIWF3XL5SpUq0a1btwLsUd569+5N2bJl8y9op927d1OmTBkmT55MlSpVCqwdeybUHh4e1jc5helmN2c1a9aMxx57zOZjDx8+HFdXV5vr30nX0y2YjAbrZNfJwci1v+VpZE2UsziYMqPS6RaybXc0GUlLz8yTPpuSxrk/fzIskHgp818D8ICLmZRr6Vy/n2fTQHCdOsTGHuLY0aOkpqby5Ref82zzltnKPNuiJYsXfYLFYuHXX37Bza04pUuXttY9mkvdZ5u3ZNHCjwFYtPBjmrd4zrr9yy8+59q1axw7epTY2EPUqVu3cE+6ENQOrsPhG8b1q4gvCP3buNoitHkLPl2UOa6fLvqYZ1tkHvPs2bPW14+jR45wOPYQFR/ysbu9u1Hm2MZax/brL7/g2eYtspU58fvvdGzXhv8t+Bg/P/8cdY/mUjfrjQrAyhXLCKhaFYBjR49ab0L8/fhxDh06QPkKFQv4LKWwFFryzo8//sjKlSsxGAyUL1+e9u3bM2fOHJKTk3Fzc6NPnz48+OCDhIWFUbt2berXrw9A586dWbhwIXv37uXLL7+kWLFinDhxAh8fH/r168eaNWtISkpizJgxuLm58dZbb/Hbb78RERFBWloaJUuWpE+fPjg5OdG3b1+aNm3Kb7/9xtNPP02jRo1y9HP16tV89913mEwmypYty4ABA4iIiOD06dMkJSWRmJhIy5YtCQkJAWDFihX8/PPPXL9+nbp169K2bVsANm3axJo1a0hLS8PPz48ePXpgNBrZuXMnn332GRkZGRQrVoxRo0YBEBcXx+jRozl37hyhoaGEhobe0rgeOXKEOXPm4OjoSOXKla3b9+7dy8qVKxk2bBgxMTF8+OGHQOZHUmPGjMHZ2Znly5ezadMmjEYjNWvWpGPHjowePZrOnTtTqVIlkpOTGT58OGFhYZw4cYLw8HDS0tKwWCwMGjQId3d3ZsyYQVJSEhkZGbRu3ZqGDRtmO8aWLVtYujRzaaCgoCA6depkfV5DQ0OJjo7G0dGRIUOG8MADD+R6jmfPns1xraSkpLBo0SJSU1MZMmQI48aNw9HRMUfdvK6Fr776iu3bt5Oamoq/vz89e/bEYDDkeP47dOjAd999h9FoZPPmzXTr1o1Lly6xZMkS0tLSKFasGP369eOBBx7IdZwvXrzIpEmTmDZtGmfOnGH27NnWN3/dunXj4YcfzvPazuujx759+9KgQQP27t0LwOuvv06pUqUICwujaNGiHDt2jIceeohmzZoxf/58kpOTKVKkCL169cLb25uIiAicnJxo2bJlns93amoq4eHhxMXF4e3tTWpqarb2J0yYgJubW46/6379+pGcnMz//vc/EhMTAejSpUu2azPL+vXrWb9+PQATJ07Ew7WQVsCwWHiwaOZLX4bFQnFnk3WCnZXibDSAV7HsZSAzQmIyGPAqZsZiAbMJnB2z9zsr/zrrd6OBbKt7pBVyHvXfc8QLiqPJzKxZs2n57FOkp6fTtWs3alavyvvvZ64S0bt3b1o2D+W7taupVtkXFxcX5i/4EEfTX3WfeTp7XYARw4fRvl1bPvlwPuXLl+eLiC9xNEHN6lVp27YttaoHYDabmT07LMdzUZAshRWOMpuZOWsWz7d4mvT0dF7u2pVa1avywZ/j2qt3b06dOkW9unVITk7GaDQSPvs9du/Zi5ubGx07dODHH3/g3LlzPOxTjrfeGk237t0zx7V9OxZ+tIBy5cvzxRcROJkhKnITo0e/hdlsxmQyER4+hzJeOW9qLFiFFIp0cGDmrFm80PKZv8a2RrVsYzt14liSkhIZPOA1AMxmM79ujbLWDX3m6Wx1AUb/dxi//bYTg8FAhQoVmfP++zg7GNj+60+0az0JBwcHjEYjYbPDKFe6ROGcawG7H9Ot/qlCmVCfOHGCJUuWMHbsWNzc3EhJSWH27Nk0btyYJk2asGHDBhYsWMAbb7xx0+McPXqU6dOn4+7uzsiRIzlw4AChoaF88803vPXWW7i5uZGcnMySJUsYOXIkTk5OLFu2jFWrVtGmTRsAHBwcGDt2bJ5tLF++nNmzZ+Pg4JDtY+3ff/+dcePGcfXqVYYOHUqtWrU4ceIECQkJjB8/HovFwuTJk4mJicHNzY3IyEjGjh2L2Wxm3rx5bN68maCgID744APGjBmDl5cXKSkp1uPHx8fz1ltvceXKFQYMGECzZs1u6WaF8PBwunXrRkBAAAsXLsy1zIoVK+jevTuVK1fm6tWrODg4sGPHDqKiohg/fjxFihTJ1pfcfPfdd4SGhvLoo4+SlpZGRkYG0dHRuLu7M3z4cAAuX76crU5SUhKffvopkyZNwtXVlXfeeYetW7dSt25drl27hp+fHy+99BKLFi3i+++/p3Xr1rm2PX/+/FyvlXbt2nH48GG6d++ea72bXQtPP/209ZqYNWsW27dvJzg4OMfz7+rqypNPPmmdgAKkpKQwbtw4DAYD33//PStWrOA///lPruN8o+LFi/Pf//4XR0dHEhISeO+995g4MTMfL7drO7dJaBYXFxcmTJjAjz/+yEcffcSwYcOAzI/ER44cidFo5O233+aVV16hdCTMR4cAACAASURBVOnSHDp0iHnz5vHWW2/d9HnOsm7dOhwdHZk6dSrHjx9n6NChOcrk9ncN8OGHH9K8eXMqV67MuXPnGDduHDNmzMhRPyQkxPrGFCDp0r2RX+zharpn+gpQ3KXwPogMeSqUXTF/BQNS06HbK72tv4OB6TOzf9KWmv5X3f0HQq2Ps/4t9oAn33z7fa51Bg97k8HD3syxvTD8PVWoID3eLJQde/4a16tp0KVHb+vvDzxYigNHTuSodzUN5n+yGCdz5u83bnct7snKNetzlA99rjWhz7XOsb0w5ZafXFCaPvkM0bufsT6+ct3Cf7r3sv7+Xvhc3gufm63OlesWa919+0Otj7P+fX/+xznauXLdQuv2nWjdvlOuxyoMRYto0luQCmVCvWfPHurXr4+bmxsARYsW5dChQwwenJmo37hxYz799NN8j+Pr62vNX6pYsSJnzpzJMek4dOgQcXFxjBw5EoC0tDT8/f/6mKZhw4Y3baN8+fLMnDmTOnXqUPeGjw+Dg4NxdHTE0dGRqlWrEhsby/79+9m1a5f1jcDVq1c5deoUv//+O0ePHrVONFNTU3Fzc+PgwYNUqVIFrz9zAIsWLWo9fq1atXBwcMDBwYHixYtz4cIFPD09GTFiBNevX+fq1aukpKQwZMgQADp27Ii/vz+XLl0iICDAOo47d+7McU6VK1fmk08+4ZFHHqFevXp4enqye/dumjRpQpEiRXL0JTf+/v4sWbKExMRE6tWrR+nSpSlfvjwLFy5k0aJF1K5dO0faxeHDh6latar1eX/00UfZt28fdevWxWw2U7t2bQB8fHzYtWtXnm3bcq1k1cvrWtizZw8rVqzg2rVrpKSkUK5cOYKDg/N8/m+UlJTEu+++y/nz50lLS7M+n7mN843S09OZP38+x44dw2g0WvNC4dau7RtlfbrSqFEjPv74rxfv+vXrYzQauXr1KgcOHGD69OnWff9kvdOYmBjrpyQVKlSgQoUKOcrk9ncNmak4N+bQX758mStXruDs7HzL7YuIyL1DAepCmlBbLJZb/jjAZDJZ85stFku2ScCNET+j0ZhrHrTFYiEwMJABAwbkevysCWRehg8fTkxMDNu2bePrr7+2Tkj+3v+sx61ateLJJ5/Mtm/NmjU89thjdOjQIdv2bdu25dnujdFoo9FozWEbP348kJnC8cMPP9C3b19ruUuXLt3SuLZq1YpatWoRHR3Nm2++yciRI/N8TkwmkzU6cP36dev2Rx55BF9fX6Kjoxk3bhy9e/emWrVqTJo0iejoaBYvXkyNGjWsUV+4eZTBZDJZ27/xfG+nvK6F1NRU5s+fz4QJE3jwwQeJiIiwpjTk9fzfaMGCBTRv3pzg4GBrugbkPs43XrOrVq2iePHiTJkyBYvFQseOHa37buXavtGNz92Nvzs5OQGZ9wi4uroyZcqUmx4nr+f7VuR1DVksljxTcERERO5HhfJZYGBgID///DMXL14EMj8y9/f3JzIyEoAtW7ZYo3ElSpTgyJEjAERFRd3SRMvJycm69Iy/vz8HDhzg1KlTAFy7do34+Phb6mdGRgbnzp2jWrVqdOrUicuXL1uPGxUVRWpqKhcvXmTv3r1UqlSJGjVqsHHjRmuZpKQkLly4QGBgIL/88gsXLlywnu/Zs2fx9/dn3759nPlzHdX80izy4+rqiouLC/v37wdg8+bc1ws9deoU5cuXp1WrVvj4+HDy5Elr37PyebP6cuP4//LLL9ZjnD59mpIlSxIaGkpwcDDHjx8nKSkJR0dHGjduTIsWLaz1svj5+RETE0NycjIZGRn89NNP1mj6P5HXtXIr9XK7FrImjm5ubly9epVff/0VyPv5d3Z2zra00eXLl61flHDjahm5jfONLl++jLu7O0ajkU2bNtl1Y2zWeERGRuLn55djv4uLC15eXvz8889A5iT32LFjOcrl9XwHBASwZcsWIDPd6fjx4znq5vZ3DVC9enXWrl1rLZdbuyIicv/QF7sUUoS6XLlyPP/884wePRqj0UjFihXp2rUrc+bMYcWKFdYbzQCeeOIJpkyZwvDhwwkMDMw3ogyZuZjjx4/H3d2dt956i759+/Lee+9ZJ07t27fPsXxTbjIyMpg1a5Y1F/jZZ5+1rmbg6+vLxIkTOXfuHK1bt8bDwwMPDw9OnjzJm29m5vA5OTnRr18/ypYtS/v27XnnnXewWCyYTCa6d+9uvflt6tSpWCwW3NzcrOkIturTp4/1psQaNWrkWmb16tXs3bsXo9GIt7c3QUFBODg4cOzYMYYNG4bZbCYoKIgOHTrQokULZsyYwaZNm6hWrZr1GJGRkWzevBmTycQDDzxAmzZtiI2NZdGiRRgMBsxmMz169MjWrru7Ox06dGDMmDFA5k2JderU+cfnmNe1kh83N7c8r4UnnniCQYMG4eXlRaVKlYC8n//atWszffp0oqKi6NatGy+++CLTp0/Hw8MDPz8/6xuk3Mb5xgX9n3rqKaZNm8Yvv/xC1apVb+nazsv169cZMWIEFouF119/Pdcy/fv3Z+7cudYbKBs1akTFihWBv6LaeT3fzZo1Izw8nMGDB1OxYkV8fX1zHD+3v+u+ffvStWtX5s+fz+DBg0lPT6dKlSr07NnT5nMVERG52xkshZn9f4+6cVUEkTvtxlU2bLFgwQIeeughmjZtept7Zp/Tyf8s5eROufduSsz5ddR3K0dT4d5YaI/0e+hbL/9+U+Ld7l6aljg7GAr1xkJ7FC1ScEkJlYd9W2DHvtH+iU8VSju2uC/WoRaRW/P5559z6NAhgoOD73RXRERE7hv/2gj1vHnzcnxZR2ho6F0Xtfs3WbJkiTXnN0uDBg144YUX8q2btRrKjfr160f58uVvax8L05QpU6zpJFk6duxIzZo171CPCpYi1AVDEeqCoQh1wbmXpiWKUGcKGLGuwI59o5jxzQqlHVv8ayfUInJ30YS6YGhCXTA0oS4499K0RBPqTJpQF+I3JYqIiIjI/ecuX4CjUCiHWkRERETEDppQi4iIiIjYQSkfIiIiImKzu/1LVwqDItQiIiIiInZQhFpEREREbKYAtSLUIiIiIiJ2UYRaRERERGymHGpNqEVERETkPhAeHk50dDTFixdn2rRpACxcuJDt27djNpspWbIkffr0wdXVFYClS5eyYcMGjEYjXbt2tX4T8ZEjRwgLCyM1NZWgoCC6du2a75sGpXyIiIiIiM0MBkOh/OSnSZMmjBgxItu26tWrM23aNKZOnUrp0qVZunQpAHFxcURGRjJ9+nTefPNN5s+fT0ZGBgBz586lV69ezJw5k1OnTrFz585829aEWkRERETueQEBARQtWjTbtho1amAymQDw9/cnKSkJgKioKBo2bIiDgwNeXl6UKlWK2NhYzp8/z5UrV/D398dgMNC4cWOioqLybVspHyIiIiJis8JMoR42bJj195CQEEJCQm657oYNG2jYsCEASUlJ+Pn5Wfd5eHiQlJSEyWTC09PTut3T09M6Cb8ZTahFRERE5J4wceJEm+otWbIEk8nEo48+CoDFYsm1XF7b86MJtYiIiIjY7G5f5eOHH35g+/btjBo1ytpXT09PEhMTrWWSkpLw8PDIsT0xMREPD49821AOtYiIiIjcl3bu3Mny5csZOnQoRYoUsW4PDg4mMjKS69evc+bMGRISEvD19cXd3R1nZ2cOHjyIxWJh06ZNBAcH59uOItQiIiIiYrO7JUD97rvvEhMTw8WLF+nduzdt27Zl6dKlpKWlMXbsWAD8/Pzo2bMn5cqVo0GDBgwcOBCj0Uj37t0xGjPjzD169CA8PJzU1FRq1qxJUFBQvm0bLLYmi4iI3Eank6/f6S7cEg9XE0mX0u90N25ZcReHO92FW+ZogtR7ZGjTM+6d/zqdzHA17U734tbdS9MSZwcDV67fG/0tWqTgkhJqvb2hwI59o+hRjxdKO7ZQyoeIiIiIiB2U8iEiIiIiNrvbb0osDIpQi4iIiIjYQRFqEREREbGZAtSKUIuIiIiI2EURahERERGxmXKoFaEWEREREbGLItQiIiIiYjMFqBWhFhERERGxiyLUIiIiImIz5VArQi0iIiIiYhdFqEVERETEZgpQK0ItIiIiImIXRahFRERExGbKoVaEWkRERETELppQi4iIiIjYQSkfInJXKO7icKe7cEtMRijuoljEv13K1bQ73YVb5uhqIuVq+p3uxi0rYr53/r4sZgNp6ZY73Y07ThkfilCLiIiIiNhFEWoRERERsZluSlSEWkRERETELopQi4iIiIjNFKBWhFpERERExC6KUIuIiIiIzZRDrQi1iIiIiIhdFKEWEREREZspQq0ItYiIiIiIXRShFhERERGbKUCtCLWIiIiIiF0UoRYRERERmymHWhFqERERERG7aEItIiIiImIHpXyIiIiIiM2U8aEItYiIiIiIXRShFhERERGb6aZERahFREREROyiCLWIiIiI2EwBakWoRURERETsogi1iIiIiNjMqBC1ItQiIiIiIvZQhFpEREREbKYAtSLUIiIiIiJ2UYRaRERERGymdagVoRYRERERsYsm1CIiIiIidlDKh4iIiIjYzKiMD0WoRURERETsoQi1iIiIiNhMNyUqQi0iIiIiYhdFqEVERETEZgpQK0ItIiIiImIXRahFRERExGYGFKJWhFpERERExA6KUIuIiIiIzbQOtSLUIvIvs+7btVSv+jBVK/syZfLEHPstFgsDB/SnamVf6gRVZ0d0NAAnTpzgqZCmVA2oQq0aVZk98z1rnV2//cZjjzQguGYgrVu1IDk5GYCorVupV7sm9WrXpG6tGixftrRwTvIOsXVsAXr16Eapkl7UrlktW528xvb4sWO4F3O2jm+/Pr0L9uTuoA3rv6VR7arUr1mFWdMn51rmp80/8sQjwTSuV4NWoU9kq1s1oEqOuueTkmj73DM0CAqg7XPP8Mf589Z9M6dNon7NKjSqXZWN69cV3IndBdavW0udmgHUCnyYGVMn5di/etUKGtUN4tH6tWn6SD1+jtxi3Ve9SiVq1qxu3Zdl2ZKvaBBcHY+iDuyI3mbdvvH772jSqC4N69SkSaO6bPphQ8GenBQqg8VisdzpToiIXE0r+DbS09MJDPDnmzXf4V22LI/Ur8PHiz6jSkCAtczaNauZEzaLZStXs/XXXxk88HU2R/5KQkICpxISqFenFol/XKRhvdpEfLWMKgEBNKpfh4mTp/Jo48f4+MMFHDt2lLfGjOXy5cs4OjpiNptJSEigXu0aHPk9HrP5/vtw0J6xBdiyeRMPuBWlS5f/sH3nHmudvMb2+LFjvNCqebayhenC5euF0k56ejoNa1UlYtlqSnuX5emmDZgzfyEPV/5rXC/88QfNmzXms69XUbZcec6ePUOJEl7Wut+u+xYX99LZ6r49chju7h70G/gGs6ZP5o8/zjPy7Qkc2B/Dq907s2ZDJKcS4mn73DNERu/FZDIVyvkCFDEXTqwvPT2d4BpVWLpyLWW8y/L4o/WZ99EiKlf5a2xTUlJwdXXFYDCwZ/cuuv3nJbbu2AtkTqi3bo3CqZhHtuMe2L8Po9HI//V/lbHjJxNUKxiAXTt3UKJkSUqXLkPM3j20eS6UmNjfC+VcAR5wKbjn8Lm52/IvdBssfyW4UNqxhSLUIvKvEbV1K5Uq+fKQjw+Ojo682K49q1Yuz1Zm1YrldOj0HwwGA/Xq1+fChT9ISEigdOnSBNWqBUCxYsWoXLkK8fEnATh08ACPPNoYgMdDnmTZ0q8BcHFxsU6er129el9/+YE9YwvwyKON8fDwyHHcvMb232LH9ige8qlEhYcyx7XVC2359puV2cos+fJznm3RirLlygNQooRXtro+Pjnrfrt6JW07dAagbYfOrP1mReb2b1bS6oW2FClShAoVH+Ihn0rs2B5VWKdbqLZv24qPTyUq/jm2L7Rpy+pVK7KVKVq0qPXv9vLlS7f0N/xw5Sr4+T+cY3v1mkGULl0GgCoBVbl67SrXrl27DWciWcLDw+nRoweDBg2ybktJSWHs2LH079+fsWPHkpKSYt23dOlS+vXrx+uvv87OnTut248cOcKgQYPo168fCxYs4FZiz5pQi8i/Rnz8ScqWLWd97O1dlpMnT+ZbJv5vZY4fO8bOnTuoUzfzY96AqtVYtTLzP+IlX31J3IkT1rJbf/2VWjWqEhwUyMyw9+/L6DTcvrH9u5uN7bGjR6kfHMSTjz/Gli2bb8dp3HUS4k9Sxrus9XFpb28SEuKzlTly+BB//PEHzz8bQrPG9Yj4bGG+dc+ePUPJUqUBKFmqNOfOns2skxBPmbI31CnjTUL8zZ+je1VCfDzeN1yPZbzL5hhbgFUrllE3qCrtWrdk1py51u0Gg4FnnnmKJo3q8tGCuTnq3cyKZUuoXr0mRYoUsf0E7iIGQ+H85KdJkyaMGDEi27Zly5YRGBjIzJkzCQwMZNmyZQDExcURGRnJ9OnTefPNN5k/fz4ZGRkAzJ07l169ejFz5kxOnTqVbbKdF02oReRfI7cow98jTvmVSUlJ4aW2rZky7V3c3NwA+GDuAj6YE0bDurVJSbmIo6OjtXzdevWI/m0vW36OYsqkCVy9evV2nc5d5XaMbW7yGttSpUtz8Mjv/LJtB5OmTOflzh2s+dX3k1sZs7S0NHbtjGZRxHI+W/oNMyZP4HDsQZvG25Y696pbPdfmLVuxdcdeFn3+NePffsu6fe33m4iK2s6XS1cx74M5/LRl0y21uy9mL6NHDmfGrDm2d15yFRAQQNGiRbNti4qK4rHHHgPgscceIyoqyrq9YcOGODg44OXlRalSpYiNjeX8+fNcuXIFf39/DAYDjRs3tta5GU2o5Z6zdetW4uLirI+/+OILdu3aBcA333xzWz5Ci4iIYMWKFTmOb49jx44RfcNNWLdq9OjRHD582K62Dx8+zIIFCwDYu3cvBw4csO5bt24dP/74Y551k5KSmDZtml3t3y28vcsSF/dXhPPkyTjKlCmTb5nSf5a5fv06bdq0pt1LHWn1/AvWMg9XrsyqNeuI3Lqdtu1e4iGfSjnarlylCq6uruzdc2dyfguavWObl7zGtkiRInh6egJQq3ZtfHwqcejgwdt1OneNMt5liT/51+tdwsmTlPozsmwtU8abpiHNcHV1xdPzQeo3fIS9u3fdtG6JEl6cPpWZbnP6VAIPlihhPVb8Da+vCfEnKVX65s/RvaqMtzcnb7ge40/G5RjbGzV6pDFHjx4h8dw5AGv6RgkvL5q3fI7obflPuk6ejKPzS22YM/fDXF8nJH/Dhg2z/qxfvz7f8hcuXMDd3R0Ad3d36xvvpKQk62sIgIeHB0lJSTm2e3p6kpSUlG87mlDLPScqKirbhLpdu3ZUr14dgNWrV9/2nLQbj3+jrI+GbtWxY8fYsWPH7erWLUtPT6dSpUp069YNyDmhbtasmfXde248PDyy5aPdy4Lr1CE29hDHjh4lNTWVL7/4nGebt8xW5tkWLVm86BMsFgu//vILbm7FKV26NBaLhd6vdKdK5Sq8/n8Ds9U5c+YMkHlNTBz/Dq/0zFxx4tjRo6SlZd5tefz4cQ4ePECFihUL/kTvAHvG9mbyGtuzZ8+Snp4OwNEjR4iNPcRDPj4FcGZ3Vs1awRw5HMvxY5njumxJBM1Cm2cr89SzLfg18ifS0tK4fPky0du34vdwZWvdo0dz1m32TAsiFmemhkQsXshToS0yt4c2Z9mSCK5du8bxY0c5cjiWoNp1CvekC0mt2nU4fMPYLvkqgmeebZGtzJHDsdZI9m87ormemoqHpyeXLl3i4sWLAFy6dIkN339HlYCqN23vwh9/0O6FlowaM476DRoVzEndIUaDoVB+ACZOnGj9CQkJsbnPeeVF27pWx/2ZzCf3nE2bNrFmzRrS0tLw8/OjR48edOnShdDQUKKjo3F0dGTIkCGcPn2abdu2ERMTw9dff82gQYP4+uuvqV27tvWd5ZgxY3Bzc+PRRx/l999/5+WXXwZg/fr1nDx5ki5duuTahyVLlvDjjz/y4IMPUqxYMXz+/M85LCyM2rVrU79+ffr27UvTpk357bffePrppylatCgRERGkpaVRsmRJ+vTpg5OTE7GxsXz00Udcu3YNs9nMyJEj+eKLL0hNTWX//v08//zz1KpViwULFnDixAnS09N58cUXqVOnDqmpqYSHhxMXF4e3tzepqak3HbudO3fy2WefkZGRQbFixRg1ahQRERGcP3+es2fPUqxYMUJCQli5ciXdunXju+++w2g0snnzZrp168bu3btxcnKiZcuWnDp1irlz55KcnJx5l/r//R9Go5FJkyYxbdo0Tpw4QXh4OGlpaVgsFgYNGoTJZGL8+PFUrlyZQ4cOUaFCBZo0acKXX37JhQsX6N+/P76+vjn6vX79emt0YeLEiTgWwiICjiYzs2bNpuWzT5Genk7Xrt2oWb0q77//PgC9e/emZfNQvlu7mmqVfXFxcWH+gg9xNMGWLT+x+NOFBAYGUj+4JgDvjBtPaGgoSyI+Izw8DIDnn3+BHt27YjDArz9voXWriTg4OGA0GgkLC6dMyQcL/kTvAHvGFqBDh5f48YcfOHfuHL4Vy/LW6DF07949z7H95adNjH5rFGazGZPJxJw571OqRM6bGguKh2thrXphYtasmXRs05yM9HS6vNyVhsHV+d8HmePas1dv6teqxrOhTxPySG2MRiOv9OhBozo1AJg1aybNQ58h/Ya6AKP+O4wO7dvzxacfUa5ceT774gs8XE00DK5Ou7ZtaVq/BiazmdmzZ1HCzTGvzhWIQvvWvSKOzJo5ixdbhZKens7LL3elTlAgH/w5tr169ebbVUtZtGghZgcHnJ2cWfzZ5xRzMnE2/ixt2ryAgcyUm/btX6JVi1AAli1byoDX+3P27Fnat25JjRo1Wb1mLTPnhXP0SCzTJo9j2uRxAKxZ8y1eXl6Fc77/UsWLF+f8+fO4u7tz/vx5a6qep6cniYmJ1nJJSUl4eHjk2J6YmJjrDdN/p2Xz5I6Li4tj0aJFDB48GLPZzLx58/Dz8yMsLIw33niD4OBgFi1ahLOzM61bt842wYWcE94JEybg5ubG1atXGTJkCDNmzMBsNvPf//6Xnj17Ur58+Rx9OHLkCGFhYYwfP5709HSGDh3Kk08+ScuWLXMcv1mzZjz33HMkJyczbdo0hg8fjpOTE8uWLSMtLY1WrVoxYMAABgwYgK+vL5cvX6ZIkSJs3ryZw4cP0717dwAWL15M2bJlady4MZcuXWLEiBFMmjSJ9evX8/vvv9OnTx+OHz/O0KFDGTduHJUq5fx4MDk5maFDhzJmzBi8vLxISUmxTvK3b9/O2LFjcXR0ZO/evaxcuZJhw4YRERFhnUAD2R6PGDGCVq1aUbduXVJTU7FYLFy4cME6oV6wYAF+fn48+uijpKWlkZGRwR9//EH//v2ZPHkyZcuWZfjw4VSoUIFXX32Vbdu2sXHjRt544418r4PCWDbvdnA0QWr6ne7F/eleGtvCWjbvdvBwNZF06R4ZWApv2bzboWgRIynX/tmnlXdKQS6b13rB9gI79o2+7lY73zJnzpyx/p8FsHDhQooVK0arVq1YtmwZKSkpdOrUiRMnTjBz5kzGjx/P+fPnefvtt5k5cyZGo5Hhw4fTtWtX/Pz8mDBhAk8//TS1/lzlKS+KUMsdt2fPHo4ePcrw4cMBSE1Nxc3NDbPZTO3amX88Pj4+/ziP2cnJiapVqxIdHY23tzfp6em5TqYB9u3bR926da13XAcH573WZcOGDQE4dOgQcXFxjBw5EsiMUvj7+xMfH4+7u7s1Kuvi4pLrcXbt2sX27dtZuXKl9bzPnTtHTEwMoaGZkY4KFSpQoUKFPPty8OBBqlSpYo1w3HgzRnBwcLab4/Jz5coVkpKSqFu3LkCudf39/VmyZAmJiYnUq1fP+nG9l5eXdWzLlStHYGAgBoOB8uXLc/bP1QNEREQK0rvvvktMTAwXL16kd+/etG3bllatWjFjxgw2bNjAgw8+yMCBmSl75cqVo0GDBgwcOBCj0Uj37t0xGjPfzPXo0YPw8HBSU1OpWbMmQUFB+batCbXccRaLhccee4wOHTpk275y5UrrHddGo9GaL/lPPPHEEyxdupQyZcrQpEmTm5a91TvZsybdFouFwMBABgwYkG3/8ePHb+k4WSkTf79x63b5p8sx3cqHVY888gi+vr5ER0czbtw4evfujZeXFw4ODtYyBoPB+thgMPzjXHMREbm33C0rwfz9/+Mso0aNynX7Cy+8wAsvvJBje6VKlf7xzfj3zucqct8KDAzkl19+4cKFC0DmsmQ3i2o6Oztz5cqVXPc5OTllW5bMz8+PxMREfvrpJxo1yvsmkCpVqrB161ZSU1O5cuUK27fn//GVv78/Bw4c4NSpUwBcu3aN+Ph4vL29OX/+PLGxsUBm5Dc9PR0nJ6ds/a5RowZr1qyxTmSPHj0KZC77s2VL5tfb/v777zedoPv7+7Nv3z7rjVs3LlifF2dn51yXbnNxccHT05OtW7cCmSta/P0Gz9OnT1OyZElCQ0MJDg6+5TcPIiIi9zNFqOWOK1u2LO3bt+edd97BYrFgMpmseca5adiwIR988AFr1qyxfnSTJSQkhPHjx+Pu7s5bb2WuF9qgQQOOHTuWY23KG/n4+NCwYUOGDBlCiRIlqFy5cr79dnNzo2/fvrz33ntcv56ZT9m+fXvKlCnDgAED+PDDD0lNTcXR0ZGRI0dSrVo1li9fzpAhQ3j++edp06YNH330EYMHDwagRIkSDBs2jGbNmhEeHs7gwYOpWLFirjf03diHnj17MnXqVCwWC25ubtYUlLzUrl2b6dOnExUVZV35I8trr73G//73PyIiIjCZTAwcODBb5CEyMpLNmzdjMpl44IEHaNOmDZcvX853rERER2x9SQAAIABJREFU5P51lwSo7yjdlCj3vYkTJ/Lss88SGBh4p7siN6GbEuVeGlvdlFhwdFNiwSjImxJf/Oiff8eCLb58+eY3Bt5J985VK/IPXbp0iddffx1HR0dNpkVERApIYa5DfbdSyofct1xdXXnvvfeybbt48SJvv/12jrKjRo2iWLFihdU1m4wYMcKaWpKlX79+ea5cIiIiIoVDKR8icldQyofcS2OrlI+Co5SPglGQKR/tP/5/9u48ruoq/+P46172VRYFWVREUNkUFRXJtIyyzMpcynFatKap1My0xWpsbOY3pllaltlMY7aYprmluWaOlpmJe+S+C6goi8gOl/v7g7xJqNFF4KLv5zx4POB+z/l+P98zN/zwueecb+08Bfjzh39/+7q6Un/etSIiIiIiNkhTPkRERETEarayD3VdUoVaRERERKQalFCLiIiIiFSDpnyIiIiIiNWMmvGhCrWIiIiISHVctkL9zjvvVGmS+fDhw69qQCIiIiJSf2hR4hUS6saNG9dmHCIiIiIi9dJlE+oBAwbUZhwiIiIiUg+pQP0HFiXu2rWL77//nnPnzjFmzBgOHTpEQUEB0dHRNRmfiIiIiIhNq9KixBUrVvDBBx8QEBDAnj17AHB0dOTzzz+v0eBERERExLYZDIZa+bJlVUqoly9fztixY+nTpw9GY3mXoKAg0tLSajQ4ERERERFbV6UpHwUFBTRs2LDCa6WlpdjbaxtrERERkeuZ9qGuYoU6IiKCxYsXV3htxYoVREVF1UhQIiIiIiL1RZVKzI888ggTJ07km2++obCwkKeffhpXV1deeOGFmo5PRERERGyYrc9vrg1VSqi9vb157bXXOHToEGfOnMHX15ewsDDLfGoRERERketVlSdBm81mSktLASgrK6uxgERERESk/lB9uooJ9bFjx5g0aRIlJSX4+PiQmZmJg4MDzz77LCEhITUcooiIiIiI7apSQj19+nR69uxJ7969MRgMmM1mli1bxvTp05k4cWJNxygiIiIiYrOqNAn65MmT3HnnnZZJ5waDgV69enHq1KkaDU5EREREbJvRYKiVL1tWpYS6Xbt2bNmypcJrW7ZsoV27djUSlIiIiIhIfXHZKR/vvPOOpSJdVlbGW2+9RWhoKL6+vmRkZHD48GHi4uJqLVARERERsT02XjyuFZdNqBs3blzh5yZNmli+Dw4Opm3btjUXlYiIiIhIPXHZhHrAgAG1GYeIiIiI1EN6sMsf2Ie6tLSUtLQ0cnJyKrweHR191YMSEREREakvqpRQ7927l8mTJ1NSUkJBQQEuLi4UFhbi6+vLu+++W9MxioiIiIiNUoG6irt8fPzxx9x9993MnDkTFxcXZs6cSb9+/bjttttqOj4REREREZtWpYQ6LS2NXr16VXitT58+LFu2rEaCEhEREZH6QftQVzGhdnV1paCgAAAvLy9SUlLIzc2lsLCwRoMTEREREbF1VZpD3blzZ7Zv307Xrl3p0aMHr776KnZ2dnTp0qWm4xMRERERG2bjxeNaUaWEevDgwZbv77rrLsLCwigsLNRe1CIiIiJy3avytnkXi4iIuNpxiIiIiEg9pH2or5BQv/LKK1UaoFdfffWqBiQi16eyMnNdh1AlZiOUldV1FFVnNOofuppw9ExeXYdQZW6O7vUq3lA/t7oOocrMGCgx1ZdfCHZ1HcA17bIJdY8ePWozDhERERGReumyCfVNN91Ui2GIiIiISH1UpS3jrnEaAxERERGRarBqUaKIiIiICGhRIqhCLSIiIiJSLapQi4iIiIjVtJlQFRPqkpIS5s+fz/fff8/58+f5+OOP2blzJydPnuT222+v6RhFRERERGxWlaZ8fPzxx5w4cYIRI0ZY5sk0adKE1atX12hwIiIiImLbjIba+bJlVapQb968malTp+Ls7GxJqH18fMjMzKzR4EREREREbF2VEmp7e3vKfvNosJycHDw8PGokKBERERGpH7TLRxWnfMTHx/Puu++Snp4OQFZWFjNmzCAhIaFGgxMRERERsXVVSqgHDRqEn58fo0ePJj8/nxEjRuDt7c2AAQNqOj4RERERsWGaQ/0HpnwMHjyYwYMHW6Z6qLwvIiIiIlLFhPr06dMVfi4oKLB87+/vf3UjEhEREZF6QzXWKibUI0aMuOyxuXPnXrVgRERERETqmyol1L9NmrOzs/niiy+IiIiokaBEREREROoLqx497uXlxeDBg3n66afp2rXr1Y5JREREROoJow3N+fjqq69Yu3YtBoOBJk2aMHToUIqLi5kyZQpnzpyhUaNGPPPMM7i7uwOwaNEi1q5di9FoZMiQIcTGxlp13Srt8nEpaWlpFBUVWdtdREREROSqyczMZMWKFUyYMIE333yTsrIyNm7cyOLFi4mJiWHq1KnExMSwePFiAFJSUti4cSOTJ0/m5ZdfZsaMGZWeu1JVVapQv/LKKxV29SgqKuLEiRP079/fqouKiIiIyLXB6upsDSgrK6O4uBg7OzuKi4vx9vZm0aJFjBs3DoDu3bszbtw4HnjgAZKSkkhISMDBwQE/Pz8aN27MwYMHadmy5R++bpUS6h49elT42dnZmWbNmhEQEPCHLygiIiIiYo0xY8ZYvk9MTCQxMdHys4+PD3fddRdPPvkkjo6OtG3blrZt23Lu3Dm8vb0B8Pb2JicnByivaIeHh1fon5mZaVVcv5tQl5WVkZyczOOPP46Dg4NVFxERERGRa1NtTqGeMGHCZY/l5uaSlJTEtGnTcHV1ZfLkyXz77beXbW82m69aXL9bpTcajezatUsPchERERERm/XTTz/h5+eHp6cn9vb2dO7cmf3799OgQQOysrIAyMrKwtPTEwBfX18yMjIs/TMzM/Hx8bHq2lWa9nLnnXcyb948SktLrbqIiIiIiFybjAZDrXz9noYNG3LgwAGKioowm8389NNPBAUFERcXx/r16wFYv349HTt2BCAuLo6NGzdSUlJCeno6J0+eJCwszKoxuOKUjw0bNtC1a1dWrlxJdnY2y5Yts2T1F0yfPt2qC4uIiIiIXC3h4eHEx8fzwgsvYGdnR0hICImJiRQWFjJlyhTWrl1Lw4YNGTVqFABNmjShS5cujBo1CqPRyKOPPorRaN0SS4P5ChNIHn74YT7++GN279592RNERkZadWERkYvlF1+9uWw1yckeiurRh3VGY/2ZrudoB8Wmuo6ianYey67rEKosKsidn1Nz6zqMKgv1c6vrEKrMy9WO7Pz68aZt5FFz6+BeWXWgxs59sX/0DP/9RnXkihXqC7m2kmYRERERkUu7YkJ9YYePK4mOjr6qAYmIiIhI/VGPPgirMVdMqEtKSnj//fcvu62IwWDg3XffrZHARERERETqgysm1M7OzkqYRURERESuoEpPShQRERERuZSqbGl3rbvi3iBX8wkyIiIiIiLXoitWqD/55JPaikNERERE6iEVqKv4pEQREREREbk0zaEWEREREatp2zxVqEVEREREqkUVahERERGxmgGVqFWhFpHryupVK4mNbk1MRDhvTJpQ6bjZbObZZ0YQExFOpw5t2b59GwApJ05wx209iI6KJC42mmnvvG3ps2vXTm7ulkDH9m3of+/d5OTkAOUPx3rs0cF0bN+G9m0imfT6a7Vyj3Vl9aqVtIlqRVTrMCa9fumxHTVyBFGtw+jYrg3bt22r0DeideW+Dwy6n84dYuncIZZWYSF07hBb4ZzHjx+noZc7Uya/UXM3VscauNjTpokHbZt6EODlVOl4gJcT0cEeRAd7ENPEg06hDbD75TP42KaeuDjaER3sQVSQu6VPmL+rpU9sU0+igz0AMAChjVyI+eWYh/O1XXdb+/UqurSPolPbCKZOfr3S8flzZ9O9S3u6d2lPr8RuJP+0s0LfqMiIy/adNnUyfp6OZGScBcp/Hwx//BG6x7fjhrgY3n5zYs3dmNS6a/u/FBGRi5hMJkY9PZyly1cTFBzMjQmduLP33URERFrarFq5goMHD7Jr936SNv/IyKeGsn7DJuzs7Rk/8Q3iO7bnbNZ5usbH0SPxViIiIhn2xGOMnzCJG7t15+OPPuStyZN4Zdw/WbjgC4qLikjatov8/Hw6xEZx331/ollISN0NQg0xmUyMHDGMZSu+Jig4mK7xHend+24iIiuO7aGDB0jec4DNP/7IiOFP8t3GHy19V6/+Gr+Ain1nzZ5r6f/Cc6Np0KBBhes+/+wz3Hb7HbV2n3UhpJELe9PyKC4tIyrYg+y8EgpKyizHT2YXcTK7CAAvV3saezlhKvt129uCYhM/p+ZWOOfB0/mW75v6Olva+3k6AvBTynns7Qy0DnAjOaVi32uFyWTihdFP88WXywkMCua2m7rQs1dvWrX+9T3bNKQ5Xy7/Bi9vb75ZvZJnRwxl5f++t/RdtXoV7j4Blfqmppxg/dpvCG7S1HKuJYvmU1xUxPpN28nPz+fGTm25t//9NG0WUtu3ftVpDrUq1CJyHdmStJnQFmE0Dw3F0dGR/vfdz1dLv6zQZtnSLxn0wIMYDAY6dY7nXHY2J0+eJCAggHbt2gPg4eFBq9YRpKWmAnBg/z663tgNgFtuuZUvFy0EwGAwkJeXR2lpKQUFBTg6OOLh6VmLd1x7kjZvpsVFYzvg/oGVxvarJV8y6IGHMBgMdI6P59y58rG90Df0Cn3NZjML5s/jvvv/ZHltyZeLad48lMjIqFq5x7rg7mRHYUkZRaVlmIHM3GK83Rwu297X3ZGM8yV/6Bo+7o6czS3v4+Jox7mCUgBKTWZKTWbcnOysjt+WbduSRPPQFoQ0L3/f3dvvPlYuW1qhTafOXfDy9gagQ8fOpKWlVuh74T37275jX3yWV/45HsNF+8kZDAby88t/HxQWFODg4ICHx7X5++B6pIRaRK4baWmpBDcJtvwcFBTMyV+S4l/bpBEc3MTyc2BQMCfTKrY5dvQoO3dup2OnzgBERkWzbOkSABYu+IKUlBMA3Nu3P25ubrRoFkjrsGY8/cxofHx8auTe6lpaWmqFcQsKCia10thWbpOWmlqlvt9v+A5/P3/CwsMByMvL481JE3l57N9r4nZshqO9keLSX6vRxaVlONhf+p9uo6G8Qp2Z92tCbcb8y5QPdxp5OFbq4+FsR0lpGUW/VLzzikyWhN3J3oibkz2Ol7lefXfqZCpBwb/+PggIDOJkWtpl23/26UxuubXn7/ZduXwpAQFBRMe0rdD/rj79cHV1Iya8Ke2jWjB0xCi8r5HfB0ZD7XzZsmvzvxIRkUu41NNfDb95IsHvtcnNzWXQwP68/sYUPH+pNk//9wz+/f573BAfR27ueRwdyxOXLUmbMdrZcfBoKj/vO8zUtyZz5PDhq3lLNqM6Y1uVvvM+n8OAgb9Wp//56t956ulncHd3/23X65aXqwPnC00VpnvsTs2loNjE3pN5+DdwwsO5YrXZ192RjNxfE/Az54spLi0jOtiDZg1dyC0shWv0qclVed9dsOHbdcz+ZCZjXx1/xb75+fm8NWkCL7xc+Q+9bVuTMNrZsWv/MZJ+2s/0d6Zw9Mi1+fvgeqSEWuqtzZs3k5KSYvl57ty57Nq1C4Bly5ZRVFRU7WvMmzePJUuWVDp/dRw9epRtFy3Gqqpx48Zx6NChyx5/8MEHqxMWUDm2LVu2sHjxYuDyY3G1xro2BAUFk3Li1/dMamoKjQMDf9MmyFJhBkhLTaFxQHmbkpISBvTvz/0DB3FPn76WNq1at2bp8lV8v2kLA+77E81DWwAw7/PZ3HpbTxwcHPDz8yM+IYFt27bU5C3WmaCg4ArjlpqaQmClsa3cJiAw8Hf7lpaW8uXihfQfcL/ltaTNP/Lyi8/TKiyEd6e+xaQJ45k+7d2auLU6VVxaVqFC7GhvpOSiivXFfN0dyMgtrvBaiak88Ss1mcnKK8HNqeLSKR83BzJ/0+d4RiHJKefZfyoPOzsDhSWXvl59FxAYTOpF/4acTEulcUBApXY/J+/imeFP8MmcBfj4+l6x79Ejhzh+7Cg33xBHh+hw0lJTSLyxM6dPn2LhvM/pkXgbDg4ONGrkR6f4BHZu31rzN1oLDAZDrXzZMiXUUm8lJSVVSKjvv/9+2rRpA8Dy5cuvepJ38fkvVlb2x/6xOXr0KNu3b79aYV1Vv40tLi6OPn36VGpX02NdUzrEdeTQwQMcPXKE4uJi5s+by529767Q5s7edzN71qeYzWY2/7gJzwYNCAgIwGw28+TjfyEiojUjRo6q0Cc9PR0ofy9MnPAvHn3scQCCmzZl/br/YTabycvLI+nHH2nZqnXt3Gwti+vYkYMXje0Xcz+vPLZ33c3sWZ9gNpv5cdMmPD3Lx/ZC3yOX6bv2mzW0bNWa4Is+Yv9m3XfsO3iUfQePMnzESJ4b8xJPDhtea/dbW3KLTDg7GHGyN2KgfL5zVl7lOdJ2RvB0sa9w7OKPyY0GaOBqT0GxyXK8gYs9BSVlFJvMl+zj6WKP2UyFBZDXknYd4jh8+CDHjpa/7xYtmEfPXr0rtEk5cZwhf76faR/MpEV4y0p9L7xnL/SNjIph9+FUtiYfYGvyAQKDglnz3Y/4+zcmqEkTNny7zvL7YGvSj4S1bFXbty01RLt8iE359ttvWbFiBaWlpYSHh/OXv/yFhx9+mF69erFt2zYcHR157rnnOH36NFu2bGH37t0sWLCA0aNHs2DBAjp06EBmZiaZmZm8+uqreHp6cuONN3L8+HEGDx4MwJo1a0hNTeXhhx++ZAwLFy5k/fr1NGzYEA8PD0JDQwGYNm0aHTp0ID4+nmHDhnHzzTezc+dObr/9dtzd3Zk3bx6lpaX4+/szdOhQnJ2dOXjwIB999BFFRUXY29szduxY5s6dS3FxMXv37uXee++lffv2fPjhh5w4cQKTycSAAQPo2LEjxcXFvPfee6SkpBAUFERxcfEl473YnDlzKoyTl5cXZ86cYfr06eTk5ODp6cnQoUNp2LAhP/zwA/Pnz8doNOLq6nrJ2IqLizl06BCPPvpohetcGIvqjPWaNWtYs2YNABMmTMCpFn4bOdnbM/Wdd+hz1+2YTCYGDxlCuzZR/Pv99wF4/IknuPuuXqxZvZw2keG4urry3xkf4mQPGzZ8z5zPPiUmJoaETu0A+Of//YtevXqxaP4cpr/3HgB97r2Xxx4dgsEAI4YP49FHHqFT+xjMZjODhwwmrl3lP8pqUm0VdRzt7HnnnXe5+86emEwmhgx5hNg2Ubz/y9g+8cQT3N27F1+vXE506zBcXV2Z8eFMHO1+7XvH7RX7XrDgi8/505/+hONl1sbZGcDewGWP14SLt6CraWYztGlSvq1diamMUD9X7O3K/48t/SUZtrczYAYiAn+Ny2AAZwc7jAZoH9KAUlMZwT7OXPizxMnBiKnMXOFeDAZwcSgfyDKzmaKSslq91wtx1Q473pk6lT/1602ZycTDg4cQ36EN//l3+Xv2r48/wQtvjic7K4OXnh0BgL29PZt+3Gzp27vXHZgu6vtbRgM0cLHDy9WOUU8P5y+PPsJNXdphNpsZMmQIN/zyu0TqP4P5UhOBROpASkoKs2bN4tlnn8Xe3p7//ve/hIeHM23aNJ5//nni4uKYNWsWLi4u9OvXr0KCC5UT3tdeew1PT08KCwt57rnnmDJlCvb29vztb3/jr3/9K02bNq0Uw+HDh5k2bRrjx48v3xbphRe49dZbufvuuyud/7bbbuOee+4hJyeHN998kxdffBFnZ2cWL15MaWkpffr0YeTIkYwcOZKwsDDy8/NxcnLiu+++q5Ckzp49m+DgYLp160ZeXh4vvfQSEydOZM2aNRw/fpyhQ4dy7NgxXnjhBf71r3/RokWLS47ffffdd8lxmjBhAvHx8dx0002sXbuWLVu28PzzzzN69GhefvllfHx8yMvLw83NjXXr1lWI7eKf582bh7Oz8yXHwpqx/q384vrxq8jJHopK6zqKqjPa+kqeizjawUUFVJu281h2XYdQZVFB7pW2zbNloX5udR1ClXm52pGdXz/etI08Lr87THW9ub525oKP7h5aK9exhirUYjOSk5M5cuQIL774IgDFxcV4enpib29Phw4dAAgNDf3D85idnZ2Jiopi27ZtBAUFYTKZLpvg7dmzh06dOuHkVP7whLi4uMueNyEhAYADBw6QkpLC2LFjgfL5ni1btiQtLQ1vb2/CwsIAcHV1veR5du3axdatW1m6dKnlvs+ePcvu3bvp1asXAM2aNaNZs2ZXvM/LjdOBAwd49tlnAejWrRufffYZAK1atWLatGl06dKFzp07X/HcVfVHxlpERORaoYRabIbZbKZ79+4MGjSowutLly61LEYwGo2YTH+8GnDLLbewaNEiAgMDuemmm67YtqoLHy4k3WazmZiYGEaOHFnh+LFjx6p0HrPZzOjRoyst4Pqj7Ozs/tA4/fWvf+XAgQNs27aN559/ntdfr/ykL2v8kbEWEZH6z8bXC9YKLUoUmxETE8OmTZs4d+4cUL492ZkzZy7b3sXFhYKCgksec3Z2prCw0PJzeHg4GRkZfP/999xwww2XPWdERASbN2+muLiYgoICtm79/RXYLVu2ZN++fZw6dQqAoqIi0tLSCAoKIisri4MHDwJQUFCAyWTC2dm5Qtxt27ZlxYoVlm2Yjhw5AkBkZCQbNmwAyh+vXNUE/VLxbdy4EYANGzbQunX5orhTp04RHh7O/fffj4eHBxkZGZViqwprx1pERORaoQq12Izg4GAGDhzI//3f/2E2m7Gzs6u0GO5iCQkJ/Pvf/2bFihWMGlVx14XExETGjx+Pt7c3f/97+X6gXbp04ejRo1fctzY0NJSEhASee+45GjVqZEk+r8TT05Nhw4bx9ttvU1JSvsJ+4MCBBAYGMnLkSGbOnElxcTGOjo6MHTuW6OhovvzyS5577jnuvfde+vfvz0cffWSZltGoUSPGjBnDbbfdxnvvvcezzz5LSEiIZerIHzVkyBCmT5/OkiVLLIsSAWbNmsXJkycBiI6OplmzZjRs2LBCbFVh7ViLiMi1wagStRYlyvVjwoQJ3HnnncTExNR1KNc8a8ZaixJrhhYl1gwtSqw5WpRYM2pyUeJb3x2psXNfbOSNzWvlOtbQlA+55uXl5fH000/j6OioZLqGaaxFRK4/evS4pnzIdcDNzY233367wmvnz5/nH//4R6W2r7zyCh4eHrUVmlVeeukly9SSC5566imb2E3jUmMtIiJyrVNCLdclDw8PJk2aVNdhWGX8+PF1HYKIiIiFplBryoeIiIiISLWoQi0iIiIiVjOiErUq1CIiIiIi1aAKtYiIiIhYTXOoVaEWEREREakWJdQiIiIiItWgKR8iIiIiYjVbf+hKbVCFWkRERESkGlShFhERERGrGbUqURVqEREREZHqUIVaRERERKymArUq1CIiIiIi1aIKtYiIiIhYTXOoVaEWEREREakWVahFRERExGoqUKtCLSIiIiJSLapQi4iIiIjVVJ3VGIiIiIiIVIsq1CIiIiJiNYMmUatCLSIiIiJSHUqoRURERESqQVM+RERERMRqmvChCrWIiIiISLWoQi0iIiIiVtOjx1WhFhERERGpFlWoRURERMRqqk+rQi0iIiIiUi2qUIuIiIiI1WxpCnVeXh7vv/8+J06cwGAw8OSTTxIYGMiUKVM4c+YMjRo14plnnsHd3R2ARYsWsXbtWoxGI0OGDCE2Ntaq6yqhFhEREZFrwsyZM4mNjWX06NGUlpZSVFTEokWLiImJoU+fPixevJjFixfzwAMPkJKSwsaNG5k8eTJZWVn885//5O2338Zo/OMTODTlQ0RERESsZjAYauXr9+Tn57Nnzx569OgBgL29PW5ubiQlJdG9e3cAunfvTlJSEgBJSUkkJCTg4OCAn58fjRs35uDBg1aNgSrUIiIiIlIvjBkzxvJ9YmIiiYmJlp/T09Px9PTkvffe49ixY4SGhjJ48GDOnTuHt7c3AN7e3uTk5ACQmZlJeHi4pb+Pjw+ZmZlWxaWEWkRERESsVpvTHSZMmHDZYyaTiSNHjvDII48QHh7OzJkzWbx48WXbm83mqxaXpnyIiIiISL3n6+uLr6+vpeocHx/PkSNHaNCgAVlZWQBkZWXh6elpaZ+RkWHpn5mZiY+Pj1XXVoVaRGxC2VWsFNQsQz2KFYzaIbZGhPq51XUIVebkYKxX8Tbt9kxdh1BlGz57nq5/fr2uw6iSgu3v1ti5qzK/uTZ4eXnh6+tLWloagYGB/PTTTwQHBxMcHMz69evp06cP69evp2PHjgDExcUxdepUevfuTVZWFidPniQsLMyqayuhFhEREZFrwiOPPMLUqVMpLS3Fz8+PoUOHYjabmTJlCmvXrqVhw4aMGjUKgCZNmtClSxdGjRqF0Wjk0UcftWqHD1BCLSIiIiLXiJCQkEvOs37llVcu2b5v37707du32tdVQi0iIiIiVrONCR91S4sSRURERESqQRVqEREREbGarSxKrEuqUIuIiIiIVIMq1CIiIiJiNVVnNQYiIiIiItWiCrWIiIiIWE1zqFWhFhERERGpFlWoRURERMRqqk+rQi0iIiIiUi2qUIuIiIiI1TSFWhVqEREREZFqUYVaRERERKxm1CxqVahFRERERKpDCbWIiIiISDVoyoeIiIiIWE2LElWhFhERERGpFlWoRURERMRqBi1KVIVaRERERKQ6VKEWEREREatpDrUq1CIiIiIi1aIKtYiIiIhYTQ92UYVaRERERKRaVKEWEREREatpDrUq1CIiIiIi1aIKtYiIiIhYTRVqVahFRERERKpFFWoRERERsZqelKgKtYiIiIhItSihFhERERGpBk35EBERERGrGTXjQxVqEREREZHqUIVaRERERKymRYmqUIuIiIiIVIsSahG5rny9eiXtYiJoG9mSNydNrHTcbDbz3KinaRvZkvi4WHZY+k6cAAAgAElEQVRs32Y59uRfHyWgsT+d2rep0OfhBwaS0Kk9CZ3aE9UylIRO7QHIyMig12230NjXk9Ejn6rZG7MBq1etpE1UK6JahzHp9QmVjpvNZkaNHEFU6zA6tmvD9m3bKvSNaF2574svPEfb6NZ0bNeG+/rfS3Z2NlA+tj0Tb6ahlzsjRwyv+ZurQ2u/XkWX9lF0ahvB1MmvVzpuNpt56bln6NQ2gu5d2rNrx3bLsXPZ2dx/3wASOkRzQ1wMST9uAmDJovnc2Kkt/g2c2LFta4Xzvf3mRDq1jaBL+yjWrlldszdXx25NiGDnorEkf/l3nh1ya6XjXh4uzH3zMTbPfZHvPn2WyBYBlmPv//3PtGkZxJYvXqrQJ6ZlEOs+Hk3SvJeY/9bjeLg5AzDwjjg2fT7G8pW3dSptWgbV7A3WEoOhdr5smRJqEblumEwmRj/9FAu/XEbSjmTmz/ucvXt2V2izetUKDh08wI6f9zF12vs8M2KY5difH3yYZctXVDrvx7M+Z+PmbWzcvI277+3L3ffcC4CzszN/+/ur/GtC5SToWmMymRg5YhhfLl3B9l27+eLzOezZXXFsV60sH9vkPQd4d/p/GDH8yQp9ly2v3PeWxFvZuiOZpO27CA9vyaSJrwHlY/vKuH/y2sQ3avdGa5nJZOKF0U8zZ8FSNiTtZOH8uezbW3Fcv1m9ksOHDvLjjt28+fZ0nn/m1z8wXn5hFD179mTj1mT+t3ErLVu1BqB1ZBQzP5tHlxturHCufXt3s2jBPL7bvIPPF37FC6NGYDKZav5G64DRaOCtMfdxz/D3aNfv/xhwewdahzau0Ob5R3uyc18Kne5/jUfHfsobz/W3HPt06SYOHE+vdN7prwzib1O/pON941nyv5088/AtAHy+YgvxAycQP3ACj/7tE46lZbJrf2rN3qTUGiXUInLd2JK0mdAWLWgeGoqjoyP9BtzPV0uXVGizbOkS/vTnBzEYDHTqHE92djanTp4EoOuN3fDx8bns+c1mM4vmf0H/+wcC4ObmRsINXXFycq65m7IRSZs306JFmGVsB9w/kK+WflmhzVdLvmTQAw9hMBjoHB/PuXPZnDx50tI39BJ9E2+9DXv78uU+nTrHk5qSApSP7Q1du+LsfG2P7bYtSTQPbUFI8/KxubfffaxctrRCmxXLl3Lfn/6MwWAgrlNnzp3L5vSpk5zPyWHTxg0MeeRRABwdHWng5QVAy1YRhIW3qnS9lcuWcm+/+3BycqJZSHOah7Zg25akmr/ROtAxOoRDJ85yNDWDklITX6zaRu+bKn761Dq0Mes27wNg/9HTNAv0wc/HA4Dvtx3CZCqrdN7wZn5s2HoQgLWb9tLnlthKbe67vQPzVm6t9Hp9Zail/9kyJdQict04mZZKUHATy89BQUGcTKtYIUqr1CaYtLSqVZG+3/Adfv7+hIWFX52A65G0tFSCfzNuqamVx/a3bdJSU6vUF+CTjz6k5+131ED0tuvUyVSCgoMtPwcEBnEyLa1im7Q0Ai8av8CgYE6mpXH06GF8fRvyl0cfoUfXjjwz/HHy8vKueL2TaWkEBv16vcCgIE6dvDarqIF+DUg5nWX5OfV0FkGNGlRo89P+VO75JSGOi2pG0wAfgvy9rnje3YdO0vumGAD63tqeYH/vSm3639aeeSu3VPcWxIYooRaR64bZbK70muE3E/Oq0uZy5s/7nP73DbQuuHquOmNblb4TX/sXdvb2DBz052pGWr9UZ1xNpSZ27dzO448/wdoNSbi6uvHOJeZg/965bH7yqpUuVfH87d2/MfNrvDxc2fT5GJ4c2J2d+1IovURV+mKPj/uMx+/rxvefPY+7qxPFJRWnzHSMbkZ+YQm7D52s7i3YDKOhdr5smbbNE6kjn376Kdu3b6ddu3Y8+OCDlY5v3ryZwMBAgn+pTo0bN44HH3yQFi1aXPVY0tPTmThxIm+++eZVP7ctCQwKJjXlhOXn1NRUGgcEVmgTVKlNCgG/aXMppaWlLPlyEd9tvDY/Hv89QUHBpPxm3AIDK4/tb9sEBAZSXFx8xb6zPvmY5cu+YsXqb6r8x821IiAw2DLNBco/ZWkcEFCxTVAQaReNX1pqSnkbg4HAoGA6de5Mdr6Ju/r0ZerkSVe8XmBQEGmpv14vLTWVxo1///1fH6WmZ1eoHgf5e5N25lyFNufzCnl83CzLz3uXvcrR1Iwrnnf/0dPcNXQaAGFN/bjjxqgKxwf07KDq9DVIFWqROrJmzRomTpx4yWQaICkpiZSL/iGV6usQ15FDBw9y9MgRiouLWfDFXO7sfVeFNr1638Wczz7FbDaz+cdNNGjQoFICcyn/W7uGli1bV/h4/noS17EjBw8esIztF3M/587ed1doc+dddzN71ieYzWZ+3LQJT88GBAQEWPoeuUTf1atW8uYbE5m/aAmurq51cWt1ql2HOA4fPsixo+Vjs2jBPHr26l2hze139GbenM8wm81s2fwjnp4N8G8cgL9/YwKDgtm3r3wO8Lfr1tKydcQVr9ezV28WLZhHUVERx44e4fDhg7SP61hj91eXtvx8jLCmjWgW6IuDvR0DerZn2bpdFdo0cHfBwd4OgCH3JrBh20HO5xVe8byNvN2B8k8JxjzWkw/mb7AcMxgM9L21HV+sunbmT4PmUAMYzJf8fEfk2pOens5rr71Gq1at2L9/Pz4+Pjz//POkpaXxwQcfUFRUhL+/P08++STu7u6MGzeOsLAwfv75Z/Lz83niiSeIiIigrKyMzz77jN27d1NSUkLPnj259dbK2y1B+cens2bNYseOHQD069ePhIQEJk6cyLZt22jatCn33nsvCQkJFfrt27ePCRMm4OrqiqurK6NHj+b999+vdjyFhYW8/vrr5OXlUVpaysCBA+nYsSPp6emMHz+esLAwjh49SkBAAMOHD8fJyYmffvqJTz/9FJPJRIsWLXjsscdITk7mf//7H6NGjQLg559/ZunSpYwZM4adO3cyb948SktL8ff3Z+jQoZdcOLZmzRrWrFkDwIQJEzCV1c6vouXLlzN61DOYTCYGDxnCSy+9zL/ffx+Ax594ArPZzIinhrNq1SpcXV3574wPiYuLA+DPgwaxfv06zp49i7+/P3//+zgeebR8wdcjQ4bQuXNnHn/iiQrXaxHanJycHIqLi/Hy8mLFylVERkbWyr1C1aerXA3Lly9n1DMjMZlMDBnyCC+9/DLv/zK2T/wytk8NH86qVStxdXVlxoczLWN7qb4ALcPDKCoqwtfXF4DOneOZ/ss5Q5uHVBjblatW19rYmsqu/LH/1bRi+XJGjx5FmcnEw4OH8OJLL/Gff5ePwV8fLx/Xp0c8xepVq3BxdeW//51Bh1/GdceOHTz5+F8pKi6mefPm/HfGh3h7e7N48SKeefppzpw5g5eXF23btmXZipUAvDZ+PB9/NBM7e3vefHMyt99Ru/PWd+2rvUKCp7szTfy9MRjgbHYep87m0PCXhPhsVi5uLo6EBJa/9wqKSjiWlmH5XdU8yBcvT1cMQEmpibQz58jIzsPPx8OSVGefzyc1/deqt7urE0F+Xuw7errW7vGCDpFNa+zc3+3P+v1GV8GNLSvPR7cVSqjlupGens6IESOYMGECISEhTJ48mbi4OJYsWcIjjzxCZGQkc+fOpaCggMGDBzNu3DhCQ0N56KGH2LZtG8uWLWPs2LGsWbOGc+fO0a9fP0pKShg7diyjRo3Cz8+v0jU3bdrE119/zcsvv0xOTg4vvvgi48ePx9vbmwcffJBPP/30svFOmzaNDh06EB8fD3BV4jGZTBQVFeHq6kpOTg4vv/wyU6dO5cyZMwwfPpx//OMftG7dmvfee4/g4GBuv/12nn76acaOHUtgYCDvvvsuzZs35/bbb2f48OFMmTIFZ2dnPvjgA1q1akVsbCxvvvkmL774Is7OzixevJjS0lL69+9fKZbfyi2qvQSlOlwcDBSU1J9fm/Z29eeDSEc7KK4nO7SdLyip6xCqzMvVjuz8ejKwQNNuz9R1CFW24bPn6frn+rEtZsH2d2vs3EqoNYdarjN+fn6EhIQAEBoayunTp8nLy7NUtbp3786UKVMs7Tt16mRpm55evt/ozp07OX78OJs2lT8gIT8/n5MnT14ygd27dy833HADRqMRLy8vIiMjOXTokKUq90dVNx6z2cycOXPYs2cPBoOBzMxMzp0rr574+vrSunX5HrXdunVj+fLltGnTBj8/P8t81u7du7Nq1SruvPNOYmNj2bp1K/Hx8Wzbto0HHniA3bt3k5KSwtixY4HyecUtW7a06l5FRKR+uM6WNlySEmq5rjg4OFi+NxqNv7uF1IX2RqORsl8+4jWbzQwZMoTY2Mp7i9a06sazYcMGcnJymDBhAvb29gwbNozi4mKg8tSA35sqkJCQwKpVq3B3d6dFixa4uLhgNpuJiYlh5MiR1tyeiIhIvVR/PgsUqQGurq64u7uzZ88eAL799lsiIq68aCc2NpbVq1dTWloKQFpaGoWFl16kEhERwQ8//EBZWRk5OTns2bOHsLCwKsXm4uJCQUHB77b7I/Hk5+fToEED7O3tSU5O5syZM5ZjZ8+eZf/+/UB54t26dWsCAwNJT0/n1KlTQPn4XKjmR0VFceTIEb755hvLHPCWLVuyb98+S/uioiLSfrNnroiIXFsMtfRly1ShluvesGHDLIsS/fz8GDp06BXb9+jRg/T0dF544QUAPD09ee655y7ZtlOnTuzfv99y/IEHHsDL68oPBbggISGBf//736xYscKy+K+68XTt2pWJEycyZswYQkJCCAoKshwLCgpi3bp1/Oc//6Fx48bcdtttODo6MnToUCZPnmxZlHhhwaPRaKR9+/asW7eOYcOGWa49bNgw3n77bUpKyueYDhw4sNL2aSIiItcSLUoUEZugRYk1Q4sSa4YWJdYcLUqsGTW5KPGHg9k1du6LdQmrWkGqLtSf37QiIiIiIjZIUz5EroLjx4/zzjvvVHjNwcGB8ePH/27fhQsX8sMPP1R4rUuXLvTt27dO4hEREfkjbH1+c23QlA8RsQma8lEzNOWjZmjKR83RlI+aUZNTPjbV0pSPeBue8qEKtYiIiIhYTyVqzaEWEREREakOVahFRERExGoGlahVoRYRERERqQ5VqEVERETEagYbK1CXlZUxZswYfHx8GDNmDLm5uUyZMoUzZ87QqFEjnnnmGdzd3QFYtGgRa9euxWg0MmTIEGJjY626pirUIiIiInLNWL58eYUnAS9evJiYmBimTp1KTEwMixcvBiAlJYWNGzcyefJkXn75ZWbMmEFZmXU7TimhFhEREZFrQkZGBtu2beOWW26xvJaUlET37t0B6N69O0lJSZbXExIScHBwwM/Pj8aNG3Pw4EGrrqspHyIiIiJitdqc8TFmzBjL94mJiSQmJlY4/tFHH/HAAw9QUFBgee3cuXN4e3sD4O3tTU5ODgCZmZmEh4db2vn4+JCZmWlVXEqoRURERKRemDBhwmWPbd26lQYNGhAaGsrPP//8u+e6ms82VEItIiIiItazkUWJ+/btY8uWLWzfvp3i4mIKCgqYOnUqDRo0ICsrC29vb7KysvD09ATA19eXjIwMS//MzEx8fHysurbmUIuIiIhIvTdo0CDef/99pk2bxsiRI4mOjmbEiBHExcWxfv16ANavX0/Hjh0BiIuLY+PGjZSUlJCens7JkycJCwuz6tqqUIuIiIiI1Wz9wS59+vRhypQprF27loYNGzJq1CgAmjRpQpcuXRg1ahRGo5FHH30Uo9G6WrMSahERERG5pkRFRREVFQWAh4cHr7zyyiXb9e3bl759+1b7ekqoRURERMRqtvZgl7qgOdQiIiIiItWgCrWIiIiIWE0FalWoRURERESqRRVqEREREbGeStSqUIuIiIiIVIcq1CIiIiJiNVvfh7o2qEItIiIiIlINSqhFRERERKpBUz5ERERExGp6sIsq1CIiIiIi1aIKtYiIiIhYTQVqVahFRERERKpFFWoRERERsZ5K1KpQi4iIiIhUhyrUIiIiImI1PdhFFWoRERERkWpRhVpERERErKZ9qFWhFhERERGpFlWoRURERMRqKlCDwWw2m+s6CBERERGpn5JTcmvlOtHB7rVyHWtoyoeIyB8wZsyYug7hmqWxrRka15qjsf2FoZa+bJgSahERERGRalBCLSIiIiJSDVqUKCLyByQmJtZ1CNcsjW3N0LjWHI1tOT3YRYsSRURERKQafk7Nq5XrRAW51cp1rKEKtYiIiIhYTQ920RxqEREREZFqUYVaRERERKymArUq1CIiIiIi1aIKtYiIiNQps9mMQRNx6y/9X6cKtYiI2B5b2YDqQhzZ2dnk5+dXel2q5/Tp0wBKpqXeU4VaRETq1IXqZFpaGgBeXl64urraRNXSYDCwZcsWPv30U8LDw/Hx8WHQoEEYDAabiK8+y83NZenSpdx22200bdpU4/k7LozPvn37OHfuHK6urkRGRmI01n1tVPtQq0ItIiJ1zGAwsH37dt544w3WrVvHyJEjyczMrNPk6kIFuqioiIMHD/LYY49x7733cvjwYT766CNL3KpUW8/Ozo68vDySk5MBVal/z4U/7j766CNOnTrFggULWLt2bV2HJb9QQi0iInXq1KlTLF68mDFjxtCqVStcXFxwcHCwHK+LpNVgMJCcnMycOXPIyMggMDCQoKAgHn/8cVJTU/nggw8s7eSPSU9PJz09HRcXF/r06UNSUhJHjx6t67BsmtlsprCwkG+//ZaxY8fi4+NDcXExcXFxlJWV1XV4GAy182XLlFCLiEid8vT0pEuXLiQnJzN//nxefPFFPDw82Lp1K0VFRXWStB4+fJi5c+fi4OBASkoKO3bsIDs7m0aNGvHYY4+RmppKSkpKrcdVHxUXF1u+P3HiBJs3b2bixIls3rwZs9lMbGysZX66LSSHtujcuXM4Ozvj6OjIwoUL+frrr3nqqafw8vLip59+0h8kNsBu3Lhx4+o6CBERuX5cmAu6Z88eUlNTadCgAbNnz2b79u1MmjQJT09PDhw4wOeff05UVBSenp61Gl9KSgpTp07lvvvuo2fPnri7u7Nz505MJhPe3t74+vqSkJCAt7d3rcZVX/3444/s3r2bwsJCZs2axZ/+9CcCAwPZvXs3Bw4cYM2aNZw4cYLOnTvj6OhY1+HanJycHN577z3Cw8MpKSnh66+/5qGHHqJFixbs3r2bGTNm0Llz5zp9P2bmlmCAGv/ydf/1kytbo4RaRERq1YW5oLNnz6ZNmzY0a9aMFi1aWOaD7t69my+//JJ+/frRunXrWonpQpJ/oUL6ww8/cPjwYW666SaCg4MB2LRpE2azmaZNm1aYkiKXlpOTQ2FhIS1atOCtt95i3bp1DBs2jMDAQAIDA2nVqhWtW7empKQEk8mEm5sbgYGBWpz4G8XFxaSmpuLl5UVsbCylpaWsXLmSjIwMlixZwoMPPkhkZGSdxpiRW1Ir11FCLSIi17XS0lLLbgT5+fnMmjWLhx56iMjISEpLS/Hx8aFt27acPn0as9lMYmIisbGxtZJcXbjGli1b+PHHH2nTpg3x8fFs376d7du307lzZ4KCgjAajTRt2hQfH58ajedaYDKZWLZsGY0bN8bDw4OioiKysrKws7MjKioKs9mMg4MDzs7OREVFcfbsWU6fPk10dLSS6V+cPXsWV1dXnJycOHPmDAsWLODmm2+mbdu2+Pn50bBhQ7p27Up0dHSd/xGihFpzqEVEpIbl5uby9ttvU1hYCJQnW/n5+ZW2+/Lz8+Oee+6hb9++REVFAbWz6O/CLiNz586lZcuW2NnZ4eHhwZNPPonZbOaNN94AID4+npCQkBqPp747f/48OTk59O/fH2dnZ5YsWUJCQgITJ05k+/btfPzxxxgMBg4ePMiePXuws7PD1dWVPXv2UFRUpJ1TKK/uz549m9dff52srCwSExPp0qUL69evp6ysjLZt2xIbG0tYWBhgA4tja2O+h43/naUKtYiI1JjS0lKcnZ1p1aoVubm5ZGRk4O/vT1ZWFqdPn6Zhw4Z4eHiwd+9ePvnkE6KionB2dq7xBCE9PZ3k5GSCg4MpKytj4cKF3HrrrURGRrJr1y6++eYbHBwcuPHGG0lKSiI4OBgvL68ajelaYDKZ+PLLL9m7dy8NGzYE4LvvviMzM5OQkBC6devGwoUL2b9/P4sXLyY2NpbGjRuTkZFBt27d8PX1rfvksI5cqDIXFhbi5uZGVFQUKSkpbN++nS1btmAymSgoKCA2NrauQ60kI08VaiXUIiJSI3JycpgxYwZ+fn4EBgbyv//9jw8//JB27drh7+/P4cOHWbVqFZmZmcybN49+/foRGhpaKwlVeno6zs7OlmkHx48fZ+/evaxevZqysjLOnj1LTk4O8fHxdOnSRdM8qiA7OxsADw8Pyy4orVq1Ijo6mi1btpCamkrLli3p0aMHBoOBxMREIiMjMZvNBAcH06BBgzq+g7plMBjYuXMnn332Gfv37ycnJ4d77rmHFi1aWBYj7ty5ky5duuDh4WFTf3hk5pViqIX/KaEWEZHrjpOTE3v27CE5OZmAgAA6deoEwNy5c7nxxhtp3749np6e2Nvb06NHD2JiYmotNm9vb9zc3HjllVdwdHQkMTERNzc34uPjLZXSb775hg4dOuDi4mJTyYstKi4uZsOGDXh7e9O0aVMaNWrE/v37OXr0KOHh4URERLBt2zaOHDlCUFAQrVq1qvBHisa3fKvGadOmMWDAANzc3Dh69KhlDn+LFi1o164dPXv2JDg42ObGKzOvtFauo4RaRESuKyaTCaPRiJeXF+vXr2fTpk1ERETQsWNHCgoKmDdvHhEREURHR9O8eXPL9ICaduFj9ezsbNzd3QkMDGThwoU4OjoSHx+Pl5cXO3bs4L///S8DBw6kefPmNpe82CI7OztLojdnzhxiYmJo0qQJBw4c4NixY4SHh9OyZUu2bt1KREQE7u7ulr7X+/heeE+mpKRgb29vSZoDAwPZtWsXvr6++Pr64uHhYdlCsq4XIf5WVn5prTzYxcfNdhNqLUoUEZGrzs7Ojl27dvHBBx/Qt29fvL29+fTTTzlx4gT33HMPXbp0YcqUKRQUFNTaIrQLSci2bduYNGkS6enpREdH89BDD7FixQq++eYbAI4fP87gwYNp3759rcRVnxUXF3Pq1CmgfPHpoUOHsLe3Z8mSJbi7u3PrrbdSUlLCsmXLsLe357HHHqNx48Z1HLVtKC0tr+peSIzd3d3ZuHEje/fuxd7enoCAAMrKyjh//nylvraUTEs5VahFROSqKisrw2AwsGrVKpo0aUJiYiJdu3bl8OHDLFu2jFatWhEfH0+HDh3w8vKq8eTgQjwXdpb46KOPePjhhy07djRq1IgmTZrw2Wef4erqSs+ePZX0VVFKSgpbtmxhx44drFmzhp49e9KwYUNOnTrFTz/9RNu2bQkMDOTgwYOEhIRoYSeQlZWFi4sLRqORHTt28NVXX3H27Fl8fX1p2bIlS5cuxcXFhaKiItauXcsNN9xg83P4s/JKa2WTD1uuUCuhFhGRq+rCY5KzsrI4e/YsYWFhODo6Ehsby1dffcWpU6eIjY3F09OzxpPp7Oxsdu7cSWBgIAaDgdTUVOzt7bn55pspLi62JNoNGzakadOmuLu706hRoxqN6Vpib2/Pxo0bWbt2LV27dqVdu3Z4e3vj7OzMmTNn2LZtG+3ataNdu3ZKpin/4+6jjz7i22+/JSgoiDlz5hAaGsq5c+fYtGkT/v7+REVFsXTpUg4fPswdd9xRq2sLrJVVS3OolVCLiMh14dy5c7z00kv4+/vTunVr1q1bh5OTE46OjmRnZ3PmzBnuuOMO/P39a+Vj63379hEYGIjRaKS4uJisrCyWLFnCzTffjJOTEwaDgd27d3PixAnatm2rZLqKLkyfMRqNuLu74+joSFFREcXFxQQFBeHj44PRaCQnJwd/f3+br7DWFoPBQKtWrUhOTmblypX07t2bnj17EhoaipubGzt27OCuu+4iISGBLl260LRpU5ubL30pWfmltbIPtRJqERG5Zl2YUgHg7OyMt7c3s2fPJioqivbt2/Pdd9+RlJTEN998w5133ml5aEttxHRh6saCBQs4e/YsXbt2JTs7m88//5ywsDCOHDnCjBkzaN++vaZ5VNHFc9GnT5/OPffcQ3h4ONnZ2ezZswcXFxfKyso4ceIEN9xwA35+fnUdsk1xcnKy7HqyZ88eEhMTcXR0xMnJic2bNxMZGYmHhwd2dnZA/ZgvnZWvCrUSahERsUphYSH29vYYDAaOHz9ueSBF06ZN8fX15T//+Q9xcXHcfvvtREdH07FjR8LDw2s8ruLiYg4fPoyTkxMpKSlkZWXh4ODAiRMnyMjIoFevXuTl5bFx40b2799Pv379aNeuXY3Hda0wGAwkJyczc+ZMBg8eTEBAAM7OzgQGBnL+/Hm+//575s6dy80330zTpk3rOlybcPHuMvn5+Xh5edGpUyd27NhBUlIS7du3Jycnh9WrV9OpUyfLbh71RXa+qVb2ofZ2s6/rW70sg1nP+BQRkT8oNzeXdevW0a1bNzw9PfnPf/5Dfn4+gwYNomHDhhiNRubNm8eKFSsYPnw4HTp0qJW4LuycsHbtWpKTk9m/fz/jxo2jcePGbNiwgf379xMSEsJNN91kmQbi6OhYLz5WtxVms5mvv/4aNzc3y/SFFStW0L17d7p160ZBQQG5ubk0b968rkO1KVu2bGHBggV4enoSGBjIww8/TG5uLpMmTSIlJYX4+Hg6depE27Zt6zrUP+zwmcJauU5oI+dauY41VKEWEZE/rLS0FH9/f8xms2Xx1O7du9m1axfNmjXD3d2d4uJiSktLCQsLq5WP/XNzc5k7dy7t27fn/PnzLFy4kMzPJ7gAACAASURBVA4dOhAbG4uLiwtNmzaloKCA3bt3k5GRQUhICHZ2dpaFiXJ5F//BYTAYyMnJYc6cOSQnJ9OoUSNiYmL47rvviIyMJCAgAG9v7zqO2Lb8/PPPzJ49mzFjxmAymZg/fz7nzp0jPj6ejh07curUKW6++Waio6PrOlSrZBfUzj7U3q6qUIuIyDXiQnJVVlbG0qVLSU1N5aabbiIyMpIZM2aQn59Po0aN2Lp1K8OGDSMkJKRWKsC5ubnk5+djMBjIysrC0fH/2bvzuKrL9P/jr3PYDvu+I5uI4IKK4K6loqaZmqmlpmlNU1OZ0zRN9atm+VZTU2rjzJiVlabjmua+ooJmiiKI7LIpiIhsIsJhO8vvD+ec0XYbOQfhevbwUeg5nYsPB3yf+1z3dVtz+vRpNBoNsbGxdO3alcbGRk6ePElYWBgBAQFtWk9HYfjapaenc/r0aVQqFSNHjsTZ2ZmWlhZcXFyorq5m0aJFLFiwAD8/P3OX3O4UFhYCNzbtbt26lSeffJJFixbRrVs3nnnmGfR6PZaW7Tcs/pTzVaZZoQ7xkBVqIYQQHYAhXDU1NWFlZUVYWBg1NTVkZ2ejUqkYN24cWq2W5uZmhg8fTmRkJGCajVWGSRPp6ens3r2bIUOGEBISwrlz56isrOT8+fMcPHiQ8ePHy0a526BQKMjKymL16tXExMSgUCj48MMPCQsLIzg4mOPHj/Ppp5/y0EMPERERYe5y2yU3NzdcXFzYtm0bw4YNo3fv3jQ1NZGcnExMTMxd1zP9bbUm2pTYnleo229lQggh2h3DdIc9e/bg5eWFl5cXkyZNYteuXSQlJaHRaBgyZIjx9qbsTTaMIpsxYwZqtZqVK1cyd+5cxo8fT1JSEikpKdx33304OzubpJ6OxDCxY/To0QD4+fmxZs0agoODcXR05LHHHqNXr17Si/4jlEol/v7+5ObmotVqKSws5He/+12HmC4jX3EJ1EIIIW5Dbm4uGzduZMGCBRw/fpxjx44xZcoURo0aRWJiIqmpqWi1WuPUDFOFq8uXLxMfH8+0adMIDAzE0dERgLVr1zJjxgwmTpxIXFwcKpVKQt9PMHSC3nyNFAoFFy5cMH4cExNjbKe5+eARua4/Ljw8HK1WS3x8PJMnTyY0NNTcJXUoVVVVLFu2jNraWhQKBXFxcUyYMIH6+no++OADKisr8fT05IUXXsDBwQGArVu3cvjwYZRKJfPnz6dv376/6LGVd/ITEUII0fEYApZarUapVDJnzhwqKio4e/Ysf/jDH4AbI/TGjh2Lq6srBQUFJq1Lq9WSkJBASUkJZWVlALi6ujJs2DDCwsJYu3YtarUaa2trQELfj9FoNMZNmtnZ2SQlJVFSUkJcXBzl5eV8+umnNDU1kZubS35+Pk1Npumd7SgiIyOZOnUqr732GjExMXSYbWymOHf8Z3zbWlhYMGfOHD744APefvtt9u/fT2lpKdu2baN379784x//oHfv3mzbtg2A0tJSjh8/zpIlS3jttdf47LPP0Ol0v+gSSA+1EEKIH6VQKMjIyGDHjh0oFApWr17NhQsXeP3113FxcSEjI4P9+/czcOBAampq+OabbxgyZEibbrIyrDJnZmbS3NxMjx49aG5uprq6GpVKhbu7OyqVCh8fH/r06YOLi4sE6Z9QX1/Pyy+/TL9+/VCr1bz//vs0Nzdz7tw5tFotc+fO5eDBg2RmZnLkyBFmzpxJ9+7dzV12u2B4PhYVFVFYWIilpSW2trbf+5zTarUd7sVdbaNpeqhdfqKH2tbW1jhhxsrKioyMDPz9/dm1axfz5s3D1tYWb29v1q1bx/jx4zl06BBdunShZ8+e2Nvbc+bMGXx9fXF3d7/t2qTlQwghxI+6fPkyX3/9Nffffz9BQUFcunSJnJwcWlpayMzMZP369cyePRsAHx8fnn/+eVSqtt2Nb+jlXrt2LY8++ihBQUEMHz6cr7/+2th2EhkZiYuLS5vW0ZE4ODgwfPhw3njjDWJjY3nxxRcJDQ0lJSWFEydOoNfrjWPfrl27hpubm7TP/Ifh+bhmzRqGDh3KZ599xpNPPkl0dPQtt9PpdFhYWNDQ0EBOTg4xMTFmqvjOUpiwi/qVV14x/ndcXBxxcXHfe7uKigrOnz9PWFgY165dMwZtV1dX6urqAKipqbnlsCk3Nzdqamp+UV0SqIUQQnwvnU5HS0sLX331FSUlJVRWVhIUFMTjjz/O559/zmeffUZLSwtz5swx9kybqie0sbGR3bt389RTTxEeHo5er8ff35/hw4dz6NAhUlJSCAwMxN7e3iT13O0MwXjatGk4OTnx2WefERUVRWhoKBERESgUCo4ePUptbS0TJ040hhMJ0zeuXV1dHQkJCbz22mtUVlZy4sQJwsLCbrmNXq9HqVSiVqt57733mDFjhhmrvnu9++67P3mbpqYmFi9ezLx587Czs/vB293JlhsJ1EIIIW5x85xplUrFzJkz2bRpE+fPn8fPzw8/Pz8ef/xxNBoNOp3O+Pa1KVcrtVotjY2NxnFjGo0GKysrPD09mTx5Mg0NDRKmb4NCoTB+/caOHUtzczMff/wxAQEBBAQEEBERgU6nM44blCD9XwqFAmdnZ0JCQti1axd5eXm8/PLLODk5cerUKbp27Yq7uzsKhYKGhgaWLFnCzJkzO9SIwfb0dNBoNCxevJjhw4czcOBAAJydnbl69Squrq5cvXrV+HPD3d2d6upq431rampwc3P7RY8rmxKFEELcwvD29fvvv88nn3xCfn4+c+fOpbKykqSkJC5evAiApaWlMUwb7mcqDg4O9OrVi3379lFbW4uVlRVZWVm8/fbbWFlZyeEiv4DhRRTAAw88wNSpU/m///s/Lly4gJ2dHdHR0QQGBpq5yvbFcJgQQGtrKxkZGTzxxBN4eXlRUFDAunXrjC0ETU1NvPnmm0ybNq1Dhen2RK/X89FHH+Hv78/EiRONvx8TE8ORI0cAOHLkCLGxscbfP378OK2trVRUVHD58uVb3lm4HXJSohBCiFtcvnyZNWvWMGzYMCwtLVmxYgWzZ88mJiaGTz/9lC5dujBx4kRsbGzMWueFCxdISkrizJkzjBgxgvj4eObMmUP//v3NWtfd4OrVqzQ0NHzvaZE3v9OwdetWtm3bxscff4y1tTVKZedeh6upqaGuro7g4GBSUlLYtGkTvr6+REVFMWzYMP71r39hb2+PXq8nPz+fmTNnGvukq6qqUKvVHfJFycWaZpM8The3H/+Zk5ubyx//+EcCAwONz+GZM2fSrVs3PvjgA6qqqvDw8OB3v/udcWzeV199RUJCAkqlknnz5hnb126XBGohhBBGJSUlLFmyhAEDBjBr1iwAysrKeO+993jllVdoaWlBr9cTFBTU5rX8WOgzaGpq4uTJkygUCjw8POjRo4dslPsZvvzyS3Jzc3nsscfo0qXLd67XzdewoqJCTpb8j+3bt5OWlsbkyZM5ePCgcbb5J598wv3338/IkSPJzc2lpqYGX19funbt2imej+0lUJuTjM0TQghh5OzsTH5+Pvn5+QwePBhra2scHR0pLS3Fx8eH0NBQk03O2LVrFwcPHiQkJAQnJ6fvDSWWlpYEBwcTFBSEp6en8fc7eoD5pQzhrmfPnuTl5ZGdnY2fn993To80tH/o9XrjSl5nptPpUCgUREREUF1dzdGjR/H09OT+++/Hw8ODyMhI1q5dS0NDAyNGjCAwMNDYi9sZnovXm7QoFLT5Lyfb9rv1r3O/dyOEEMLI0D+7YMECfH19Wbp0KWfPnuXcuXOkpqaarA7DG6fTp0/Hx8eHHTt2UFpa+oO3//ZBDJ0hwPxShmuTnZ1NQ0MD58+f51//+hclJSXfe3ulUklDQwNJSUm/+MCLu11LSwsXLlygtbWVwsJCBg8eTHR0NCUlJRQVFaHVagkMDOS5557jyJEjlJeXd9pr1ZnJCrUQQgjgv6uSCoWCQYMGkZmZyd69e7G3t2fy5MnG8XRtHVhvDn25ubkUFxdz+vRpunfv/p2VVJ1OZwx9qamp+Pn5SaD+CeXl5Xz44YfMmzePGTNmcOnSJZKSkggODjZOPzBcV7VazTvvvENsbGynbfu4fv06x44d4+jRo2zdupVhw4YxYMAAqqqqOH78OP7+/jg5OeHm5sbIkSM75SFCdU1akzyOrFALIYRodwwrwTdvpVEqlcbVtWeeeYZevXpx7tw5wsPDTVpbeXk5K1euZOrUqSxevJjIyEjWr19/y0r1t0Ofs7Nzp98093PY2dnh6emJlZUVAPPmzQNgyZIlFBcX33JdFy1axKOPPkpkZKQZKzYvFxcXgoODSU5OJiYmBg8PDwBmzJhBSEgIa9eupbi4GKDNDzRqv9rJ2eNmJD95hBCikzEE6ObmGxuJvr2a9u1QrVQqWbJkiXH12hQk9N05hq93Y2MjDQ0NODk54ejoSFFREfX19QDcd9992NnZYWFhgVKppKmpib/85S/MmDGj0454u/mFZmRkJAsXLkSv1xMfH8+lS5eAG21Jcvy6AJnyIYQQndKZM2fYs2cP4eHhDBw48HtHebW2thoDbVlZWZvOdja0kjQ2NqLT6bC3t2f58uVERUXRp08fHBwcOHv2LF9++SVPP/00AQEBNDU18ac//Yn58+d32tD3cyUnJ3Po0CF0Oh3Dhw/HxsaGw4cPExAQgK2tLWlpacydO9d4DLNWq6W0tNQk01zaG7Va/YOn6+Xn57N//366du2KUqkkMzOT3/zmNz96Gl9ncKm2xSSP4+9i/dM3MhPpoRZCiE6moKCArVu3MmrUKAoKCrhy5QoWFhbGHlnDMcmWlpbU1dWxfPlyYmNj2/TtbIVCQXJyMuvXrycpKQmALl26cOTIEcrKysjPz+fw4cM89thjhISEGO/TvXv3X3wQQ2eRl5fHpk2beOGFFygtLeXkyZPMnTsXX19f9Ho9VVVVjB49mh49egA3wrSFhYXJprm0J6WlpaxcuZKwsLDvPWnT3d0dNzc3zp8/T3Z2Nvfcc4/x+diZXTdVD7XKwiSP80vICrUQQnQi1dXVLF26lKioKKZNm8a1a9fYu3cvGo2GPn360Lt3b+NqseGY5AcffJBevXq1aV15eXmsXr2al19+mS1btpCXl8df//pXCgsLKSkp4eLFi/Tt25eoqCjgv6FPfJehHcYgKyuL6upqLC0t2b17NwsWLMDHx4fa2tpOGZp/SGtrK59++imxsbFERUVRUVHxgzPQW1tb0Wq1qFSqTjFn+qeUmWiF2q8dr1C33+2SQggh7jgLCwvCwsL4+uuv6d+/PyEhIdx///1s3bqVlJQUgoODcXR0pKGhgcWLFzN9+vQ26U3+duhrbW1l7NixZGRkkJ+fz/PPPw/cWBHs2rXr934e4rs0Gg25ubn4+flRW1tLZWUlFhYWHDlyhNbWVl544QU8PDxITk4mPj6e559/Hjs7O9nMCVhZWeHl5cXKlSuxs7Pj//2///eDt7WwsDC2Q3X2MC1ukEAthBAdmGH1rKysDKVSiaurK3PnzsXR0ZEvv/yS6dOnExISwpQpU6ivr8fR0RGdTseaNWuYNm1am4RpCX1tp7m5mfr6epYtW8bly5f585//jJeXF0lJSWi1Wmpra7lw4QIbNmxg9uzZcmjLfxi+TwYPHsyuXbuM3yvw3XdDDC8Gm5qayM/Pp3fv3uYqu92Q1xTSQy2EEB2W4S/+1NRUli9fTlNTE5s2bWLIkCFERUVRVVVFfHw8Xbp0wcfHB0dHR+DGiluvXr3w9fVtk7oaGxspLS1lw4YNHDhwgAcffJBu3bqRkZGBSqUiMDCQoqIiNmzYwNSpUwkKCpJVwJ/J2tqa+vp6vvrqKyIiIujVqxeOjo7079+f/Px8zp8/T25uLpMnTyY6OlraFbj1mHVLS0tGjRpFVVUVGzZsYPTo0VhYWKDVao3Tbwxzz99//30GDBggbTNAfbNpeqgdpYdaCCGEqdTX1xtXHouKili2bBl/+MMfKCwsZPny5Tg4OPD+++/j4ODAV199Rd++fQkNDTVpjbm5ubz55pv06dOHRx99FD8/P7RaLWvXrkWj0XD16lVGjRpFv379JPT9DIZrlJ2djZeXF1evXqWkpITCwkKGDh1Kz549qa2tNU6jsLZuv72opmS4bunp6eTm5qLT6Zg0aRJ2dnZ8+OGHFBcX884776BUKo0r1Q0NDXzwwQdMnTrVuJGzsyu/1mqSx/FxtjLJ4/wSskIthBAdSEtLCytWrCAsLAw7OzsUCgX9+vXj6tWrbN68mWXLlpGXl8e6desYNWoUffr0Mb613dZuDn2enp4MGjQIe3t7Tp48iUqlwtvbm6CgIPr378/AgQONG8IkTP80hUJBWloan3zyCd27dycyMhJbW1vq6urIycmhtLSU+Ph4evToYXwnQty4bjk5OXz00UcMHTqU9PR0CgsLcXBw4L777uPMmTNs3bqVcePGGds83nvvPaZNmyZh+ib1zaY5at2hHa9QS6AWQogOxMLCgt69e6NWqzl69ChRUVE4OzuTmJhISEgI3bt3p7W1lQsXLtC1a1c8PT1NVpuEvrajVqv57LPPmDdvnnEii6Ojo/E6njx5kjFjxhjnTIv/vsBLSEggLCyMsWPHMnjwYIqLi0lOTmbw4MEMHTqU7t27G9s6MjMzGThwoMlPDm3v6lt0Jjko0cGm/QZq2eEhhBAdhKGDz87Ojrq6Og4cOMDevXuBG8cnl5eXs3v3bhISEvjtb39LZGQkpuz6U6vVbNu2jd/85jfGjVy+vr7ExMQQHh5OWloa9957r/FoZ/HTbj4+XqFQ4O/vD9x4pwJufN3vvfdeXn31VWJjY0369W7vDO98+Pn5UVZWRlVVFRYWFjz00EPU1tZy8eJFgFsOt+nbt6/MnRbfSwK1EEJ0AIZAdfbsWbKzs+nevTsLFiwgMTGRQ4cOcc899+Dv709RURETJkwwaTuFhL47z3CNrl69il6vx97enpCQEL744gu0Wi3W1tZkZWWxaNEiGhoasLS8MdRL2me+y9/fHysrKzIyMigtLaWsrIympiZsbW0BuWY/hwkWp2nvXwVp+RBCiA5AoVCQmprK6tWr6d27Nz4+Pri5uREUFMTWrVsBGD9+PAMHDsTf398kG/0Mj3H16lVUKhXW1taUlJRw8uRJYmJisLKyIisri88++4yYmBhsbGxQKBQSYH4GhULBmTNn+OKLLygvLyclJYWRI0dSWlrK5s2b0Wg0bNmyhalTpxIcHCzX9EcY9hAUFRWRkJBAcnIykydPlraO29Bgqh7qdtzyIVM+hBCiA6ivr+dvf/sbjz32GGFhYRQUFFBdXU3Pnj0pKytjxYoVvPTSS3h4eJh0nvOZM2fYtm0bERERNDc3M2rUKA4ePEh+fj733HMP8fHxzJw5k5iYGJPV1BGUlJSwdOlSXnrpJfbt20dZWRkvvPACKpWK+Ph4VCoVbm5u9OrVS6ak/Iibr01TUxPNzc20tLTg6ekp1+02XKkzzZQPb6f2O+VDArUQQnQADQ0NfP755/j6+lJWVoZWq+XixYuMHTuW++67j4aGBuzt7U1ak4S+tlFbW0tZWRmlpaUEBASwZs0aFi5ciI+PD8XFxXTp0kUOwbmJ4bmlVquxtLT8wZGB334Ofvs0T/HDKq6bJlB7ObbfQC0nJQohxF3o5hMQnZycsLOz45577uHcuXOMHDmS3r17c/z4cU6dOkVcXJxx/rCp1NbWUl9fz7hx46ipqeHcuXMsXLgQW1tbiouLiYuLuyWsSJj+cYZwl5OTQ0JCAmPGjGHnzp0AvPvuu9jb25Oenk5iYiLz58+XKSk3USgUnDp1iqNHj6JUKhkwYADR0dHf+Z4wfE+1trZiZWUlYVrcFnm2CCHEXcjQM/3BBx+wc+dOli9fTkBAANOnT6d3795kZ2ezZcsW7r33XiwtLU0SWHW6G32UOTk5rFu3DisrK2Ntr7/+Oj4+PqSnp7N9+3YaGhravJ6OoKWlBb1ej1KpJC8vj6ysLIYOHUq3bt24//77CQkJISMjg/T0dNasWcPgwYMlTH9LQUEB27Zt4+mnn8bS0pIjR47ccpQ4cMsJiH/+85+pqqoyU7V3J4WJ/mnPJFALIcRdqKioiHXr1vHSSy9hY2NDQUEBH374IWVlZTQ2NrJv3z5mz55N375927wWCX1to66ujn379tHY2AjA4cOH2bt3r/HFUUxMDMOHDyc+Pp7jx4/z8MMPy5SU71FTU8PIkSPJycnhypUrPPnkk9jY2FBTUwNgPFZcrVazZMkSZs+eLaMbxW2THmohhLgLaDQaWltbsbW1pb6+nkuXLmFvb8/Vq1f597//zbPPPsuuXbuoqKhgwYIF2NnZYWtr2+a9yXV1dSQmJhrbSj766COSk5NZuHAhUVFRVFVVcf78efbt24enpycxMTHExMRIz/RPaG1tRalUUlNTg1KppL6+nqCgIFasWEF5eTkvv/yysRdYq9WiUChQKpVyXfluL3ReXh5ffvkl9fX1vPDCC3h5eXHy5EmOHTvGs88+i0qlor6+nsWLFzNjxgwiIyPNWP3dqbJeY5LH8XRov53KMjZPCCHaOZ1Ox7lz5ygpKSEvL49jx44xePBg3NzcSEhIYPjw4URGRlJcXExTUxNBQUF4e3sDbdub3Nraio2NDR4eHmi1WiorK4mLi6OiooKUlBTjSrS/vz/Dhg0jJibGZCP77mbXr19n+/btAISGhrJ161bS09Px8fFh1KhRZGRkkJiYSGxsLJaWliiVSuP1lOt64xpkZmaSmZlJVVUVUVFR5Obm4u3tjbOzMxUVFaxdu5YHHniAwMBA9Ho9O3fuZMSIEfTs2dPc5d+V1C2mGZtnb91+GyskUAshRDunUCi4du0a27dvJykpibi4OLp27Yper+fkyZNcvnwZgMTERObOnWuSk9wk9LWd+vp6ioqKuHLlCvb29sbjsHNycvDw8GDMmDEkJyeTmJjI8OHD5Xr+h+GFWn5+Pv/617/w8PAgMTGR+vp6Hn74YS5cuEB6ejr5+flMmjSJ/v37Azeej2FhYfj6+pr5M7h7NbboTHKwi50EaiGEEL+ETqdDoVDg7OzMpUuXsLKywsPDAwcHBxwcHIiIiCA1NZWLFy9y7733mmyFTUJf29DpdNjb29PU1EROTg65ubkEBgYycOBA8vLyyMvLw9XVlfHjxxMUFGQ8lETcCMYFBQWcOHGCsWPHMnbsWKKioti4cSNqtZqpU6cycOBA+vTpY1yZNtzv25sUxe0x1Qq1BGohhBC3zbDRLy0tjYMHDzJz5kxcXV3JycmhpqaG4OBg9Ho9vr6+jBo1iqCgIJO0U0joazsKhYK0tDQ2btxITEwMRUVFXL16FWdnZ4YOHUpmZiZ5eXn07NlTNs7dxPC8P3nyJF9//TXe3t6EhITg5ORE3759Wb16NVeuXKFv375YWFgYT+SUF3p3RmOrDoWCNv8lgVoIIcRtUygUpKSksGbNGu677z78/Pzw8vLC3t6e3NxckpOTWblyJUOGDMHT09N4H1PUJaGvbWi1Wvbv38/AgQOJi4sjMjKSS5cucfbsWXx9fRk6dCj+/v7yIuU/bj7e3sbGhvDwcFxdXfnmm2/w9fXFxcUFR0dHYmJicHd3x8PDQ0J0G2hslRVqCdRCCNFOtbS0sGfPHubMmUNoaChnz55l8+bN9O/fn+7du2Ntbc0999xj8qkEEvrajlKp5Pz585w7d47evXvj6uqKi4sLO3bsoLm5mYiICNzc3MxdZrtheHH36aefkpOTQ1JSEuPGjcPW1pZ9+/bh7u6Om5sbjo6O8uKuDTW16k0yh9pWArUQQojbZWFhQXp6Ojt37iQrK4vm5mbUajVHjx5l7NixBAcH4+XlZfK6JPS1LVtbWyoqKqitrSUoKIjW1lbOnz/PlClTcHd3N3d57UpZWRnLli3j8ccfp1+/fjQ0NLB582YeeeQRGhsbOXz4MLGxsT943Li4MxpbTTOBWQK1EEKIn2R4+7qwsJDi4mLs7OwYPHgw9vb2DBkyhGHDhtGtWzfS0tLo378/NjY2ZqtVQl/bcXZ2Rq1Wk5+fz7Zt20hMTOSBBx4gIiLC3KW1CzfvE2hqaqKyspLx48djb29PZGQkRUVFtLS0MHr0aHlxZyJNGr1JeqhtrdpvoG6/E7KFEKKTUSgUnD59mo0bNxIZGcnBgwcZPnw4gwYNwtLSkmPHjrF9+3amT59u9pMGg4KCCAsLIysriz/96U80NTUxc+ZM/P39zVrX3aa1tRUrKyvjx4awOGTIEAYOHEhpaSk2Njb4+PiYscr2RaFQkJ2dzaVLl/D09OTMmTMkJCQwcuRIABwdHY2nIJrjHRzROckKtRBCtBOlpaVs2bKFF198EZ1OxzfffINer0etVtOlSxfS09MZPHiw8XhpU22uam1tvWWsmOGxu3TpQt++fenWrRsjRowgPDzcJPV0FOXl5ezZs4eIiAjjnO6bv6ZKpRIXFxccHByA754A2NncPGf6k08+Qa1W09LSgpeXF/v370er1XLt2jUOHjzIqFGj8PLy6tTXy5SaTNXyISvUQgghfoqDgwNz5szh8uXL7Nixg1dffZXExET27t2LXq9n0qRJgGmDVXl5OQkJCUybNg1LS8vvhD4LCwuCgoKMH3f20Hc7qqqquHLlinGF+tvXTqvVYmFhYfx3Z7+uhjnTmzZtYuHChQQFBXH06FEqKiqIjY2lqKiI5uZmHnnkETnxUJhc+436QgjRwRkOligpKaGiogKNRkNgYCCXL18mNjYWPz8/wsPD8ff3p1u3bsb7mTJY3Rz6FAqFsWYDrVZ7y787e+j7OdRqNQC9evVCo9GwZs0a4NZrp9PpsLCwoL6+nnfeeYf6+nqz1NreNDQ0kJGRQXp6OgBDhw7Fx8cHOzs7wsPDmTFjBv379//Ovzj91gAAIABJREFU81SItiYtH0IIYSaGnunVq1cbJxJ06dIFW1tbPvzwQzQaDXv37mXatGmEhYWZtDa1Wo2VlRVeXl4cO3aMwsJC+vTp84Ohb9GiRURHR8s0he9x88pzZWUlX331FZcuXSI8PJyAgADKy8sJCQkxrlTrdDqUSiUNDQ38/e9/Z8qUKQQEBJjzU2g3fHx8CA4OZufOndjb2xMcHExAQAB1dXVERETg7OwMyAs7U2s20aZEVTtu+ZBALYQQZlJdXc2qVat49dVXuXTpEsXFxYwePRp/f3969epFRUUFY8aMoXfv3m1ei4S+ttHa2kpycjIBAQFkZ2eTmJjI4MGDiY+Pp6ysjPT0dM6dO4ePjw++vr7AjTBYX1/P3//+dx566CFpX/gWPz8/PD09+eqrr7CysiIkJITAwECcnJzMXVqn1aQxzTsC7TlQSw+1EEKYiV6vp2vXrmRkZJCcnMyzzz6Lk5MTmZmZhIeHm2xMWmtrKykpKQwaNIjs7GxOnTrFkCFD2LhxI3V1ddTW1nLx4kW6detGv379gBsb5m4OfaY+XOZu0NLSgrW1NaWlpSxYsACVSsVTTz1FWFgYL730EuXl5aSlpZGTk8OhQ4cIDQ3F2dkZvV7PF198weTJk+W6/oDo6Gh0Oh1r164lKioKFxcXlMr2G7Y6OgXyjoCsUAshhJnY2dkRHx/P4cOHee211/Dy8iIzM5ONGzfSp08f7O3t27wGQ+hLSkpi+fLlZGZmMnXqVMLDwxkwYAAuLi7U19eTl5dHTU0NPXv2RKVSodfr+fTTTxk3bpysoH6Puro6Dhw4QLdu3QgMDCQ5ORmtVssjjzwCgJWVFa6urkRGRhIaGkpxcTFhYWE4OjqiUCjo27cvfn5+Zv4s2jc/Pz+GDBmCi4uLtHiYWbPJVqjb79dZArUQQrSxpqYm9Hr9946ec3Z2prW1lYyMDFpaWli/fj3Tpk27ZRNiW5HQ13YsLCxwc3NDo9FQVFTEpEmTqK6uZuvWrcZDeSoqKrC3t8fDw4OjR4+i0WgIDw9Hr9djaSlvIP8c5jzcSPxXi9Y0PdQ2lhKohRCiU7p48SKff/45PXv2xNbWFp1Od8voOVdXV7p27UppaSkAI0aMIDo62iTj5yT03Xkajca46m9lZcXOnTspKCggICCAe++9l8LCQg4cOIC1tTXLli2jd+/eWFtbc+TIEcaPH4+Tk5Ostoq7jqlWqNtzoFboZbaMEEK0CZ1Ox/Lly3FxcWHixIlYWVlhZ2f3s+7bloFao9Gg0WhQqVQ0NzezZcsWamtrmTBhAsHBwXz66adcunSJMWPGsH79ev7whz/g4eHB+++/z+OPPy6bD3+AVqslIyMDlUpFRUUFJSUlTJ06lZ07d1JXV8eoUaPo2rUr27Zto6KigpiYGKKjo4EbXxN5gSLuVtebdCZ5HEdV++2Tl0AthBBtKDs7my1btnD+/HnefPPNHzya2zA1o61J6GtbmZmZrF+/ntraWubMmcOgQYNobW1l8+bNqNVqhg0bRvfu3Y1Hjhv+CpZVaXE3k0AtB7sIIUSbu3DhAuHh4bS0tBh/7+a1DEOYVqvVFBQUtOmhFBYWFlhaWrJmzRo2btxIWFgYdnZ2TJ06FQcHBxITEzl37hxTpkxh/vz5xvYTafH4cYavWa9evfD398fb2xtra2uuXr2KlZUVDz/8MFZWVhw5coT6+nrj+MFvnzwpxF1JYaJf7Zj0UAshxB12c7uGvb09gwYNws7OjqSkJKysrPDx8TGeOqjX641h+s033yQ6Ohp3d/c2rcvLy4tz586hVCqJjIxEpVJhb29Pz549ycrKorCwkO7du2NrawtI6PsphutaXl6OpaUl0dHReHt7k5iYiFKpJCgoiOvXrxMUFER4eHibfX2FMJcWrfRQS8uHEELcQYZwdfbsWVJSUrCxsWHgwIGEhYWxe/duLl26ZGyjuHlletGiRcyYMaPNZk/fHPoMY8by8/M5dOgQ/fr1Y8SIEdTV1dHY2IhOpzMeMiJ+nOG6pqWl8fHHH9OzZ088PT2ZPHkyubm5HD16FFdXVw4fPswbb7xBaGiouUsW4o6rbzZNlHSwab+BWlaohRDiDlIoFGRlZbF69WpiY2OxtLTkk08+ISQkhKFDh1JWVkZOTg4RERGoVCrUajVvvfUWs2bNavMwnZaWxuLFiyktLeXixYsMGTIEBwcHTp48SV5eHsuXL2fw4MEEBga2SR0dkUKhoKCggLNnzzJlyhRCQ0MpKysjLS2NUaNGERAQgEajIS4uzmQH9Qhhai1a0zyOtaxQCyFE53HgwAEaGhp48MEHAThz5gxr1qzhjTfeQKvVotfr8fT0BKC8vJy6ujrCw8PbtKaCggKSk5ONJx2mpqbS3NzMrFmzKC8vp7CwEE9PT5Mcc96R6PV6nn32Wdzd3XnzzTcBKCoqIikpievXr/Pwww/j4uJyy+2lfUZ0NLJCLZsShRDijtPpdBQXFxs/7tevn3Gyg4eHhzFM63Q6fHx82jxM6/V6lixZQnZ2NhEREURERDBo0CBsbGxYtWoVzs7OjBo1yhimZZ3lxxmuT35+Pvn5+cyePZvz58+za9cuAEJDQ4mNjcXe3p7r16/fcl8J06IjMsWhLu39W0daPoQQ4g4LCgpi27ZtXLhwgcjISAoLC4mPj2fAgAE4OTkZb9eW4cqwEpqfn091dTVdu3YlPj4ea2trwsPDcXV1NY7O8/X1xdnZ2SR1dQQKhYLk5GT+/e9/o9VqOX/+PH379mX37t3o9XoiIiJwd3cnLCwMDw8Pc5crRJtrNVXLh0X7/dkkLR9CCHEHGTYatrS08P777+Pq6kpJSQkzZswwznM2leTkZDZv3kxUVBRXr17Fw8OD+Ph4HnjgAaZMmQKAWq3+2YfNiBsaGhpYsWIFv/71r0lPT2fLli28//77XLp0id///vdMnz6dqVOnmrtMIUxG3WKaKGlnLYFaCCE6nB/qh9VqtVhYWKDT6WhpaaGxsRFXV1eT9s9K6Gs7TU1NrF69GmtrawoLC3n22Wfx8fGhrKyMsrIyrK2tiYqKMneZQpiMBGrpoRZCiNtmWIdobGy85fd1uhunhVlYWBjnS6tUKlxdXW+5nylYWFhgZ2fHpk2b2L17Ny+++CJwo13hxRdfJCwszGS1dDQqlYrAwEDOnj3L9OnT8fHxITs7m3feeQdfX1+ioqKkD110LnKwC3LslRBC3CaFQkFKSgqbN2+md+/eBAQEMGLECJRKpbHlQ6FQ3NL+0dLSgoODg8lqNIS+/fv3M3/+fGPoW758Oa+88gr+/v4yceJ/MGjQIGpra9m+fTtpaWmkpqYyf/5849Hycl2F6Fyk5UMIIW5TaWkpO3bsoE+fPjQ2NlJQUEBoaChjx44F/rtSrVQqaWhoYPny5cyZMwdvb2+T1llbW8u+ffvIz88nKCiI1NRU5s6da/Je7o6qqamJoqIi6uvrcXNzIywsTF6kiE6psdU0j2NrZZrH+SUkUAshxM+k0+moqqrihRde4P7772fWrFnU1dVx7tw5UlJSCAwMZMKECcZQZTgBcdq0afTo0cMsNUvoE0K0tfYUqNPS0li5ciU6nY7Ro0cbN2C3NWn5EEKIn0mpVOLl5cWDDz7I9u3biYuLw8vLi8jISLRaLadPn6ayshJPT0/UajXvvvsuM2fOJDIy0mw1q1Sq74R5CdNCiDupvfxI0el0fPbZZ7z++uu4u7vz6quvEhMTQ0BAQJs/tsyhFkKIH2FYzS0qKiI9PR2AIUOGYGNjw7JlyxgwYABubm64uLjQq1cv3N3d0el07Nq1i1GjRpk1TAshhClodKZ5HMufGKWRn59PSUkJ48ePN7bclZWVmeTnsKxQCyHEj1AoFJw+fZq1a9cSFhbG6dOn8fDwYNasWSgUCn7/+9+zaNEifHx8jPdRKpVMmDABlUplxsqFEMI0VCZKk42NjfzlL38xfhwXF0dcXJzx45qaGtzd3Y0fu7u7k5+fb5LaJFALIcS31NfXY2lpiUqlQqPRkJyczG9+8xvCw8MpKioiNTWVQ4cOMWHCBK5fv05FRcUtgRqQMC2EEHeYra0t77777g/++fdtCzRVi5vMoRZCiJtoNBrWr19PfX09er0eCwsL6urqyMzMBCA0NBQfHx/jxw8//LDMHRZCiHbA3d2d6upq48fV1dXGcwDamgRqIYS4iaWlJXPmzEGn07F9+3b0ej2TJk2ioqKCEydOAODn54dGo+H69evG+8lGPyGEMK+uXbty+fJlKioq0Gg0HD9+nJiYGJM8trR8CCHEfxg2IFpbW9PS0kJycjI2Njb069ePHj16sHPnTpKTkykoKGDu3Lk4Ojqau2QhhBD/YWFhweOPP87bb7+NTqdj5MiRdOnSxSSPLXOohRCC/4bps2fPUlpayv33309xcTGrV68mNjaWESNG0NLSwqVLl3B0dCQwMFDmOQshhACk5UMIIYAbLRtpaWl8/vnnhIaGAhAUFMQTTzxBamoqe/bswc7Ojp49exIYGGi8jxBCCCGBWgjR6en1epqbm9m7dy+PPvookZGRpKSksGnTJurq6vjVr35FVlYWtbW15i5VCCFEOyQtH0KITsvQslFfX4+DgwP79+/n+PHj2Nvb4+TkhJ2dHXV1dTz33HM0NjZia2tr7pKFEEK0Q7IpUQjRKRnC9OnTp9m3bx8LFiygf//+uLi44O3tTXBwMNnZ2Xz55Zeo1WqZKy2EEOIHSaAWQnRKCoWCjIwMNm7cyJNPPomzszMtLS3ExsaiVCrJyMhg1apVzJw5Ezs7O3OXK4QQoh2Tlg8hRKdy82SO+Ph4tFot3bt3p7i4mMTERKKioujTpw9ZWVkEBAQQHR0t0zyEEEL8KAnUQohOJz8/ny5dulBUVMSBAwe4fPkyo0ePRq/XU1xczJQpU/Dw8ECplH3bQgghfpq0fAghOp2EhATOnz/Pm2++SWhoKGq1Gjc3N8rKyoiPj6exsVHCtBBCiJ9NVqiFEB2eoWVDp9MZg/KKFSsoKSnhtddeQ6VScfr0adatW8esWbNMdlStEEKIjkECtRCiUygoKCAvL497773XuMlwxYoVlJeX8/LLL1NaWopGoyE8PFx6poUQQtwWeU9TCNFhGdYLWlpaUKlUHDt2jK+//hq1Wg3AxIkTqays5G9/+xshISGEh4cDcgKiEEKI2yOBWgjRIRlWmdPT01m1ahUBAQE88cQTJCUl8fXXX9Pc3IxarWbYsGE8/PDDEqKFEEL8YtLyIYTosLKzs/noo4946qmn6NmzJwCFhYVs3rwZR0dHzp49y3PPPUfv3r3NXKkQQoi7mQRqIUSHo9PpUCgUfPXVVzg4ODBu3Dg0Gg1KpRKlUkltbS1qtZrm5mZCQkLMXa4QQoi7nLR8CCE6DMP6gEajQaFQ4OjoSEFBAQ0NDVhaWqJUKjl37hy1tbX4+flJmBZCCHFHSKAWQnQIhp7p06dP849//IOmpiYCAwOxsbEhIyOD2tpaLly4wOrVq9FqteYuVwghRAciB7sIIToEhUJBWloaGzdu5LHHHkOlUhEREUFFRQW5ubns2bMHnU7H5MmT6dq1q7nLFUII0YFID7UQ4q519epVjhw5wpQpUwDYvHkzgYGBBAcHk5+fzzfffMOIESPo3r07ra2tKJVKPDw8ZM60EEKIO0pWqIUQdy2dTkd0dDQ1NTW4uLjg6urKoUOHUKvVDBgwAD8/P5KSkujVqxeurq7G+0mYFkIIcSdJoBZC3HWqqqpISEhg+vTpuLu78+GHH2Jtbc2vfvUrwsPDsba2xtvbmytXrrB06VKuX7+Og4ODucsWQgjRQcmmRCHEXUev15Oamsq6desAmDJlChqNhlWrVuHn54e3tzcnT57kvffe48EHH8TX19fMFQshhOjIpIdaCHFXMfQ/5+bmsmLFCoYOHcrUqVMpLy9n27ZtWFtbM2/ePE6fPo2dnR29evWSnmkhhBBtSgK1EOKuc/r0aY4ePYqdnR15eXlER0fz6KOPUl5ezsaNG3F2dmbevHnmLlMIIUQnIYFaCHFXaWpq4u2332bmzJn06NGDsrIyVqxYQc+ePZk2bRrl5eU0NzcTFBRk7lKFEEJ0ErIpUQhxV1Eqldjb2+Po6AiAj48Pw4cPZ926dWg0Gh555BEzVyiEEKKzkU2JQoi7guF0Q2tra0JDQ/nnP/9JfX29cbb0mDFj6Nu3r5mrFEII0RlJy4cQol1LSkoiOjoaa2trAKqrq3F3d2ft2rWcOnWKe+65h8OHD/P8888THh5u5mqFEEJ0RhKohRDtVk1NDVu2bMHe3p5Zs2Zx4MAB8vLyePrpp7G0tOTEiRNYWFhgb29Pz549zV2uEEKITkoCtRCi3dJqtVy+fJlTp06Rk5ODWq3m5ZdfxsnJydylCSGEEEbSQy2EaJd0Oh0WFhYEBATQ2trKxYsXCQkJMYZpnU5n5gqFEEKIGyRQCyHaJaXyxo+n3bt3k5mZyTPPPIObmxurVq1Co9EY/1wIIYQwN/kbSQhhdjevNldXV3PlyhXjxzU1NSxcuJCoqCj69euHhYUFFRUV5ihTCCGE+F7SQy2EMKu6ujpOnTrFyJEjycrK4osvvkCn0zFw4MDvzJTW6/U0NzejUqnMVK0QQgjxXXKwixDCrAoKCigoKKChoYGcnBxefPFFbGxsWLx4MZaWljz00EMoFAoAFAqFhGkhhBDtjrR8CCHMwvDmWHR0NAMGDODKlSs0NjZia2uLu7s7Cxcu5OzZs2zYsMHMlQohhBA/TgK1EMKsSkpK6NOnD0OGDMHNzY0TJ05QU1ODt7c3zz77LGlpaZSXl5u7TCGEEOIHSQ+1EMJszpw5wxdffMFzzz1HWFgYp06dIjs7G29vbwYOHIibmxstLS3GUxKFEEKI9kh6qIUQZlFVVcXatWt55plnCAsLA2DAgAEoFApSU1PRarWMGzcOS0v5MSWEEKJ9k7+phBAmpdfrUSgUNDc34+7uTnh4OIBxJTo2NhYbGxtcXV2xsrIyc7VCCCHET5MeaiGESRi6y5qbmwHw9vZGr9dz4MABAKytrUlLS2PVqlVERUXRpUsXs9UqhBBC3A7poRZCmExaWhoJCQmEhITg6emJtbU1Z86cAaB///6sX7+emTNn0r9/fzNXKoQQQvx8skIthDCJ3NxcVq9ezaRJk8jKyiIzM5Pu3bszYcIE9Ho9eXl5zJo1i/79+yOv84UQQtxNpIdaCNGmDOG4vLyc2bNno9fruX79Or/+9a9xcnIC4Kmnnrrl9oaDXIQQQoi7gQRqIUSbMoRjV1dXVq5ciaWlJW+88QbOzs6cPn2aCxcuMHnyZOMGRAnTQggh7jYSqIUQd5xhlTkvL4/i4mJCQkJwdnamd+/eqFQq9Ho9+fn5bNiwgZkzZ8o0DyGEEHc12ZQohGgTqamprFmzhtGjR5OQkMADDzyAm5sbeXl5pKSk4ODgwNixY4mNjZU2DyGEEHc1WaEWQtxxarWaU6dO8dprr1FVVUVCQgL9+/fH0dGRqKgo7rvvPpRKJXZ2dhKmhRBC3PVkhVoI0Sa+/PJLLl26RGVlJb/97W/x9PTk9OnTuLu7ExISYu7yhBBCiDtGxuYJIf5nhtflNTU1XL58GQAPDw+qq6t58MEH8fT0pKCggDVr1hgPdhFCCCE6ClmhFkLcEampqaxduxYLCwt69epFeHg4ubm51NXVodFouHTpEjNnziQmJsbcpQohhBB3lARqIcT/rLS0lHXr1vHoo4/i5ubG5s2bsbe3Z9CgQQBUVlbi4uJCYGCg9EwLIYTocKTlQwjxP2loaGDPnj3GVg+VSsWUKVM4e/YsKSkp+Pr6EhUVRWBgICBzpoUQQnQ8EqiFELfN8MZWcXExNjY2DBs2jK5du3Lq1CkqKipwcHDgnnvuQa1Wo9PpzFytEEII0bYkUAshbptCoeD06dN8+OGHXLhwgR49ejBs2DCqq6tZuXIlBw8eZPfu3XTr1g2lUn7MCCGE6Nikh1oIcdtKS0tZvHgxv//97/H39+f69etYWVlx5coVY/90bGws/fv3R6fTSagWQgjRocnfckKI21ZXV4e3tzctLS1s3LiRRYsW8fzzz+Pg4MB9992Hvb09JSUl1NTUSJgWQgjR4cnfdEKI29ajRw8sLS1Zt24dPj4+vPHGGwwZMoSMjAx69uxJ9+7daWxsxNra2tylCiGEEG1OWj6EELfl5haOlpYWrK2tKSoq4p///CdPPfUUERERADQ1NaFSqcxZqhBCCGESskIthPhJGo3G+N9KpdI4ucPa2prc3FyWLl3K7NmziYiIMP6ZhGkhhBCdhaxQCyF+VFZWFlevXmXQoEFYWlp+721KSkqMh7aAzJoWQgjRucgKtRDiB5WXl5OQkEBgYOD3hmnDanRgYCA6nQ6FQiFhWgghRKcjgVoI8R06nY7Kykr+8pe/YGFhYQzMP0StVnP06FFaW1tNWKUQQgjRPkigFkIYGVo2lEolnp6ePPTQQ2RlZVFQUPCd8XeGzYlqtZq//vWv+Pn5YWVlZY6yhRBCCLP6/oZIIUSno9frUSgUZGZmcu7cOXx9fRk2bBgAy5YtY+HChQQHB6PX69Hr9SiVShoaGliyZAmzZ88mPDzczJ+BEEIIYR6yQi2EAG5sJExJSWHNmjW4u7sTHx/P1q1biYuLY8yYMbz77rsUFRWhUChQKpU0NTWxdOlSHnroISIjI81dvhBCCGE2skItRCdnWJnW6/VkZGTwyiuvUFxcTFNTE2PGjAFgwoQJwI3Z0gYNDQ3MmTOHLl26mKVuIYQQor2QsXlCdFKGcKxSqSgtLSUgIIBVq1ZRXl6OWq1mwYIFeHp6kpqailKppG/fvgAyGk8IIYT4Fmn5EKKTKioq4pNPPuHo0aN88MEH1NTUMGjQIK5evcrQoUPx9PQkJyeHVatW3XKEuIzGE0IIIW4lK9RCdGJLliwhOTmZF154gQEDBlBXV0daWhp79+7F29ubixcvMnv2bKKjo81dqhBCCNFuSaAWopMx9EwDJCcnk5yczOXLl1m4cCEeHh4A1NXV0dDQgF6vx8/Pz5zlCiGEEO2eBGohOqG8vDyqq6vp0aMHzs7OrF27lszMTP70pz9x/vx5CgsLmThxornLFEIIIe4KEqiF6GSys7NZsWIFbm5u2NvbM2rUKPr27cu///1vCgsLuXbtGo888ggDBgwwd6lCCCHEXUECtRCdyIULF1i7di3z58/Hz8+PnTt3cvnyZQYMGEDfvn0pLS1FqVTi5+d3S2uIEEIIIX6YTPkQooO7+TVzSUkJeXl5ZGdnAzBx4kT8/Pw4duwYycnJBAQEGHumJUwLIYQQP48c7CJEB6dQKMjIyMDGxoYRI0bQ0tLCqVOncHJyYsCAAUycOJEdO3bg7e1t7lKFEEKIu5IEaiE6gYqKCj755BPeeust4uLisLS0JCEhAa1Wy+DBg5k0aZK5SxRCCCHuWhKohejA1Go1KpWK0aNHo1AoeOutt3jttde499570Wq1HD58mMjISJycnFAqpQNMCCGE+CVkU6IQHVRZWRn79+9n4MCBREREoFQqOXz4MCtXruSPf/wj3bp1o6amBjc3N3OXKoQQQtzVZElKiA7k5tfHrq6uKJVKTp06RX5+PhqNhlGjRhEeHs5bb71FY2OjhGkhhBDiDpAVaiE6CMOYu7y8PK5du4atrS29evVi06ZNqNVq+vXrh7W1NSkpKQwaNIiwsDBzlyyEEEJ0CBKohehA0tLSWLNmDf369SM3N5eAgACefvppduzYQWlpKTk5OcyfP5/o6GgAmTUthBBC3AESqIXoIPR6PUuXLmXw4MEMHDgQgNdff50ePXowa9YsNBoNtbW1eHh4mLlSIYQQomORHmoh7mKG18NZWVkkJSXh5OSEjY2N8c+ffvppamtr0Wq1WFpaSpgWQggh2oAEaiHuYgqFgpSUFL744gs8PDzw8/NjxYoV1NTUAFBTU0NlZSXNzc1mrlQIIYTouGQOtRB3saamJg4fPswTTzxBt27d6NatG9evX+ftt9+mT58+nDlzhjlz5mBnZ2fuUoUQQogOSwK1EHe569evG1eg9Xo906dPx9PTk9DQUIYNG0ZoaKhsPhRCCCHakLR8CHEXU6lUDB48mNzcXEpLS41j844fP46TkxOhoaEAEqaFEEKINiRTPoS4y9XU1HDgwAFycnLo3r07J06cuGU0nhBCCCHalgRqITqApqYmCgsLuXbtGp6ennTr1s3cJQkhhBCdhgRqIYQQQggh/gfSQy2EEEIIIcT/QAK1EEIIIYQQ/wMJ1EIIIYQQQvwPJFALIYQQQgjxP5BALYQQQgghxP9AArUQQogObdmyZWzYsAGAnJwcFi5caJLHnTFjBuXl5d/7Z3/+8585dOjQz/r/PPvss6Snp/+iGv6X+wohfj45elwIIYTZPfvss9TW1qJUKlGpVPTr14/HH38clUp1Rx8nMjKSpUuX/uTtEhMTOXToEG+++eYdfXwhRMckK9RCCCHahZdffpk1a9bwt7/9jcLCQrZs2fKd22i1WjNUJoQQP05WqIUQQrQrbm5u9O3bl4sXLwI3Wicef/xx9uzZg1arZdmyZaSkpLBhwwYqKysJCAjgySefJCgoCIDz58/z0UcfcfnyZfr164dCoTD+v7OysvjnP//JRx99BEBVVRWrVq0iJycHvV7P0KFDGTduHCtWrECj0TBnzhwsLCxYtWoVra2trF+/nhMnTqDRaIiNjWXevHlYW1sDsGPHDnbt2oVCoeDhhx/+2Z9veXk5H3/8McXFxSgUCvr06cMTTzyBvb298TaFhYWsXLmS2tpaYmNj+dWvfmV83B+7FkII05AVaiGEEO1KVVUVZ86cITg42Ph7ycnJ/PWvf+WD/9/evUZFdZ0NHP8zIA53EVBEUe6isQTFqogaL1hNSGqehy/hAAASmUlEQVRBSXBZ1IrLpGhMDKIpZrFIV5osJNQqJlFq8EJNrVJFkXpJUFFqNLUasQWqhFIRuTgzghC5z3k/uDivCHhNgn3f5/eJmb3PPvsya/GcPc+cs24dJSUlfPLJJyxZsoS0tDSCg4NZu3YtLS0ttLa2kpSUxMSJE0lLSyMwMJCzZ892eR6j0UhiYiKOjo589NFHbNq0iaCgIDUo9fHxIT09nW3btgGwc+dOKioqSEpKYsOGDRgMBjIyMgD4+uuvycrK4p133mH9+vVcunTpkcYcGhrK5s2bWbduHXq9nj179nQoz8vLY82aNaSkpFBRUcHevXsB7jsXQogfjgTUQgghngpJSUksXLiQ+Ph4hg8fTlhYmFoWGhqKtbU15ubm5OTkEBwcjLe3NxqNhsmTJ2NmZsaVK1e4fPkybW1thISEYGZmxrhx4/D09OzyfMXFxRgMBiIjI9FqtZibm+Pr69tlXUVRyMnJYcGCBVhbW2NhYUFYWBh//etfATh9+jSTJ09m8ODBaLVawsPDH3rczs7O+Pn50atXL2xtbQkJCaGgoKBDnRkzZuDo6Ii1tTWhoaHqee83F0KIH46kfAghhHgqxMbG4ufn12WZg4OD+rdOpyM3N5fDhw+r77W2tmIwGDAxMaFv374d0jwcHR27bFOn0+Hk5ISpqekD+3br1i2ampp4++231fcURcFoNAJw8+ZNPDw81DInJ6cHttmutraWrVu3UlhYSGNjI0ajEWtr6w517h6Dk5MTBoNBHUN3cyGE+OFIQC2EEOKpd3eA7ODgQFhYWIcd7HYFBQUYDAYURVGP0ev1ODs7d6rr6OiITqejra3tgUG1jY0N5ubm/Pa3v6Vv376dyu3t7dHr9eprnU730GP77LPPAPjwww+xsbHhq6++Ii0trUOdu9vT6XRqH+43F0KIH46kfAghhPivMm3aND7//HOuXLmCoig0NjZy/vx5Ghoa8PHxQaPRcOjQIdra2jh79izFxcVdtuPl5YW9vT07d+6ksbGR5uZmioqKAOjTpw8Gg4HW1lYANBoN06ZNY9u2bdTW1gJgMBj4+uuvAQgMDOTEiRNcu3aNpqamTjnQ99PQ0IBWq8XKygqDwUBWVlanOkeOHEGv11NfX8++ffsIDAx84FwIIX44skMthBDiv4qnpyevvvoqaWlpVFRUqLnPw4YNw8zMjJUrV7J582Z27drFyJEjGTNmTJftaDQaVq9eTVpaGtHR0ZiYmBAUFISvry8jRoxQf5yo0Wj49NNPmTdvHhkZGaxZs4a6ujr69u3L9OnT8ff3Z+TIkYSEhPDuu++i0Wh45ZVXyMvLe6jxhIeHs3HjRhYsWICzszOTJk0iOzu7Q50JEybw3nvvcfPmTUaPHs3s2bMfOBdCiB+OiaIoSk93QgghhBBCiP9WkvIhhBBCCCHEE5CAWgghhBBCiCcgAbUQQgghhBBPQAJqIYQQQgghnoAE1EIIIUQP0Ol0REZGqg+HeRxLly4lPz//O+yVEOJxyG3zhBBC9DhFUdi5cyfHjh0DYOrUqcybN6/DA13udvr0afbs2YNer8fBwYG5c+eqt8d7mLb+8pe/kJ2dza1bt3B0dCQ2NhYXFxcADh06xMGDB6mvr2fAgAEsXLiw20eSPwlHR0fS09O/83Z7wqVLl/j000/R6XR4e3sTHR3d7dMiDx8+zIkTJ7h69SpBQUEsXbq0Q3lOTg6ZmZnU1NTg6+vLL3/5S/VBNtnZ2Rw6dIi6ujq0Wi2BgYFERkZiamra6YmTgwcPZv78+Xh7e3/v4xcCRQghxP9rRqNRaWtr69E+HD16VFm+fLmi0+kUvV6vvPnmm8qRI0e6rKvX65WIiAjl/PnzitFoVP7+978r8+bNU2pqah6qrS+++EKJiYlRysrKFKPRqFRUVCh1dXWKoijK5cuXlZ///OfKN998oxiNRuXIkSPKokWLenx+uhMdHa1cvHixR/tQW1urzJ8/Xzl9+rTS1NSk7NixQ4mLi+u2/pkzZ5SzZ88qqampysaNGzuU/fOf/1SioqKUq1evKi0tLUpqaqoSHx+vlldUVCj19fWKoihKXV2dkpCQoGRlZSmKoiiVlZVKVlaWYjAYlLa2NuXzzz9XFi1apDQ0NHwPoxaiI9mhFkKIp0BmZiY5OTnU1tZ22nEF+OKLL8jOzlZ3ZF9//XU8PDzQ6XRs27aNwsJCFEUhKCiIqKgodu/eTWVlJcuXLwegurqaZcuW8cc//hFTU1MSEhIYOnQoBQUFlJSUkJycTGFhIQcOHECv12Nra8usWbOYPn262oe//e1v7N69m+rqamxtbYmKiqKhoYHMzEwSExPVellZWRQWFrJq1aqHHn9ubi4vvfQSDg4OALz00kvk5OTwk5/8pFNdvV6PlZUVI0eOBGDUqFH07t2bqqoq7Ozs7tuW0WgkIyOD6OhoBg0aBNDhseQ3btxg0KBBeHh4ADBp0iS2bNlCbW0t9vb2ZGZmUlRUxNtvv93lOJYuXcqMGTM4efIkVVVVjB8/nrlz5/Lxxx9TVFSEt7c3K1aswNrautOanDhxgoyMDG7duoWNjQ0RERFMnDjxvut/t+LiYrZu3Up5eTnm5uaMHTuWBQsWYGZmhqIobN++nby8PFpaWnBycmL58uUMHjyY8+fPk56ejl6vx8LCgpCQEH76058+9Np99dVXuLq6qk9vDA8PJyoqivLycgYOHNip/tixYwEoKSnp8Lh2gHPnzjFu3DhcXV0BmD17Nq+99hqVlZU4Ozt3WCtFUdBoNFRWVgLQv39/XnzxRbU8ODiY9PR0rl+/3mmuhPiuSUAthBBPgf79+/Puu+/Sp08fzpw5Q0pKChs2bMDe3p4vv/ySPXv2EBsbi6enJ1VVVZiammI0GklMTOSZZ57ho48+QqPRUFJS8tDnPHnyJHFxcbi4uKAoCnZ2dqxevZr+/ftTWFjI+++/j6enJx4eHhQXF7Nx40ZiYmIYMWIENTU1NDQ00K9fP1JTU7l27ZoaoJ46dYqwsDDgzoVCZmZmt33Ytm0bAGVlZQwZMkR9f8iQIZSVlXV5jKenJwMHDuTcuXOMGjWKc+fO0atXLwYPHvzAtgwGA3q9nrKyMj7++GM0Gg3PPfccc+bMQaPR4O/vz/79+7ly5Qqenp4cP34cNzc3+vTpA8DPfvazB87r2bNneeeddzAajaxatYrS0lJee+01Bg0axPvvv8+hQ4cIDw/vcExjYyNbt27lgw8+wMXFhZs3b1JfXw/Q7frfS6PRsGDBAjw9PdHr9XzwwQccOXKEkJAQLl68SGFhIevXr8fS0pLy8nKsrKwA2LRpEytWrGDYsGHU19dTXV0N3MnxXrlyZbfjXLx4MRMmTOg031qtFmdnZ8rKyroMqB9HWVmZGkzn5eXx+9//noaGBmxsbIiMjOzymNLSUlpbWzsE4UJ8XySgFkKIp0D77h7A+PHj2bdvH8XFxfz4xz/m2LFjzJo1Cy8vL+B/d1QvX76MwWBQc0iBR8r1nTx5sroTCHd2etsNHz4cPz8/ioqK8PDw4NixY0yZMgU/Pz8ANae1vb+nTp1i7ty5lJWVcePGDQICAoA7AejDBKGNjY1YWlqqry0tLWlsbERRlE551O1B8Pr162lpacHMzIy33noLrVb7wLbad0QvXrzIhx9+yLfffstvfvMb+vbtS3BwMBYWFowdO5b4+HgURcHKyopf/epX3eZyd2XmzJlqAO7r64udnR3u7u4AjBkzhkuXLnV5nImJCVevXsXR0RF7e3vs7e0Bul3/e929C9uvXz+Cg4MpKCggJCQEMzMzGhsbKS8vx8vLS734ATA1NeXatWsMGTIEa2trrK2tgTs53u0XPPfT2NiIra1th/fa5/xRjRw5kt/97ndMnz6dAQMGkJGRgYmJCU1NTWqdCRMmMGHCBCoqKsjNzVXn+m63b98mJSWFOXPmdPgsCPF9kYBaCCGeArm5uRw8eJAbN24Ad4KUuro64M5OYf/+/Tsdo9PpcHJy6nK38mG0p0S0u3DhAhkZGVy/fh1FUWhqalJ3ffV6vZpica/24DYiIoKTJ08SGBhIr169HqkvWq2WhoYG9XVDQwNarbbLQDY/P58//OEPJCQk4O7uTklJCWvXriUuLg43N7f7tmVubg7ArFmzsLKywsrKiuDgYC5cuEBwcDA5OTkcP36c5ORknJ2dyc/PJzExkcTExA4XEfdjZ2en/m1ubt7p9d3B4d3jf/PNN8nKymLTpk0MHTqU+fPnM3DgwG7X/17Xr19nx44dfPPNNzQ3N9PW1qYG2SNGjGDGjBnqDwfHjBlDZGQklpaWxMTEsHfvXj777DMGDx7MvHnz8PHxeaixtvf97vmGOwFt+wXOo/jRj35EeHg4ycnJ3L59m5CQELRabafPKsCAAQNwdXVly5YtHXbSm5ubSUxMxNvbm9DQ0EfugxCPQwJqIYToYTdu3GDz5s3Ex8fj4+ODRqMhNjYWRVGAOzuFVVVVnY5zdHREp9PR1tbWKajWarU0Nzerr2tqajodf3ew2tLSQnJyMsuWLWP06NGYmZmxdu1atdzBwUHNVb2Xj48PZmZmFBYWkpeXxxtvvKGW7d27l3379nU79va7XLi6ulJaWqruwpaWlnbYPb9baWkpw4YNw9PTEwAvLy+8vLzIz8/Hzc3tvm25uLhgZtb9v77//Oc/BAQEqHf88Pf3p0+fPly+fJlx48Z1e9x3wd/fH39/f5qbm9m1axebN2/m17/+dbfrf68tW7bg5ubGG2+8gYWFBdnZ2Zw5c0Ytf+GFF3jhhReora1l3bp1HDhwgIiICLy8vFi1ahWtra0cPnyYdevW8cknn6DT6VixYkW351uyZAkTJ07E1dWV3Nxc9f3Gxkaqqqq6Xb8HmTlzJjNnzgTuXCTs3bu327ba2to6zE1LSwtJSUn07duXJUuWPNb5hXgcElALIUQPa2pqwsTERP3a/Pjx4x3yh6dOncqOHTvw9fXF3d1dzaH18vLC3t6enTt38vLLL6s51L6+vri5ubF//350Oh2Wlpb3zWMGaG1tpaWlBVtbW0xNTblw4QL5+fnqDvXUqVN57733CAgI4JlnnlFzqNtzZJ977jnS0tIwNTXtkHYSFham5lPfz6RJk8jOzlbTTg4ePKgGVffy8vJi//79lJaW4ubmxr///W+KioqYMWPGA9vq3bs348eP58CBA7i7u3P79m1ycnLUH+F5enqyb98+nn/+efr168elS5eoqKhQA7rdu3dTUFBAQkLCA8f0KGpqaiguLmbEiBGYm5uj1WrRaO48KqK79b/3tnQNDQ1YWlqi1WopLy/n6NGj6mequLgYRVFwd3end+/e9OrVC41GQ2trK19++SUBAQFYWlpiaWmpnvdhb+s3ZswY0tPTOXPmDKNGjSIjI4MhQ4Z0mz/d1tZGW1sbRqMRo9FIc3MzpqammJqa0tzcTGVlJa6uruj1elJTU3n++efVNJScnBxGjx6NnZ0d165dIzMzk2effRa48xlOTk6mV69eLFu2TB2HED8ECaiFEKKHDRo0iBdffJE1a9ag0WiYNGkSQ4cOVcsDAwOpq6tj/fr1GAwG+vXrx7Jly3BycmL16tWkpaURHR2NiYkJQUFB+Pr64ufnR2BgICtXrsTGxoZZs2Zx7ty5bvtgYWHBL37xC9atW0dLSwsBAQGMHj1aLffy8iI6Oprt27dTXV2NnZ0dUVFRatA0adIk/vSnPzF79uzHmoPp06dTXV1NTEwMANOmTetwh5G33nqL0NBQJk6cyPDhw9W0gNraWmxtbQkNDVUDqwe1tWjRIlJTU3n11VexsrJi2rRpTJkyBbhzYVBVVUVCQgLffvstDg4OLFmyRB2nXq/vsDbfFUVRyMrKIiUlBRMTE9zc3Fi8eDFw//W/W2RkJKmpqezfvx93d3fGjx/PP/7xD+BOsL19+3aqqqowNzfn2WefVS8iTp48SVpaGkajERcXF15//fVH6rutrS0xMTGkpaWRkpKCt7d3p28pioqKiIuLA+DPf/4zGRkZavmpU6eYM2cOL7/8Mi0tLWzYsIGqqiq0Wi1TpkwhIiJCrfuvf/2LXbt2qXnb48aN45VXXgHu/Kbg/PnzmJubs3DhQvWYuLg4hg0b9khjEuJRmSjt3ykKIYQQj6m5uZnFixeTmJjIgAEDero735vY2Fji4+OxsbHp6a4IIZ4iskMthBDiiR09ehRPT8//08E0QFJSUk93QQjxFJKAWgghxBNZunQpiqIQGxvb010RQogeISkfQgghhBBCPAH5CawQQgghhBBPQAJqIYQQQgghnoAE1EIIIYQQQjwBCaiFEEIIIYR4AhJQCyGEEEII8QT+B9Rpsve2TgiUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(cm,label_final,title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_targets1 = np.array(fin_targets)\n",
    "fin_outputs1 = np.array(fin_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(len(label_final)):\n",
    "    fpr[i], tpr[i], _ = roc_curve(fin_targets1[:, i], fin_outputs1[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot of a ROC curve for a specific class\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(fpr[0], tpr[0], label='counter_speech+discussion_of_eastasian_prejudice, ROC curve (AUC score = %0.2f)' % roc_auc[0])\n",
    "plt.plot(fpr[1], tpr[1], label='entity_directed_criticism, ROC curve (AUC scoure = %0.2f)' % roc_auc[1])\n",
    "plt.plot(fpr[2], tpr[2], label='entity_directed_hostility, ROC curve (AUC score = %0.2f)' % roc_auc[2])\n",
    "plt.plot(fpr[3], tpr[3], label='neutral, ROC curve (AUC score = %0.2f)' % roc_auc[3])\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "                                                  precision    recall  f1-score   support\n",
      "\n",
      "counter_speech+discussion_of_eastasian_prejudice     0.7065    0.6842    0.6952       190\n",
      "                       entity_directed_criticism     0.3333    0.0195    0.0369       205\n",
      "                       entity_directed_hostility     0.6120    0.7579    0.6772       537\n",
      "                               none_of_the_above     0.8860    0.9197    0.9025      1868\n",
      "\n",
      "                                        accuracy                         0.8068      2800\n",
      "                                       macro avg     0.6345    0.5953    0.5780      2800\n",
      "                                    weighted avg     0.7808    0.8068    0.7819      2800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print('Classification Report:')\n",
    "y_true1 = y_true.argmax(axis=1)\n",
    "y_pred1 = y_pred.argmax(axis=1)\n",
    "target_names = label_final\n",
    "print(classification_report(y_true1, y_pred1, target_names=target_names,digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning(freeze parameters in RoBERTa model to initilize the parameters in linear layers) version 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Finetuning(model, \n",
    "             optimizer, \n",
    "             train_iter, \n",
    "             valid_iter,\n",
    "             valid_period = int(len(train_dataset)/TRAIN_BATCH_SIZE),\n",
    "             scheduler = None,\n",
    "             num_epochs = 5):\n",
    "    # Pretrain linear layers, do not train bert\n",
    "    for param in model.roberta.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    \n",
    "    # Initialize losses and loss histories\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0   \n",
    "    global_step = 0  \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        count = 0\n",
    "        for _,data in enumerate(train_iter, 0):\n",
    "            source = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            target = data['targets'].to(device, dtype = torch.float)\n",
    "            y_pred = model(input_ids=source,  \n",
    "                           attention_mask=mask)\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()(y_pred, target)\n",
    "            print('batch_no [{}/{}]:'.format(count, int(len(train_dataset)/TRAIN_BATCH_SIZE)),'training_loss:',loss)\n",
    "            count+=1\n",
    "            loss.backward()\n",
    "            \n",
    "            \n",
    "            # Optimizer and scheduler step\n",
    "            optimizer.step()    \n",
    "            scheduler.step()\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Update train loss and global step\n",
    "            train_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            # Validation loop. Save progress and evaluate model performance.\n",
    "            if global_step % valid_period == 0:\n",
    "                model.eval()\n",
    "                \n",
    "                with torch.no_grad():                    \n",
    "                    for _,data in enumerate(valid_iter, 0):\n",
    "                        source = data['ids'].to(device, dtype = torch.long)\n",
    "                        mask = data['mask'].to(device, dtype = torch.long)\n",
    "                        target = data['targets'].to(device, dtype = torch.float)\n",
    "                        y_pred = model(input_ids=source, \n",
    "                                       attention_mask=mask)\n",
    "\n",
    "                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss()(y_pred, target)\n",
    "\n",
    "                        \n",
    "                        valid_loss += loss.item()\n",
    "\n",
    "                # Store train and validation loss history\n",
    "                train_loss = train_loss / valid_period\n",
    "                valid_loss = valid_loss / len(valid_iter)\n",
    "\n",
    "                # print summary\n",
    "                print('Epoch [{}/{}], global step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_iter),\n",
    "                              train_loss, valid_loss))\n",
    "                \n",
    "    \n",
    "    print('finetuning done!')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================= Start finetuning ==============================\n",
      "batch_no [0/1000]: training_loss: tensor(0.2886, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [1/1000]: training_loss: tensor(0.2721, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [2/1000]: training_loss: tensor(0.2784, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [3/1000]: training_loss: tensor(0.2891, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [4/1000]: training_loss: tensor(0.2919, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [5/1000]: training_loss: tensor(0.3168, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [6/1000]: training_loss: tensor(0.3828, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [7/1000]: training_loss: tensor(0.3004, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [8/1000]: training_loss: tensor(0.3593, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [9/1000]: training_loss: tensor(0.3971, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [10/1000]: training_loss: tensor(0.4406, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [11/1000]: training_loss: tensor(0.3646, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [12/1000]: training_loss: tensor(0.3128, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [13/1000]: training_loss: tensor(0.3351, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [14/1000]: training_loss: tensor(0.3152, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [15/1000]: training_loss: tensor(0.3703, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [16/1000]: training_loss: tensor(0.3315, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [17/1000]: training_loss: tensor(0.3510, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [18/1000]: training_loss: tensor(0.3795, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [19/1000]: training_loss: tensor(0.3464, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [20/1000]: training_loss: tensor(0.2992, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [21/1000]: training_loss: tensor(0.3183, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [22/1000]: training_loss: tensor(0.3245, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [23/1000]: training_loss: tensor(0.3340, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [24/1000]: training_loss: tensor(0.3215, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [25/1000]: training_loss: tensor(0.3694, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [26/1000]: training_loss: tensor(0.4029, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [27/1000]: training_loss: tensor(0.2892, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [28/1000]: training_loss: tensor(0.2553, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [29/1000]: training_loss: tensor(0.4430, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [30/1000]: training_loss: tensor(0.3060, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [31/1000]: training_loss: tensor(0.3193, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [32/1000]: training_loss: tensor(0.2706, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [33/1000]: training_loss: tensor(0.3467, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [34/1000]: training_loss: tensor(0.2355, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [35/1000]: training_loss: tensor(0.3523, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [36/1000]: training_loss: tensor(0.3110, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [37/1000]: training_loss: tensor(0.3088, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [38/1000]: training_loss: tensor(0.2606, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [39/1000]: training_loss: tensor(0.3102, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [40/1000]: training_loss: tensor(0.2540, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [41/1000]: training_loss: tensor(0.3599, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [42/1000]: training_loss: tensor(0.3316, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [43/1000]: training_loss: tensor(0.3477, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [44/1000]: training_loss: tensor(0.2738, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [45/1000]: training_loss: tensor(0.2915, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [46/1000]: training_loss: tensor(0.2337, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [47/1000]: training_loss: tensor(0.3865, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [48/1000]: training_loss: tensor(0.3123, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [49/1000]: training_loss: tensor(0.3506, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [50/1000]: training_loss: tensor(0.3547, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [51/1000]: training_loss: tensor(0.3059, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [52/1000]: training_loss: tensor(0.3023, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [53/1000]: training_loss: tensor(0.3601, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [54/1000]: training_loss: tensor(0.3286, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [55/1000]: training_loss: tensor(0.2750, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [56/1000]: training_loss: tensor(0.3477, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [57/1000]: training_loss: tensor(0.3610, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [58/1000]: training_loss: tensor(0.3431, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [59/1000]: training_loss: tensor(0.2999, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [60/1000]: training_loss: tensor(0.3484, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [61/1000]: training_loss: tensor(0.3474, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [62/1000]: training_loss: tensor(0.2418, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [63/1000]: training_loss: tensor(0.3001, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [64/1000]: training_loss: tensor(0.3014, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [65/1000]: training_loss: tensor(0.3230, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [66/1000]: training_loss: tensor(0.2737, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [67/1000]: training_loss: tensor(0.2932, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [68/1000]: training_loss: tensor(0.3410, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [69/1000]: training_loss: tensor(0.2816, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [70/1000]: training_loss: tensor(0.2784, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [71/1000]: training_loss: tensor(0.3139, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [72/1000]: training_loss: tensor(0.2954, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [73/1000]: training_loss: tensor(0.3392, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [74/1000]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [75/1000]: training_loss: tensor(0.3410, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [76/1000]: training_loss: tensor(0.3325, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [77/1000]: training_loss: tensor(0.3080, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [78/1000]: training_loss: tensor(0.2609, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [79/1000]: training_loss: tensor(0.2802, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [80/1000]: training_loss: tensor(0.2243, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [81/1000]: training_loss: tensor(0.3150, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [82/1000]: training_loss: tensor(0.3974, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [83/1000]: training_loss: tensor(0.3097, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [84/1000]: training_loss: tensor(0.3463, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [85/1000]: training_loss: tensor(0.3213, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [86/1000]: training_loss: tensor(0.3558, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [87/1000]: training_loss: tensor(0.2861, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [88/1000]: training_loss: tensor(0.2767, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [89/1000]: training_loss: tensor(0.2890, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [90/1000]: training_loss: tensor(0.3585, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [91/1000]: training_loss: tensor(0.3754, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [92/1000]: training_loss: tensor(0.3724, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [93/1000]: training_loss: tensor(0.3098, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [94/1000]: training_loss: tensor(0.3461, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [95/1000]: training_loss: tensor(0.2596, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [96/1000]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [97/1000]: training_loss: tensor(0.3583, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [98/1000]: training_loss: tensor(0.3864, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [99/1000]: training_loss: tensor(0.3413, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [100/1000]: training_loss: tensor(0.3370, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [101/1000]: training_loss: tensor(0.3071, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [102/1000]: training_loss: tensor(0.3134, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [103/1000]: training_loss: tensor(0.3617, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [104/1000]: training_loss: tensor(0.2868, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [105/1000]: training_loss: tensor(0.2661, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [106/1000]: training_loss: tensor(0.3646, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [107/1000]: training_loss: tensor(0.3703, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [108/1000]: training_loss: tensor(0.3905, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [109/1000]: training_loss: tensor(0.3711, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [110/1000]: training_loss: tensor(0.3290, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [111/1000]: training_loss: tensor(0.2913, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [112/1000]: training_loss: tensor(0.3538, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [113/1000]: training_loss: tensor(0.2646, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [114/1000]: training_loss: tensor(0.2948, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [115/1000]: training_loss: tensor(0.3328, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [116/1000]: training_loss: tensor(0.3706, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [117/1000]: training_loss: tensor(0.2710, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [118/1000]: training_loss: tensor(0.2746, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [119/1000]: training_loss: tensor(0.2563, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [120/1000]: training_loss: tensor(0.3996, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [121/1000]: training_loss: tensor(0.2725, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [122/1000]: training_loss: tensor(0.3676, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [123/1000]: training_loss: tensor(0.3149, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [124/1000]: training_loss: tensor(0.2885, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [125/1000]: training_loss: tensor(0.3464, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [126/1000]: training_loss: tensor(0.2629, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [127/1000]: training_loss: tensor(0.3987, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [128/1000]: training_loss: tensor(0.2825, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [129/1000]: training_loss: tensor(0.4573, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [130/1000]: training_loss: tensor(0.3539, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [131/1000]: training_loss: tensor(0.2888, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [132/1000]: training_loss: tensor(0.3487, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [133/1000]: training_loss: tensor(0.3761, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [134/1000]: training_loss: tensor(0.3364, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [135/1000]: training_loss: tensor(0.3849, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [136/1000]: training_loss: tensor(0.3071, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [137/1000]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [138/1000]: training_loss: tensor(0.4270, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [139/1000]: training_loss: tensor(0.3324, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [140/1000]: training_loss: tensor(0.2879, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [141/1000]: training_loss: tensor(0.2223, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [142/1000]: training_loss: tensor(0.2765, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [143/1000]: training_loss: tensor(0.2966, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [144/1000]: training_loss: tensor(0.2925, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [145/1000]: training_loss: tensor(0.2621, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [146/1000]: training_loss: tensor(0.4235, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [147/1000]: training_loss: tensor(0.4451, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [148/1000]: training_loss: tensor(0.3080, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [149/1000]: training_loss: tensor(0.3294, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [150/1000]: training_loss: tensor(0.3659, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [151/1000]: training_loss: tensor(0.2815, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [152/1000]: training_loss: tensor(0.3524, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [153/1000]: training_loss: tensor(0.3455, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [154/1000]: training_loss: tensor(0.3809, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [155/1000]: training_loss: tensor(0.4009, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [156/1000]: training_loss: tensor(0.2769, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [157/1000]: training_loss: tensor(0.3226, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [158/1000]: training_loss: tensor(0.2591, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [159/1000]: training_loss: tensor(0.3324, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [160/1000]: training_loss: tensor(0.2833, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [161/1000]: training_loss: tensor(0.3001, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [162/1000]: training_loss: tensor(0.3019, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [163/1000]: training_loss: tensor(0.3456, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [164/1000]: training_loss: tensor(0.3136, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [165/1000]: training_loss: tensor(0.3830, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [166/1000]: training_loss: tensor(0.3019, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [167/1000]: training_loss: tensor(0.3093, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [168/1000]: training_loss: tensor(0.2812, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [169/1000]: training_loss: tensor(0.3069, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [170/1000]: training_loss: tensor(0.3611, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [171/1000]: training_loss: tensor(0.2923, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [172/1000]: training_loss: tensor(0.3692, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [173/1000]: training_loss: tensor(0.3520, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [174/1000]: training_loss: tensor(0.2777, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [175/1000]: training_loss: tensor(0.3268, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [176/1000]: training_loss: tensor(0.3245, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [177/1000]: training_loss: tensor(0.3996, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [178/1000]: training_loss: tensor(0.2405, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [179/1000]: training_loss: tensor(0.3912, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [180/1000]: training_loss: tensor(0.3415, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [181/1000]: training_loss: tensor(0.2552, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [182/1000]: training_loss: tensor(0.3244, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [183/1000]: training_loss: tensor(0.2672, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [184/1000]: training_loss: tensor(0.3399, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [185/1000]: training_loss: tensor(0.3300, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [186/1000]: training_loss: tensor(0.3013, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [187/1000]: training_loss: tensor(0.3153, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [188/1000]: training_loss: tensor(0.3473, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [189/1000]: training_loss: tensor(0.3075, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [190/1000]: training_loss: tensor(0.2652, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [191/1000]: training_loss: tensor(0.3771, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [192/1000]: training_loss: tensor(0.3134, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [193/1000]: training_loss: tensor(0.3679, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [194/1000]: training_loss: tensor(0.2509, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [195/1000]: training_loss: tensor(0.2912, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [196/1000]: training_loss: tensor(0.2830, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [197/1000]: training_loss: tensor(0.2708, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [198/1000]: training_loss: tensor(0.3281, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [199/1000]: training_loss: tensor(0.2897, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [200/1000]: training_loss: tensor(0.2579, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [201/1000]: training_loss: tensor(0.3197, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [202/1000]: training_loss: tensor(0.3136, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [203/1000]: training_loss: tensor(0.3353, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [204/1000]: training_loss: tensor(0.3823, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [205/1000]: training_loss: tensor(0.3478, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [206/1000]: training_loss: tensor(0.3046, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [207/1000]: training_loss: tensor(0.2645, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [208/1000]: training_loss: tensor(0.3310, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [209/1000]: training_loss: tensor(0.2507, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [210/1000]: training_loss: tensor(0.3716, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [211/1000]: training_loss: tensor(0.3194, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [212/1000]: training_loss: tensor(0.2863, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [213/1000]: training_loss: tensor(0.3591, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [214/1000]: training_loss: tensor(0.2666, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [215/1000]: training_loss: tensor(0.3017, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [216/1000]: training_loss: tensor(0.3502, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [217/1000]: training_loss: tensor(0.2618, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [218/1000]: training_loss: tensor(0.2954, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [219/1000]: training_loss: tensor(0.2946, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [220/1000]: training_loss: tensor(0.2808, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [221/1000]: training_loss: tensor(0.3099, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [222/1000]: training_loss: tensor(0.3655, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [223/1000]: training_loss: tensor(0.2665, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [224/1000]: training_loss: tensor(0.3533, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [225/1000]: training_loss: tensor(0.3360, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [226/1000]: training_loss: tensor(0.3998, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [227/1000]: training_loss: tensor(0.3542, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [228/1000]: training_loss: tensor(0.3957, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [229/1000]: training_loss: tensor(0.2469, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [230/1000]: training_loss: tensor(0.3289, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [231/1000]: training_loss: tensor(0.2919, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [232/1000]: training_loss: tensor(0.2585, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [233/1000]: training_loss: tensor(0.3197, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [234/1000]: training_loss: tensor(0.3343, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [235/1000]: training_loss: tensor(0.3652, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [236/1000]: training_loss: tensor(0.2788, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [237/1000]: training_loss: tensor(0.2871, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [238/1000]: training_loss: tensor(0.2417, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [239/1000]: training_loss: tensor(0.3319, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [240/1000]: training_loss: tensor(0.3117, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [241/1000]: training_loss: tensor(0.3409, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [242/1000]: training_loss: tensor(0.2629, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [243/1000]: training_loss: tensor(0.3171, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [244/1000]: training_loss: tensor(0.4434, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [245/1000]: training_loss: tensor(0.3141, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [246/1000]: training_loss: tensor(0.3934, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [247/1000]: training_loss: tensor(0.2369, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [248/1000]: training_loss: tensor(0.3599, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [249/1000]: training_loss: tensor(0.4602, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [250/1000]: training_loss: tensor(0.3409, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [251/1000]: training_loss: tensor(0.2517, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [252/1000]: training_loss: tensor(0.3376, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [253/1000]: training_loss: tensor(0.3360, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [254/1000]: training_loss: tensor(0.2563, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [255/1000]: training_loss: tensor(0.3109, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [256/1000]: training_loss: tensor(0.3226, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [257/1000]: training_loss: tensor(0.3526, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [258/1000]: training_loss: tensor(0.3656, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [259/1000]: training_loss: tensor(0.4038, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [260/1000]: training_loss: tensor(0.3497, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [261/1000]: training_loss: tensor(0.2826, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [262/1000]: training_loss: tensor(0.3883, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [263/1000]: training_loss: tensor(0.3505, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [264/1000]: training_loss: tensor(0.3121, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [265/1000]: training_loss: tensor(0.2784, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [266/1000]: training_loss: tensor(0.2703, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [267/1000]: training_loss: tensor(0.3091, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [268/1000]: training_loss: tensor(0.3139, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [269/1000]: training_loss: tensor(0.2874, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [270/1000]: training_loss: tensor(0.2805, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [271/1000]: training_loss: tensor(0.2995, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [272/1000]: training_loss: tensor(0.3707, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [273/1000]: training_loss: tensor(0.2942, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [274/1000]: training_loss: tensor(0.3827, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [275/1000]: training_loss: tensor(0.2558, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [276/1000]: training_loss: tensor(0.3896, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [277/1000]: training_loss: tensor(0.3142, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [278/1000]: training_loss: tensor(0.2891, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [279/1000]: training_loss: tensor(0.3769, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [280/1000]: training_loss: tensor(0.3087, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [281/1000]: training_loss: tensor(0.3747, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [282/1000]: training_loss: tensor(0.3085, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [283/1000]: training_loss: tensor(0.2669, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [284/1000]: training_loss: tensor(0.3734, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [285/1000]: training_loss: tensor(0.4189, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [286/1000]: training_loss: tensor(0.2790, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [287/1000]: training_loss: tensor(0.3549, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [288/1000]: training_loss: tensor(0.3085, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [289/1000]: training_loss: tensor(0.2309, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [290/1000]: training_loss: tensor(0.3316, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [291/1000]: training_loss: tensor(0.2665, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [292/1000]: training_loss: tensor(0.3087, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [293/1000]: training_loss: tensor(0.3223, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [294/1000]: training_loss: tensor(0.3108, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [295/1000]: training_loss: tensor(0.2754, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [296/1000]: training_loss: tensor(0.3811, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [297/1000]: training_loss: tensor(0.3185, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [298/1000]: training_loss: tensor(0.3346, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [299/1000]: training_loss: tensor(0.3813, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [300/1000]: training_loss: tensor(0.2330, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [301/1000]: training_loss: tensor(0.3819, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [302/1000]: training_loss: tensor(0.3255, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [303/1000]: training_loss: tensor(0.2867, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [304/1000]: training_loss: tensor(0.3270, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [305/1000]: training_loss: tensor(0.4628, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [306/1000]: training_loss: tensor(0.2191, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [307/1000]: training_loss: tensor(0.3679, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [308/1000]: training_loss: tensor(0.3354, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [309/1000]: training_loss: tensor(0.2843, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [310/1000]: training_loss: tensor(0.3472, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [311/1000]: training_loss: tensor(0.3267, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [312/1000]: training_loss: tensor(0.3356, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [313/1000]: training_loss: tensor(0.3396, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [314/1000]: training_loss: tensor(0.2974, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [315/1000]: training_loss: tensor(0.2876, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [316/1000]: training_loss: tensor(0.3899, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [317/1000]: training_loss: tensor(0.2613, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [318/1000]: training_loss: tensor(0.3429, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [319/1000]: training_loss: tensor(0.2965, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [320/1000]: training_loss: tensor(0.2674, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [321/1000]: training_loss: tensor(0.2457, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [322/1000]: training_loss: tensor(0.5130, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [323/1000]: training_loss: tensor(0.3267, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [324/1000]: training_loss: tensor(0.2985, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [325/1000]: training_loss: tensor(0.3371, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [326/1000]: training_loss: tensor(0.2903, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [327/1000]: training_loss: tensor(0.3047, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [328/1000]: training_loss: tensor(0.2967, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [329/1000]: training_loss: tensor(0.3979, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [330/1000]: training_loss: tensor(0.3206, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [331/1000]: training_loss: tensor(0.2744, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [332/1000]: training_loss: tensor(0.3365, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [333/1000]: training_loss: tensor(0.2977, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [334/1000]: training_loss: tensor(0.3511, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [335/1000]: training_loss: tensor(0.3877, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [336/1000]: training_loss: tensor(0.2569, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [337/1000]: training_loss: tensor(0.3029, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [338/1000]: training_loss: tensor(0.3162, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [339/1000]: training_loss: tensor(0.2641, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [340/1000]: training_loss: tensor(0.3276, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [341/1000]: training_loss: tensor(0.3012, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [342/1000]: training_loss: tensor(0.2732, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [343/1000]: training_loss: tensor(0.3634, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [344/1000]: training_loss: tensor(0.4259, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [345/1000]: training_loss: tensor(0.2223, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [346/1000]: training_loss: tensor(0.3078, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [347/1000]: training_loss: tensor(0.3351, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [348/1000]: training_loss: tensor(0.2582, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [349/1000]: training_loss: tensor(0.3606, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [350/1000]: training_loss: tensor(0.2584, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [351/1000]: training_loss: tensor(0.3022, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [352/1000]: training_loss: tensor(0.3555, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [353/1000]: training_loss: tensor(0.2898, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [354/1000]: training_loss: tensor(0.3081, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [355/1000]: training_loss: tensor(0.3353, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [356/1000]: training_loss: tensor(0.3179, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [357/1000]: training_loss: tensor(0.3508, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [358/1000]: training_loss: tensor(0.3060, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [359/1000]: training_loss: tensor(0.2721, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [360/1000]: training_loss: tensor(0.3558, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [361/1000]: training_loss: tensor(0.3374, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [362/1000]: training_loss: tensor(0.3164, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [363/1000]: training_loss: tensor(0.3449, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [364/1000]: training_loss: tensor(0.4038, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [365/1000]: training_loss: tensor(0.3433, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [366/1000]: training_loss: tensor(0.3319, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [367/1000]: training_loss: tensor(0.2872, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [368/1000]: training_loss: tensor(0.3159, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [369/1000]: training_loss: tensor(0.3678, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [370/1000]: training_loss: tensor(0.2812, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [371/1000]: training_loss: tensor(0.2654, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [372/1000]: training_loss: tensor(0.3302, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [373/1000]: training_loss: tensor(0.3261, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [374/1000]: training_loss: tensor(0.3506, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [375/1000]: training_loss: tensor(0.2554, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [376/1000]: training_loss: tensor(0.3210, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [377/1000]: training_loss: tensor(0.3128, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [378/1000]: training_loss: tensor(0.3406, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [379/1000]: training_loss: tensor(0.2878, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [380/1000]: training_loss: tensor(0.3244, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [381/1000]: training_loss: tensor(0.3457, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [382/1000]: training_loss: tensor(0.4044, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [383/1000]: training_loss: tensor(0.3162, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [384/1000]: training_loss: tensor(0.3368, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [385/1000]: training_loss: tensor(0.3095, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [386/1000]: training_loss: tensor(0.2805, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [387/1000]: training_loss: tensor(0.3240, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [388/1000]: training_loss: tensor(0.3118, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [389/1000]: training_loss: tensor(0.4206, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [390/1000]: training_loss: tensor(0.2965, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [391/1000]: training_loss: tensor(0.3583, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [392/1000]: training_loss: tensor(0.3255, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [393/1000]: training_loss: tensor(0.3984, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [394/1000]: training_loss: tensor(0.3039, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [395/1000]: training_loss: tensor(0.2880, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [396/1000]: training_loss: tensor(0.3120, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [397/1000]: training_loss: tensor(0.2877, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [398/1000]: training_loss: tensor(0.2940, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [399/1000]: training_loss: tensor(0.3391, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [400/1000]: training_loss: tensor(0.2766, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [401/1000]: training_loss: tensor(0.3402, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [402/1000]: training_loss: tensor(0.3110, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [403/1000]: training_loss: tensor(0.3994, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [404/1000]: training_loss: tensor(0.2938, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [405/1000]: training_loss: tensor(0.3167, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [406/1000]: training_loss: tensor(0.4054, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [407/1000]: training_loss: tensor(0.2873, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [408/1000]: training_loss: tensor(0.2660, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [409/1000]: training_loss: tensor(0.3338, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [410/1000]: training_loss: tensor(0.2837, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [411/1000]: training_loss: tensor(0.3427, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [412/1000]: training_loss: tensor(0.3190, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [413/1000]: training_loss: tensor(0.3488, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [414/1000]: training_loss: tensor(0.2916, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [415/1000]: training_loss: tensor(0.3785, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [416/1000]: training_loss: tensor(0.2967, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [417/1000]: training_loss: tensor(0.3782, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [418/1000]: training_loss: tensor(0.2623, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [419/1000]: training_loss: tensor(0.4070, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [420/1000]: training_loss: tensor(0.3881, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [421/1000]: training_loss: tensor(0.3515, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [422/1000]: training_loss: tensor(0.2678, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [423/1000]: training_loss: tensor(0.3403, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [424/1000]: training_loss: tensor(0.3570, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [425/1000]: training_loss: tensor(0.3903, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [426/1000]: training_loss: tensor(0.3361, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [427/1000]: training_loss: tensor(0.3306, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [428/1000]: training_loss: tensor(0.2786, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [429/1000]: training_loss: tensor(0.2603, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [430/1000]: training_loss: tensor(0.2864, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [431/1000]: training_loss: tensor(0.2884, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [432/1000]: training_loss: tensor(0.2861, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [433/1000]: training_loss: tensor(0.2999, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [434/1000]: training_loss: tensor(0.3661, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [435/1000]: training_loss: tensor(0.3444, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [436/1000]: training_loss: tensor(0.3087, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [437/1000]: training_loss: tensor(0.2841, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [438/1000]: training_loss: tensor(0.3397, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [439/1000]: training_loss: tensor(0.3113, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [440/1000]: training_loss: tensor(0.2891, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [441/1000]: training_loss: tensor(0.3023, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [442/1000]: training_loss: tensor(0.3682, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [443/1000]: training_loss: tensor(0.3947, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [444/1000]: training_loss: tensor(0.3054, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [445/1000]: training_loss: tensor(0.3429, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [446/1000]: training_loss: tensor(0.3243, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [447/1000]: training_loss: tensor(0.2886, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [448/1000]: training_loss: tensor(0.3376, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [449/1000]: training_loss: tensor(0.2538, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [450/1000]: training_loss: tensor(0.3218, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [451/1000]: training_loss: tensor(0.3367, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [452/1000]: training_loss: tensor(0.2733, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [453/1000]: training_loss: tensor(0.2863, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [454/1000]: training_loss: tensor(0.3256, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [455/1000]: training_loss: tensor(0.2809, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [456/1000]: training_loss: tensor(0.3526, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [457/1000]: training_loss: tensor(0.3140, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [458/1000]: training_loss: tensor(0.3330, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [459/1000]: training_loss: tensor(0.2685, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [460/1000]: training_loss: tensor(0.3249, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [461/1000]: training_loss: tensor(0.3444, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [462/1000]: training_loss: tensor(0.2871, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [463/1000]: training_loss: tensor(0.3135, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [464/1000]: training_loss: tensor(0.3677, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [465/1000]: training_loss: tensor(0.2886, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [466/1000]: training_loss: tensor(0.3304, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [467/1000]: training_loss: tensor(0.2868, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [468/1000]: training_loss: tensor(0.2530, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [469/1000]: training_loss: tensor(0.3772, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [470/1000]: training_loss: tensor(0.2909, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [471/1000]: training_loss: tensor(0.3352, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [472/1000]: training_loss: tensor(0.3304, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [473/1000]: training_loss: tensor(0.2962, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [474/1000]: training_loss: tensor(0.4459, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [475/1000]: training_loss: tensor(0.2723, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [476/1000]: training_loss: tensor(0.3138, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [477/1000]: training_loss: tensor(0.3513, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [478/1000]: training_loss: tensor(0.2959, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [479/1000]: training_loss: tensor(0.3668, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [480/1000]: training_loss: tensor(0.3425, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [481/1000]: training_loss: tensor(0.3164, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [482/1000]: training_loss: tensor(0.3303, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [483/1000]: training_loss: tensor(0.3907, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [484/1000]: training_loss: tensor(0.2701, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [485/1000]: training_loss: tensor(0.2737, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [486/1000]: training_loss: tensor(0.2578, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [487/1000]: training_loss: tensor(0.2734, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [488/1000]: training_loss: tensor(0.3343, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [489/1000]: training_loss: tensor(0.2643, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [490/1000]: training_loss: tensor(0.3255, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [491/1000]: training_loss: tensor(0.3332, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [492/1000]: training_loss: tensor(0.3427, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [493/1000]: training_loss: tensor(0.2342, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [494/1000]: training_loss: tensor(0.3667, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [495/1000]: training_loss: tensor(0.4029, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [496/1000]: training_loss: tensor(0.2854, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [497/1000]: training_loss: tensor(0.3464, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [498/1000]: training_loss: tensor(0.3818, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [499/1000]: training_loss: tensor(0.2623, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [500/1000]: training_loss: tensor(0.2682, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [501/1000]: training_loss: tensor(0.3214, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [502/1000]: training_loss: tensor(0.2909, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [503/1000]: training_loss: tensor(0.2622, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [504/1000]: training_loss: tensor(0.2568, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [505/1000]: training_loss: tensor(0.2714, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [506/1000]: training_loss: tensor(0.3158, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [507/1000]: training_loss: tensor(0.3069, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [508/1000]: training_loss: tensor(0.4000, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [509/1000]: training_loss: tensor(0.3142, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [510/1000]: training_loss: tensor(0.2755, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [511/1000]: training_loss: tensor(0.3009, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [512/1000]: training_loss: tensor(0.4023, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [513/1000]: training_loss: tensor(0.3145, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [514/1000]: training_loss: tensor(0.3539, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [515/1000]: training_loss: tensor(0.2964, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [516/1000]: training_loss: tensor(0.2958, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [517/1000]: training_loss: tensor(0.2454, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [518/1000]: training_loss: tensor(0.3976, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [519/1000]: training_loss: tensor(0.3076, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [520/1000]: training_loss: tensor(0.2888, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [521/1000]: training_loss: tensor(0.2980, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [522/1000]: training_loss: tensor(0.3425, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [523/1000]: training_loss: tensor(0.3165, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [524/1000]: training_loss: tensor(0.3684, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [525/1000]: training_loss: tensor(0.2829, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [526/1000]: training_loss: tensor(0.2554, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [527/1000]: training_loss: tensor(0.3466, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [528/1000]: training_loss: tensor(0.2685, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [529/1000]: training_loss: tensor(0.3184, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [530/1000]: training_loss: tensor(0.3382, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [531/1000]: training_loss: tensor(0.3514, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [532/1000]: training_loss: tensor(0.3502, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [533/1000]: training_loss: tensor(0.3749, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [534/1000]: training_loss: tensor(0.2457, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [535/1000]: training_loss: tensor(0.3429, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [536/1000]: training_loss: tensor(0.4051, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [537/1000]: training_loss: tensor(0.2700, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [538/1000]: training_loss: tensor(0.3602, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [539/1000]: training_loss: tensor(0.3414, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [540/1000]: training_loss: tensor(0.3492, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [541/1000]: training_loss: tensor(0.3178, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [542/1000]: training_loss: tensor(0.2266, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [543/1000]: training_loss: tensor(0.2789, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [544/1000]: training_loss: tensor(0.3844, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [545/1000]: training_loss: tensor(0.2954, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [546/1000]: training_loss: tensor(0.3852, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [547/1000]: training_loss: tensor(0.4303, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [548/1000]: training_loss: tensor(0.2879, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [549/1000]: training_loss: tensor(0.3611, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [550/1000]: training_loss: tensor(0.3181, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [551/1000]: training_loss: tensor(0.2370, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [552/1000]: training_loss: tensor(0.3005, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [553/1000]: training_loss: tensor(0.2979, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [554/1000]: training_loss: tensor(0.3187, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [555/1000]: training_loss: tensor(0.3989, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [556/1000]: training_loss: tensor(0.3457, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [557/1000]: training_loss: tensor(0.3057, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [558/1000]: training_loss: tensor(0.2896, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [559/1000]: training_loss: tensor(0.2996, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [560/1000]: training_loss: tensor(0.2955, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [561/1000]: training_loss: tensor(0.2714, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [562/1000]: training_loss: tensor(0.2720, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [563/1000]: training_loss: tensor(0.3132, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [564/1000]: training_loss: tensor(0.3431, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [565/1000]: training_loss: tensor(0.2841, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [566/1000]: training_loss: tensor(0.3697, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [567/1000]: training_loss: tensor(0.2831, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [568/1000]: training_loss: tensor(0.3388, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [569/1000]: training_loss: tensor(0.3989, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [570/1000]: training_loss: tensor(0.3539, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [571/1000]: training_loss: tensor(0.2869, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [572/1000]: training_loss: tensor(0.2636, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [573/1000]: training_loss: tensor(0.3211, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [574/1000]: training_loss: tensor(0.3506, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [575/1000]: training_loss: tensor(0.3196, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [576/1000]: training_loss: tensor(0.2517, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [577/1000]: training_loss: tensor(0.3753, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [578/1000]: training_loss: tensor(0.3101, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [579/1000]: training_loss: tensor(0.3217, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [580/1000]: training_loss: tensor(0.3188, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [581/1000]: training_loss: tensor(0.3020, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [582/1000]: training_loss: tensor(0.2954, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [583/1000]: training_loss: tensor(0.2880, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [584/1000]: training_loss: tensor(0.2806, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [585/1000]: training_loss: tensor(0.3895, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [586/1000]: training_loss: tensor(0.4226, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [587/1000]: training_loss: tensor(0.3389, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [588/1000]: training_loss: tensor(0.3434, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [589/1000]: training_loss: tensor(0.3435, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [590/1000]: training_loss: tensor(0.2931, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [591/1000]: training_loss: tensor(0.3109, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [592/1000]: training_loss: tensor(0.2906, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [593/1000]: training_loss: tensor(0.3947, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [594/1000]: training_loss: tensor(0.2585, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [595/1000]: training_loss: tensor(0.3439, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [596/1000]: training_loss: tensor(0.2866, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [597/1000]: training_loss: tensor(0.2868, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [598/1000]: training_loss: tensor(0.2891, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [599/1000]: training_loss: tensor(0.3166, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [600/1000]: training_loss: tensor(0.3108, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [601/1000]: training_loss: tensor(0.3080, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [602/1000]: training_loss: tensor(0.3538, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [603/1000]: training_loss: tensor(0.3544, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [604/1000]: training_loss: tensor(0.3210, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [605/1000]: training_loss: tensor(0.3107, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [606/1000]: training_loss: tensor(0.2604, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [607/1000]: training_loss: tensor(0.3229, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [608/1000]: training_loss: tensor(0.2568, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [609/1000]: training_loss: tensor(0.2482, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [610/1000]: training_loss: tensor(0.3075, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [611/1000]: training_loss: tensor(0.2775, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [612/1000]: training_loss: tensor(0.3763, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [613/1000]: training_loss: tensor(0.3098, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [614/1000]: training_loss: tensor(0.2385, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [615/1000]: training_loss: tensor(0.3372, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [616/1000]: training_loss: tensor(0.2731, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [617/1000]: training_loss: tensor(0.3070, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [618/1000]: training_loss: tensor(0.3154, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [619/1000]: training_loss: tensor(0.2818, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [620/1000]: training_loss: tensor(0.3638, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [621/1000]: training_loss: tensor(0.3308, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [622/1000]: training_loss: tensor(0.3393, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [623/1000]: training_loss: tensor(0.3761, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [624/1000]: training_loss: tensor(0.2960, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [625/1000]: training_loss: tensor(0.2931, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [626/1000]: training_loss: tensor(0.2970, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [627/1000]: training_loss: tensor(0.3342, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [628/1000]: training_loss: tensor(0.2567, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [629/1000]: training_loss: tensor(0.2888, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [630/1000]: training_loss: tensor(0.3148, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [631/1000]: training_loss: tensor(0.2778, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [632/1000]: training_loss: tensor(0.3400, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [633/1000]: training_loss: tensor(0.3567, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [634/1000]: training_loss: tensor(0.2697, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [635/1000]: training_loss: tensor(0.2444, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [636/1000]: training_loss: tensor(0.3664, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [637/1000]: training_loss: tensor(0.3265, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [638/1000]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [639/1000]: training_loss: tensor(0.2777, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [640/1000]: training_loss: tensor(0.3360, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [641/1000]: training_loss: tensor(0.3342, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [642/1000]: training_loss: tensor(0.3130, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [643/1000]: training_loss: tensor(0.3550, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [644/1000]: training_loss: tensor(0.2858, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [645/1000]: training_loss: tensor(0.2623, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [646/1000]: training_loss: tensor(0.2715, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [647/1000]: training_loss: tensor(0.2803, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [648/1000]: training_loss: tensor(0.3218, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [649/1000]: training_loss: tensor(0.2310, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [650/1000]: training_loss: tensor(0.2956, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [651/1000]: training_loss: tensor(0.4220, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [652/1000]: training_loss: tensor(0.3339, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [653/1000]: training_loss: tensor(0.3204, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [654/1000]: training_loss: tensor(0.3124, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [655/1000]: training_loss: tensor(0.3400, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [656/1000]: training_loss: tensor(0.3271, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [657/1000]: training_loss: tensor(0.2332, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [658/1000]: training_loss: tensor(0.2810, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [659/1000]: training_loss: tensor(0.3356, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [660/1000]: training_loss: tensor(0.3001, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [661/1000]: training_loss: tensor(0.3160, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [662/1000]: training_loss: tensor(0.2898, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [663/1000]: training_loss: tensor(0.3463, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [664/1000]: training_loss: tensor(0.3140, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [665/1000]: training_loss: tensor(0.2867, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [666/1000]: training_loss: tensor(0.2916, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [667/1000]: training_loss: tensor(0.3192, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [668/1000]: training_loss: tensor(0.2863, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [669/1000]: training_loss: tensor(0.3755, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [670/1000]: training_loss: tensor(0.3290, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [671/1000]: training_loss: tensor(0.4052, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [672/1000]: training_loss: tensor(0.2197, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [673/1000]: training_loss: tensor(0.2751, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [674/1000]: training_loss: tensor(0.3790, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [675/1000]: training_loss: tensor(0.3623, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [676/1000]: training_loss: tensor(0.2839, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [677/1000]: training_loss: tensor(0.2843, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [678/1000]: training_loss: tensor(0.3314, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [679/1000]: training_loss: tensor(0.3141, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [680/1000]: training_loss: tensor(0.3633, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [681/1000]: training_loss: tensor(0.3487, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [682/1000]: training_loss: tensor(0.2633, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [683/1000]: training_loss: tensor(0.2951, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [684/1000]: training_loss: tensor(0.2698, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [685/1000]: training_loss: tensor(0.2264, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [686/1000]: training_loss: tensor(0.3413, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [687/1000]: training_loss: tensor(0.3395, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [688/1000]: training_loss: tensor(0.3260, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [689/1000]: training_loss: tensor(0.2845, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [690/1000]: training_loss: tensor(0.2648, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [691/1000]: training_loss: tensor(0.3141, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [692/1000]: training_loss: tensor(0.3973, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [693/1000]: training_loss: tensor(0.3665, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [694/1000]: training_loss: tensor(0.3814, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [695/1000]: training_loss: tensor(0.2324, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [696/1000]: training_loss: tensor(0.3229, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [697/1000]: training_loss: tensor(0.3150, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [698/1000]: training_loss: tensor(0.3596, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [699/1000]: training_loss: tensor(0.2940, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [700/1000]: training_loss: tensor(0.3280, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [701/1000]: training_loss: tensor(0.2755, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [702/1000]: training_loss: tensor(0.4233, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [703/1000]: training_loss: tensor(0.3584, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [704/1000]: training_loss: tensor(0.3424, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [705/1000]: training_loss: tensor(0.3327, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [706/1000]: training_loss: tensor(0.3073, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [707/1000]: training_loss: tensor(0.3652, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [708/1000]: training_loss: tensor(0.3149, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [709/1000]: training_loss: tensor(0.3057, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [710/1000]: training_loss: tensor(0.2659, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [711/1000]: training_loss: tensor(0.3566, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [712/1000]: training_loss: tensor(0.4279, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [713/1000]: training_loss: tensor(0.3116, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [714/1000]: training_loss: tensor(0.3145, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [715/1000]: training_loss: tensor(0.2868, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [716/1000]: training_loss: tensor(0.3391, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [717/1000]: training_loss: tensor(0.2438, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [718/1000]: training_loss: tensor(0.3768, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [719/1000]: training_loss: tensor(0.3334, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [720/1000]: training_loss: tensor(0.2839, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [721/1000]: training_loss: tensor(0.2375, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [722/1000]: training_loss: tensor(0.3180, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [723/1000]: training_loss: tensor(0.2909, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [724/1000]: training_loss: tensor(0.3459, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [725/1000]: training_loss: tensor(0.2733, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [726/1000]: training_loss: tensor(0.2888, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [727/1000]: training_loss: tensor(0.4484, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [728/1000]: training_loss: tensor(0.3174, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [729/1000]: training_loss: tensor(0.2941, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [730/1000]: training_loss: tensor(0.3399, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [731/1000]: training_loss: tensor(0.3651, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [732/1000]: training_loss: tensor(0.4120, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [733/1000]: training_loss: tensor(0.3456, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [734/1000]: training_loss: tensor(0.4483, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [735/1000]: training_loss: tensor(0.3693, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [736/1000]: training_loss: tensor(0.3438, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [737/1000]: training_loss: tensor(0.3468, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [738/1000]: training_loss: tensor(0.2953, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [739/1000]: training_loss: tensor(0.3766, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [740/1000]: training_loss: tensor(0.3978, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [741/1000]: training_loss: tensor(0.2580, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [742/1000]: training_loss: tensor(0.3317, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [743/1000]: training_loss: tensor(0.3047, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [744/1000]: training_loss: tensor(0.2882, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [745/1000]: training_loss: tensor(0.3363, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [746/1000]: training_loss: tensor(0.3271, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [747/1000]: training_loss: tensor(0.3605, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [748/1000]: training_loss: tensor(0.4007, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [749/1000]: training_loss: tensor(0.3006, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [750/1000]: training_loss: tensor(0.3709, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [751/1000]: training_loss: tensor(0.3502, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [752/1000]: training_loss: tensor(0.2517, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [753/1000]: training_loss: tensor(0.2903, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [754/1000]: training_loss: tensor(0.3442, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [755/1000]: training_loss: tensor(0.3946, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [756/1000]: training_loss: tensor(0.3662, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [757/1000]: training_loss: tensor(0.2894, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [758/1000]: training_loss: tensor(0.3421, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [759/1000]: training_loss: tensor(0.2444, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [760/1000]: training_loss: tensor(0.2504, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [761/1000]: training_loss: tensor(0.2758, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [762/1000]: training_loss: tensor(0.2989, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [763/1000]: training_loss: tensor(0.3020, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [764/1000]: training_loss: tensor(0.3700, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [765/1000]: training_loss: tensor(0.2854, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [766/1000]: training_loss: tensor(0.3099, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [767/1000]: training_loss: tensor(0.2950, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [768/1000]: training_loss: tensor(0.2662, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [769/1000]: training_loss: tensor(0.3820, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [770/1000]: training_loss: tensor(0.3060, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [771/1000]: training_loss: tensor(0.2835, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [772/1000]: training_loss: tensor(0.3981, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [773/1000]: training_loss: tensor(0.3213, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [774/1000]: training_loss: tensor(0.2998, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [775/1000]: training_loss: tensor(0.2705, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [776/1000]: training_loss: tensor(0.3512, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [777/1000]: training_loss: tensor(0.2628, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [778/1000]: training_loss: tensor(0.3225, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [779/1000]: training_loss: tensor(0.2388, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [780/1000]: training_loss: tensor(0.2894, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [781/1000]: training_loss: tensor(0.2990, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [782/1000]: training_loss: tensor(0.3652, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [783/1000]: training_loss: tensor(0.3138, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [784/1000]: training_loss: tensor(0.3002, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [785/1000]: training_loss: tensor(0.2846, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [786/1000]: training_loss: tensor(0.3212, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [787/1000]: training_loss: tensor(0.2681, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [788/1000]: training_loss: tensor(0.3033, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [789/1000]: training_loss: tensor(0.3594, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [790/1000]: training_loss: tensor(0.3229, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [791/1000]: training_loss: tensor(0.2555, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [792/1000]: training_loss: tensor(0.4082, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [793/1000]: training_loss: tensor(0.2961, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [794/1000]: training_loss: tensor(0.2651, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [795/1000]: training_loss: tensor(0.3407, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [796/1000]: training_loss: tensor(0.2897, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [797/1000]: training_loss: tensor(0.3549, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [798/1000]: training_loss: tensor(0.2851, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [799/1000]: training_loss: tensor(0.2577, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [800/1000]: training_loss: tensor(0.2966, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [801/1000]: training_loss: tensor(0.3096, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [802/1000]: training_loss: tensor(0.2637, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [803/1000]: training_loss: tensor(0.2642, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [804/1000]: training_loss: tensor(0.3601, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [805/1000]: training_loss: tensor(0.3209, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [806/1000]: training_loss: tensor(0.3835, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [807/1000]: training_loss: tensor(0.2961, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [808/1000]: training_loss: tensor(0.2750, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [809/1000]: training_loss: tensor(0.2241, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [810/1000]: training_loss: tensor(0.3082, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [811/1000]: training_loss: tensor(0.3655, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [812/1000]: training_loss: tensor(0.2990, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [813/1000]: training_loss: tensor(0.4154, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [814/1000]: training_loss: tensor(0.3511, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [815/1000]: training_loss: tensor(0.2973, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [816/1000]: training_loss: tensor(0.3759, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [817/1000]: training_loss: tensor(0.3012, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [818/1000]: training_loss: tensor(0.2507, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [819/1000]: training_loss: tensor(0.3467, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [820/1000]: training_loss: tensor(0.3424, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [821/1000]: training_loss: tensor(0.2801, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [822/1000]: training_loss: tensor(0.2883, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [823/1000]: training_loss: tensor(0.3170, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [824/1000]: training_loss: tensor(0.3073, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [825/1000]: training_loss: tensor(0.2774, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [826/1000]: training_loss: tensor(0.2427, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [827/1000]: training_loss: tensor(0.3366, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [828/1000]: training_loss: tensor(0.3209, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [829/1000]: training_loss: tensor(0.3052, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [830/1000]: training_loss: tensor(0.2935, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [831/1000]: training_loss: tensor(0.3241, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [832/1000]: training_loss: tensor(0.3895, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [833/1000]: training_loss: tensor(0.3055, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [834/1000]: training_loss: tensor(0.3491, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [835/1000]: training_loss: tensor(0.2852, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [836/1000]: training_loss: tensor(0.2499, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [837/1000]: training_loss: tensor(0.3604, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [838/1000]: training_loss: tensor(0.3152, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [839/1000]: training_loss: tensor(0.3358, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [840/1000]: training_loss: tensor(0.2900, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [841/1000]: training_loss: tensor(0.2493, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [842/1000]: training_loss: tensor(0.3217, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [843/1000]: training_loss: tensor(0.2931, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [844/1000]: training_loss: tensor(0.3644, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [845/1000]: training_loss: tensor(0.3273, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [846/1000]: training_loss: tensor(0.2949, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [847/1000]: training_loss: tensor(0.2789, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [848/1000]: training_loss: tensor(0.3413, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [849/1000]: training_loss: tensor(0.3273, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [850/1000]: training_loss: tensor(0.2482, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [851/1000]: training_loss: tensor(0.2881, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [852/1000]: training_loss: tensor(0.2778, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [853/1000]: training_loss: tensor(0.2825, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [854/1000]: training_loss: tensor(0.2608, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [855/1000]: training_loss: tensor(0.3112, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [856/1000]: training_loss: tensor(0.4091, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [857/1000]: training_loss: tensor(0.3766, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [858/1000]: training_loss: tensor(0.2526, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [859/1000]: training_loss: tensor(0.4071, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [860/1000]: training_loss: tensor(0.3466, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [861/1000]: training_loss: tensor(0.3632, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [862/1000]: training_loss: tensor(0.3127, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [863/1000]: training_loss: tensor(0.3828, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [864/1000]: training_loss: tensor(0.3239, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [865/1000]: training_loss: tensor(0.3643, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [866/1000]: training_loss: tensor(0.3327, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [867/1000]: training_loss: tensor(0.3714, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [868/1000]: training_loss: tensor(0.3208, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [869/1000]: training_loss: tensor(0.2901, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [870/1000]: training_loss: tensor(0.3438, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [871/1000]: training_loss: tensor(0.2732, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [872/1000]: training_loss: tensor(0.3713, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [873/1000]: training_loss: tensor(0.3335, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [874/1000]: training_loss: tensor(0.2994, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [875/1000]: training_loss: tensor(0.2799, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [876/1000]: training_loss: tensor(0.3432, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [877/1000]: training_loss: tensor(0.3265, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [878/1000]: training_loss: tensor(0.3971, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [879/1000]: training_loss: tensor(0.2779, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [880/1000]: training_loss: tensor(0.3309, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [881/1000]: training_loss: tensor(0.2544, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [882/1000]: training_loss: tensor(0.3420, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [883/1000]: training_loss: tensor(0.3646, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [884/1000]: training_loss: tensor(0.3671, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [885/1000]: training_loss: tensor(0.3451, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [886/1000]: training_loss: tensor(0.3349, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [887/1000]: training_loss: tensor(0.2536, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [888/1000]: training_loss: tensor(0.3013, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [889/1000]: training_loss: tensor(0.3656, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [890/1000]: training_loss: tensor(0.3569, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [891/1000]: training_loss: tensor(0.3384, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [892/1000]: training_loss: tensor(0.3235, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [893/1000]: training_loss: tensor(0.3743, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [894/1000]: training_loss: tensor(0.4112, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [895/1000]: training_loss: tensor(0.3430, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [896/1000]: training_loss: tensor(0.2640, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [897/1000]: training_loss: tensor(0.2992, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [898/1000]: training_loss: tensor(0.3415, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [899/1000]: training_loss: tensor(0.2620, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [900/1000]: training_loss: tensor(0.3582, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [901/1000]: training_loss: tensor(0.3229, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [902/1000]: training_loss: tensor(0.2778, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [903/1000]: training_loss: tensor(0.2500, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [904/1000]: training_loss: tensor(0.3359, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [905/1000]: training_loss: tensor(0.3138, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [906/1000]: training_loss: tensor(0.3464, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [907/1000]: training_loss: tensor(0.3074, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [908/1000]: training_loss: tensor(0.2928, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [909/1000]: training_loss: tensor(0.2811, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [910/1000]: training_loss: tensor(0.4184, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [911/1000]: training_loss: tensor(0.3059, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [912/1000]: training_loss: tensor(0.2560, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [913/1000]: training_loss: tensor(0.2985, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [914/1000]: training_loss: tensor(0.3443, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [915/1000]: training_loss: tensor(0.3336, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [916/1000]: training_loss: tensor(0.2880, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [917/1000]: training_loss: tensor(0.3474, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [918/1000]: training_loss: tensor(0.3380, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [919/1000]: training_loss: tensor(0.2715, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [920/1000]: training_loss: tensor(0.2880, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [921/1000]: training_loss: tensor(0.2749, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [922/1000]: training_loss: tensor(0.3884, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [923/1000]: training_loss: tensor(0.3172, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [924/1000]: training_loss: tensor(0.3773, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [925/1000]: training_loss: tensor(0.3797, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [926/1000]: training_loss: tensor(0.3914, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [927/1000]: training_loss: tensor(0.2897, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [928/1000]: training_loss: tensor(0.3324, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [929/1000]: training_loss: tensor(0.3678, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [930/1000]: training_loss: tensor(0.3064, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [931/1000]: training_loss: tensor(0.3472, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [932/1000]: training_loss: tensor(0.4024, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [933/1000]: training_loss: tensor(0.2449, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [934/1000]: training_loss: tensor(0.2985, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [935/1000]: training_loss: tensor(0.3126, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [936/1000]: training_loss: tensor(0.2625, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [937/1000]: training_loss: tensor(0.3280, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [938/1000]: training_loss: tensor(0.2575, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [939/1000]: training_loss: tensor(0.2952, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [940/1000]: training_loss: tensor(0.2901, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [941/1000]: training_loss: tensor(0.3293, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [942/1000]: training_loss: tensor(0.2906, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [943/1000]: training_loss: tensor(0.3237, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [944/1000]: training_loss: tensor(0.3394, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [945/1000]: training_loss: tensor(0.3314, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [946/1000]: training_loss: tensor(0.2997, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [947/1000]: training_loss: tensor(0.2429, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [948/1000]: training_loss: tensor(0.2662, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [949/1000]: training_loss: tensor(0.3517, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [950/1000]: training_loss: tensor(0.2998, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [951/1000]: training_loss: tensor(0.3818, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [952/1000]: training_loss: tensor(0.2465, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [953/1000]: training_loss: tensor(0.3363, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [954/1000]: training_loss: tensor(0.3077, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [955/1000]: training_loss: tensor(0.3862, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [956/1000]: training_loss: tensor(0.3804, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [957/1000]: training_loss: tensor(0.3387, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [958/1000]: training_loss: tensor(0.2868, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [959/1000]: training_loss: tensor(0.4235, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [960/1000]: training_loss: tensor(0.4096, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [961/1000]: training_loss: tensor(0.3561, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [962/1000]: training_loss: tensor(0.3096, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [963/1000]: training_loss: tensor(0.3852, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [964/1000]: training_loss: tensor(0.3019, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [965/1000]: training_loss: tensor(0.3385, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [966/1000]: training_loss: tensor(0.3110, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [967/1000]: training_loss: tensor(0.3374, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [968/1000]: training_loss: tensor(0.3749, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [969/1000]: training_loss: tensor(0.3390, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [970/1000]: training_loss: tensor(0.3008, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [971/1000]: training_loss: tensor(0.3923, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [972/1000]: training_loss: tensor(0.2830, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [973/1000]: training_loss: tensor(0.3452, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [974/1000]: training_loss: tensor(0.2613, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [975/1000]: training_loss: tensor(0.3216, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [976/1000]: training_loss: tensor(0.3337, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [977/1000]: training_loss: tensor(0.3790, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [978/1000]: training_loss: tensor(0.3263, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [979/1000]: training_loss: tensor(0.2580, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [980/1000]: training_loss: tensor(0.2426, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [981/1000]: training_loss: tensor(0.2911, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [982/1000]: training_loss: tensor(0.2926, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [983/1000]: training_loss: tensor(0.3797, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [984/1000]: training_loss: tensor(0.3256, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [985/1000]: training_loss: tensor(0.3496, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [986/1000]: training_loss: tensor(0.3358, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [987/1000]: training_loss: tensor(0.3231, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [988/1000]: training_loss: tensor(0.3106, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [989/1000]: training_loss: tensor(0.3611, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [990/1000]: training_loss: tensor(0.3637, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [991/1000]: training_loss: tensor(0.3121, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [992/1000]: training_loss: tensor(0.2948, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [993/1000]: training_loss: tensor(0.2581, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [994/1000]: training_loss: tensor(0.2470, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [995/1000]: training_loss: tensor(0.2953, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [996/1000]: training_loss: tensor(0.2653, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [997/1000]: training_loss: tensor(0.2781, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [998/1000]: training_loss: tensor(0.4367, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [999/1000]: training_loss: tensor(0.3095, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Epoch [1/3], global step [1000/3000], Train Loss: 0.3198, Valid Loss: 0.3322\n",
      "batch_no [0/1000]: training_loss: tensor(0.2991, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [1/1000]: training_loss: tensor(0.3428, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [2/1000]: training_loss: tensor(0.3182, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3/1000]: training_loss: tensor(0.3353, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [4/1000]: training_loss: tensor(0.2330, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [5/1000]: training_loss: tensor(0.3246, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [6/1000]: training_loss: tensor(0.3336, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [7/1000]: training_loss: tensor(0.2715, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [8/1000]: training_loss: tensor(0.3596, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [9/1000]: training_loss: tensor(0.3415, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [10/1000]: training_loss: tensor(0.3750, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [11/1000]: training_loss: tensor(0.2682, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [12/1000]: training_loss: tensor(0.3509, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [13/1000]: training_loss: tensor(0.2714, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [14/1000]: training_loss: tensor(0.2755, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [15/1000]: training_loss: tensor(0.3336, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [16/1000]: training_loss: tensor(0.2560, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [17/1000]: training_loss: tensor(0.3177, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [18/1000]: training_loss: tensor(0.3676, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [19/1000]: training_loss: tensor(0.3439, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [20/1000]: training_loss: tensor(0.3159, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [21/1000]: training_loss: tensor(0.2789, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [22/1000]: training_loss: tensor(0.3030, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [23/1000]: training_loss: tensor(0.2696, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [24/1000]: training_loss: tensor(0.4026, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [25/1000]: training_loss: tensor(0.2629, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [26/1000]: training_loss: tensor(0.2677, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [27/1000]: training_loss: tensor(0.2516, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [28/1000]: training_loss: tensor(0.3509, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [29/1000]: training_loss: tensor(0.3211, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [30/1000]: training_loss: tensor(0.2983, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [31/1000]: training_loss: tensor(0.3914, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [32/1000]: training_loss: tensor(0.2610, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [33/1000]: training_loss: tensor(0.2564, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [34/1000]: training_loss: tensor(0.2563, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [35/1000]: training_loss: tensor(0.3049, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [36/1000]: training_loss: tensor(0.2929, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [37/1000]: training_loss: tensor(0.2291, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [38/1000]: training_loss: tensor(0.2457, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [39/1000]: training_loss: tensor(0.2969, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [40/1000]: training_loss: tensor(0.3356, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [41/1000]: training_loss: tensor(0.3496, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [42/1000]: training_loss: tensor(0.2716, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [43/1000]: training_loss: tensor(0.3085, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [44/1000]: training_loss: tensor(0.2531, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [45/1000]: training_loss: tensor(0.3566, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [46/1000]: training_loss: tensor(0.2953, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [47/1000]: training_loss: tensor(0.3485, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [48/1000]: training_loss: tensor(0.3057, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [49/1000]: training_loss: tensor(0.3490, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [50/1000]: training_loss: tensor(0.3150, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [51/1000]: training_loss: tensor(0.2669, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [52/1000]: training_loss: tensor(0.3718, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [53/1000]: training_loss: tensor(0.3380, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [54/1000]: training_loss: tensor(0.3845, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [55/1000]: training_loss: tensor(0.3078, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [56/1000]: training_loss: tensor(0.2698, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [57/1000]: training_loss: tensor(0.3975, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [58/1000]: training_loss: tensor(0.3148, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [59/1000]: training_loss: tensor(0.3918, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [60/1000]: training_loss: tensor(0.2642, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [61/1000]: training_loss: tensor(0.4096, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [62/1000]: training_loss: tensor(0.3053, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [63/1000]: training_loss: tensor(0.3328, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [64/1000]: training_loss: tensor(0.3191, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [65/1000]: training_loss: tensor(0.3108, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [66/1000]: training_loss: tensor(0.3851, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [67/1000]: training_loss: tensor(0.2591, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [68/1000]: training_loss: tensor(0.3392, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [69/1000]: training_loss: tensor(0.2912, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [70/1000]: training_loss: tensor(0.2633, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [71/1000]: training_loss: tensor(0.3016, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [72/1000]: training_loss: tensor(0.3124, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [73/1000]: training_loss: tensor(0.2968, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [74/1000]: training_loss: tensor(0.2788, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [75/1000]: training_loss: tensor(0.3461, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [76/1000]: training_loss: tensor(0.3419, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [77/1000]: training_loss: tensor(0.2988, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [78/1000]: training_loss: tensor(0.2645, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [79/1000]: training_loss: tensor(0.3163, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [80/1000]: training_loss: tensor(0.3168, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [81/1000]: training_loss: tensor(0.2796, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [82/1000]: training_loss: tensor(0.2530, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [83/1000]: training_loss: tensor(0.3792, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [84/1000]: training_loss: tensor(0.2781, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [85/1000]: training_loss: tensor(0.2630, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [86/1000]: training_loss: tensor(0.2674, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [87/1000]: training_loss: tensor(0.3026, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [88/1000]: training_loss: tensor(0.3335, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [89/1000]: training_loss: tensor(0.3127, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [90/1000]: training_loss: tensor(0.3503, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [91/1000]: training_loss: tensor(0.3169, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [92/1000]: training_loss: tensor(0.3207, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [93/1000]: training_loss: tensor(0.2664, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [94/1000]: training_loss: tensor(0.3299, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [95/1000]: training_loss: tensor(0.3450, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [96/1000]: training_loss: tensor(0.2856, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [97/1000]: training_loss: tensor(0.2971, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [98/1000]: training_loss: tensor(0.3167, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [99/1000]: training_loss: tensor(0.2499, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [100/1000]: training_loss: tensor(0.3744, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [101/1000]: training_loss: tensor(0.2677, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [102/1000]: training_loss: tensor(0.3238, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [103/1000]: training_loss: tensor(0.2990, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [104/1000]: training_loss: tensor(0.3297, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [105/1000]: training_loss: tensor(0.3764, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [106/1000]: training_loss: tensor(0.3680, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [107/1000]: training_loss: tensor(0.3610, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [108/1000]: training_loss: tensor(0.3302, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [109/1000]: training_loss: tensor(0.4166, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [110/1000]: training_loss: tensor(0.3548, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [111/1000]: training_loss: tensor(0.3311, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [112/1000]: training_loss: tensor(0.3080, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [113/1000]: training_loss: tensor(0.3522, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [114/1000]: training_loss: tensor(0.2300, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [115/1000]: training_loss: tensor(0.2953, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [116/1000]: training_loss: tensor(0.2735, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [117/1000]: training_loss: tensor(0.3411, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [118/1000]: training_loss: tensor(0.2291, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [119/1000]: training_loss: tensor(0.3903, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [120/1000]: training_loss: tensor(0.2780, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [121/1000]: training_loss: tensor(0.3483, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [122/1000]: training_loss: tensor(0.3221, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [123/1000]: training_loss: tensor(0.3284, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [124/1000]: training_loss: tensor(0.2926, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [125/1000]: training_loss: tensor(0.3596, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [126/1000]: training_loss: tensor(0.4046, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [127/1000]: training_loss: tensor(0.3142, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [128/1000]: training_loss: tensor(0.3579, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [129/1000]: training_loss: tensor(0.3503, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [130/1000]: training_loss: tensor(0.3140, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [131/1000]: training_loss: tensor(0.4442, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [132/1000]: training_loss: tensor(0.3472, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [133/1000]: training_loss: tensor(0.3322, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [134/1000]: training_loss: tensor(0.2578, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [135/1000]: training_loss: tensor(0.3087, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [136/1000]: training_loss: tensor(0.2795, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [137/1000]: training_loss: tensor(0.3326, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [138/1000]: training_loss: tensor(0.2845, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [139/1000]: training_loss: tensor(0.3328, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [140/1000]: training_loss: tensor(0.3124, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [141/1000]: training_loss: tensor(0.3317, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [142/1000]: training_loss: tensor(0.3361, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [143/1000]: training_loss: tensor(0.3106, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [144/1000]: training_loss: tensor(0.2890, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [145/1000]: training_loss: tensor(0.2808, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [146/1000]: training_loss: tensor(0.3072, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [147/1000]: training_loss: tensor(0.2736, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [148/1000]: training_loss: tensor(0.3068, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [149/1000]: training_loss: tensor(0.2481, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [150/1000]: training_loss: tensor(0.2873, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [151/1000]: training_loss: tensor(0.3505, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [152/1000]: training_loss: tensor(0.3505, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [153/1000]: training_loss: tensor(0.3168, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [154/1000]: training_loss: tensor(0.3112, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [155/1000]: training_loss: tensor(0.3095, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [156/1000]: training_loss: tensor(0.3127, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [157/1000]: training_loss: tensor(0.4018, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [158/1000]: training_loss: tensor(0.3698, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [159/1000]: training_loss: tensor(0.2839, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [160/1000]: training_loss: tensor(0.3400, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [161/1000]: training_loss: tensor(0.3722, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [162/1000]: training_loss: tensor(0.3611, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [163/1000]: training_loss: tensor(0.3250, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [164/1000]: training_loss: tensor(0.3704, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [165/1000]: training_loss: tensor(0.2830, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [166/1000]: training_loss: tensor(0.2517, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [167/1000]: training_loss: tensor(0.3684, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [168/1000]: training_loss: tensor(0.3420, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [169/1000]: training_loss: tensor(0.2470, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [170/1000]: training_loss: tensor(0.3095, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [171/1000]: training_loss: tensor(0.3066, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [172/1000]: training_loss: tensor(0.2996, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [173/1000]: training_loss: tensor(0.3431, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [174/1000]: training_loss: tensor(0.3635, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [175/1000]: training_loss: tensor(0.3212, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [176/1000]: training_loss: tensor(0.2692, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [177/1000]: training_loss: tensor(0.3129, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [178/1000]: training_loss: tensor(0.3230, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [179/1000]: training_loss: tensor(0.3150, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [180/1000]: training_loss: tensor(0.3880, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [181/1000]: training_loss: tensor(0.3195, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [182/1000]: training_loss: tensor(0.3186, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [183/1000]: training_loss: tensor(0.2350, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [184/1000]: training_loss: tensor(0.3174, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [185/1000]: training_loss: tensor(0.2452, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [186/1000]: training_loss: tensor(0.3216, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [187/1000]: training_loss: tensor(0.3459, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [188/1000]: training_loss: tensor(0.2574, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [189/1000]: training_loss: tensor(0.3167, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [190/1000]: training_loss: tensor(0.2434, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [191/1000]: training_loss: tensor(0.3286, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [192/1000]: training_loss: tensor(0.2974, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [193/1000]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [194/1000]: training_loss: tensor(0.3281, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [195/1000]: training_loss: tensor(0.3251, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [196/1000]: training_loss: tensor(0.2934, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [197/1000]: training_loss: tensor(0.3104, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [198/1000]: training_loss: tensor(0.3811, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [199/1000]: training_loss: tensor(0.4371, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [200/1000]: training_loss: tensor(0.3273, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [201/1000]: training_loss: tensor(0.2493, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [202/1000]: training_loss: tensor(0.3576, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [203/1000]: training_loss: tensor(0.3055, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [204/1000]: training_loss: tensor(0.2942, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [205/1000]: training_loss: tensor(0.3280, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [206/1000]: training_loss: tensor(0.2976, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [207/1000]: training_loss: tensor(0.3497, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [208/1000]: training_loss: tensor(0.3245, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [209/1000]: training_loss: tensor(0.2861, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [210/1000]: training_loss: tensor(0.3595, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [211/1000]: training_loss: tensor(0.2529, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [212/1000]: training_loss: tensor(0.3239, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [213/1000]: training_loss: tensor(0.3391, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [214/1000]: training_loss: tensor(0.2751, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [215/1000]: training_loss: tensor(0.3261, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [216/1000]: training_loss: tensor(0.2775, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [217/1000]: training_loss: tensor(0.3869, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [218/1000]: training_loss: tensor(0.3257, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [219/1000]: training_loss: tensor(0.2671, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [220/1000]: training_loss: tensor(0.3654, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [221/1000]: training_loss: tensor(0.3383, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [222/1000]: training_loss: tensor(0.2394, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [223/1000]: training_loss: tensor(0.3428, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [224/1000]: training_loss: tensor(0.2495, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [225/1000]: training_loss: tensor(0.2924, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [226/1000]: training_loss: tensor(0.3445, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [227/1000]: training_loss: tensor(0.3401, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [228/1000]: training_loss: tensor(0.3473, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [229/1000]: training_loss: tensor(0.3535, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [230/1000]: training_loss: tensor(0.2981, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [231/1000]: training_loss: tensor(0.3615, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [232/1000]: training_loss: tensor(0.3630, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [233/1000]: training_loss: tensor(0.2948, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [234/1000]: training_loss: tensor(0.3667, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [235/1000]: training_loss: tensor(0.4046, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [236/1000]: training_loss: tensor(0.3131, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [237/1000]: training_loss: tensor(0.3325, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [238/1000]: training_loss: tensor(0.2956, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [239/1000]: training_loss: tensor(0.3036, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [240/1000]: training_loss: tensor(0.3185, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [241/1000]: training_loss: tensor(0.3204, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [242/1000]: training_loss: tensor(0.3214, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [243/1000]: training_loss: tensor(0.3028, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [244/1000]: training_loss: tensor(0.2844, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [245/1000]: training_loss: tensor(0.3248, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [246/1000]: training_loss: tensor(0.3620, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [247/1000]: training_loss: tensor(0.3577, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [248/1000]: training_loss: tensor(0.3295, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [249/1000]: training_loss: tensor(0.3477, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [250/1000]: training_loss: tensor(0.2898, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [251/1000]: training_loss: tensor(0.3356, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [252/1000]: training_loss: tensor(0.3105, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [253/1000]: training_loss: tensor(0.2744, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [254/1000]: training_loss: tensor(0.2930, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [255/1000]: training_loss: tensor(0.2490, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [256/1000]: training_loss: tensor(0.3200, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [257/1000]: training_loss: tensor(0.3230, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [258/1000]: training_loss: tensor(0.3361, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [259/1000]: training_loss: tensor(0.2868, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [260/1000]: training_loss: tensor(0.3182, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [261/1000]: training_loss: tensor(0.3450, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [262/1000]: training_loss: tensor(0.3794, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [263/1000]: training_loss: tensor(0.3138, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [264/1000]: training_loss: tensor(0.3121, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [265/1000]: training_loss: tensor(0.3918, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [266/1000]: training_loss: tensor(0.3539, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [267/1000]: training_loss: tensor(0.3587, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [268/1000]: training_loss: tensor(0.3381, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [269/1000]: training_loss: tensor(0.2242, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [270/1000]: training_loss: tensor(0.2734, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [271/1000]: training_loss: tensor(0.4745, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [272/1000]: training_loss: tensor(0.2488, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [273/1000]: training_loss: tensor(0.2521, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [274/1000]: training_loss: tensor(0.4049, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [275/1000]: training_loss: tensor(0.2510, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [276/1000]: training_loss: tensor(0.3710, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [277/1000]: training_loss: tensor(0.3007, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [278/1000]: training_loss: tensor(0.2710, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [279/1000]: training_loss: tensor(0.3001, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [280/1000]: training_loss: tensor(0.2983, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [281/1000]: training_loss: tensor(0.2918, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [282/1000]: training_loss: tensor(0.3771, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [283/1000]: training_loss: tensor(0.3029, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [284/1000]: training_loss: tensor(0.3284, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [285/1000]: training_loss: tensor(0.2778, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [286/1000]: training_loss: tensor(0.3380, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [287/1000]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [288/1000]: training_loss: tensor(0.3149, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [289/1000]: training_loss: tensor(0.3357, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [290/1000]: training_loss: tensor(0.3507, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [291/1000]: training_loss: tensor(0.3063, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [292/1000]: training_loss: tensor(0.3573, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [293/1000]: training_loss: tensor(0.3115, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [294/1000]: training_loss: tensor(0.4013, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [295/1000]: training_loss: tensor(0.3124, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [296/1000]: training_loss: tensor(0.4116, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [297/1000]: training_loss: tensor(0.2998, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [298/1000]: training_loss: tensor(0.2967, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [299/1000]: training_loss: tensor(0.2932, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [300/1000]: training_loss: tensor(0.3128, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [301/1000]: training_loss: tensor(0.3212, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [302/1000]: training_loss: tensor(0.2735, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [303/1000]: training_loss: tensor(0.2692, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [304/1000]: training_loss: tensor(0.2724, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [305/1000]: training_loss: tensor(0.3643, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [306/1000]: training_loss: tensor(0.2617, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [307/1000]: training_loss: tensor(0.2771, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [308/1000]: training_loss: tensor(0.3004, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [309/1000]: training_loss: tensor(0.3268, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [310/1000]: training_loss: tensor(0.4027, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [311/1000]: training_loss: tensor(0.3540, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [312/1000]: training_loss: tensor(0.3088, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [313/1000]: training_loss: tensor(0.2685, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [314/1000]: training_loss: tensor(0.3306, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [315/1000]: training_loss: tensor(0.3028, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [316/1000]: training_loss: tensor(0.2508, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [317/1000]: training_loss: tensor(0.3082, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [318/1000]: training_loss: tensor(0.3235, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [319/1000]: training_loss: tensor(0.2733, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [320/1000]: training_loss: tensor(0.2706, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [321/1000]: training_loss: tensor(0.2931, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [322/1000]: training_loss: tensor(0.2853, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [323/1000]: training_loss: tensor(0.2421, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [324/1000]: training_loss: tensor(0.2664, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [325/1000]: training_loss: tensor(0.3810, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [326/1000]: training_loss: tensor(0.3018, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [327/1000]: training_loss: tensor(0.2555, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [328/1000]: training_loss: tensor(0.3291, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [329/1000]: training_loss: tensor(0.2859, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [330/1000]: training_loss: tensor(0.3132, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [331/1000]: training_loss: tensor(0.2798, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [332/1000]: training_loss: tensor(0.3050, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [333/1000]: training_loss: tensor(0.2851, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [334/1000]: training_loss: tensor(0.2660, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [335/1000]: training_loss: tensor(0.4724, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [336/1000]: training_loss: tensor(0.2734, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [337/1000]: training_loss: tensor(0.2854, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [338/1000]: training_loss: tensor(0.3625, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [339/1000]: training_loss: tensor(0.3436, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [340/1000]: training_loss: tensor(0.2903, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [341/1000]: training_loss: tensor(0.3104, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [342/1000]: training_loss: tensor(0.3463, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [343/1000]: training_loss: tensor(0.3056, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [344/1000]: training_loss: tensor(0.2579, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [345/1000]: training_loss: tensor(0.2858, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [346/1000]: training_loss: tensor(0.2778, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [347/1000]: training_loss: tensor(0.3059, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [348/1000]: training_loss: tensor(0.2923, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [349/1000]: training_loss: tensor(0.3326, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [350/1000]: training_loss: tensor(0.3017, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [351/1000]: training_loss: tensor(0.3006, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [352/1000]: training_loss: tensor(0.3508, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [353/1000]: training_loss: tensor(0.3310, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [354/1000]: training_loss: tensor(0.3378, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [355/1000]: training_loss: tensor(0.3191, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [356/1000]: training_loss: tensor(0.2639, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [357/1000]: training_loss: tensor(0.3529, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [358/1000]: training_loss: tensor(0.2710, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [359/1000]: training_loss: tensor(0.3512, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [360/1000]: training_loss: tensor(0.2315, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [361/1000]: training_loss: tensor(0.3556, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [362/1000]: training_loss: tensor(0.2909, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [363/1000]: training_loss: tensor(0.3693, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [364/1000]: training_loss: tensor(0.3016, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [365/1000]: training_loss: tensor(0.2238, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [366/1000]: training_loss: tensor(0.3143, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [367/1000]: training_loss: tensor(0.3634, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [368/1000]: training_loss: tensor(0.3070, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [369/1000]: training_loss: tensor(0.2593, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [370/1000]: training_loss: tensor(0.2768, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [371/1000]: training_loss: tensor(0.2984, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [372/1000]: training_loss: tensor(0.3119, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [373/1000]: training_loss: tensor(0.2603, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [374/1000]: training_loss: tensor(0.2949, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [375/1000]: training_loss: tensor(0.3337, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [376/1000]: training_loss: tensor(0.3652, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [377/1000]: training_loss: tensor(0.3218, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [378/1000]: training_loss: tensor(0.3089, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [379/1000]: training_loss: tensor(0.3739, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [380/1000]: training_loss: tensor(0.3671, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [381/1000]: training_loss: tensor(0.3898, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [382/1000]: training_loss: tensor(0.3316, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [383/1000]: training_loss: tensor(0.2700, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [384/1000]: training_loss: tensor(0.2847, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [385/1000]: training_loss: tensor(0.2939, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [386/1000]: training_loss: tensor(0.3118, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [387/1000]: training_loss: tensor(0.2771, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [388/1000]: training_loss: tensor(0.2643, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [389/1000]: training_loss: tensor(0.3610, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [390/1000]: training_loss: tensor(0.3319, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [391/1000]: training_loss: tensor(0.3210, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [392/1000]: training_loss: tensor(0.2958, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [393/1000]: training_loss: tensor(0.2515, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [394/1000]: training_loss: tensor(0.2479, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [395/1000]: training_loss: tensor(0.2660, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [396/1000]: training_loss: tensor(0.2871, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [397/1000]: training_loss: tensor(0.3059, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [398/1000]: training_loss: tensor(0.3121, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [399/1000]: training_loss: tensor(0.3090, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [400/1000]: training_loss: tensor(0.3409, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [401/1000]: training_loss: tensor(0.2910, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [402/1000]: training_loss: tensor(0.3490, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [403/1000]: training_loss: tensor(0.3572, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [404/1000]: training_loss: tensor(0.3313, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [405/1000]: training_loss: tensor(0.4330, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [406/1000]: training_loss: tensor(0.3280, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [407/1000]: training_loss: tensor(0.3575, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [408/1000]: training_loss: tensor(0.3638, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [409/1000]: training_loss: tensor(0.2824, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [410/1000]: training_loss: tensor(0.3312, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [411/1000]: training_loss: tensor(0.2615, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [412/1000]: training_loss: tensor(0.3579, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [413/1000]: training_loss: tensor(0.2741, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [414/1000]: training_loss: tensor(0.3056, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [415/1000]: training_loss: tensor(0.3418, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [416/1000]: training_loss: tensor(0.3387, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [417/1000]: training_loss: tensor(0.2385, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [418/1000]: training_loss: tensor(0.3287, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [419/1000]: training_loss: tensor(0.3703, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [420/1000]: training_loss: tensor(0.3575, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [421/1000]: training_loss: tensor(0.2853, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [422/1000]: training_loss: tensor(0.3052, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [423/1000]: training_loss: tensor(0.2674, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [424/1000]: training_loss: tensor(0.3201, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [425/1000]: training_loss: tensor(0.3130, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [426/1000]: training_loss: tensor(0.2788, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [427/1000]: training_loss: tensor(0.3466, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [428/1000]: training_loss: tensor(0.2877, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [429/1000]: training_loss: tensor(0.3187, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [430/1000]: training_loss: tensor(0.3155, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [431/1000]: training_loss: tensor(0.3265, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [432/1000]: training_loss: tensor(0.3145, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [433/1000]: training_loss: tensor(0.3390, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [434/1000]: training_loss: tensor(0.3559, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [435/1000]: training_loss: tensor(0.2674, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [436/1000]: training_loss: tensor(0.3200, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [437/1000]: training_loss: tensor(0.2702, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [438/1000]: training_loss: tensor(0.3108, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [439/1000]: training_loss: tensor(0.3228, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [440/1000]: training_loss: tensor(0.2724, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [441/1000]: training_loss: tensor(0.3080, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [442/1000]: training_loss: tensor(0.3937, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [443/1000]: training_loss: tensor(0.2268, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [444/1000]: training_loss: tensor(0.3205, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [445/1000]: training_loss: tensor(0.3549, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [446/1000]: training_loss: tensor(0.3227, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [447/1000]: training_loss: tensor(0.3524, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [448/1000]: training_loss: tensor(0.3285, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [449/1000]: training_loss: tensor(0.3367, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [450/1000]: training_loss: tensor(0.3437, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [451/1000]: training_loss: tensor(0.3002, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [452/1000]: training_loss: tensor(0.2822, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [453/1000]: training_loss: tensor(0.3079, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [454/1000]: training_loss: tensor(0.2965, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [455/1000]: training_loss: tensor(0.2775, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [456/1000]: training_loss: tensor(0.2959, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [457/1000]: training_loss: tensor(0.2997, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [458/1000]: training_loss: tensor(0.2760, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [459/1000]: training_loss: tensor(0.3617, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [460/1000]: training_loss: tensor(0.2894, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [461/1000]: training_loss: tensor(0.4348, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [462/1000]: training_loss: tensor(0.3645, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [463/1000]: training_loss: tensor(0.2779, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [464/1000]: training_loss: tensor(0.3224, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [465/1000]: training_loss: tensor(0.3204, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [466/1000]: training_loss: tensor(0.3483, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [467/1000]: training_loss: tensor(0.3504, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [468/1000]: training_loss: tensor(0.2896, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [469/1000]: training_loss: tensor(0.2553, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [470/1000]: training_loss: tensor(0.3391, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [471/1000]: training_loss: tensor(0.2944, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [472/1000]: training_loss: tensor(0.3145, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [473/1000]: training_loss: tensor(0.3881, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [474/1000]: training_loss: tensor(0.3340, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [475/1000]: training_loss: tensor(0.3987, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [476/1000]: training_loss: tensor(0.2874, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [477/1000]: training_loss: tensor(0.3002, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [478/1000]: training_loss: tensor(0.2890, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [479/1000]: training_loss: tensor(0.3381, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [480/1000]: training_loss: tensor(0.4797, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [481/1000]: training_loss: tensor(0.3418, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [482/1000]: training_loss: tensor(0.3208, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [483/1000]: training_loss: tensor(0.2967, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [484/1000]: training_loss: tensor(0.3328, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [485/1000]: training_loss: tensor(0.3260, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [486/1000]: training_loss: tensor(0.3046, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [487/1000]: training_loss: tensor(0.3586, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [488/1000]: training_loss: tensor(0.2752, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [489/1000]: training_loss: tensor(0.3028, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [490/1000]: training_loss: tensor(0.3501, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [491/1000]: training_loss: tensor(0.3142, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [492/1000]: training_loss: tensor(0.3208, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [493/1000]: training_loss: tensor(0.2936, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [494/1000]: training_loss: tensor(0.2553, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [495/1000]: training_loss: tensor(0.3208, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [496/1000]: training_loss: tensor(0.3298, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [497/1000]: training_loss: tensor(0.2475, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [498/1000]: training_loss: tensor(0.2862, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [499/1000]: training_loss: tensor(0.3440, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [500/1000]: training_loss: tensor(0.3062, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [501/1000]: training_loss: tensor(0.2817, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [502/1000]: training_loss: tensor(0.2812, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [503/1000]: training_loss: tensor(0.3191, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [504/1000]: training_loss: tensor(0.3296, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [505/1000]: training_loss: tensor(0.3318, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [506/1000]: training_loss: tensor(0.2893, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [507/1000]: training_loss: tensor(0.2438, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [508/1000]: training_loss: tensor(0.4296, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [509/1000]: training_loss: tensor(0.2669, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [510/1000]: training_loss: tensor(0.3388, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [511/1000]: training_loss: tensor(0.3363, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [512/1000]: training_loss: tensor(0.3208, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [513/1000]: training_loss: tensor(0.2606, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [514/1000]: training_loss: tensor(0.2739, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [515/1000]: training_loss: tensor(0.3549, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [516/1000]: training_loss: tensor(0.3172, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [517/1000]: training_loss: tensor(0.3626, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [518/1000]: training_loss: tensor(0.3331, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [519/1000]: training_loss: tensor(0.3718, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [520/1000]: training_loss: tensor(0.2709, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [521/1000]: training_loss: tensor(0.3293, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [522/1000]: training_loss: tensor(0.2945, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [523/1000]: training_loss: tensor(0.4126, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [524/1000]: training_loss: tensor(0.2281, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [525/1000]: training_loss: tensor(0.2419, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [526/1000]: training_loss: tensor(0.3815, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [527/1000]: training_loss: tensor(0.3511, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [528/1000]: training_loss: tensor(0.4018, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [529/1000]: training_loss: tensor(0.3408, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [530/1000]: training_loss: tensor(0.3676, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [531/1000]: training_loss: tensor(0.3125, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [532/1000]: training_loss: tensor(0.2611, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [533/1000]: training_loss: tensor(0.2411, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [534/1000]: training_loss: tensor(0.2868, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [535/1000]: training_loss: tensor(0.3481, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [536/1000]: training_loss: tensor(0.3775, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [537/1000]: training_loss: tensor(0.2268, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [538/1000]: training_loss: tensor(0.2740, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [539/1000]: training_loss: tensor(0.2951, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [540/1000]: training_loss: tensor(0.2395, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [541/1000]: training_loss: tensor(0.3224, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [542/1000]: training_loss: tensor(0.3561, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [543/1000]: training_loss: tensor(0.3008, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [544/1000]: training_loss: tensor(0.3178, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [545/1000]: training_loss: tensor(0.2722, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [546/1000]: training_loss: tensor(0.2640, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [547/1000]: training_loss: tensor(0.2987, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [548/1000]: training_loss: tensor(0.3687, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [549/1000]: training_loss: tensor(0.2874, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [550/1000]: training_loss: tensor(0.3663, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [551/1000]: training_loss: tensor(0.2937, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [552/1000]: training_loss: tensor(0.2465, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [553/1000]: training_loss: tensor(0.3123, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [554/1000]: training_loss: tensor(0.3272, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [555/1000]: training_loss: tensor(0.3117, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [556/1000]: training_loss: tensor(0.3158, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [557/1000]: training_loss: tensor(0.3523, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [558/1000]: training_loss: tensor(0.2764, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [559/1000]: training_loss: tensor(0.3504, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [560/1000]: training_loss: tensor(0.3322, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [561/1000]: training_loss: tensor(0.2884, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [562/1000]: training_loss: tensor(0.2713, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [563/1000]: training_loss: tensor(0.3398, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [564/1000]: training_loss: tensor(0.2682, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [565/1000]: training_loss: tensor(0.4009, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [566/1000]: training_loss: tensor(0.2992, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [567/1000]: training_loss: tensor(0.3218, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [568/1000]: training_loss: tensor(0.3624, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [569/1000]: training_loss: tensor(0.2989, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [570/1000]: training_loss: tensor(0.3176, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [571/1000]: training_loss: tensor(0.3748, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [572/1000]: training_loss: tensor(0.3423, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [573/1000]: training_loss: tensor(0.3333, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [574/1000]: training_loss: tensor(0.3097, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [575/1000]: training_loss: tensor(0.3182, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [576/1000]: training_loss: tensor(0.2409, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [577/1000]: training_loss: tensor(0.3079, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [578/1000]: training_loss: tensor(0.2619, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [579/1000]: training_loss: tensor(0.3408, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [580/1000]: training_loss: tensor(0.3147, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [581/1000]: training_loss: tensor(0.2958, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [582/1000]: training_loss: tensor(0.2925, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [583/1000]: training_loss: tensor(0.2740, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [584/1000]: training_loss: tensor(0.3194, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [585/1000]: training_loss: tensor(0.2704, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [586/1000]: training_loss: tensor(0.4495, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [587/1000]: training_loss: tensor(0.3345, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [588/1000]: training_loss: tensor(0.3396, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [589/1000]: training_loss: tensor(0.2924, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [590/1000]: training_loss: tensor(0.2552, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [591/1000]: training_loss: tensor(0.3326, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [592/1000]: training_loss: tensor(0.2276, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [593/1000]: training_loss: tensor(0.3363, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [594/1000]: training_loss: tensor(0.2814, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [595/1000]: training_loss: tensor(0.3613, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [596/1000]: training_loss: tensor(0.3439, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [597/1000]: training_loss: tensor(0.3016, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [598/1000]: training_loss: tensor(0.3557, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [599/1000]: training_loss: tensor(0.2893, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [600/1000]: training_loss: tensor(0.3444, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [601/1000]: training_loss: tensor(0.3633, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [602/1000]: training_loss: tensor(0.3960, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [603/1000]: training_loss: tensor(0.2953, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [604/1000]: training_loss: tensor(0.2635, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [605/1000]: training_loss: tensor(0.3938, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [606/1000]: training_loss: tensor(0.2501, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [607/1000]: training_loss: tensor(0.3244, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [608/1000]: training_loss: tensor(0.3332, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [609/1000]: training_loss: tensor(0.2688, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [610/1000]: training_loss: tensor(0.2479, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [611/1000]: training_loss: tensor(0.3728, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [612/1000]: training_loss: tensor(0.2632, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [613/1000]: training_loss: tensor(0.3336, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [614/1000]: training_loss: tensor(0.3038, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [615/1000]: training_loss: tensor(0.4104, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [616/1000]: training_loss: tensor(0.3481, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [617/1000]: training_loss: tensor(0.3058, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [618/1000]: training_loss: tensor(0.3552, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [619/1000]: training_loss: tensor(0.3280, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [620/1000]: training_loss: tensor(0.2888, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [621/1000]: training_loss: tensor(0.2431, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [622/1000]: training_loss: tensor(0.3308, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [623/1000]: training_loss: tensor(0.3145, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [624/1000]: training_loss: tensor(0.2824, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [625/1000]: training_loss: tensor(0.2287, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [626/1000]: training_loss: tensor(0.3718, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [627/1000]: training_loss: tensor(0.3024, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [628/1000]: training_loss: tensor(0.2913, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [629/1000]: training_loss: tensor(0.2865, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [630/1000]: training_loss: tensor(0.3205, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [631/1000]: training_loss: tensor(0.3164, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [632/1000]: training_loss: tensor(0.3705, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [633/1000]: training_loss: tensor(0.3539, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [634/1000]: training_loss: tensor(0.2957, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [635/1000]: training_loss: tensor(0.2702, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [636/1000]: training_loss: tensor(0.3256, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [637/1000]: training_loss: tensor(0.3974, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [638/1000]: training_loss: tensor(0.3800, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [639/1000]: training_loss: tensor(0.3050, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [640/1000]: training_loss: tensor(0.3160, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [641/1000]: training_loss: tensor(0.3633, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [642/1000]: training_loss: tensor(0.3769, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [643/1000]: training_loss: tensor(0.4125, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [644/1000]: training_loss: tensor(0.4043, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [645/1000]: training_loss: tensor(0.2888, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [646/1000]: training_loss: tensor(0.2928, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [647/1000]: training_loss: tensor(0.3028, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [648/1000]: training_loss: tensor(0.3401, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [649/1000]: training_loss: tensor(0.3679, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [650/1000]: training_loss: tensor(0.3170, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [651/1000]: training_loss: tensor(0.2499, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [652/1000]: training_loss: tensor(0.3413, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [653/1000]: training_loss: tensor(0.2524, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [654/1000]: training_loss: tensor(0.3422, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [655/1000]: training_loss: tensor(0.2917, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [656/1000]: training_loss: tensor(0.4058, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [657/1000]: training_loss: tensor(0.2666, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [658/1000]: training_loss: tensor(0.3019, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [659/1000]: training_loss: tensor(0.3716, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [660/1000]: training_loss: tensor(0.3321, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [661/1000]: training_loss: tensor(0.3720, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [662/1000]: training_loss: tensor(0.3108, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [663/1000]: training_loss: tensor(0.3742, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [664/1000]: training_loss: tensor(0.2716, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [665/1000]: training_loss: tensor(0.3183, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [666/1000]: training_loss: tensor(0.2929, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [667/1000]: training_loss: tensor(0.3207, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [668/1000]: training_loss: tensor(0.2698, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [669/1000]: training_loss: tensor(0.3750, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [670/1000]: training_loss: tensor(0.2986, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [671/1000]: training_loss: tensor(0.3343, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [672/1000]: training_loss: tensor(0.3088, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [673/1000]: training_loss: tensor(0.3623, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [674/1000]: training_loss: tensor(0.3665, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [675/1000]: training_loss: tensor(0.3066, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [676/1000]: training_loss: tensor(0.2607, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [677/1000]: training_loss: tensor(0.2349, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [678/1000]: training_loss: tensor(0.2761, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [679/1000]: training_loss: tensor(0.2761, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [680/1000]: training_loss: tensor(0.2525, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [681/1000]: training_loss: tensor(0.2959, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [682/1000]: training_loss: tensor(0.4019, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [683/1000]: training_loss: tensor(0.2909, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [684/1000]: training_loss: tensor(0.2991, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [685/1000]: training_loss: tensor(0.2711, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [686/1000]: training_loss: tensor(0.3468, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [687/1000]: training_loss: tensor(0.3418, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [688/1000]: training_loss: tensor(0.3284, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [689/1000]: training_loss: tensor(0.3338, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [690/1000]: training_loss: tensor(0.2665, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [691/1000]: training_loss: tensor(0.3270, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [692/1000]: training_loss: tensor(0.2583, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [693/1000]: training_loss: tensor(0.2905, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [694/1000]: training_loss: tensor(0.3377, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [695/1000]: training_loss: tensor(0.2895, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [696/1000]: training_loss: tensor(0.3741, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [697/1000]: training_loss: tensor(0.2948, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [698/1000]: training_loss: tensor(0.2782, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [699/1000]: training_loss: tensor(0.2617, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [700/1000]: training_loss: tensor(0.3440, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [701/1000]: training_loss: tensor(0.3307, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [702/1000]: training_loss: tensor(0.2636, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [703/1000]: training_loss: tensor(0.3144, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [704/1000]: training_loss: tensor(0.2622, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [705/1000]: training_loss: tensor(0.2804, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [706/1000]: training_loss: tensor(0.3378, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [707/1000]: training_loss: tensor(0.2932, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [708/1000]: training_loss: tensor(0.2404, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [709/1000]: training_loss: tensor(0.2742, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [710/1000]: training_loss: tensor(0.3290, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [711/1000]: training_loss: tensor(0.2290, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [712/1000]: training_loss: tensor(0.2946, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [713/1000]: training_loss: tensor(0.3300, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [714/1000]: training_loss: tensor(0.2748, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [715/1000]: training_loss: tensor(0.3237, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [716/1000]: training_loss: tensor(0.3695, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [717/1000]: training_loss: tensor(0.2828, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [718/1000]: training_loss: tensor(0.3438, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [719/1000]: training_loss: tensor(0.3162, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [720/1000]: training_loss: tensor(0.2693, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [721/1000]: training_loss: tensor(0.3123, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [722/1000]: training_loss: tensor(0.3723, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [723/1000]: training_loss: tensor(0.3421, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [724/1000]: training_loss: tensor(0.2865, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [725/1000]: training_loss: tensor(0.2889, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [726/1000]: training_loss: tensor(0.3056, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [727/1000]: training_loss: tensor(0.2684, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [728/1000]: training_loss: tensor(0.3089, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [729/1000]: training_loss: tensor(0.2837, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [730/1000]: training_loss: tensor(0.3014, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [731/1000]: training_loss: tensor(0.2824, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [732/1000]: training_loss: tensor(0.2457, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [733/1000]: training_loss: tensor(0.2529, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [734/1000]: training_loss: tensor(0.4786, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [735/1000]: training_loss: tensor(0.2538, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [736/1000]: training_loss: tensor(0.3004, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [737/1000]: training_loss: tensor(0.2807, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [738/1000]: training_loss: tensor(0.3815, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [739/1000]: training_loss: tensor(0.3715, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [740/1000]: training_loss: tensor(0.3933, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [741/1000]: training_loss: tensor(0.3326, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [742/1000]: training_loss: tensor(0.2653, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [743/1000]: training_loss: tensor(0.2546, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [744/1000]: training_loss: tensor(0.2633, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [745/1000]: training_loss: tensor(0.2720, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [746/1000]: training_loss: tensor(0.2839, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [747/1000]: training_loss: tensor(0.3362, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [748/1000]: training_loss: tensor(0.3175, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [749/1000]: training_loss: tensor(0.2720, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [750/1000]: training_loss: tensor(0.2765, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [751/1000]: training_loss: tensor(0.3800, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [752/1000]: training_loss: tensor(0.2862, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [753/1000]: training_loss: tensor(0.2905, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [754/1000]: training_loss: tensor(0.2738, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [755/1000]: training_loss: tensor(0.3128, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [756/1000]: training_loss: tensor(0.2534, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [757/1000]: training_loss: tensor(0.3232, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [758/1000]: training_loss: tensor(0.2733, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [759/1000]: training_loss: tensor(0.2884, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [760/1000]: training_loss: tensor(0.2926, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [761/1000]: training_loss: tensor(0.3857, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [762/1000]: training_loss: tensor(0.3370, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [763/1000]: training_loss: tensor(0.3498, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [764/1000]: training_loss: tensor(0.2771, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [765/1000]: training_loss: tensor(0.2940, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [766/1000]: training_loss: tensor(0.3381, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [767/1000]: training_loss: tensor(0.2828, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [768/1000]: training_loss: tensor(0.3157, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [769/1000]: training_loss: tensor(0.2362, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [770/1000]: training_loss: tensor(0.4128, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [771/1000]: training_loss: tensor(0.3275, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [772/1000]: training_loss: tensor(0.3417, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [773/1000]: training_loss: tensor(0.2943, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [774/1000]: training_loss: tensor(0.3351, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [775/1000]: training_loss: tensor(0.3167, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [776/1000]: training_loss: tensor(0.2423, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [777/1000]: training_loss: tensor(0.2910, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [778/1000]: training_loss: tensor(0.3167, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [779/1000]: training_loss: tensor(0.2489, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [780/1000]: training_loss: tensor(0.3129, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [781/1000]: training_loss: tensor(0.3967, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [782/1000]: training_loss: tensor(0.3334, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [783/1000]: training_loss: tensor(0.2744, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [784/1000]: training_loss: tensor(0.4147, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [785/1000]: training_loss: tensor(0.2632, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [786/1000]: training_loss: tensor(0.2336, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [787/1000]: training_loss: tensor(0.3062, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [788/1000]: training_loss: tensor(0.2322, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [789/1000]: training_loss: tensor(0.3304, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [790/1000]: training_loss: tensor(0.2872, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [791/1000]: training_loss: tensor(0.2635, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [792/1000]: training_loss: tensor(0.3047, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [793/1000]: training_loss: tensor(0.2604, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [794/1000]: training_loss: tensor(0.4255, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [795/1000]: training_loss: tensor(0.3517, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [796/1000]: training_loss: tensor(0.2743, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [797/1000]: training_loss: tensor(0.3235, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [798/1000]: training_loss: tensor(0.3187, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [799/1000]: training_loss: tensor(0.2582, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [800/1000]: training_loss: tensor(0.2924, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [801/1000]: training_loss: tensor(0.3051, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [802/1000]: training_loss: tensor(0.3856, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [803/1000]: training_loss: tensor(0.3341, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [804/1000]: training_loss: tensor(0.3271, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [805/1000]: training_loss: tensor(0.3520, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [806/1000]: training_loss: tensor(0.2813, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [807/1000]: training_loss: tensor(0.2822, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [808/1000]: training_loss: tensor(0.4072, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [809/1000]: training_loss: tensor(0.2584, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [810/1000]: training_loss: tensor(0.3007, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [811/1000]: training_loss: tensor(0.3025, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [812/1000]: training_loss: tensor(0.3090, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [813/1000]: training_loss: tensor(0.3063, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [814/1000]: training_loss: tensor(0.3958, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [815/1000]: training_loss: tensor(0.3679, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [816/1000]: training_loss: tensor(0.3286, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [817/1000]: training_loss: tensor(0.3121, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [818/1000]: training_loss: tensor(0.3355, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [819/1000]: training_loss: tensor(0.3048, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [820/1000]: training_loss: tensor(0.3108, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [821/1000]: training_loss: tensor(0.2398, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [822/1000]: training_loss: tensor(0.2461, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [823/1000]: training_loss: tensor(0.2943, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [824/1000]: training_loss: tensor(0.3321, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [825/1000]: training_loss: tensor(0.2664, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [826/1000]: training_loss: tensor(0.3194, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [827/1000]: training_loss: tensor(0.2996, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [828/1000]: training_loss: tensor(0.2392, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [829/1000]: training_loss: tensor(0.3101, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [830/1000]: training_loss: tensor(0.2891, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [831/1000]: training_loss: tensor(0.3241, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [832/1000]: training_loss: tensor(0.2634, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [833/1000]: training_loss: tensor(0.3265, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [834/1000]: training_loss: tensor(0.3069, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [835/1000]: training_loss: tensor(0.2715, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [836/1000]: training_loss: tensor(0.2610, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [837/1000]: training_loss: tensor(0.2781, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [838/1000]: training_loss: tensor(0.3910, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [839/1000]: training_loss: tensor(0.3624, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [840/1000]: training_loss: tensor(0.3557, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [841/1000]: training_loss: tensor(0.2992, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [842/1000]: training_loss: tensor(0.2468, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [843/1000]: training_loss: tensor(0.3272, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [844/1000]: training_loss: tensor(0.2536, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [845/1000]: training_loss: tensor(0.3002, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [846/1000]: training_loss: tensor(0.3274, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [847/1000]: training_loss: tensor(0.2709, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [848/1000]: training_loss: tensor(0.2909, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [849/1000]: training_loss: tensor(0.3322, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [850/1000]: training_loss: tensor(0.3459, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [851/1000]: training_loss: tensor(0.3898, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [852/1000]: training_loss: tensor(0.3868, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [853/1000]: training_loss: tensor(0.2676, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [854/1000]: training_loss: tensor(0.3661, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [855/1000]: training_loss: tensor(0.3570, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [856/1000]: training_loss: tensor(0.3036, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [857/1000]: training_loss: tensor(0.4434, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [858/1000]: training_loss: tensor(0.3781, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [859/1000]: training_loss: tensor(0.3248, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [860/1000]: training_loss: tensor(0.2927, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [861/1000]: training_loss: tensor(0.2997, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [862/1000]: training_loss: tensor(0.2446, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [863/1000]: training_loss: tensor(0.2834, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [864/1000]: training_loss: tensor(0.3895, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [865/1000]: training_loss: tensor(0.3127, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [866/1000]: training_loss: tensor(0.2933, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [867/1000]: training_loss: tensor(0.3428, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [868/1000]: training_loss: tensor(0.3181, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [869/1000]: training_loss: tensor(0.2569, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [870/1000]: training_loss: tensor(0.2991, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [871/1000]: training_loss: tensor(0.3391, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [872/1000]: training_loss: tensor(0.4064, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [873/1000]: training_loss: tensor(0.3802, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [874/1000]: training_loss: tensor(0.3000, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [875/1000]: training_loss: tensor(0.2740, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [876/1000]: training_loss: tensor(0.2830, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [877/1000]: training_loss: tensor(0.3301, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [878/1000]: training_loss: tensor(0.3339, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [879/1000]: training_loss: tensor(0.2586, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [880/1000]: training_loss: tensor(0.3093, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [881/1000]: training_loss: tensor(0.3031, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [882/1000]: training_loss: tensor(0.2861, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [883/1000]: training_loss: tensor(0.2736, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [884/1000]: training_loss: tensor(0.2578, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [885/1000]: training_loss: tensor(0.2723, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [886/1000]: training_loss: tensor(0.4227, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [887/1000]: training_loss: tensor(0.4317, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [888/1000]: training_loss: tensor(0.3700, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [889/1000]: training_loss: tensor(0.3041, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [890/1000]: training_loss: tensor(0.2607, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [891/1000]: training_loss: tensor(0.3243, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [892/1000]: training_loss: tensor(0.3165, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [893/1000]: training_loss: tensor(0.3454, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [894/1000]: training_loss: tensor(0.3418, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [895/1000]: training_loss: tensor(0.3213, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [896/1000]: training_loss: tensor(0.3133, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [897/1000]: training_loss: tensor(0.3141, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [898/1000]: training_loss: tensor(0.3204, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [899/1000]: training_loss: tensor(0.3195, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [900/1000]: training_loss: tensor(0.3977, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [901/1000]: training_loss: tensor(0.3395, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [902/1000]: training_loss: tensor(0.2821, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [903/1000]: training_loss: tensor(0.2566, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [904/1000]: training_loss: tensor(0.2936, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [905/1000]: training_loss: tensor(0.3309, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [906/1000]: training_loss: tensor(0.2866, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [907/1000]: training_loss: tensor(0.2917, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [908/1000]: training_loss: tensor(0.3785, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [909/1000]: training_loss: tensor(0.2607, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [910/1000]: training_loss: tensor(0.3076, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [911/1000]: training_loss: tensor(0.3861, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [912/1000]: training_loss: tensor(0.2772, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [913/1000]: training_loss: tensor(0.3036, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [914/1000]: training_loss: tensor(0.3268, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [915/1000]: training_loss: tensor(0.3238, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [916/1000]: training_loss: tensor(0.3379, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [917/1000]: training_loss: tensor(0.2698, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [918/1000]: training_loss: tensor(0.2566, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [919/1000]: training_loss: tensor(0.3141, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [920/1000]: training_loss: tensor(0.3172, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [921/1000]: training_loss: tensor(0.3283, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [922/1000]: training_loss: tensor(0.2517, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [923/1000]: training_loss: tensor(0.3133, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [924/1000]: training_loss: tensor(0.3171, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [925/1000]: training_loss: tensor(0.3232, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [926/1000]: training_loss: tensor(0.3109, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [927/1000]: training_loss: tensor(0.4013, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [928/1000]: training_loss: tensor(0.3324, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [929/1000]: training_loss: tensor(0.2537, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [930/1000]: training_loss: tensor(0.2583, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [931/1000]: training_loss: tensor(0.3051, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [932/1000]: training_loss: tensor(0.2950, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [933/1000]: training_loss: tensor(0.3813, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [934/1000]: training_loss: tensor(0.3427, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [935/1000]: training_loss: tensor(0.3693, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [936/1000]: training_loss: tensor(0.3061, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [937/1000]: training_loss: tensor(0.2622, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [938/1000]: training_loss: tensor(0.3547, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [939/1000]: training_loss: tensor(0.3480, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [940/1000]: training_loss: tensor(0.3308, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [941/1000]: training_loss: tensor(0.3741, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [942/1000]: training_loss: tensor(0.3346, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [943/1000]: training_loss: tensor(0.2957, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [944/1000]: training_loss: tensor(0.2885, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [945/1000]: training_loss: tensor(0.2790, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [946/1000]: training_loss: tensor(0.3472, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [947/1000]: training_loss: tensor(0.2943, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [948/1000]: training_loss: tensor(0.2885, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [949/1000]: training_loss: tensor(0.2534, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [950/1000]: training_loss: tensor(0.2747, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [951/1000]: training_loss: tensor(0.2625, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [952/1000]: training_loss: tensor(0.3485, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [953/1000]: training_loss: tensor(0.3165, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [954/1000]: training_loss: tensor(0.3557, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [955/1000]: training_loss: tensor(0.3145, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [956/1000]: training_loss: tensor(0.3099, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [957/1000]: training_loss: tensor(0.3106, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [958/1000]: training_loss: tensor(0.4253, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [959/1000]: training_loss: tensor(0.3598, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [960/1000]: training_loss: tensor(0.3233, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [961/1000]: training_loss: tensor(0.2769, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [962/1000]: training_loss: tensor(0.3843, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [963/1000]: training_loss: tensor(0.2528, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [964/1000]: training_loss: tensor(0.2589, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [965/1000]: training_loss: tensor(0.3175, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [966/1000]: training_loss: tensor(0.3245, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [967/1000]: training_loss: tensor(0.2620, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [968/1000]: training_loss: tensor(0.3319, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [969/1000]: training_loss: tensor(0.3095, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [970/1000]: training_loss: tensor(0.2745, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [971/1000]: training_loss: tensor(0.2558, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [972/1000]: training_loss: tensor(0.3256, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [973/1000]: training_loss: tensor(0.3401, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [974/1000]: training_loss: tensor(0.3709, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [975/1000]: training_loss: tensor(0.3640, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [976/1000]: training_loss: tensor(0.2855, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [977/1000]: training_loss: tensor(0.3385, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [978/1000]: training_loss: tensor(0.3026, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [979/1000]: training_loss: tensor(0.3118, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [980/1000]: training_loss: tensor(0.2804, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [981/1000]: training_loss: tensor(0.3210, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [982/1000]: training_loss: tensor(0.2949, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [983/1000]: training_loss: tensor(0.2850, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [984/1000]: training_loss: tensor(0.3446, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [985/1000]: training_loss: tensor(0.2845, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [986/1000]: training_loss: tensor(0.4180, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [987/1000]: training_loss: tensor(0.3758, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [988/1000]: training_loss: tensor(0.3170, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [989/1000]: training_loss: tensor(0.3073, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [990/1000]: training_loss: tensor(0.3538, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [991/1000]: training_loss: tensor(0.2529, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [992/1000]: training_loss: tensor(0.2618, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [993/1000]: training_loss: tensor(0.3116, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [994/1000]: training_loss: tensor(0.3402, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [995/1000]: training_loss: tensor(0.3119, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [996/1000]: training_loss: tensor(0.3949, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [997/1000]: training_loss: tensor(0.3303, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [998/1000]: training_loss: tensor(0.3393, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [999/1000]: training_loss: tensor(0.3630, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Epoch [2/3], global step [2000/3000], Train Loss: 0.3151, Valid Loss: 0.3276\n",
      "batch_no [0/1000]: training_loss: tensor(0.3120, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [1/1000]: training_loss: tensor(0.3167, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [2/1000]: training_loss: tensor(0.3357, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [3/1000]: training_loss: tensor(0.2811, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [4/1000]: training_loss: tensor(0.2556, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [5/1000]: training_loss: tensor(0.3201, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [6/1000]: training_loss: tensor(0.2797, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [7/1000]: training_loss: tensor(0.2856, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [8/1000]: training_loss: tensor(0.2450, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [9/1000]: training_loss: tensor(0.3264, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [10/1000]: training_loss: tensor(0.2692, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [11/1000]: training_loss: tensor(0.3120, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [12/1000]: training_loss: tensor(0.2673, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [13/1000]: training_loss: tensor(0.3173, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [14/1000]: training_loss: tensor(0.2887, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [15/1000]: training_loss: tensor(0.3893, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [16/1000]: training_loss: tensor(0.3183, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [17/1000]: training_loss: tensor(0.3419, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [18/1000]: training_loss: tensor(0.3260, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [19/1000]: training_loss: tensor(0.2851, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [20/1000]: training_loss: tensor(0.2937, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [21/1000]: training_loss: tensor(0.3845, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [22/1000]: training_loss: tensor(0.2457, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [23/1000]: training_loss: tensor(0.2902, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [24/1000]: training_loss: tensor(0.3248, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [25/1000]: training_loss: tensor(0.3607, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [26/1000]: training_loss: tensor(0.2945, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [27/1000]: training_loss: tensor(0.2658, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [28/1000]: training_loss: tensor(0.3248, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [29/1000]: training_loss: tensor(0.3063, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [30/1000]: training_loss: tensor(0.3312, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [31/1000]: training_loss: tensor(0.2747, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [32/1000]: training_loss: tensor(0.2829, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [33/1000]: training_loss: tensor(0.2817, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [34/1000]: training_loss: tensor(0.2582, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [35/1000]: training_loss: tensor(0.2741, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [36/1000]: training_loss: tensor(0.2136, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [37/1000]: training_loss: tensor(0.3938, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [38/1000]: training_loss: tensor(0.3668, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [39/1000]: training_loss: tensor(0.2770, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [40/1000]: training_loss: tensor(0.3670, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [41/1000]: training_loss: tensor(0.2626, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [42/1000]: training_loss: tensor(0.2904, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [43/1000]: training_loss: tensor(0.4040, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [44/1000]: training_loss: tensor(0.2917, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [45/1000]: training_loss: tensor(0.3144, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [46/1000]: training_loss: tensor(0.2351, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [47/1000]: training_loss: tensor(0.3562, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [48/1000]: training_loss: tensor(0.3008, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [49/1000]: training_loss: tensor(0.3602, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [50/1000]: training_loss: tensor(0.3997, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [51/1000]: training_loss: tensor(0.2596, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [52/1000]: training_loss: tensor(0.2624, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [53/1000]: training_loss: tensor(0.3515, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [54/1000]: training_loss: tensor(0.3196, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [55/1000]: training_loss: tensor(0.3107, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [56/1000]: training_loss: tensor(0.3700, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [57/1000]: training_loss: tensor(0.3011, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [58/1000]: training_loss: tensor(0.3184, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [59/1000]: training_loss: tensor(0.3107, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [60/1000]: training_loss: tensor(0.3404, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [61/1000]: training_loss: tensor(0.3725, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [62/1000]: training_loss: tensor(0.3664, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [63/1000]: training_loss: tensor(0.2822, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [64/1000]: training_loss: tensor(0.3889, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [65/1000]: training_loss: tensor(0.2750, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [66/1000]: training_loss: tensor(0.2931, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [67/1000]: training_loss: tensor(0.3436, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [68/1000]: training_loss: tensor(0.2623, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [69/1000]: training_loss: tensor(0.2212, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [70/1000]: training_loss: tensor(0.4010, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [71/1000]: training_loss: tensor(0.3560, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [72/1000]: training_loss: tensor(0.3316, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [73/1000]: training_loss: tensor(0.2585, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [74/1000]: training_loss: tensor(0.3361, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [75/1000]: training_loss: tensor(0.3423, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [76/1000]: training_loss: tensor(0.2806, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [77/1000]: training_loss: tensor(0.2653, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [78/1000]: training_loss: tensor(0.3368, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [79/1000]: training_loss: tensor(0.2617, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [80/1000]: training_loss: tensor(0.2778, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [81/1000]: training_loss: tensor(0.3091, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [82/1000]: training_loss: tensor(0.3256, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [83/1000]: training_loss: tensor(0.2754, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [84/1000]: training_loss: tensor(0.3548, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [85/1000]: training_loss: tensor(0.3231, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [86/1000]: training_loss: tensor(0.2868, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [87/1000]: training_loss: tensor(0.2754, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [88/1000]: training_loss: tensor(0.3018, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [89/1000]: training_loss: tensor(0.2401, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [90/1000]: training_loss: tensor(0.2551, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [91/1000]: training_loss: tensor(0.3389, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [92/1000]: training_loss: tensor(0.3049, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [93/1000]: training_loss: tensor(0.4713, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [94/1000]: training_loss: tensor(0.3413, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [95/1000]: training_loss: tensor(0.2617, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [96/1000]: training_loss: tensor(0.3334, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [97/1000]: training_loss: tensor(0.3043, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [98/1000]: training_loss: tensor(0.3131, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [99/1000]: training_loss: tensor(0.2385, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [100/1000]: training_loss: tensor(0.2905, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [101/1000]: training_loss: tensor(0.2678, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [102/1000]: training_loss: tensor(0.3027, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [103/1000]: training_loss: tensor(0.2720, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [104/1000]: training_loss: tensor(0.3159, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [105/1000]: training_loss: tensor(0.2858, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [106/1000]: training_loss: tensor(0.2767, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [107/1000]: training_loss: tensor(0.2430, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [108/1000]: training_loss: tensor(0.3931, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [109/1000]: training_loss: tensor(0.3030, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [110/1000]: training_loss: tensor(0.3153, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [111/1000]: training_loss: tensor(0.2277, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [112/1000]: training_loss: tensor(0.3317, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [113/1000]: training_loss: tensor(0.2862, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [114/1000]: training_loss: tensor(0.2697, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [115/1000]: training_loss: tensor(0.2713, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [116/1000]: training_loss: tensor(0.3785, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [117/1000]: training_loss: tensor(0.2827, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [118/1000]: training_loss: tensor(0.3219, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [119/1000]: training_loss: tensor(0.3067, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [120/1000]: training_loss: tensor(0.3860, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [121/1000]: training_loss: tensor(0.2718, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [122/1000]: training_loss: tensor(0.3232, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [123/1000]: training_loss: tensor(0.3026, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [124/1000]: training_loss: tensor(0.2952, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [125/1000]: training_loss: tensor(0.3699, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [126/1000]: training_loss: tensor(0.2495, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [127/1000]: training_loss: tensor(0.2428, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [128/1000]: training_loss: tensor(0.2964, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [129/1000]: training_loss: tensor(0.3926, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [130/1000]: training_loss: tensor(0.3541, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [131/1000]: training_loss: tensor(0.3340, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [132/1000]: training_loss: tensor(0.3080, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [133/1000]: training_loss: tensor(0.3362, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [134/1000]: training_loss: tensor(0.2657, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [135/1000]: training_loss: tensor(0.3600, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [136/1000]: training_loss: tensor(0.3029, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [137/1000]: training_loss: tensor(0.2908, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [138/1000]: training_loss: tensor(0.3496, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [139/1000]: training_loss: tensor(0.2868, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [140/1000]: training_loss: tensor(0.2667, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [141/1000]: training_loss: tensor(0.3932, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [142/1000]: training_loss: tensor(0.3226, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [143/1000]: training_loss: tensor(0.2835, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [144/1000]: training_loss: tensor(0.2976, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [145/1000]: training_loss: tensor(0.3085, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [146/1000]: training_loss: tensor(0.2941, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [147/1000]: training_loss: tensor(0.2471, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [148/1000]: training_loss: tensor(0.3142, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [149/1000]: training_loss: tensor(0.2761, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [150/1000]: training_loss: tensor(0.3584, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [151/1000]: training_loss: tensor(0.2545, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [152/1000]: training_loss: tensor(0.2734, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [153/1000]: training_loss: tensor(0.3594, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [154/1000]: training_loss: tensor(0.3281, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [155/1000]: training_loss: tensor(0.3591, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [156/1000]: training_loss: tensor(0.2628, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [157/1000]: training_loss: tensor(0.3248, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [158/1000]: training_loss: tensor(0.3695, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [159/1000]: training_loss: tensor(0.3190, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [160/1000]: training_loss: tensor(0.2895, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [161/1000]: training_loss: tensor(0.3285, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [162/1000]: training_loss: tensor(0.3523, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [163/1000]: training_loss: tensor(0.2937, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [164/1000]: training_loss: tensor(0.2348, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [165/1000]: training_loss: tensor(0.2626, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [166/1000]: training_loss: tensor(0.4281, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [167/1000]: training_loss: tensor(0.3217, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [168/1000]: training_loss: tensor(0.3182, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [169/1000]: training_loss: tensor(0.2481, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [170/1000]: training_loss: tensor(0.3583, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [171/1000]: training_loss: tensor(0.3235, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [172/1000]: training_loss: tensor(0.3301, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [173/1000]: training_loss: tensor(0.2093, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [174/1000]: training_loss: tensor(0.3498, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [175/1000]: training_loss: tensor(0.2885, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [176/1000]: training_loss: tensor(0.3007, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [177/1000]: training_loss: tensor(0.3642, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [178/1000]: training_loss: tensor(0.2947, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [179/1000]: training_loss: tensor(0.2940, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [180/1000]: training_loss: tensor(0.2769, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [181/1000]: training_loss: tensor(0.3759, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [182/1000]: training_loss: tensor(0.2907, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [183/1000]: training_loss: tensor(0.3175, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [184/1000]: training_loss: tensor(0.2456, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [185/1000]: training_loss: tensor(0.2956, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [186/1000]: training_loss: tensor(0.3261, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [187/1000]: training_loss: tensor(0.2540, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [188/1000]: training_loss: tensor(0.2742, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [189/1000]: training_loss: tensor(0.2392, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [190/1000]: training_loss: tensor(0.3379, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [191/1000]: training_loss: tensor(0.2276, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [192/1000]: training_loss: tensor(0.3035, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [193/1000]: training_loss: tensor(0.2773, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [194/1000]: training_loss: tensor(0.3125, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [195/1000]: training_loss: tensor(0.3233, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [196/1000]: training_loss: tensor(0.3410, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [197/1000]: training_loss: tensor(0.2497, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [198/1000]: training_loss: tensor(0.3130, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [199/1000]: training_loss: tensor(0.3203, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [200/1000]: training_loss: tensor(0.3494, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [201/1000]: training_loss: tensor(0.3760, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [202/1000]: training_loss: tensor(0.3051, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [203/1000]: training_loss: tensor(0.3075, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [204/1000]: training_loss: tensor(0.3634, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [205/1000]: training_loss: tensor(0.2841, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [206/1000]: training_loss: tensor(0.3378, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [207/1000]: training_loss: tensor(0.2780, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [208/1000]: training_loss: tensor(0.3215, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [209/1000]: training_loss: tensor(0.3068, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [210/1000]: training_loss: tensor(0.3675, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [211/1000]: training_loss: tensor(0.3586, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [212/1000]: training_loss: tensor(0.2583, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [213/1000]: training_loss: tensor(0.2655, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [214/1000]: training_loss: tensor(0.2825, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [215/1000]: training_loss: tensor(0.2729, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [216/1000]: training_loss: tensor(0.3242, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [217/1000]: training_loss: tensor(0.3382, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [218/1000]: training_loss: tensor(0.3063, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [219/1000]: training_loss: tensor(0.3005, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [220/1000]: training_loss: tensor(0.3331, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [221/1000]: training_loss: tensor(0.2513, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [222/1000]: training_loss: tensor(0.3310, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [223/1000]: training_loss: tensor(0.2539, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [224/1000]: training_loss: tensor(0.3093, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [225/1000]: training_loss: tensor(0.2827, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [226/1000]: training_loss: tensor(0.2987, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [227/1000]: training_loss: tensor(0.2847, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [228/1000]: training_loss: tensor(0.3446, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [229/1000]: training_loss: tensor(0.2930, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [230/1000]: training_loss: tensor(0.2608, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [231/1000]: training_loss: tensor(0.2898, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [232/1000]: training_loss: tensor(0.2757, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [233/1000]: training_loss: tensor(0.4216, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [234/1000]: training_loss: tensor(0.2841, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [235/1000]: training_loss: tensor(0.3602, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [236/1000]: training_loss: tensor(0.3706, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [237/1000]: training_loss: tensor(0.3090, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [238/1000]: training_loss: tensor(0.2837, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [239/1000]: training_loss: tensor(0.2741, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [240/1000]: training_loss: tensor(0.2787, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [241/1000]: training_loss: tensor(0.3381, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [242/1000]: training_loss: tensor(0.3519, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [243/1000]: training_loss: tensor(0.3590, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [244/1000]: training_loss: tensor(0.3417, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [245/1000]: training_loss: tensor(0.3866, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [246/1000]: training_loss: tensor(0.2824, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [247/1000]: training_loss: tensor(0.2717, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [248/1000]: training_loss: tensor(0.3219, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [249/1000]: training_loss: tensor(0.3443, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [250/1000]: training_loss: tensor(0.2850, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [251/1000]: training_loss: tensor(0.2545, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [252/1000]: training_loss: tensor(0.3964, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [253/1000]: training_loss: tensor(0.3319, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [254/1000]: training_loss: tensor(0.2940, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [255/1000]: training_loss: tensor(0.3132, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [256/1000]: training_loss: tensor(0.2742, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [257/1000]: training_loss: tensor(0.3586, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [258/1000]: training_loss: tensor(0.2909, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [259/1000]: training_loss: tensor(0.2298, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [260/1000]: training_loss: tensor(0.3653, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [261/1000]: training_loss: tensor(0.3524, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [262/1000]: training_loss: tensor(0.3914, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [263/1000]: training_loss: tensor(0.3146, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [264/1000]: training_loss: tensor(0.2860, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [265/1000]: training_loss: tensor(0.3409, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [266/1000]: training_loss: tensor(0.2861, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [267/1000]: training_loss: tensor(0.3156, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [268/1000]: training_loss: tensor(0.3245, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [269/1000]: training_loss: tensor(0.2982, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [270/1000]: training_loss: tensor(0.3120, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [271/1000]: training_loss: tensor(0.3244, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [272/1000]: training_loss: tensor(0.3213, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [273/1000]: training_loss: tensor(0.3291, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [274/1000]: training_loss: tensor(0.3760, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [275/1000]: training_loss: tensor(0.3242, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [276/1000]: training_loss: tensor(0.3344, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [277/1000]: training_loss: tensor(0.2678, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [278/1000]: training_loss: tensor(0.3121, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [279/1000]: training_loss: tensor(0.3785, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [280/1000]: training_loss: tensor(0.3425, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [281/1000]: training_loss: tensor(0.2634, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [282/1000]: training_loss: tensor(0.2645, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [283/1000]: training_loss: tensor(0.2711, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [284/1000]: training_loss: tensor(0.3321, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [285/1000]: training_loss: tensor(0.3779, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [286/1000]: training_loss: tensor(0.2861, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [287/1000]: training_loss: tensor(0.2930, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [288/1000]: training_loss: tensor(0.2605, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [289/1000]: training_loss: tensor(0.2626, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [290/1000]: training_loss: tensor(0.3683, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [291/1000]: training_loss: tensor(0.3084, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [292/1000]: training_loss: tensor(0.2777, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [293/1000]: training_loss: tensor(0.3951, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [294/1000]: training_loss: tensor(0.3076, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [295/1000]: training_loss: tensor(0.2537, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [296/1000]: training_loss: tensor(0.3727, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [297/1000]: training_loss: tensor(0.3646, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [298/1000]: training_loss: tensor(0.3779, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [299/1000]: training_loss: tensor(0.2863, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [300/1000]: training_loss: tensor(0.4546, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [301/1000]: training_loss: tensor(0.3458, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [302/1000]: training_loss: tensor(0.3015, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [303/1000]: training_loss: tensor(0.3343, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [304/1000]: training_loss: tensor(0.3474, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [305/1000]: training_loss: tensor(0.3355, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [306/1000]: training_loss: tensor(0.2895, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [307/1000]: training_loss: tensor(0.3186, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [308/1000]: training_loss: tensor(0.3361, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [309/1000]: training_loss: tensor(0.2607, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [310/1000]: training_loss: tensor(0.2947, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [311/1000]: training_loss: tensor(0.3445, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [312/1000]: training_loss: tensor(0.3160, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [313/1000]: training_loss: tensor(0.3116, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [314/1000]: training_loss: tensor(0.2605, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [315/1000]: training_loss: tensor(0.2712, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [316/1000]: training_loss: tensor(0.2582, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [317/1000]: training_loss: tensor(0.2977, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [318/1000]: training_loss: tensor(0.2753, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [319/1000]: training_loss: tensor(0.3428, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [320/1000]: training_loss: tensor(0.3605, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [321/1000]: training_loss: tensor(0.3055, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [322/1000]: training_loss: tensor(0.2534, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [323/1000]: training_loss: tensor(0.3288, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [324/1000]: training_loss: tensor(0.2981, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [325/1000]: training_loss: tensor(0.3609, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [326/1000]: training_loss: tensor(0.2362, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [327/1000]: training_loss: tensor(0.2270, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [328/1000]: training_loss: tensor(0.2421, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [329/1000]: training_loss: tensor(0.2472, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [330/1000]: training_loss: tensor(0.2965, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [331/1000]: training_loss: tensor(0.3187, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [332/1000]: training_loss: tensor(0.2689, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [333/1000]: training_loss: tensor(0.2843, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [334/1000]: training_loss: tensor(0.3879, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [335/1000]: training_loss: tensor(0.2729, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [336/1000]: training_loss: tensor(0.3528, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [337/1000]: training_loss: tensor(0.4907, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [338/1000]: training_loss: tensor(0.2709, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [339/1000]: training_loss: tensor(0.2966, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [340/1000]: training_loss: tensor(0.2846, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [341/1000]: training_loss: tensor(0.2756, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [342/1000]: training_loss: tensor(0.3228, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [343/1000]: training_loss: tensor(0.3665, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [344/1000]: training_loss: tensor(0.2326, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [345/1000]: training_loss: tensor(0.2600, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [346/1000]: training_loss: tensor(0.4160, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [347/1000]: training_loss: tensor(0.2506, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [348/1000]: training_loss: tensor(0.2851, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [349/1000]: training_loss: tensor(0.2961, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [350/1000]: training_loss: tensor(0.3497, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [351/1000]: training_loss: tensor(0.4558, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [352/1000]: training_loss: tensor(0.2286, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [353/1000]: training_loss: tensor(0.3129, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [354/1000]: training_loss: tensor(0.2701, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [355/1000]: training_loss: tensor(0.3440, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [356/1000]: training_loss: tensor(0.2182, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [357/1000]: training_loss: tensor(0.3198, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [358/1000]: training_loss: tensor(0.3253, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [359/1000]: training_loss: tensor(0.2479, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [360/1000]: training_loss: tensor(0.3023, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [361/1000]: training_loss: tensor(0.2729, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [362/1000]: training_loss: tensor(0.3137, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [363/1000]: training_loss: tensor(0.3312, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [364/1000]: training_loss: tensor(0.2589, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [365/1000]: training_loss: tensor(0.3629, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [366/1000]: training_loss: tensor(0.3262, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [367/1000]: training_loss: tensor(0.3021, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [368/1000]: training_loss: tensor(0.2722, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [369/1000]: training_loss: tensor(0.2660, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [370/1000]: training_loss: tensor(0.3008, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [371/1000]: training_loss: tensor(0.3047, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [372/1000]: training_loss: tensor(0.2119, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [373/1000]: training_loss: tensor(0.3918, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [374/1000]: training_loss: tensor(0.3122, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [375/1000]: training_loss: tensor(0.3201, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [376/1000]: training_loss: tensor(0.2468, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [377/1000]: training_loss: tensor(0.2467, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [378/1000]: training_loss: tensor(0.3754, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [379/1000]: training_loss: tensor(0.3106, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [380/1000]: training_loss: tensor(0.2508, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [381/1000]: training_loss: tensor(0.3936, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [382/1000]: training_loss: tensor(0.2770, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [383/1000]: training_loss: tensor(0.2263, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [384/1000]: training_loss: tensor(0.3309, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [385/1000]: training_loss: tensor(0.3484, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [386/1000]: training_loss: tensor(0.3198, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [387/1000]: training_loss: tensor(0.3630, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [388/1000]: training_loss: tensor(0.2340, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [389/1000]: training_loss: tensor(0.2965, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [390/1000]: training_loss: tensor(0.2800, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [391/1000]: training_loss: tensor(0.3078, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [392/1000]: training_loss: tensor(0.3803, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [393/1000]: training_loss: tensor(0.3232, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [394/1000]: training_loss: tensor(0.3682, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [395/1000]: training_loss: tensor(0.3759, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [396/1000]: training_loss: tensor(0.3138, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [397/1000]: training_loss: tensor(0.3062, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [398/1000]: training_loss: tensor(0.2721, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [399/1000]: training_loss: tensor(0.2788, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [400/1000]: training_loss: tensor(0.3664, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [401/1000]: training_loss: tensor(0.3315, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [402/1000]: training_loss: tensor(0.3923, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [403/1000]: training_loss: tensor(0.3551, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [404/1000]: training_loss: tensor(0.2313, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [405/1000]: training_loss: tensor(0.2451, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [406/1000]: training_loss: tensor(0.3528, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [407/1000]: training_loss: tensor(0.3271, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [408/1000]: training_loss: tensor(0.3902, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [409/1000]: training_loss: tensor(0.2269, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [410/1000]: training_loss: tensor(0.3109, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [411/1000]: training_loss: tensor(0.3169, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [412/1000]: training_loss: tensor(0.3335, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [413/1000]: training_loss: tensor(0.3055, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [414/1000]: training_loss: tensor(0.3916, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [415/1000]: training_loss: tensor(0.3810, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [416/1000]: training_loss: tensor(0.2992, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [417/1000]: training_loss: tensor(0.3110, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [418/1000]: training_loss: tensor(0.3694, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [419/1000]: training_loss: tensor(0.3489, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [420/1000]: training_loss: tensor(0.2799, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [421/1000]: training_loss: tensor(0.2965, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [422/1000]: training_loss: tensor(0.2608, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [423/1000]: training_loss: tensor(0.2770, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [424/1000]: training_loss: tensor(0.2378, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [425/1000]: training_loss: tensor(0.2531, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [426/1000]: training_loss: tensor(0.3187, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [427/1000]: training_loss: tensor(0.3031, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [428/1000]: training_loss: tensor(0.2735, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [429/1000]: training_loss: tensor(0.2810, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [430/1000]: training_loss: tensor(0.3159, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [431/1000]: training_loss: tensor(0.3026, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [432/1000]: training_loss: tensor(0.2669, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [433/1000]: training_loss: tensor(0.3197, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [434/1000]: training_loss: tensor(0.3696, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [435/1000]: training_loss: tensor(0.3819, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [436/1000]: training_loss: tensor(0.3176, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [437/1000]: training_loss: tensor(0.3015, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [438/1000]: training_loss: tensor(0.3861, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [439/1000]: training_loss: tensor(0.3437, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [440/1000]: training_loss: tensor(0.2711, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [441/1000]: training_loss: tensor(0.2781, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [442/1000]: training_loss: tensor(0.2442, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [443/1000]: training_loss: tensor(0.3188, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [444/1000]: training_loss: tensor(0.3265, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [445/1000]: training_loss: tensor(0.2450, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [446/1000]: training_loss: tensor(0.3780, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [447/1000]: training_loss: tensor(0.2566, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [448/1000]: training_loss: tensor(0.3391, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [449/1000]: training_loss: tensor(0.2229, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [450/1000]: training_loss: tensor(0.3316, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [451/1000]: training_loss: tensor(0.2870, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [452/1000]: training_loss: tensor(0.2259, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [453/1000]: training_loss: tensor(0.3112, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [454/1000]: training_loss: tensor(0.2304, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [455/1000]: training_loss: tensor(0.3149, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [456/1000]: training_loss: tensor(0.3574, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [457/1000]: training_loss: tensor(0.3030, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [458/1000]: training_loss: tensor(0.3204, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [459/1000]: training_loss: tensor(0.2993, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [460/1000]: training_loss: tensor(0.2732, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [461/1000]: training_loss: tensor(0.2750, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [462/1000]: training_loss: tensor(0.3498, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [463/1000]: training_loss: tensor(0.2701, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [464/1000]: training_loss: tensor(0.3388, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [465/1000]: training_loss: tensor(0.3279, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [466/1000]: training_loss: tensor(0.3375, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [467/1000]: training_loss: tensor(0.3252, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [468/1000]: training_loss: tensor(0.2735, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [469/1000]: training_loss: tensor(0.2907, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [470/1000]: training_loss: tensor(0.2898, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [471/1000]: training_loss: tensor(0.3675, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [472/1000]: training_loss: tensor(0.3785, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [473/1000]: training_loss: tensor(0.3368, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [474/1000]: training_loss: tensor(0.3187, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [475/1000]: training_loss: tensor(0.2177, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [476/1000]: training_loss: tensor(0.3372, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [477/1000]: training_loss: tensor(0.3018, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [478/1000]: training_loss: tensor(0.3428, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [479/1000]: training_loss: tensor(0.3752, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [480/1000]: training_loss: tensor(0.3012, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [481/1000]: training_loss: tensor(0.3476, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [482/1000]: training_loss: tensor(0.3745, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [483/1000]: training_loss: tensor(0.2675, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [484/1000]: training_loss: tensor(0.2283, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [485/1000]: training_loss: tensor(0.3038, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [486/1000]: training_loss: tensor(0.3107, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [487/1000]: training_loss: tensor(0.2587, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [488/1000]: training_loss: tensor(0.3307, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [489/1000]: training_loss: tensor(0.2153, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [490/1000]: training_loss: tensor(0.2835, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [491/1000]: training_loss: tensor(0.3050, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [492/1000]: training_loss: tensor(0.3781, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [493/1000]: training_loss: tensor(0.2983, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [494/1000]: training_loss: tensor(0.2525, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [495/1000]: training_loss: tensor(0.3287, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [496/1000]: training_loss: tensor(0.2309, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [497/1000]: training_loss: tensor(0.2707, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [498/1000]: training_loss: tensor(0.2592, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [499/1000]: training_loss: tensor(0.2944, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [500/1000]: training_loss: tensor(0.3107, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [501/1000]: training_loss: tensor(0.3400, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [502/1000]: training_loss: tensor(0.2803, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [503/1000]: training_loss: tensor(0.3444, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [504/1000]: training_loss: tensor(0.2791, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [505/1000]: training_loss: tensor(0.4224, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [506/1000]: training_loss: tensor(0.2764, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [507/1000]: training_loss: tensor(0.3514, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [508/1000]: training_loss: tensor(0.3322, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [509/1000]: training_loss: tensor(0.4005, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [510/1000]: training_loss: tensor(0.2594, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [511/1000]: training_loss: tensor(0.2508, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [512/1000]: training_loss: tensor(0.2850, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [513/1000]: training_loss: tensor(0.3586, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [514/1000]: training_loss: tensor(0.4389, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [515/1000]: training_loss: tensor(0.2968, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [516/1000]: training_loss: tensor(0.3897, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [517/1000]: training_loss: tensor(0.2748, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [518/1000]: training_loss: tensor(0.2711, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [519/1000]: training_loss: tensor(0.2365, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [520/1000]: training_loss: tensor(0.2891, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [521/1000]: training_loss: tensor(0.3278, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [522/1000]: training_loss: tensor(0.2470, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [523/1000]: training_loss: tensor(0.2737, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [524/1000]: training_loss: tensor(0.2693, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [525/1000]: training_loss: tensor(0.3728, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [526/1000]: training_loss: tensor(0.3462, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [527/1000]: training_loss: tensor(0.2980, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [528/1000]: training_loss: tensor(0.3312, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [529/1000]: training_loss: tensor(0.4360, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [530/1000]: training_loss: tensor(0.2430, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [531/1000]: training_loss: tensor(0.2840, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [532/1000]: training_loss: tensor(0.3554, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [533/1000]: training_loss: tensor(0.2755, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [534/1000]: training_loss: tensor(0.2805, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [535/1000]: training_loss: tensor(0.2816, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [536/1000]: training_loss: tensor(0.2555, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [537/1000]: training_loss: tensor(0.2827, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [538/1000]: training_loss: tensor(0.3384, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [539/1000]: training_loss: tensor(0.3146, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [540/1000]: training_loss: tensor(0.2579, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [541/1000]: training_loss: tensor(0.2696, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [542/1000]: training_loss: tensor(0.3834, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [543/1000]: training_loss: tensor(0.2529, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [544/1000]: training_loss: tensor(0.3963, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [545/1000]: training_loss: tensor(0.3093, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [546/1000]: training_loss: tensor(0.2846, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [547/1000]: training_loss: tensor(0.2829, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [548/1000]: training_loss: tensor(0.3056, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [549/1000]: training_loss: tensor(0.2481, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [550/1000]: training_loss: tensor(0.2340, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [551/1000]: training_loss: tensor(0.3166, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [552/1000]: training_loss: tensor(0.3007, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [553/1000]: training_loss: tensor(0.3009, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [554/1000]: training_loss: tensor(0.4373, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [555/1000]: training_loss: tensor(0.2833, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [556/1000]: training_loss: tensor(0.2590, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [557/1000]: training_loss: tensor(0.3289, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [558/1000]: training_loss: tensor(0.3541, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [559/1000]: training_loss: tensor(0.3331, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [560/1000]: training_loss: tensor(0.2857, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [561/1000]: training_loss: tensor(0.3007, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [562/1000]: training_loss: tensor(0.2968, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [563/1000]: training_loss: tensor(0.2668, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [564/1000]: training_loss: tensor(0.3440, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [565/1000]: training_loss: tensor(0.2776, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [566/1000]: training_loss: tensor(0.3777, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [567/1000]: training_loss: tensor(0.3106, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [568/1000]: training_loss: tensor(0.3117, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [569/1000]: training_loss: tensor(0.3805, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [570/1000]: training_loss: tensor(0.3152, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [571/1000]: training_loss: tensor(0.3075, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [572/1000]: training_loss: tensor(0.2436, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [573/1000]: training_loss: tensor(0.2720, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [574/1000]: training_loss: tensor(0.2491, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [575/1000]: training_loss: tensor(0.3585, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [576/1000]: training_loss: tensor(0.2882, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [577/1000]: training_loss: tensor(0.3896, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [578/1000]: training_loss: tensor(0.3715, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [579/1000]: training_loss: tensor(0.3142, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [580/1000]: training_loss: tensor(0.3376, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [581/1000]: training_loss: tensor(0.2787, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [582/1000]: training_loss: tensor(0.2989, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [583/1000]: training_loss: tensor(0.3346, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [584/1000]: training_loss: tensor(0.2586, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [585/1000]: training_loss: tensor(0.3221, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [586/1000]: training_loss: tensor(0.2833, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [587/1000]: training_loss: tensor(0.2916, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [588/1000]: training_loss: tensor(0.3219, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [589/1000]: training_loss: tensor(0.2982, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [590/1000]: training_loss: tensor(0.2899, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [591/1000]: training_loss: tensor(0.2854, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [592/1000]: training_loss: tensor(0.2496, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [593/1000]: training_loss: tensor(0.3177, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [594/1000]: training_loss: tensor(0.3313, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [595/1000]: training_loss: tensor(0.2429, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [596/1000]: training_loss: tensor(0.3647, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [597/1000]: training_loss: tensor(0.2663, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [598/1000]: training_loss: tensor(0.3532, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [599/1000]: training_loss: tensor(0.3215, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [600/1000]: training_loss: tensor(0.2873, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [601/1000]: training_loss: tensor(0.2513, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [602/1000]: training_loss: tensor(0.3372, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [603/1000]: training_loss: tensor(0.2249, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [604/1000]: training_loss: tensor(0.2217, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [605/1000]: training_loss: tensor(0.4443, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [606/1000]: training_loss: tensor(0.3170, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [607/1000]: training_loss: tensor(0.3999, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [608/1000]: training_loss: tensor(0.2614, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [609/1000]: training_loss: tensor(0.3176, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [610/1000]: training_loss: tensor(0.2483, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [611/1000]: training_loss: tensor(0.3156, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [612/1000]: training_loss: tensor(0.3122, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [613/1000]: training_loss: tensor(0.3272, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [614/1000]: training_loss: tensor(0.2604, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [615/1000]: training_loss: tensor(0.3278, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [616/1000]: training_loss: tensor(0.2618, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [617/1000]: training_loss: tensor(0.2236, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [618/1000]: training_loss: tensor(0.3946, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [619/1000]: training_loss: tensor(0.3417, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [620/1000]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [621/1000]: training_loss: tensor(0.2745, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [622/1000]: training_loss: tensor(0.3193, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [623/1000]: training_loss: tensor(0.2704, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [624/1000]: training_loss: tensor(0.2743, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [625/1000]: training_loss: tensor(0.2753, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [626/1000]: training_loss: tensor(0.3135, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [627/1000]: training_loss: tensor(0.2985, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [628/1000]: training_loss: tensor(0.3571, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [629/1000]: training_loss: tensor(0.2728, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [630/1000]: training_loss: tensor(0.2689, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [631/1000]: training_loss: tensor(0.2797, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [632/1000]: training_loss: tensor(0.2312, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [633/1000]: training_loss: tensor(0.2603, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [634/1000]: training_loss: tensor(0.2624, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [635/1000]: training_loss: tensor(0.2849, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [636/1000]: training_loss: tensor(0.4200, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [637/1000]: training_loss: tensor(0.3836, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [638/1000]: training_loss: tensor(0.3653, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [639/1000]: training_loss: tensor(0.3605, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [640/1000]: training_loss: tensor(0.2831, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [641/1000]: training_loss: tensor(0.4179, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [642/1000]: training_loss: tensor(0.3362, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [643/1000]: training_loss: tensor(0.2916, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [644/1000]: training_loss: tensor(0.3211, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [645/1000]: training_loss: tensor(0.3201, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [646/1000]: training_loss: tensor(0.2517, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [647/1000]: training_loss: tensor(0.3345, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [648/1000]: training_loss: tensor(0.2910, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [649/1000]: training_loss: tensor(0.2920, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [650/1000]: training_loss: tensor(0.2286, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [651/1000]: training_loss: tensor(0.3133, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [652/1000]: training_loss: tensor(0.3191, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [653/1000]: training_loss: tensor(0.2375, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [654/1000]: training_loss: tensor(0.3461, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [655/1000]: training_loss: tensor(0.2535, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [656/1000]: training_loss: tensor(0.2688, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [657/1000]: training_loss: tensor(0.3312, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [658/1000]: training_loss: tensor(0.3713, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [659/1000]: training_loss: tensor(0.2209, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [660/1000]: training_loss: tensor(0.3157, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [661/1000]: training_loss: tensor(0.2593, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [662/1000]: training_loss: tensor(0.3321, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [663/1000]: training_loss: tensor(0.2911, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [664/1000]: training_loss: tensor(0.3679, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [665/1000]: training_loss: tensor(0.3064, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [666/1000]: training_loss: tensor(0.2290, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [667/1000]: training_loss: tensor(0.2860, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [668/1000]: training_loss: tensor(0.3211, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [669/1000]: training_loss: tensor(0.3195, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [670/1000]: training_loss: tensor(0.3307, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [671/1000]: training_loss: tensor(0.3068, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [672/1000]: training_loss: tensor(0.2476, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [673/1000]: training_loss: tensor(0.2474, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [674/1000]: training_loss: tensor(0.2768, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [675/1000]: training_loss: tensor(0.3119, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [676/1000]: training_loss: tensor(0.2801, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [677/1000]: training_loss: tensor(0.2966, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [678/1000]: training_loss: tensor(0.2877, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [679/1000]: training_loss: tensor(0.2146, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [680/1000]: training_loss: tensor(0.3234, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [681/1000]: training_loss: tensor(0.3124, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [682/1000]: training_loss: tensor(0.2572, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [683/1000]: training_loss: tensor(0.3518, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [684/1000]: training_loss: tensor(0.2638, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [685/1000]: training_loss: tensor(0.2594, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [686/1000]: training_loss: tensor(0.2499, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [687/1000]: training_loss: tensor(0.3437, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [688/1000]: training_loss: tensor(0.2548, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [689/1000]: training_loss: tensor(0.2935, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [690/1000]: training_loss: tensor(0.3643, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [691/1000]: training_loss: tensor(0.2809, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [692/1000]: training_loss: tensor(0.2353, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [693/1000]: training_loss: tensor(0.3349, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [694/1000]: training_loss: tensor(0.3503, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [695/1000]: training_loss: tensor(0.2864, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [696/1000]: training_loss: tensor(0.3349, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [697/1000]: training_loss: tensor(0.2805, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [698/1000]: training_loss: tensor(0.4178, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [699/1000]: training_loss: tensor(0.2536, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [700/1000]: training_loss: tensor(0.2473, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [701/1000]: training_loss: tensor(0.3610, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [702/1000]: training_loss: tensor(0.3040, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [703/1000]: training_loss: tensor(0.2643, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [704/1000]: training_loss: tensor(0.2703, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [705/1000]: training_loss: tensor(0.3515, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [706/1000]: training_loss: tensor(0.2513, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [707/1000]: training_loss: tensor(0.3430, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [708/1000]: training_loss: tensor(0.2415, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [709/1000]: training_loss: tensor(0.4100, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [710/1000]: training_loss: tensor(0.2570, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [711/1000]: training_loss: tensor(0.3137, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [712/1000]: training_loss: tensor(0.3639, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [713/1000]: training_loss: tensor(0.2707, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [714/1000]: training_loss: tensor(0.2875, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [715/1000]: training_loss: tensor(0.3498, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [716/1000]: training_loss: tensor(0.2782, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [717/1000]: training_loss: tensor(0.2924, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [718/1000]: training_loss: tensor(0.3484, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [719/1000]: training_loss: tensor(0.2461, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [720/1000]: training_loss: tensor(0.2894, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [721/1000]: training_loss: tensor(0.3764, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [722/1000]: training_loss: tensor(0.2871, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [723/1000]: training_loss: tensor(0.2494, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [724/1000]: training_loss: tensor(0.2645, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [725/1000]: training_loss: tensor(0.2921, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [726/1000]: training_loss: tensor(0.2610, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [727/1000]: training_loss: tensor(0.3246, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [728/1000]: training_loss: tensor(0.2704, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [729/1000]: training_loss: tensor(0.2533, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [730/1000]: training_loss: tensor(0.2140, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [731/1000]: training_loss: tensor(0.3258, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [732/1000]: training_loss: tensor(0.2365, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [733/1000]: training_loss: tensor(0.3236, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [734/1000]: training_loss: tensor(0.2534, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [735/1000]: training_loss: tensor(0.3398, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [736/1000]: training_loss: tensor(0.3079, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [737/1000]: training_loss: tensor(0.2932, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [738/1000]: training_loss: tensor(0.3425, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [739/1000]: training_loss: tensor(0.3304, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [740/1000]: training_loss: tensor(0.3065, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [741/1000]: training_loss: tensor(0.2262, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [742/1000]: training_loss: tensor(0.3257, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [743/1000]: training_loss: tensor(0.2395, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [744/1000]: training_loss: tensor(0.3298, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [745/1000]: training_loss: tensor(0.2623, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [746/1000]: training_loss: tensor(0.2849, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [747/1000]: training_loss: tensor(0.3770, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [748/1000]: training_loss: tensor(0.2742, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [749/1000]: training_loss: tensor(0.3878, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [750/1000]: training_loss: tensor(0.3092, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [751/1000]: training_loss: tensor(0.4060, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [752/1000]: training_loss: tensor(0.2569, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [753/1000]: training_loss: tensor(0.2870, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [754/1000]: training_loss: tensor(0.3116, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [755/1000]: training_loss: tensor(0.3094, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [756/1000]: training_loss: tensor(0.2733, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [757/1000]: training_loss: tensor(0.3055, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [758/1000]: training_loss: tensor(0.3313, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [759/1000]: training_loss: tensor(0.3412, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [760/1000]: training_loss: tensor(0.3205, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [761/1000]: training_loss: tensor(0.2830, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [762/1000]: training_loss: tensor(0.2975, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [763/1000]: training_loss: tensor(0.3005, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [764/1000]: training_loss: tensor(0.2747, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [765/1000]: training_loss: tensor(0.2782, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [766/1000]: training_loss: tensor(0.2827, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [767/1000]: training_loss: tensor(0.2590, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [768/1000]: training_loss: tensor(0.3548, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [769/1000]: training_loss: tensor(0.3226, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [770/1000]: training_loss: tensor(0.2805, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [771/1000]: training_loss: tensor(0.2843, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [772/1000]: training_loss: tensor(0.2866, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [773/1000]: training_loss: tensor(0.3019, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [774/1000]: training_loss: tensor(0.3565, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [775/1000]: training_loss: tensor(0.2884, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [776/1000]: training_loss: tensor(0.3108, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [777/1000]: training_loss: tensor(0.3384, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [778/1000]: training_loss: tensor(0.2879, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [779/1000]: training_loss: tensor(0.3041, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [780/1000]: training_loss: tensor(0.2978, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [781/1000]: training_loss: tensor(0.2881, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [782/1000]: training_loss: tensor(0.3266, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [783/1000]: training_loss: tensor(0.3204, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [784/1000]: training_loss: tensor(0.2977, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [785/1000]: training_loss: tensor(0.2927, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [786/1000]: training_loss: tensor(0.3667, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [787/1000]: training_loss: tensor(0.4602, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [788/1000]: training_loss: tensor(0.3684, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [789/1000]: training_loss: tensor(0.2699, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [790/1000]: training_loss: tensor(0.2727, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [791/1000]: training_loss: tensor(0.4028, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [792/1000]: training_loss: tensor(0.2622, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [793/1000]: training_loss: tensor(0.3458, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [794/1000]: training_loss: tensor(0.2713, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [795/1000]: training_loss: tensor(0.2805, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [796/1000]: training_loss: tensor(0.3267, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [797/1000]: training_loss: tensor(0.2333, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [798/1000]: training_loss: tensor(0.3502, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [799/1000]: training_loss: tensor(0.3493, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [800/1000]: training_loss: tensor(0.4200, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [801/1000]: training_loss: tensor(0.2975, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [802/1000]: training_loss: tensor(0.3106, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [803/1000]: training_loss: tensor(0.3866, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [804/1000]: training_loss: tensor(0.4183, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [805/1000]: training_loss: tensor(0.3174, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [806/1000]: training_loss: tensor(0.3180, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [807/1000]: training_loss: tensor(0.2881, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [808/1000]: training_loss: tensor(0.2427, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [809/1000]: training_loss: tensor(0.2864, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [810/1000]: training_loss: tensor(0.2713, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [811/1000]: training_loss: tensor(0.4150, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [812/1000]: training_loss: tensor(0.3102, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [813/1000]: training_loss: tensor(0.2446, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [814/1000]: training_loss: tensor(0.3245, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [815/1000]: training_loss: tensor(0.2260, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [816/1000]: training_loss: tensor(0.2970, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [817/1000]: training_loss: tensor(0.4068, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [818/1000]: training_loss: tensor(0.3373, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [819/1000]: training_loss: tensor(0.3532, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [820/1000]: training_loss: tensor(0.2625, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [821/1000]: training_loss: tensor(0.2359, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [822/1000]: training_loss: tensor(0.3919, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [823/1000]: training_loss: tensor(0.3314, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [824/1000]: training_loss: tensor(0.3674, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [825/1000]: training_loss: tensor(0.3223, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [826/1000]: training_loss: tensor(0.3096, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [827/1000]: training_loss: tensor(0.3164, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [828/1000]: training_loss: tensor(0.3364, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [829/1000]: training_loss: tensor(0.3904, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [830/1000]: training_loss: tensor(0.3315, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [831/1000]: training_loss: tensor(0.2891, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [832/1000]: training_loss: tensor(0.2584, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [833/1000]: training_loss: tensor(0.3645, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [834/1000]: training_loss: tensor(0.3363, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [835/1000]: training_loss: tensor(0.3434, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [836/1000]: training_loss: tensor(0.3197, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [837/1000]: training_loss: tensor(0.3799, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [838/1000]: training_loss: tensor(0.3434, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [839/1000]: training_loss: tensor(0.2817, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [840/1000]: training_loss: tensor(0.2886, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [841/1000]: training_loss: tensor(0.2929, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [842/1000]: training_loss: tensor(0.2087, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [843/1000]: training_loss: tensor(0.2524, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [844/1000]: training_loss: tensor(0.3510, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [845/1000]: training_loss: tensor(0.2702, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [846/1000]: training_loss: tensor(0.2760, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [847/1000]: training_loss: tensor(0.2233, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [848/1000]: training_loss: tensor(0.4338, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [849/1000]: training_loss: tensor(0.2689, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [850/1000]: training_loss: tensor(0.3032, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [851/1000]: training_loss: tensor(0.3793, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [852/1000]: training_loss: tensor(0.2161, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [853/1000]: training_loss: tensor(0.3121, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [854/1000]: training_loss: tensor(0.2665, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [855/1000]: training_loss: tensor(0.2500, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [856/1000]: training_loss: tensor(0.3093, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [857/1000]: training_loss: tensor(0.3568, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [858/1000]: training_loss: tensor(0.3104, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [859/1000]: training_loss: tensor(0.3296, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [860/1000]: training_loss: tensor(0.3673, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [861/1000]: training_loss: tensor(0.3673, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [862/1000]: training_loss: tensor(0.3672, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [863/1000]: training_loss: tensor(0.2959, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [864/1000]: training_loss: tensor(0.3330, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [865/1000]: training_loss: tensor(0.3971, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [866/1000]: training_loss: tensor(0.3557, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [867/1000]: training_loss: tensor(0.2399, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [868/1000]: training_loss: tensor(0.2513, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [869/1000]: training_loss: tensor(0.3468, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [870/1000]: training_loss: tensor(0.3217, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [871/1000]: training_loss: tensor(0.2907, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [872/1000]: training_loss: tensor(0.2346, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [873/1000]: training_loss: tensor(0.3284, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [874/1000]: training_loss: tensor(0.3187, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [875/1000]: training_loss: tensor(0.3031, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [876/1000]: training_loss: tensor(0.3110, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [877/1000]: training_loss: tensor(0.2848, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [878/1000]: training_loss: tensor(0.3508, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [879/1000]: training_loss: tensor(0.3282, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [880/1000]: training_loss: tensor(0.4017, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [881/1000]: training_loss: tensor(0.2823, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [882/1000]: training_loss: tensor(0.2674, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [883/1000]: training_loss: tensor(0.3976, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [884/1000]: training_loss: tensor(0.2566, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [885/1000]: training_loss: tensor(0.2629, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [886/1000]: training_loss: tensor(0.3264, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [887/1000]: training_loss: tensor(0.2682, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [888/1000]: training_loss: tensor(0.2752, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [889/1000]: training_loss: tensor(0.2629, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [890/1000]: training_loss: tensor(0.2592, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [891/1000]: training_loss: tensor(0.2244, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [892/1000]: training_loss: tensor(0.2955, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [893/1000]: training_loss: tensor(0.3284, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [894/1000]: training_loss: tensor(0.3274, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [895/1000]: training_loss: tensor(0.3194, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [896/1000]: training_loss: tensor(0.2436, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [897/1000]: training_loss: tensor(0.2451, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [898/1000]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [899/1000]: training_loss: tensor(0.2670, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [900/1000]: training_loss: tensor(0.3543, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [901/1000]: training_loss: tensor(0.2980, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [902/1000]: training_loss: tensor(0.2503, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [903/1000]: training_loss: tensor(0.2409, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [904/1000]: training_loss: tensor(0.2602, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [905/1000]: training_loss: tensor(0.3641, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [906/1000]: training_loss: tensor(0.3091, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [907/1000]: training_loss: tensor(0.3029, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [908/1000]: training_loss: tensor(0.2886, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [909/1000]: training_loss: tensor(0.2557, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [910/1000]: training_loss: tensor(0.3511, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [911/1000]: training_loss: tensor(0.2885, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [912/1000]: training_loss: tensor(0.2843, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [913/1000]: training_loss: tensor(0.2803, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [914/1000]: training_loss: tensor(0.3165, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [915/1000]: training_loss: tensor(0.3392, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [916/1000]: training_loss: tensor(0.3071, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [917/1000]: training_loss: tensor(0.3989, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [918/1000]: training_loss: tensor(0.2981, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [919/1000]: training_loss: tensor(0.2577, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [920/1000]: training_loss: tensor(0.3085, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [921/1000]: training_loss: tensor(0.2850, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [922/1000]: training_loss: tensor(0.3972, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [923/1000]: training_loss: tensor(0.2668, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [924/1000]: training_loss: tensor(0.2420, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [925/1000]: training_loss: tensor(0.2570, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [926/1000]: training_loss: tensor(0.3503, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [927/1000]: training_loss: tensor(0.2454, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [928/1000]: training_loss: tensor(0.3446, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [929/1000]: training_loss: tensor(0.2944, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [930/1000]: training_loss: tensor(0.3497, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [931/1000]: training_loss: tensor(0.3051, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [932/1000]: training_loss: tensor(0.2659, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [933/1000]: training_loss: tensor(0.3170, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [934/1000]: training_loss: tensor(0.3164, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [935/1000]: training_loss: tensor(0.2766, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [936/1000]: training_loss: tensor(0.2511, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [937/1000]: training_loss: tensor(0.2831, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [938/1000]: training_loss: tensor(0.2831, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [939/1000]: training_loss: tensor(0.3151, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [940/1000]: training_loss: tensor(0.2710, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [941/1000]: training_loss: tensor(0.2959, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [942/1000]: training_loss: tensor(0.2914, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [943/1000]: training_loss: tensor(0.2713, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [944/1000]: training_loss: tensor(0.2883, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [945/1000]: training_loss: tensor(0.3406, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [946/1000]: training_loss: tensor(0.3859, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [947/1000]: training_loss: tensor(0.3525, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [948/1000]: training_loss: tensor(0.2754, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [949/1000]: training_loss: tensor(0.2680, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [950/1000]: training_loss: tensor(0.2542, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [951/1000]: training_loss: tensor(0.4897, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [952/1000]: training_loss: tensor(0.3561, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [953/1000]: training_loss: tensor(0.2963, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [954/1000]: training_loss: tensor(0.3068, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [955/1000]: training_loss: tensor(0.2139, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [956/1000]: training_loss: tensor(0.3003, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [957/1000]: training_loss: tensor(0.3458, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [958/1000]: training_loss: tensor(0.3037, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [959/1000]: training_loss: tensor(0.3212, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [960/1000]: training_loss: tensor(0.2409, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [961/1000]: training_loss: tensor(0.2256, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [962/1000]: training_loss: tensor(0.2609, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [963/1000]: training_loss: tensor(0.3749, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [964/1000]: training_loss: tensor(0.3178, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [965/1000]: training_loss: tensor(0.3476, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [966/1000]: training_loss: tensor(0.2796, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [967/1000]: training_loss: tensor(0.3480, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [968/1000]: training_loss: tensor(0.2767, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [969/1000]: training_loss: tensor(0.3541, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [970/1000]: training_loss: tensor(0.3505, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [971/1000]: training_loss: tensor(0.2874, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [972/1000]: training_loss: tensor(0.3471, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [973/1000]: training_loss: tensor(0.2367, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [974/1000]: training_loss: tensor(0.2741, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [975/1000]: training_loss: tensor(0.3148, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [976/1000]: training_loss: tensor(0.2602, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [977/1000]: training_loss: tensor(0.2873, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [978/1000]: training_loss: tensor(0.2710, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [979/1000]: training_loss: tensor(0.2730, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [980/1000]: training_loss: tensor(0.3446, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [981/1000]: training_loss: tensor(0.3964, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [982/1000]: training_loss: tensor(0.4875, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [983/1000]: training_loss: tensor(0.2926, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [984/1000]: training_loss: tensor(0.3536, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [985/1000]: training_loss: tensor(0.2911, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [986/1000]: training_loss: tensor(0.2683, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [987/1000]: training_loss: tensor(0.3671, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [988/1000]: training_loss: tensor(0.3497, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [989/1000]: training_loss: tensor(0.3706, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [990/1000]: training_loss: tensor(0.3119, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [991/1000]: training_loss: tensor(0.2915, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [992/1000]: training_loss: tensor(0.2540, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [993/1000]: training_loss: tensor(0.3919, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [994/1000]: training_loss: tensor(0.3935, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [995/1000]: training_loss: tensor(0.2731, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [996/1000]: training_loss: tensor(0.2724, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [997/1000]: training_loss: tensor(0.2812, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [998/1000]: training_loss: tensor(0.2890, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [999/1000]: training_loss: tensor(0.3332, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "Epoch [3/3], global step [3000/3000], Train Loss: 0.3080, Valid Loss: 0.3195\n",
      "finetuning done!\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 3\n",
    "steps_per_epoch = len(train_dataset)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=steps_per_epoch*1, \n",
    "                                          num_training_steps=steps_per_epoch*NUM_EPOCHS)\n",
    "print(\"======================= Start finetuning ==============================\")\n",
    "model_fintuned = Finetuning(model=model, \n",
    "      train_iter=training_loader, \n",
    "      valid_iter=val_loader, \n",
    "      optimizer=optimizer, \n",
    "      scheduler=scheduler, \n",
    "      num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "350it [00:11, 30.29it/s]\n"
     ]
    }
   ],
   "source": [
    "fin_targets_fine = []\n",
    "fin_outputs_fine = []\n",
    "with torch.no_grad():                    \n",
    "    for _,data in tqdm(enumerate(test_loader, 0)):\n",
    "        source = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        target = data['targets'].to(device, dtype = torch.float)\n",
    "        y_pred = model_fintuned(input_ids=source, \n",
    "                       attention_mask=mask)\n",
    "        fin_targets_fine.extend(target.cpu().detach().numpy().tolist())\n",
    "        fin_outputs_fine.extend(torch.sigmoid(y_pred).cpu().detach().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_final_fine = []\n",
    "for i in fin_outputs_fine:\n",
    "    temp = [0 for i in range(len(label_final))]\n",
    "    index = i.index(max(i))\n",
    "    temp[index] = 1\n",
    "    outputs_final_fine.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.8121428571428572\n",
      "F1 Score (Micro) = 0.8121428571428572\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "accuracy_fine = metrics.accuracy_score(fin_targets_fine, outputs_final_fine)\n",
    "f1_score_micro_fine = metrics.f1_score(fin_targets_fine, outputs_final_fine, average='micro')\n",
    "print(f\"Accuracy Score = {accuracy_fine}\")\n",
    "print(f\"F1 Score (Micro) = {f1_score_micro_fine}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_targets1_fine = np.array(fin_targets_fine)\n",
    "fin_outputs1_fine = np.array(fin_outputs_fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr_fine = dict()\n",
    "tpr_fine = dict()\n",
    "roc_auc_fine = dict()\n",
    "for i in range(len(label_final)):\n",
    "    fpr_fine[i], tpr_fine[i], _ = roc_curve(fin_targets1[:, i], fin_outputs1[:, i])\n",
    "    roc_auc_fine[i] = auc(fpr[i], tpr[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in-domain test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot of a ROC curve for a specific class\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(fpr_fine[0], tpr_fine[0], label='counter_speech+discussion_of_eastasian_prejudice, ROC curve (AUC score = %0.2f)' % roc_auc_fine[0])\n",
    "plt.plot(fpr_fine[1], tpr_fine[1], label='entity_directed_criticism, ROC curve (AUC scoure = %0.2f)' % roc_auc_fine[1])\n",
    "plt.plot(fpr_fine[2], tpr_fine[2], label='entity_directed_hostility, ROC curve (AUC score = %0.2f)' % roc_auc_fine[2])\n",
    "plt.plot(fpr_fine[3], tpr_fine[3], label='neutral, ROC curve (AUC score = %0.2f)' % roc_auc_fine[3])\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_fine = np.array(outputs_final_fine)\n",
    "y_true_fine = np.array(fin_targets_fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "                                                  precision    recall  f1-score   support\n",
      "\n",
      "counter_speech+discussion_of_eastasian_prejudice     0.7151    0.6737    0.6938       190\n",
      "                       entity_directed_criticism     0.5385    0.0341    0.0642       205\n",
      "                       entity_directed_hostility     0.6233    0.7579    0.6840       537\n",
      "                               none_of_the_above     0.8859    0.9272    0.9061      1868\n",
      "\n",
      "                                        accuracy                         0.8121      2800\n",
      "                                       macro avg     0.6907    0.5982    0.5870      2800\n",
      "                                    weighted avg     0.7985    0.8121    0.7875      2800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print('Classification Report:')\n",
    "y_true2 = y_true_fine.argmax(axis=1)\n",
    "y_pred2 = y_pred_fine.argmax(axis=1)\n",
    "target_names = label_final\n",
    "print(classification_report(y_true2, y_pred2, target_names=target_names,digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross test on 2400 Covid data (version 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_2400 = pd.read_csv('Data/test5.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fucking piece of shit your whole community is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im not being funny but coronavirus in china ir...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>got on the victoria line today to seven sister...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it s appalling that the media amp libtards bit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dude fuck the chinese man fuck em and if you t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  attack\n",
       "0   fucking piece of shit your whole community is...       1\n",
       "1  im not being funny but coronavirus in china ir...       0\n",
       "2  got on the victoria line today to seven sister...       1\n",
       "3  it s appalling that the media amp libtards bit...       1\n",
       "4  dude fuck the chinese man fuck em and if you t...       1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_2400.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_2400.rename(columns= {'attack':'label'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_2400 = CustomDataset(sample_2400, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader_2400 = DataLoader(training_2400, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "290it [00:09, 30.88it/s]\n"
     ]
    }
   ],
   "source": [
    "fin_targets_2400 = []\n",
    "fin_outputs_2400 = []\n",
    "with torch.no_grad():                    \n",
    "    for _,data in tqdm(enumerate(test_loader_2400, 0)):\n",
    "        source = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        target = data['targets'].to(device, dtype = torch.float)\n",
    "        y_pred = model(input_ids=source, \n",
    "                       attention_mask=mask)\n",
    "        fin_targets_2400.extend(target.cpu().detach().numpy().tolist())\n",
    "        fin_outputs_2400.extend(torch.sigmoid(y_pred).cpu().detach().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_final_2400 = []\n",
    "for i in fin_outputs_2400:\n",
    "    temp = [0 for i in range(len(label_final))]\n",
    "    index = i.index(max(i))\n",
    "    temp[index] = 1\n",
    "    outputs_final_2400.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_2400_final = []\n",
    "prob_2400 = []\n",
    "for m in outputs_final_2400:\n",
    "    if m[-1] == 1:\n",
    "        label_2400_final.append(0)\n",
    "    elif m[1] == 1:\n",
    "        label_2400_final.append(0)\n",
    "    else:\n",
    "        label_2400_final.append(1)\n",
    "for m in fin_outputs_2400:\n",
    "    prob_2400.append((m[1]+m[2])/sum(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_2400 = sample_2400['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.5434    0.6932    0.6092       678\n",
      "           0     0.8569    0.7593    0.8052      1641\n",
      "\n",
      "    accuracy                         0.7400      2319\n",
      "   macro avg     0.7001    0.7263    0.7072      2319\n",
      "weighted avg     0.7653    0.7400    0.7479      2319\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Text(0, 0.5, 'Attack'), Text(0, 1.5, 'Not attack')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhUAAAH0CAYAAACKFN/EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzde1xUdf7H8fcMoIgoAoKaYaZiSd4otERTSlp3t4uuZdrFRK1MK/OSly01y+pXmZdIzVK8pFuaplRmZYhpSiqK90tqWopiKIOiAsLA/P5wmxWvNM2ZAef13Mc8Hs73nDnf72F3lg/v7/ecY7LZbDYBAAD8RWZ3DwAAAFwbKCoAAIBTUFQAAACnoKgAAABOQVEBAACcgqICAAA4BUUF4ERWq1W9evVScHCwTCaTfvjhB6cct27dunr99dedcqyyLi4uTrGxse4eBgAHUFTgmpeVlaWhQ4fqpptukq+vr0JDQ9W2bVt9/PHHslqtTu3r888/1yeffKKvvvpKGRkZio6OdspxU1NTNXDgQKcc60p++OEHmUwmVaxYUcePHy+xrbCwUDVq1JDJZNLcuXNLfczVq1fLZDLp119/LdX+7733nhYsWPBnhg2gjPB29wAAI6Wnp6t169by9vbWa6+9psjISPn4+CglJUXvvvuumjZtqubNmzutv71796p27dpOKyb+EBIS4tTjXU3NmjX18ccfa9CgQfa2xYsXq1KlSob1WVhYKG9vbwUEBBjWBwBjkVTgmta3b1+dPXtWaWlpeuyxxxQREaHw8HD16NFDGzduVHh4uKRzv9CGDx+u2rVrq0KFCoqIiNAnn3xS4lgmk0lTpkxR9+7dVaVKFYWFhemdd96xb4+JidHIkSO1f/9+mUwm1a1b197+5JNPljjW66+/bt8uSTt27FCHDh1UrVo1Va5cWY0aNdKcOXPs2y+c/jh16pT69OmjkJAQ+fr6KioqSsuWLbNv//XXX2UymfTZZ5/p/vvvl5+fn+rVq1fimFfSu3dvTZs2rUTbRx99pN69e1+073vvvafmzZvL399fNWvWVLdu3ZSRkWEfx5133ilJuvHGG2UymRQTEyPpf9Mc77//vurWrauKFSvqzJkzJaY/zp49q8jISHXq1MneX15enho3bqyuXbuW6lwAuA5FBa5ZFotFS5cu1XPPPXfJv359fHxUuXJlSdJLL72kadOmaeLEidq+fbsef/xxPf7441q+fHmJz7z66qtq27atNm/erCFDhmjYsGFasWKFJGnRokUaPHiw6tatq4yMDKWmppZ6rI888oiCg4OVkpKibdu2afz48QoMDLzs/r169dJ3332nuXPnatOmTWrdurXuu+8+7d69u8R+w4cPV/fu3bV161Y9/PDD6tmzp/bu3XvV8fxRGKxevVqS9Msvv2jlypXq1avXJfd/9913tW3bNi1evFgHDx5Ut27dJElhYWH64osvJEnr169XRkaGFi1aZP/c+vXrlZycrMTERG3ZskW+vr4ljluxYkXNnz9fy5cv16RJkyRJ/fv3V25urj766KOrngcAF7MB16h169bZJNk+//zzK+535swZW4UKFWyTJ08u0d6pUyfbXXfdZX8vyfb888+X2Oemm26yDR8+3P7+lVdesdWvX7/EPu3atbP17t27RNuYMWNsN9xwg/191apVbTNnzrzsGG+44QbbmDFjbDabzbZ3716bJNvXX39dYp/IyEhbz549bTabzXbgwAGbJNu4cePs2wsLC22VK1e2TZ069bL9rFixwibJdujQIVvfvn1tTzzxhM1ms9mGDRtmu//+++0/hzlz5lz2GGlpaTZJtvT0dJvNZrP9+OOPNkm2AwcOlNivR48etoCAANupU6cuam/fvn2JtlmzZtkqVqxoGzlypM3Hx8e2bt26y/YPwH1IKnDNsv33WXkmk+mK++3bt08FBQVq27ZtifZ27dppx44dJdouXH9Ru3Zt/f777395rC+++KKefPJJxcTEaPTo0UpLS7vsvjt37pSki8bbtm3bK47X29tbNWrUKPV4+/TpowULFujYsWOaNWuWnnrqqUvu98MPP6hDhw4KCwtTlSpV1KZNG0nSb7/9dtU+GjVqJH9//6vu16NHD3Xs2FFjxozRmDFj1LJly1KdAwDXoqjANSs8PFxms/miX7SXc2HxYbPZLmqrUKHCRZ8pLi6+4nHNZrO9wPlDYWFhifcjR47Unj179PDDD2v79u264447NGLEiFKN29nj/UOzZs3UuHFjPfLII/L29tY///nPi/Y5ePCg/vnPf6pu3bqaN2+eNmzYoC+//FKSVFBQcNU+/ph+uprTp08rLS1NXl5e2rNnT6k+A8D1KCpwzQoKCtI//vEPTZo0SSdPnrxoe2Fhoc6cOaMGDRqoYsWKWrlyZYntq1at0i233PKXxxEaGqojR46UaLtUElGvXj3169dPCxcu1GuvvaYPPvjgksf7Y0yrVq0q0f7jjz86Zbzn69Onj5YvX65evXrJy8vrou2pqanKy8vTxIkT1bp1a910000XJSF/FDZFRUUOj6Nv377y8vJScnKy5s6dq3nz5jl8LADGoajANW3KlCny8fHRbbfdpk8++UQ7d+7Uvn37NHfuXEVFRWnv3r3y8/NT//79NXLkSC1YsEB79+7Vm2++qS+++EIvvfTSXx5DbGyskpKS9Nlnn2nfvn1666239OOPP9q3nz59Ws8++6ySk5N14MABbdq0Sd9++60iIiIuebz69eurS5cu6tevn7777jvt3r1bL7zwgrZv364hQ4b85fGeLy4uTseOHdPIkSMvuT08PFwmk0njxo3TgQMHlJiYqNdee63EPjfccIPMZrOWLl2qzMzMSxZ4VzJ37lwtWLBA8+bNU9u2bfXmm2+qT58+OnDggMPnBcAYFBW4ptWpU0dpaWnq2LGjRo8erVtvvVXR0dGaNm2ahgwZosaNG0uS3njjDT311FMaMGCAbrnlFs2dO1dz585V+/bt//IYevTooWeffVbPPfecoqKidOjQIfXv39++3dvbW9nZ2erdu7caNWqkDh06qEaNGhdd0nq+6dOnq0OHDnr88cfVrFkzrVmzRkuWLNHNN9/8l8d7Pi8vL1WvXl0+Pj6X3N60aVO9//77+vDDDxUREaF3331XEydOLLFPjRo19H//93966623VKtWLXXs2LHU/e/bt0/9+vXT2LFj7etDBg0apOjoaHXr1u2iaSQA7mWyXTjZCwAA4ACSCgAA4BQUFQAAwCkoKgAAgFNQVAAAAKegqAAAAE5RLh59/tO+E+4eAlAuRNat5u4hAOWCrwt++1WKfM6Q4+ZtmmTIcZ2BpAIAADhFuUgqAAAod0ye93e7550xAAAwBEkFAABGuOCpwZ6ApAIAADgFSQUAAEbwwDUVFBUAABiB6Q8AAADHkFQAAGAED5z+8LwzBgAAhiCpAADACB64poKiAgAAIzD9AQAA4BiSCgAAjMD0BwAAKM+mTJmitLQ0BQQEaNy4cZKkOXPmaOPGjfL29laNGjXUr18/Va5cWZK0ePFiJScny2w2q2fPnmrevLkkaf/+/Zo8ebIKCgoUGRmpnj17ynSVQonpDwAAjGAyG/O6ipiYGL300ksl2po2bapx48bp3XffVa1atbR48WJJUnp6ulJSUjR+/Hi9/PLLSkhIUHFxsSRp2rRp6tOnj+Lj43X06FFt3rz5qn1TVAAAYASTyZjXVURERMjf379EW7NmzeTl5SVJatiwoSwWiyQpNTVV0dHR8vHxUWhoqGrWrKl9+/YpOztbeXl5atiwoUwmk9q2bavU1NSr9k1RAQCAB0lOTrZPcVgsFgUHB9u3BQUFyWKxXNQeHBxsL0SuhDUVAAAYwaBLSpOSkpSUlGR/Hxsbq9jY2FJ9dtGiRfLy8tKdd94pSbLZbJfc73LtV0NRAQBAOfJniojz/fDDD9q4caNGjRplX3AZHBysrKws+z4Wi0VBQUEXtWdlZSkoKOiqfTD9AQCAEdy0puJSNm/erC+++ELDhg1TxYoV7e1RUVFKSUlRYWGhMjMzlZGRoQYNGigwMFCVKlXSnj17ZLPZtGrVKkVFRV39lG2OZhwu9NO+E+4eAlAuRNat5u4hAOWCrwty+kp3jjLkuHk/vnbF7RMnTtTOnTt16tQpBQQE6OGHH9bixYtltVrtCzjDw8P19NNPSzo3JbJixQqZzWbFxcUpMjJSkvTLL79oypQpKigoUPPmzdWrV6+rXlJKUQFcQygqgNJxSVHRdrQhx81bZcxxnYE1FQAAGIFnfwAAADiGpAIAACOYPe/ZHyQVAADAKUgqAAAwggeuqaCoAADACB746HPPK6MAAIAhSCoAADCCB05/eN4ZAwAAQ5BUAABgBA9cU0FRAQCAEZj+AAAAcAxJBQAARvDA6Q+SCgAA4BQkFQAAGMED11RQVAAAYASmPwAAABxDUgEAgBE8cPrD884YAAAYgqQCAAAjsKYCAADAMSQVAAAYwQPXVFBUAABgBA8sKjzvjAEAgCFIKgAAMAILNQEAABxDUgEAgBE8cE0FRQUAAEZg+gMAAMAxJBUAABjBA6c/PO+MAQCAIUgqAAAwggeuqaCoAADAACYPLCqY/gAAAE5BUgEAgAFIKgAAABxEUgEAgBE8L6ggqQAAAM5BUgEAgAE8cU0FRQUAAAbwxKKC6Q8AAOAUJBUAABiApAIAAMBBJBUAABjAE5MKigoAAIzgeTUF0x8AAMA5SCoAADCAJ05/kFQAAACnIKkAAMAAnphUUFQAAGAATywqmP4AAABOQVIBAIABSCoAAAAcRFIBAIARPC+oIKkAAADOQVIBAIABPHFNBUUFAAAG8MSigukPAADgFCQVAAAYgKQCAADAQSQVAAAYwfOCCooKAACMwPQHAACAg0gqAAAwAEkFAACAg0gqAAAwgCcmFRQVAAAYwBOLCqY/AACAU5BUAABgBM8LKkgqAACAc5BUAABgANZUAAAAOIikAgAAA3hiUkFRAQCAATyxqGD6AwAAOAVJBQAARnBTUDFlyhSlpaUpICBA48aNkySdPn1aEyZM0LFjxxQSEqKBAwfK399fkrR48WIlJyfLbDarZ8+eat68uSRp//79mjx5sgoKChQZGamePXteNX0hqQAA4BoSExOjl156qURbYmKimjRpovj4eDVp0kSJiYmSpPT0dKWkpGj8+PF6+eWXlZCQoOLiYknStGnT1KdPH8XHx+vo0aPavHnzVfumqAAAwAAmk8mQ19VERETYU4g/pKamql27dpKkdu3aKTU11d4eHR0tHx8fhYaGqmbNmtq3b5+ys7OVl5enhg0bymQyqW3btvbPXAnTHwAAGKAsLdQ8efKkAgMDJUmBgYHKycmRJFksFoWHh9v3CwoKksVikZeXl4KDg+3twcHBslgsV+2HogIAgHIkKSlJSUlJ9vexsbGKjY116Fg2m+1PtV8NRQWuqLioSKMHxCkwOEQDR4/XlLdeVkb6b5Kk3DOn5VfZX2MmzZUkLflsllYt+0pms1mP9RmsJrfd4c6hAy5xNCNDL/97qLKyjstkMuuhLg/rse49dPLECQ19caCOHD6s62rX1thxE1U1IECFhYV6ddQI7dq1U0VFVt3/QCf1fqqPu08DBjAqqXCkiAgICFB2drYCAwOVnZ2tqlWrSjqXQGRlZdn3s1gsCgoKuqg9KytLQUFBV+2HNRW4omVfztd1YXXt7/sNf0NjJs3VmElzFdX6LkVFx0iSDh/cr3WrvtcbH3yqwa+9p4+nvKPioiL3DBpwIS9vL704dLgSv/pGcz+dr3mffqJf9u3TjOkfqeXtrfTVN8vU8vZWSpj+kSTp++++VUFhgT5P/EqffrZICz+br8OH0918FrjWRUVFaeXKlZKklStXqkWLFvb2lJQUFRYWKjMzUxkZGWrQoIECAwNVqVIl7dmzRzabTatWrVJUVNRV+6GowGVZjv+uLalr1LZDx4u22Ww2pf6YpNvb/U2StGntKt3e9h75+FRQSM3rVOO667V/z05XDxlwuZCQUDWKuEWSVLmyv+rVq6fMzN+1YsVyPdCpkyTpgU6dtCL5XFxtMpmUl5snq9Wqs2fz5e3jI//K/pc9Psovdy3UnDhxokaMGKEjR47omWeeUXJysjp16qStW7eqf//+2rp1qzr993+bYWFhatWqlQYNGqQ33nhDvXv3ltl8rjR48skn9eGHH6p///6qUaOGIiMjr9q3S6Y/Nm3adNFgli1bpr/97W+u6B4O+uSjCera8znl5eVetG3Pjs2qWi1INWvXkSRlZx1T/Zsa27cHBocqOyvTZWMFyoLDh9O1e9cuNWnaTJasLIWEhEo6V3j8scgt9m8dtGLFcsXGtFFefr6GDP23AqpVc+ewYRQ3rdMcMGDAJdtHjRp1yfbOnTurc+fOF7XXr1/ffp+L0nJJUvH5559r+/bt9veJiYnasGGDK7qGgzavX62qAUGqG97oktvXrlxmTymkyyzqKUMrnwGj5Z45o8ED+mvI8JcuupzvfNu3bZWX2azvV/yopd8t18ezZyj90CEXjhQwjkuSiqFDh+rtt9/W448/rs2bN+vw4cMaOnToFT9z/urWjk8Od8UwcZ69O7do07pV2rIhRYUFZ5Wfd0Yfjn1FfYa8qqIiqzamrNDo92bb9w+qHirL8d/t77OzMhUYFOKOoQMuV1hYqEED+uuf996v2HvOFdtBwcE6dixTISGhOnYs077I7Zuvlyi6zZ3y8fFRcHCwmkfeqh07tun6sDB3ngIMUJYuKXUVlyQVVatW1dChQ5WQkKDs7GwNHjxY3t5XrmdiY2P11ltv6a233nLFEHGBLnHPasLHSzRuZqL6DntdjZpGqc+QVyVJOzalqtb1dRVUvYZ9/8jb22rdqu9VWFigY0eP6PfDh1SvYYS7hg+4jM1m0+hRL6tevXp6Iq6nvT3mrrv15X/vWvhlYqLuuqu9JKlmrVpav26dbDabcnNztW3LFt14Yz23jB1wNkOTiieeeEImk0k2m00mk0lWq1W///671q5dK5PJpNmzZ1/9IChz1q36vsTUhyTVvqGeWrSJ1UvPdJOXl5e69xsis5eXm0YIuM6mtI1a8uUXCm/YUA93Preo+fkBg9Tryac1ZNAAJS5aqJq1aund8e9Jkro98phGjfi3One8T7LZ1PFfndXwppvdeQowiCcmFSabo3e4cKGf9p1w9xCAciGyLgv+gNLwdcHkf/3B3xhy3F/G/cOQ4zqDS6Y/1q9fr9zc/11BcObMGa1fv94VXQMA4BYmkzGvsswlRcWCBQvk5+dnf1+5cmUtXLjQFV0DAOAW7rpPhTu5pKi41AxLEXdbBADgmuKSS0rr1aun2bNnq0OHDjKZTPrmm29Urx6rnQEA164yHioYwiVFRa9evfT5559r4sSJstlsatasmbp16+aKrgEAgIu4pKjw9fXVY4895oquAAAoE8r6+gcjuKSoyMnJ0RdffKH09HQVFBTY21955RVXdA8AgMt5YE3hmoWa8fHxql27tjIzM9WlSxeFhISofv36rugaAAC4iEuKilOnTunuu++Wl5eXIiIi1K9fP+3du9cVXQMA4BZms8mQV1nmkumPP57zERgYqLS0NAUGBtofAwwAAK4NLikqOnfurNzcXHXv3l0zZ85Ubm6uevTo4YquAQBwC09cU+GSoqJy5cry8/NTnTp17Iszd+/e7YquAQBwC0+8+sMlaypmzpxZqjYAAFB+GZpU7NmzRz///LNycnK0ZMkSe3tubq6Ki4uN7BoAALfywKDC2KLCarUqPz9fRUVFysvLs7f7+flp0KBBRnYNAABczNCiIiIiQhEREapQoYI6duxYYttPP/2kWrVqGdk9AABuw5oKg6SkpFzUlpiY6IquAQCAixiaVGzatEmbNm2SxWLRjBkz7O35+fny8vIysmsAANzKE5MKQ4uKwMBA1atXTxs2bCjxqPPjx4+rQoUKRnYNAIBbeWBNYWxRUbduXdWtW1dt2rRRenq6Vq9erZ9++kmhoaG6/fbbjewaAAC4mKFFxZEjR5SSkqI1a9bI399f0dHRstlsPJ0UAHDNY/rDyQYOHKibb75Zw4YNU82aNSVJX3/9tZFdAgAANzG0qBg8eLDWrFmjV199Vc2aNVPr1q1ls9mM7BIAgDLBA4MKY4uKli1bqmXLlsrPz1dqaqq+/vprnTx5UtOmTVPLli3VrFkzI7sHAMBtmP4wiK+vr+68807deeedOn36tH766SclJiZSVAAAcA1xSVFxPn9/f91zzz265557XN01AAAu44FBhWvuqAkAAK59Lk8qAADwBKypAAAATuGBNQXTHwAAwDlIKgAAMIAnTn+QVAAAAKcgqQAAwAAeGFSQVAAAAOcgqQAAwACeuKaCogIAAAN4YE3B9AcAAHAOkgoAAAzgidMfJBUAAMApSCoAADCABwYVFBUAABiB6Q8AAAAHkVQAAGAAkgoAAAAHkVQAAGAADwwqKCoAADAC0x8AAAAOIqkAAMAAHhhUkFQAAADnIKkAAMAAnrimgqICAAADeGBNwfQHAABwDpIKAAAMYPbAqIKkAgAAOAVJBQAABvDAoIKkAgAAOAdJBQAABuCSUgAA4BRmz6spmP4AAADOQVIBAIABPHH6g6QCAAA4BUkFAAAG8MCggqICAAAjmOR5VQXTHwAAwClIKgAAMACXlAIAADiIpAIAAAN44iWlFBUAABjAA2sKpj8AAIBzkFQAAGAAswdGFRQVAABcY5YsWaLk5GSZTCaFhYWpX79+Kigo0IQJE3Ts2DGFhIRo4MCB8vf3lyQtXrxYycnJMpvN6tmzp5o3b+5Qv0x/AABgAJPJmNfVWCwWffPNN3rrrbc0btw4FRcXKyUlRYmJiWrSpIni4+PVpEkTJSYmSpLS09OVkpKi8ePH6+WXX1ZCQoKKi4sdOudSFRXbt29XZmamJCk7O1uTJk3SlClTdOLECYc6BQAAxikuLlZBQYGKiopUUFCgwMBApaamql27dpKkdu3aKTU1VZKUmpqq6Oho+fj4KDQ0VDVr1tS+ffsc6rdURUVCQoLM5nO7fvzxxyoqKpLJZNKHH37oUKcAAFzrTCaTIa+rCQoK0v3336++ffvq6aeflp+fn5o1a6aTJ08qMDBQkhQYGKicnBxJ55KN4ODgEp+3WCwOnXOp1lRYLBZVr15dRUVF2rJli6ZMmSJvb2/16dPHoU4BALjWGbVOMykpSUlJSfb3sbGxio2Ntb8/ffq0UlNTNXnyZPn5+Wn8+PFatWrVZY9ns9mcNrZSFRWVKlXSiRMndOjQIV1//fXy9fWV1WqV1Wp12kAAAMDVXVhEXGjbtm0KDQ1V1apVJUm333679uzZo4CAAGVnZyswMFDZ2dn27cHBwcrKyrJ/3mKxKCgoyKGxlWr64+9//7v+/e9/Kz4+Xh06dJAk7d69W7Vr13aoUwAArnVmk8mQ19VUr15de/fu1dmzZ2Wz2bRt2zbVrl1bUVFRWrlypSRp5cqVatGihSQpKipKKSkpKiwsVGZmpjIyMtSgQQOHztlkK2XuceTIEZnNZtWsWdP+3mq1qk6dOg51/Gf8tI8FoUBpRNat5u4hAOWCrwtuqNB19iZDjju/R+RV9/nss8+UkpIiLy8v1a1bV88884zy8/M1YcIEHT9+XNWrV9egQYPsl5QuWrRIK1askNlsVlxcnCIjr97HpZS6qHAnigqgdCgqgNJxRVHRzaCiYl4pigp3ueyPtW/fvqU6wAcffOC0wQAAcK3ggWLnef755105DgAAUM5dtqiIiIhw5TgAALimmD0vqCjdJaWFhYVauHCh1qxZo1OnTmn27NnasmWLMjIy9Pe//93oMQIAgHKgVJeUzp49W4cOHVL//v3tc0RhYWFatmyZoYMDAKC8ctcdNd2pVEnF+vXrFR8fL19fX/sJ/ZXbeAIAcK0r47//DVGqpMLb2/uiJ5bl5OSoSpUqhgwKAACUP6UqKu644w5NmjSpxJNKExISFB0dbejgAAAorzxx+qNURcWjjz6q0NBQDR48WLm5uerfv78CAwPVpUsXo8cHAADKiVKtqfD29lZcXJzi4uLs0x5lvVoCAMCduKT0CjIyMvTTTz/Zn17WqlUr1apVy8ixAQCAcqRU0x+rV6/W0KFD9dtvv8nX11cHDx7UsGHDtHr1aqPHBwBAueSJaypKlVTMmzdP//73v0vcZXPXrl2aNGmS2rRpY9jgAAAor8r2r39jlCqpyMvLU8OGDUu0hYeHKz8/35BBAQCA8qdURcV9992nTz/9VAUFBZKkgoICzZs3T/fdd5+hgwMAoLwym0yGvMqyUj/6/MSJE1q6dKn8/f11+vRpSVK1atX0r3/9y9gRAgCAcoFHnwMAYIAyHioYgkefAwBggLJ+pYYRSn2fil9//VW7du3SqVOnZLPZ7O1du3Y1ZGAAAKB8KVVRkZSUpNmzZ6tp06bavHmzmjdvrq1btyoqKsro8QEAUC55YFBRuqs/vvjiC7300ksaMmSIKlSooCFDhmjQoEHy8vIyenwAAKCcKFVRkZOTo0aNGkk6N0dUXFysyMhIbdy40dDBAQBQXnFJ6WUEBQUpMzNToaGhqlWrljZs2KAqVarI27vUSzIAAPAoZfz3vyFKVRV07NhRhw8fVmhoqB566CGNHz9eVqtVcXFxBg8PAACUF6UqKmJiYuz/joyM1MyZM2W1WlWhQgWjxgUAQLnGJaWl/ZC3t2w2mx555BHNnz/f2WO6SJVKTLMApRHY4jl3DwEoF/I2TXL3EK5J/LYGAMAApboS4hrjiecMAAAMQFIBAIABWFNxgVGjRl32h1JcXGzIgAAAuBaYPa+muHJRcffdd1/xw+3bt3fqYAAAQPl1xaLi/EtJAQBA6XliUsFCTQAA4BQs1AQAwAAs1AQAAE7B9AcAAICDSpVUFBYWauHChVqzZo1OnTql2bNna8uWLcrIyNDf//53o8cIAEC544GzH6VLKmbPnq1Dhw6pf//+9jmisLAwLVu2zNDBAQCA8qNUScX69esVHx8vX19fe1ERFBQki8Vi6OAAACivzB4YVZSqqPD29r7oDpo5OTmqUqWKITGHS5IAACAASURBVIMCAKC888RFi6U65zvuuEOTJk1SZmamJCk7O1sJCQmKjo42dHAAAKD8KFVR8eijjyo0NFSDBw9Wbm6u+vfvr8DAQHXp0sXo8QEAUC6ZTMa8yrJST3/ExcUpLi7OPu3hiTf1AAAAl1eqouL3338v8T4vL8/+7xo1ajh3RAAAXANYqHkZ/fv3v+y2+fPnO20wAACg/CpVUXFh4XDixAktWLBAjRo1MmRQAACUdx4YVDh2xUu1atUUFxenTz75xNnjAQDgmmA2GfMqyxy+jPbIkSM6e/asM8cCAADKsVJNf4waNarE1R5nz57VoUOH9NBDDxk2MAAAyjMWal7G3XffXeK9r6+vbrjhBtWqVcuQQQEAgPLnqkVFcXGxtm/frj59+sjHx8cVYwIAoNzzwKDi6kWF2WzW1q1budkVAAB/QllfVGmEUi3UvPfee/XZZ5/JarUaPR4AAFBOXTGpWL16tdq0aaNvv/1WJ06c0Ndff62qVauW2OeDDz4wdIAAAJRHJnleVHHFomLatGlq06aNnn/+eVeNBwAAlFNXLCpsNpskKSIiwiWDAQDgWuGJayquWFT8ceXHlTRu3NipAwIA4FpAUXGBwsJCTZ061Z5YXMhkMmnSpEmGDAwAAJQvVywqfH19KRoAAHCAJ96KweFnfwAAAJyvVAs1AQDAn+OJayqumFR8/PHHrhoHAAAo50r1QDEAAPDneOCSCooKAACM4ImPPmehJgAAcAqSCgAADMBCTQAAAAeRVAAAYAAPXFJBUQEAgBHMHvjoc6Y/AACAU5BUAABgAE+c/iCpAAAATkFSAQCAATzxklKKCgAADMAdNQEAABxEUgEAgAHcGVScOXNGU6dO1aFDh2QymdS3b19dd911mjBhgo4dO6aQkBANHDhQ/v7+kqTFixcrOTlZZrNZPXv2VPPmzR3ql6ICAIBrzMyZM9W8eXMNHjxYVqtVZ8+e1eLFi9WkSRN16tRJiYmJSkxM1OOPP6709HSlpKRo/Pjxys7O1pgxY/Tee+/JbP7zkxlMfwAAYACzyWTI62pyc3O1a9cu3X333ZIkb29vVa5cWampqWrXrp0kqV27dkpNTZUkpaamKjo6Wj4+PgoNDVXNmjW1b98+h86ZpAIAgGtIZmamqlatqilTpui3335TvXr1FBcXp5MnTyowMFCSFBgYqJycHEmSxWJReHi4/fNBQUGyWCwO9U1RAQCAAYxaU5GUlKSkpCT7+9jYWMXGxtrfFxUV6cCBA+rVq5fCw8M1c+ZMJSYmXvZ4NpvNaWOjqAAAwABGrS+4sIi4UHBwsIKDg+3pwx133KHExEQFBAQoOztbgYGBys7OVtWqVe37Z2Vl2T9vsVgUFBTk0NhYUwEAwDWkWrVqCg4O1pEjRyRJ27Zt0/XXX6+oqCitXLlSkrRy5Uq1aNFCkhQVFaWUlBQVFhYqMzNTGRkZatCggUN9k1QAAGAAkxuvKe3Vq5fi4+NltVoVGhqqfv36yWazacKECUpOTlb16tU1aNAgSVJYWJhatWqlQYMGyWw2q3fv3g5d+SFJJpszJ1MMsv3waXcPASgXWtw33N1DAMqFvE2TDO9j9oZDhhy3R1SYIcd1BpIKAAAM4Hk36aaoAADAEDz7AwAAwEEkFQAAGMDzcgqSCgAA4CQkFQAAGMADl1RQVAAAYAR33qfCXZj+AAAATkFSAQCAATzxr3ZPPGcAAGAAkgoAAAzAmgoAAAAHkVQAAGAAz8spKCoAADAE0x8AAAAOIqkAAMAAnvhXuyeeMwAAMABJBQAABvDENRUUFQAAGMDzSgqmPwAAgJOQVAAAYAAPnP0gqQAAAM5BUgEAgAHMHriqgqICAAADMP0BAADgIJIKAAAMYPLA6Q+SCgAA4BQkFQAAGMAT11RQVAAAYABPvPqD6Q8AAOAUJBUAABjAE6c/SCoAAIBTkFQAAGAAkgoAAAAHkVQAAGAAT7z5FUUFAAAGMHteTcH0BwAAcA6SCgAADOCJ0x8kFQAAwClIKgAAMIAnXlJKUQEAgAGY/gAAAHAQSQUAAAbgklIAAAAHkVQAAGAAT1xTQVGBSyooOKuRLzylwsICFRUVqVW79uoW94x+/WWPPpzwpvLzchVS4zoNePl1+VX2V+bRI3oh7iFdF3aDJKlhRBP1GfiSm88CMMbUVx7TP9o21jHLKUV1eVOS9OaATvpn28YqKCzSgfTjevqVuTp5Os/+mbCagUr7fITemLpUE+cslyT5eHtpwvCH1TYqXMXFxRo9eYkSl292yznB+bj6A/gvH58KGj1+qipV8pPVWqgR/Xvr1patNf39d9TjmQG6pdltWv7NF/pi/sd6pFc/SVKN667XuGmfunnkgPHmfLVWU+ev1PQxT9jblq/drZHvf6miomK93r+jhvT6m0bEf2Hf/s6LD2rZmh0ljjPsyQ46Zjmlpp1ek8lkUlCAn8vOATCCS9ZU7N+//6K2DRs2uKJrOMhkMqlSpXP/B1dktcpqtUom6cih3xTR9FZJUrPbbtfaH5PdOUzALdak/SLLydwSbcvX7lZRUbEkaf22A6pdo5p92/0xTXUg/bh2/nK0xGd6dGylsTOWSZJsNpuyTpwxeORwJZNBr7LMJUXFhx9+qIMHD9rfr169WosWLXJF1/gLioqKNPipR9Sr8z1qFnWHGjZqojp16ys1ZaUkKWVlko5n/m7fP/PoYb349KMaOeAp7dy6yV3DBtzuiY6t9N2anZIkP98KGtzzHr3x4dIS+wT4V5IkvfLsfUr5ZJj+804vhQZVcflYAWdySVExaNAgTZ48Wenp6UpKStKyZcs0YsQIV3SNv8DLy0vjpn2qjz77Rnt3b9fBA/vUb+gofZv4mYb0eUz5ubny9vGRJAUGVdeHn36tdz/6RHH9BmniGy8r98xpN58B4HpDe3dQUVGx5i1NlSSN7Huv3p+brDN5BSX28/Y26/qagfpp835FP/q21m39Vf838F/uGDIMYjaZDHmVZS5ZU1GjRg298MILGjt2rKpXr64RI0aoQoUKV/xMUlKSkpKSJEmPP08B4k6V/auocbMobVqfoo5dn9CosVMknZsK2bh2tSTJp0IF+fz3v9P6DRup5nXX60j6QTW4KcJt4wZc7bH7b9c/2zbWP/rE29taNL5B/4ptrjcGdFJAlUoqLrYpv6BQU+ev0pm8s/oieYskadH3aerRqZW7hg44haFFxeDBg2U6r6o6ffq0iouL9dJL564KePfddy/72djYWMXGxkqSth/mL15XO3kiW97e3qrsX0Vnz+Zra9o6derWQyezLQoIDFJxcbEWzk3Q3x540L6/f5Wq8vLy0tEj6cpIP6gatWq7+SwA17knupEGx8Xqb0++p7z8Qnt7bO+J9n+/3OefOpN7VlPnr5IkLV21XW2jwrUydY9iWt6k3fszXD5uGKdsZwrGMLSoGD58uJGHh4Gys45r0tuvqKi4SLZim6JjYhXVqq2WfP6Jvv1igSTp9jZ36e6/PyBJ2rk1TfNmTpWXl5fMZrOeHviSqlQNcOcpAIaZ/X9xuvO2cFWv5q99347RmKlLNaTn31SxgreWfPCcJGn9tl/V/415VzzOiPcSlfB6D4198UEdzz6tPqPnumL4cBUPrCpMNpvNZnQne/bsUVhYmCpVOrcwKS8vT+np6QoPDy/V50kqgNJpcR+FPFAaeZsmGd7H2l9OGHLcO+pXu/pObuKShZrTp0+Xr6+v/X3FihU1ffp0V3QNAIBbmAz6T1nmkqLCZrOVWFthNptVVFTkiq4BAICLuKSoqFGjhpYuXSrrf2+itHTpUoWGhrqiawAA3MJkMuZVlrnkktKnnnpKM2fO1KJFi2QymdS4cWP16dPHFV0DAOAWZfz3vyFcUlQEBARowIABrugKAAC4iUuKioKCAiUnJys9PV0FBf+7q1y/fv1c0T0AAK7ngVGFS9ZUTJo0SSdOnNCWLVsUEREhi8Viv7wUAABcG1xSVBw9elTdunVTxYoVFRMTo+HDh5d4wBgAANcaT7yk1CXTH15eXpKkypUr6+DBg6pWrZqOHTvmiq4BAHCLsn6lhhFcUlTExsbq9OnT6tq1q9555x3l5+era9eurugaAAC4iEuKiiZNmsjf318RERGaNOncrVEzMzNd0TUAAG7hgUGFa9ZUjBs3rlRtAACg/DI0qTh8+LAOHTqk3NxcrVu3zt6el5enwsLCK3wSAIByzgOjCkOLiiNHjigtLU1nzpzRxo0b7e2+vr7cURMAgGuMoUVFixYt1KJFC+3cuVMREREltu3evdvIrgEAcKuyfvmnEVyypmL27NkXtc2cOdMVXQMA4BY8UMzJ9uzZo59//lk5OTlasmSJvT03N1fFxcVGdg0AAFzM0KLCarUqPz9fRUVFysvLs7f7+flp0KBBRnYNAIBblfFQwRCGFhURERGKiIhQTEyMQkJCjOwKAAC4mUtuflWxYkXNmTPnoqeUvvLKK67oHgAA1/PAqMIlCzXj4+NVu3ZtZWZmqkuXLgoJCVH9+vVd0TUAAG7BA8UMcurUKd19991aunSpfUqElAIAAOMUFxdr+PDhCgoK0vDhw3X69GlNmDBBx44dU0hIiAYOHCh/f39J0uLFi5WcnCyz2ayePXuqefPmDvXpkqTC2/tc7RIYGKi0tDQdOHBAFovFFV0DAOAW7r6kdOnSpapdu7b9fWJiopo0aaL4+Hg1adJEiYmJkqT09HSlpKRo/Pjxevnll5WQkODwFZouKSo6d+6s3Nxcde/eXV999ZWmTp2qHj16uKJrAAA8TlZWltLS0tS+fXt7W2pqqtq1aydJateunVJTU+3t0dHR8vHxUWhoqGrWrKl9+/Y51K9Lpj9uu+02SVKdOnWY9gAAeASjVj8kJSUpKSnJ/j42NlaxsbEl9pk1a5Yef/zxErdzOHnypAIDAyWdmznIycmRJFksFoWHh9v3CwoKcng2wSVFBQAAHsegquJSRcT5Nm7cqICAANWrV087duy46vFsNpvTxkZRAQDANeTnn3/Whg0btGnTJhUUFCgvL0/x8fEKCAhQdna2AgMDlZ2drapVq0qSgoODlZWVZf+8xWJRUFCQQ327ZE1FZmZmqdoAALhWuOuS0kcffVRTp07V5MmTNWDAADVu3Fj9+/dXVFSUVq5cKUlauXKlWrRoIUmKiopSSkqKCgsLlZmZqYyMDDVo0MChc3ZJUTFu3LhStQEAAGN06tRJW7duVf/+/bV161Z16tRJkhQWFqZWrVpp0KBBeuONN9S7d2+ZzY6VB4ZOfxw+fFiHDh1Sbm6u1q1bZ2/Py8tTYWGhkV0DAOBWZeGJorfccotuueUWSVKVKlU0atSoS+7XuXNnde7c+S/3Z2hRceTIEaWlpenMmTPauHGjvd3X11d9+vQxsmsAAOBihhYVLVq0UIsWLbRnzx41bNjQyK4AAChTykBQ4XIuufojODhYY8eO1c8//yyTyaSbbrpJPXv2VHBwsCu6BwDA9TywqnDJQs0pU6YoKipKH374oaZOnaqoqChNmTLFFV0DAAAXcUlRkZOTo7vuukteXl7y8vJSTEyM/U5eAABcizzxKaUuKSqqVq2qVatWqbi4WMXFxVq1apWqVKniiq4BAICLuGRNRd++fZWQkKDZs2fLZDKpYcOG6tu3ryu6BgDALcrCJaWu5pKionr16ho2bJgrugIAoEzwwJrC2KJi4cKFV9z+0EMPGdk9AABwIUOLiooVK17UdvbsWSUnJ+vUqVMUFQCAa5cHRhWGFhX333+//d95eXlaunSpVqxYoejo6BLbAABA+Wf4morTp09ryZIl+vHHH9WuXTu9/fbb8vf3N7pbAADcqqxf/mkEQ4uKOXPmaP369Wrfvr3GjRsnX19fI7sDAKDM4OoPJ1uyZIm8vb21aNEiLV682N5us9lkMpk0e/ZsI7sHAAAuZGhRMX/+fCMPDwBAmeWBQYVr7qgJAACufS65+RUAAB7HA6MKkgoAAOAUJBUAABiAS0oBAIBTeOIlpUx/AAAApyCpAADAAB4YVJBUAAAA5yCpAADACB4YVVBUAABgAE+8+oPpDwAA4BQkFQAAGIBLSgEAABxEUgEAgAE8MKigqAAAwAhMfwAAADiIpAIAAEN4XlRBUgEAAJyCpAIAAAOwpgIAAMBBJBUAABjAA4MKigoAAIzA9AcAAICDSCoAADAATykFAABwEEkFAABG8LyggqICAAAjeGBNwfQHAABwDpIKAAAMwCWlAAAADiKpAADAAJ54SSlFBQAARvC8moLpDwAA4BwkFQAAGMADgwqSCgAA4BwkFQAAGIBLSgEAABxEUgEAgAG4pBQAADgF0x8AAAAOoqgAAABOQVEBAACcgjUVAAAYwBPXVFBUAABgAE+8+oPpDwAA4BQkFQAAGMATpz9IKgAAgFOQVAAAYAAPDCooKgAAMIQHVhVMfwAAAKcgqQAAwABcUgoAAOAgkgoAAAzAJaUAAAAOIqkAAMAAHhhUUFQAAGAID6wqmP4AAABOQVIBAIABuKQUAADAQSQVAAAYwBMvKTXZbDabuweB8ikpKUmxsbHuHgZQ5vFdgadg+gMOS0pKcvcQgHKB7wo8BUUFAABwCooKAADgFBQVcBhzxEDp8F2Bp2ChJgAAcAqSCgAA4BQUFdD69ev18MMP6/Dhw5KkX3/9VWlpafbtO3bs0M8//+zw8bt37/6XxwgY5eGHH9bHH39sf//ll1/qs88+u+Jn1q9fr/T09D/Vz4XfI0eO8YfMzEwNHjzYoc8CRqKogFavXq2bb75Za9askXSuqNi0aZN9+18tKoCyzMfHR+vWrVNOTk6pP5OamvqXiwpHjgGUddxR08Pl5+fr559/1iuvvKJ33nlHnTt31vz581VQUKDdu3erdevW+v7772U2m/Xjjz+qV69eOnPmjBYtWiSr1aoqVaro+eefV7Vq1ZSfn68ZM2bol19+kclk0kMPPaQ77rjD3ldOTo7efvttPfjgg7r11lvdeNbA/5jNZsXGxurrr7/WI488UmLbsWPH9MEHHygnJ0dVq1ZVv379lJWVpQ0bNmjnzp36/PPPNXjwYNWsWdP+mQ0bNlz0/SgoKCjxPerZs+dFx9i+fbuWL18uq9WqGjVq6Pnnn1fFihV14sQJTZs2TZmZmZKkJ598UoGBgfb+fv/9d40bN05PP/20GjRo4JofGnAZFBUebv369WrevLmuu+46+fv76+DBg+ratat++eUX9e7dW5JUUFAgX19fPfDAA5Kk06dP64033pDJZNLy5cv15Zdf6oknntDChQvl5+encePG2ff7w4kTJ/TOO++oW7duatq0qetPFLiCDh06aMiQIerYsWOJ9oSEBLVt21YxMTFKTk7WjBkzNHToUEVFRem2224rUTT/4eabb77k9+Oee+4p8T268BiVK1e2XyUyb948JScn6x//+IdmzpypiIgIDRkyRMXFxcrPz7d/t44cOaKJEyeqX79+qlu3roE/IaB0KCo83Jo1a3TvvfdKkqKjo7VmzRqFhYVd8TMWi0UTJ05Udna2rFarQkNDJUnbtm3TgAED7Pv5+/tLkoqKijRmzBj17t1bERERBp0J4Dg/Pz+1bdtWS5cuVYUKFezte/fu1YsvvihJatu2rf7zn/9c9ViX+35czaFDhzRv3jydOXNG+fn5atasmSRp+/bteu655ySdS1X8/Px0+vRp5eTk6J133tHgwYOv+p0FXIWiwoOdOnVK27dv16FDh2QymVRcXCzp3MK1K5kxY4buu+8+RUVFaceOHVqwYIF9m+kST9Dx8vLSjTfeqM2bN1NUoMy69957NWzYMMXExPyl41zp+3ElkydP1pAhQ1S3bl398MMP2rFjxxX39/PzU3BwsH7++WeKCpQZLNT0YGvXrlW7du00ZcoUTZ48WR988IFCQ0N1/Phx5eXl2ferVKmS8vPz7e9zc3MVFBQkSVq5cqW9vWnTpvr222/t78+f/ujXr5+OHDmixMREI08JcJi/v79atWql5ORke1vDhg2VkpIi6X8LmqVz34nzvyPnu9z348Lv0YXHyM/PV2BgoKxWq3788Ud7e5MmTbRs2TJJUnFxsXJzcyVJ3t7eGjJkiFauXKnVq1f/pXMHnMVr9OjRo909CLjHnDlz1L59+xKLzPLy8nTs2DEdPHhQ3333nfz9/RUREaGFCxfq+++/V1hYmMLDwzV16lStXbtWtWrVUnZ2tmJiYtSwYUOtXbtW8+fPV1JSkkJCQnT99ddr8eLFevDBB9WyZUstXrxYeXl5LChDmbF48WJ17txZklSnTh0lJibqpptu0i233KKGDRtqwYIFWrJkiY4ePaqnn35afn5+qlSpkj755BMlJyerSZMm9qk+SapWrdolvx9VqlQp8T2qU6dOiWOEhIRoypQp2rhxo8LCwmS1WtWyZUvdfPPN+vbbb7Vo0SItX75c4eHh8vX1tU9d3n777UpISFBAQIBq167trh8jIIk7agIAACdh+gMAADgFRQUAAHAKigoAAOAUFBUAAMApKCoAAIBTUFQALjJ58mTNmzdPkrRr1y698MILLun34Ycf1tGjR516zPPPxZWfBVC2cUdN4DzPPvusTpw4IbPZLF9fX0VGRqpXr17y9fV1aj+NGjXSe++9d9X9fvjhBy1fvlxjxoxxav9/GD16tO688061b9/ekOMD8CwkFcAFhg0bpjlz5ujtt9/WL7/8os8///yifYqKitwwMgAo20gqgMsICgpS8+bNdejQIUnnphF69eqlpUuXqqioSJMnT9bGjRs1b948HTt2TNdff72eeuop3XDDDZKkAwcOaOrUqcrIyFBkZGSJ56Ls2LFD77//vqZOnSpJOn78uGbNmqVdu3bJZrOpdevW6tChg6ZNmyar1aru3bvLy8tLs2bNUmFhoT799FP99NNPslqtatGiheLi4uwPwvryyy+1ZMkSmUwmde3a1eHzHz9+vHbt2qWCggLVrVtXTz75ZIlnTOTk5GjMmDHau3evbrzxRj333HMKCQmRJB0+fFgzZszQ/v37VbVqVXXt2lXR0dEX9ZGTk6MpU6Zo9+7dMplMCgsL0+jRo2U28/cOUB7xzQUu4/jx49q0aVOJR0qnpqbqzTff1IQJE7R//3598MEHevrppzVjxgzFxsbqnXfeUWFhoaxWq8aOHas777xTM2bMUKtWrbRu3bpL9lNcXKy3335b1atX1+TJkzV16lS1bt3aXqQ0bNhQc+bM0axZsyRJ//nPf5SRkaGxY8cqPj5eFotFCxculCRt3rxZX331lUaMGKH33ntP27Ztc/j8mzdvrvj4eE2fPl033nij4uPjS2xfvXq1HnzwQSUkJKhu3br27fn5+Xr99dfVpk0bTZ8+XS+88IISEhLsxdn5lixZoqCgIE2fPl3Tpk3TI488csmH0gEoHygqgAuMHTtWcXFxGjVqlCIiIuzPhZCkf/3rX/L391eFChW0fPlyxcbGKjw8XGazWTExMfL29tbevXu1Z88eFRUV6d5775W3t7fuuOMO1a9f/5L97du3TxaLRd27d5evr68qVKhgf3DVhWw2m5YvX64ePXrI399flSpVUufOnbVmzRpJUkpKimJiYlSnTh35+vqqS5cuDv8c7r77blWqVEk+Pj7q0qWLfvvtN/vDrCTp1ltvVUREhHx8fPTII49oz549On78uNLS0hQSEqK77rpLXl5eqlevnm6//XatXbv2oj68vLx04sQJHT9+XN7e3mrUqBFFBVCOMf0BXGDIkCFq2rTpJbcFBwfb/338+HGtXLmyxJNZrVarLBaLTCaTgoKCSvyCrF69+iWPefz4cYWEhMjLy+uqY8vJydHZs2c1fPhwe5vNZrM/tj47O1v16tWzb/tjOuLPKi4u1qeffqq1a9cqJyfHfh45OTny8/OTVPJn4evrK39/f2VnZ+vYsWPau3ev4uLi7NuLiorUtm3bi/p54IEHtGDBAr3++uuSpNjYWHXq1MmhMQNwP4oK4E84v0gIDg5W586dSyQZf9i5c6csFotsNpv9M1lZWSWeCPuH6tWr6/jx4yoqKrpqYVGlShVVqFBB48ePtz9e+3yBgYHKysqyvz9+/Hipz+18q1ev1oYNGzRy5EiFhIQoNzdXPXv2LLHP+f3k5+fr9OnTCgwMVHBwsCIiIjRy5Mir9lOpUiU98cQTeuKJJ3To0CG9+uqrql+/vpo0aeLQuAG4F9MfgIPat2+v77//Xnv37pXNZlN+fr7S0tKUl5enhg0bymw265tvvlFRUZHWrVunffv2XfI4DRo0UGBgoP7zn/8oPz9fBQUF2r17t6Rzj9G2WCyyWq2SJLPZrPbt22vWrFk6efKkJMlisWjz5s2SpFatWumHH35Qenq6zp49qwULFlz1PIqKilRQUGB/Wa1W5eXlydvbW/7+/jp79qw+/fTTiz63adMm7d69W1arVfPmzVN4eLiqV6+u2267TRkZGVq1apWsVqusVqv27dun9PT0i46xceNGHT16VDabTZUqVZLZbGaRJlCOkVQADqpfv7769OmjGTNmKCMjw74WolGjRvL29taLL76oDz/8UPPmzVNkZKRatmx5yeOYzWYNGzZMM2bMUL9+/WQymdS6dWvdfPPNaty4sX3BptlsVkJCgh577DEtXLhQL7/8sk6dOqWgoCDdc889at68uSIjI3Xvvffq1VdfldlsVteuXbV69eornsf06dM1ffp0+/s2bdro6aef1pYtW/TMM8/I399fXbt21bJly0p8rnXr1lqwYIH27NmjevXqqX///pLOpQ8jRozQ7NmzNXv2bNlsNt1www3q0aPHRX1nZGTo/9uxgxOAYRgIgsrbpbgPl+ki3YBSQ+AgCcxUcKDPor13nXNqjFFrrZpzPj0F8BFXd/fbIwCA//NnBAAiRAUAECEqAIAIUQEARIgKACBCoUj1AAAAABZJREFUVAAAEaICAIgQFQBAhKgAACJunP4cLUy+L2IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1440x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test3 double text\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true_2400, label_2400_final, labels=[1,0], digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_true_2400, label_2400_final, labels=[1,0])\n",
    "plt.figure(1, figsize=(20,8))\n",
    "\n",
    "ax= plt.subplot(121)\n",
    "sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "ax.xaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "ax.yaxis.set_ticklabels(['Attack', 'Not attack'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_true_2400, prob_2400)\n",
    "lw = 2\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(fpr_rt_lm, tpr_rt_lm, color='darkorange',\n",
    "         lw=lw, label='2400 data roc curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid()\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune the linear layers on 2400 data(version3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fucking piece of shit your whole community is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im not being funny but coronavirus in china ir...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>got on the victoria line today to seven sister...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it s appalling that the media amp libtards bit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dude fuck the chinese man fuck em and if you t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  label\n",
       "0   fucking piece of shit your whole community is...      1\n",
       "1  im not being funny but coronavirus in china ir...      0\n",
       "2  got on the victoria line today to seven sister...      1\n",
       "3  it s appalling that the media amp libtards bit...      1\n",
       "4  dude fuck the chinese man fuck em and if you t...      1"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_2400.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_calabrated = []\n",
    "for i in sample_2400['label']:\n",
    "    if i == 1:\n",
    "        label_calabrated.append([0,1,1,0])\n",
    "    else:\n",
    "        label_calabrated.append([1,0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_2400['label'] = label_calabrated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (2319, 2)\n",
      "TRAIN Dataset: (1623, 2)\n",
      "Val Dataset: (209, 2)\n",
      "TEST Dataset: (487, 2)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.7\n",
    "val_size = 0.3\n",
    "train_dataset_2400 =sample_2400.sample(frac=train_size,random_state=200)\n",
    "test_dataset_2400 = sample_2400.drop(train_dataset_2400.index).reset_index(drop=True)\n",
    "\n",
    "val_dataset_2400=test_dataset_2400.sample(frac=val_size,random_state=200)\n",
    "test_dataset_2400 =test_dataset_2400.drop(val_dataset_2400.index).reset_index(drop=True)\n",
    "\n",
    "train_dataset_2400 = train_dataset_2400.reset_index(drop=True)\n",
    "val_dataset_2400 = val_dataset_2400.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(sample_2400.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset_2400.shape))\n",
    "print(\"Val Dataset: {}\".format(val_dataset_2400.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset_2400.shape))\n",
    "\n",
    "training_set_2400 = CustomDataset(train_dataset_2400, tokenizer, MAX_LEN)\n",
    "testing_set_2400 = CustomDataset(test_dataset_2400, tokenizer, MAX_LEN)\n",
    "val_set_2400 = CustomDataset(val_dataset_2400, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "val_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "training_loader_2400 = DataLoader(training_set_2400, **train_params)\n",
    "val_loader_2400 = DataLoader(val_set_2400, **val_params)\n",
    "test_loader_2400 = DataLoader(testing_set_2400, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================= Start finetuning ==============================\n",
      "batch_no [0/1000]: training_loss: tensor(0.6921, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [1/1000]: training_loss: tensor(0.7719, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [2/1000]: training_loss: tensor(0.8380, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [3/1000]: training_loss: tensor(0.6809, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [4/1000]: training_loss: tensor(0.7969, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [5/1000]: training_loss: tensor(0.7094, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [6/1000]: training_loss: tensor(0.7403, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [7/1000]: training_loss: tensor(0.7655, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [8/1000]: training_loss: tensor(0.7540, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [9/1000]: training_loss: tensor(0.6419, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [10/1000]: training_loss: tensor(0.8227, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [11/1000]: training_loss: tensor(0.6559, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [12/1000]: training_loss: tensor(0.5691, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [13/1000]: training_loss: tensor(0.8177, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [14/1000]: training_loss: tensor(0.6927, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [15/1000]: training_loss: tensor(0.7736, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [16/1000]: training_loss: tensor(0.6898, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [17/1000]: training_loss: tensor(0.7795, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [18/1000]: training_loss: tensor(0.6679, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [19/1000]: training_loss: tensor(0.8265, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [20/1000]: training_loss: tensor(0.7217, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [21/1000]: training_loss: tensor(0.7786, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [22/1000]: training_loss: tensor(0.6771, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [23/1000]: training_loss: tensor(0.7335, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [24/1000]: training_loss: tensor(0.7382, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [25/1000]: training_loss: tensor(0.6201, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [26/1000]: training_loss: tensor(0.8000, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [27/1000]: training_loss: tensor(0.6849, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [28/1000]: training_loss: tensor(0.7890, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [29/1000]: training_loss: tensor(0.7413, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [30/1000]: training_loss: tensor(0.6722, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [31/1000]: training_loss: tensor(0.6051, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [32/1000]: training_loss: tensor(0.8367, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [33/1000]: training_loss: tensor(0.7549, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [34/1000]: training_loss: tensor(0.8063, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [35/1000]: training_loss: tensor(0.7375, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [36/1000]: training_loss: tensor(0.7327, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [37/1000]: training_loss: tensor(0.7260, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [38/1000]: training_loss: tensor(0.8852, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [39/1000]: training_loss: tensor(0.6794, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [40/1000]: training_loss: tensor(0.8418, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [41/1000]: training_loss: tensor(0.7477, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [42/1000]: training_loss: tensor(0.7773, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [43/1000]: training_loss: tensor(0.7548, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [44/1000]: training_loss: tensor(0.9455, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [45/1000]: training_loss: tensor(0.7345, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [46/1000]: training_loss: tensor(0.6842, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [47/1000]: training_loss: tensor(0.7569, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [48/1000]: training_loss: tensor(0.7181, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [49/1000]: training_loss: tensor(0.7372, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [50/1000]: training_loss: tensor(0.6744, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [51/1000]: training_loss: tensor(0.7434, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [52/1000]: training_loss: tensor(0.7553, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [53/1000]: training_loss: tensor(0.7409, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [54/1000]: training_loss: tensor(0.8392, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [55/1000]: training_loss: tensor(0.6918, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [56/1000]: training_loss: tensor(0.7640, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [57/1000]: training_loss: tensor(0.6414, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [58/1000]: training_loss: tensor(0.7454, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [59/1000]: training_loss: tensor(0.7945, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [60/1000]: training_loss: tensor(0.7624, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [61/1000]: training_loss: tensor(0.8212, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [62/1000]: training_loss: tensor(0.7116, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [63/1000]: training_loss: tensor(0.6750, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [64/1000]: training_loss: tensor(0.7903, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [65/1000]: training_loss: tensor(0.7706, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [66/1000]: training_loss: tensor(0.6744, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [67/1000]: training_loss: tensor(0.7059, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [68/1000]: training_loss: tensor(0.8569, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [69/1000]: training_loss: tensor(0.7930, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [70/1000]: training_loss: tensor(0.7600, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [71/1000]: training_loss: tensor(0.7876, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [72/1000]: training_loss: tensor(0.8407, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [73/1000]: training_loss: tensor(0.6926, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [74/1000]: training_loss: tensor(0.8955, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [75/1000]: training_loss: tensor(0.7511, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [76/1000]: training_loss: tensor(0.6431, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [77/1000]: training_loss: tensor(0.6047, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [78/1000]: training_loss: tensor(0.7239, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [79/1000]: training_loss: tensor(0.7369, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [80/1000]: training_loss: tensor(0.7956, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [81/1000]: training_loss: tensor(0.6740, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [82/1000]: training_loss: tensor(0.7905, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [83/1000]: training_loss: tensor(0.7655, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [84/1000]: training_loss: tensor(0.7613, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [85/1000]: training_loss: tensor(0.7151, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [86/1000]: training_loss: tensor(0.7585, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [87/1000]: training_loss: tensor(0.7083, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [88/1000]: training_loss: tensor(0.6675, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [89/1000]: training_loss: tensor(0.8400, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [90/1000]: training_loss: tensor(0.7839, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [91/1000]: training_loss: tensor(0.6418, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [92/1000]: training_loss: tensor(0.8375, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [93/1000]: training_loss: tensor(0.8253, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [94/1000]: training_loss: tensor(0.7946, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [95/1000]: training_loss: tensor(0.6826, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [96/1000]: training_loss: tensor(0.7625, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [97/1000]: training_loss: tensor(0.7547, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [98/1000]: training_loss: tensor(0.7417, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [99/1000]: training_loss: tensor(0.7709, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [100/1000]: training_loss: tensor(0.6830, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [101/1000]: training_loss: tensor(0.5580, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [0/1000]: training_loss: tensor(0.7050, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [1/1000]: training_loss: tensor(0.7584, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [2/1000]: training_loss: tensor(0.6714, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [3/1000]: training_loss: tensor(0.7372, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [4/1000]: training_loss: tensor(0.7951, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [5/1000]: training_loss: tensor(0.8268, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [6/1000]: training_loss: tensor(0.6823, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [7/1000]: training_loss: tensor(0.7244, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [8/1000]: training_loss: tensor(0.7917, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [9/1000]: training_loss: tensor(0.7646, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [10/1000]: training_loss: tensor(0.7502, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [11/1000]: training_loss: tensor(0.7615, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [12/1000]: training_loss: tensor(0.7685, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [13/1000]: training_loss: tensor(0.7958, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [14/1000]: training_loss: tensor(0.7455, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [15/1000]: training_loss: tensor(0.7070, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [16/1000]: training_loss: tensor(0.6996, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [17/1000]: training_loss: tensor(0.7778, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [18/1000]: training_loss: tensor(0.6273, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [19/1000]: training_loss: tensor(0.7847, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [20/1000]: training_loss: tensor(0.6668, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [21/1000]: training_loss: tensor(0.7792, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [22/1000]: training_loss: tensor(0.7319, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [23/1000]: training_loss: tensor(0.6937, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [24/1000]: training_loss: tensor(0.7836, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [25/1000]: training_loss: tensor(0.6828, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [26/1000]: training_loss: tensor(0.7088, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [27/1000]: training_loss: tensor(0.7362, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [28/1000]: training_loss: tensor(0.7155, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [29/1000]: training_loss: tensor(0.7071, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [30/1000]: training_loss: tensor(0.7672, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [31/1000]: training_loss: tensor(0.7195, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [32/1000]: training_loss: tensor(0.7735, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [33/1000]: training_loss: tensor(0.7821, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [34/1000]: training_loss: tensor(0.7488, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [35/1000]: training_loss: tensor(0.7301, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [36/1000]: training_loss: tensor(0.7814, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [37/1000]: training_loss: tensor(0.6229, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [38/1000]: training_loss: tensor(0.7353, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [39/1000]: training_loss: tensor(0.7324, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [40/1000]: training_loss: tensor(0.9210, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [41/1000]: training_loss: tensor(0.6505, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [42/1000]: training_loss: tensor(0.8200, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [43/1000]: training_loss: tensor(0.8150, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [44/1000]: training_loss: tensor(0.8038, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [45/1000]: training_loss: tensor(0.7015, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [46/1000]: training_loss: tensor(0.7115, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [47/1000]: training_loss: tensor(0.7993, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [48/1000]: training_loss: tensor(0.6702, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [49/1000]: training_loss: tensor(0.7580, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [50/1000]: training_loss: tensor(0.7857, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [51/1000]: training_loss: tensor(0.6025, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [52/1000]: training_loss: tensor(0.8367, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [53/1000]: training_loss: tensor(0.6931, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [54/1000]: training_loss: tensor(0.6825, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [55/1000]: training_loss: tensor(0.7890, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [56/1000]: training_loss: tensor(0.6927, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [57/1000]: training_loss: tensor(0.7482, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [58/1000]: training_loss: tensor(0.6807, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [59/1000]: training_loss: tensor(0.7742, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [60/1000]: training_loss: tensor(0.7003, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [61/1000]: training_loss: tensor(0.7881, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [62/1000]: training_loss: tensor(0.8223, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [63/1000]: training_loss: tensor(0.7028, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [64/1000]: training_loss: tensor(0.8000, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [65/1000]: training_loss: tensor(0.7494, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [66/1000]: training_loss: tensor(0.6915, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [67/1000]: training_loss: tensor(0.7710, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [68/1000]: training_loss: tensor(0.7942, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [69/1000]: training_loss: tensor(0.8262, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [70/1000]: training_loss: tensor(0.7201, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [71/1000]: training_loss: tensor(0.7618, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [72/1000]: training_loss: tensor(0.8550, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [73/1000]: training_loss: tensor(0.8279, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [74/1000]: training_loss: tensor(0.6715, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [75/1000]: training_loss: tensor(0.7694, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [76/1000]: training_loss: tensor(0.7986, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [77/1000]: training_loss: tensor(0.7535, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [78/1000]: training_loss: tensor(0.7293, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [79/1000]: training_loss: tensor(0.7297, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [80/1000]: training_loss: tensor(0.7233, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [81/1000]: training_loss: tensor(0.7746, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [82/1000]: training_loss: tensor(0.6975, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [83/1000]: training_loss: tensor(0.8008, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [84/1000]: training_loss: tensor(0.7538, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [85/1000]: training_loss: tensor(0.6898, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [86/1000]: training_loss: tensor(0.7382, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [87/1000]: training_loss: tensor(0.7025, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [88/1000]: training_loss: tensor(0.7369, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [89/1000]: training_loss: tensor(0.8238, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [90/1000]: training_loss: tensor(0.7022, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [91/1000]: training_loss: tensor(0.6826, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [92/1000]: training_loss: tensor(0.6257, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [93/1000]: training_loss: tensor(0.7161, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [94/1000]: training_loss: tensor(0.7624, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [95/1000]: training_loss: tensor(0.7950, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [96/1000]: training_loss: tensor(0.6812, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [97/1000]: training_loss: tensor(0.6893, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [98/1000]: training_loss: tensor(0.7431, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [99/1000]: training_loss: tensor(0.8474, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [100/1000]: training_loss: tensor(0.7653, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [101/1000]: training_loss: tensor(0.8805, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [0/1000]: training_loss: tensor(0.7785, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [1/1000]: training_loss: tensor(0.7459, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [2/1000]: training_loss: tensor(0.8340, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [3/1000]: training_loss: tensor(0.7416, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [4/1000]: training_loss: tensor(0.7209, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [5/1000]: training_loss: tensor(0.8654, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [6/1000]: training_loss: tensor(0.7702, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [7/1000]: training_loss: tensor(0.7995, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [8/1000]: training_loss: tensor(0.7278, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [9/1000]: training_loss: tensor(0.6753, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [10/1000]: training_loss: tensor(0.7414, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [11/1000]: training_loss: tensor(0.8679, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [12/1000]: training_loss: tensor(0.7204, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [13/1000]: training_loss: tensor(0.7386, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [14/1000]: training_loss: tensor(0.8242, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [15/1000]: training_loss: tensor(0.8800, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [16/1000]: training_loss: tensor(0.9620, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [17/1000]: training_loss: tensor(0.7963, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [18/1000]: training_loss: tensor(0.7389, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [19/1000]: training_loss: tensor(0.6535, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [20/1000]: training_loss: tensor(0.6490, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [21/1000]: training_loss: tensor(0.9369, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [22/1000]: training_loss: tensor(0.6828, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [23/1000]: training_loss: tensor(0.7119, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [24/1000]: training_loss: tensor(0.7244, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [25/1000]: training_loss: tensor(0.7491, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [26/1000]: training_loss: tensor(0.7613, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [27/1000]: training_loss: tensor(0.6689, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [28/1000]: training_loss: tensor(0.7427, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [29/1000]: training_loss: tensor(0.7268, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [30/1000]: training_loss: tensor(0.8970, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [31/1000]: training_loss: tensor(0.6854, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [32/1000]: training_loss: tensor(0.7734, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [33/1000]: training_loss: tensor(0.6786, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [34/1000]: training_loss: tensor(0.8658, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [35/1000]: training_loss: tensor(0.7617, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [36/1000]: training_loss: tensor(0.7107, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [37/1000]: training_loss: tensor(0.7060, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [38/1000]: training_loss: tensor(0.7610, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [39/1000]: training_loss: tensor(0.7779, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [40/1000]: training_loss: tensor(0.6613, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [41/1000]: training_loss: tensor(0.7358, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [42/1000]: training_loss: tensor(0.7143, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [43/1000]: training_loss: tensor(0.6615, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [44/1000]: training_loss: tensor(0.7678, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [45/1000]: training_loss: tensor(0.6636, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [46/1000]: training_loss: tensor(0.7943, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [47/1000]: training_loss: tensor(0.5615, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [48/1000]: training_loss: tensor(0.7205, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [49/1000]: training_loss: tensor(0.7234, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [50/1000]: training_loss: tensor(0.6429, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [51/1000]: training_loss: tensor(0.7213, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [52/1000]: training_loss: tensor(0.7086, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [53/1000]: training_loss: tensor(0.7736, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [54/1000]: training_loss: tensor(0.7207, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [55/1000]: training_loss: tensor(0.6565, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [56/1000]: training_loss: tensor(0.7744, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [57/1000]: training_loss: tensor(0.7872, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [58/1000]: training_loss: tensor(0.6847, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [59/1000]: training_loss: tensor(0.7285, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [60/1000]: training_loss: tensor(0.8887, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [61/1000]: training_loss: tensor(0.7420, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [62/1000]: training_loss: tensor(0.8610, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [63/1000]: training_loss: tensor(0.7305, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [64/1000]: training_loss: tensor(0.7759, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [65/1000]: training_loss: tensor(0.7097, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [66/1000]: training_loss: tensor(0.7591, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [67/1000]: training_loss: tensor(0.6501, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [68/1000]: training_loss: tensor(0.8341, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [69/1000]: training_loss: tensor(0.7135, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [70/1000]: training_loss: tensor(0.7733, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [71/1000]: training_loss: tensor(0.7589, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [72/1000]: training_loss: tensor(0.6903, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [73/1000]: training_loss: tensor(0.6288, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [74/1000]: training_loss: tensor(0.6701, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [75/1000]: training_loss: tensor(0.7545, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [76/1000]: training_loss: tensor(0.6866, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [77/1000]: training_loss: tensor(0.7407, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [78/1000]: training_loss: tensor(0.7619, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [79/1000]: training_loss: tensor(0.6844, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [80/1000]: training_loss: tensor(0.8162, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [81/1000]: training_loss: tensor(0.6787, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [82/1000]: training_loss: tensor(0.7154, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [83/1000]: training_loss: tensor(0.6488, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [84/1000]: training_loss: tensor(0.8805, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [85/1000]: training_loss: tensor(0.7957, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [86/1000]: training_loss: tensor(0.7094, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [87/1000]: training_loss: tensor(0.9003, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [88/1000]: training_loss: tensor(0.7381, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [89/1000]: training_loss: tensor(0.7059, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [90/1000]: training_loss: tensor(0.6534, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [91/1000]: training_loss: tensor(0.6969, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [92/1000]: training_loss: tensor(0.7625, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [93/1000]: training_loss: tensor(0.7437, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [94/1000]: training_loss: tensor(0.7717, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [95/1000]: training_loss: tensor(0.7028, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [96/1000]: training_loss: tensor(0.7872, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [97/1000]: training_loss: tensor(0.6731, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [98/1000]: training_loss: tensor(0.7597, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [99/1000]: training_loss: tensor(0.6926, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [100/1000]: training_loss: tensor(0.6987, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [101/1000]: training_loss: tensor(0.8417, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [0/1000]: training_loss: tensor(0.7978, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [1/1000]: training_loss: tensor(0.7851, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [2/1000]: training_loss: tensor(0.6331, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [3/1000]: training_loss: tensor(0.6878, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [4/1000]: training_loss: tensor(0.8006, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [5/1000]: training_loss: tensor(0.7498, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [6/1000]: training_loss: tensor(0.7362, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [7/1000]: training_loss: tensor(0.7007, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [8/1000]: training_loss: tensor(0.7863, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [9/1000]: training_loss: tensor(0.7375, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [10/1000]: training_loss: tensor(0.6937, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [11/1000]: training_loss: tensor(0.7127, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [12/1000]: training_loss: tensor(0.6886, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [13/1000]: training_loss: tensor(0.6657, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [14/1000]: training_loss: tensor(0.7076, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [15/1000]: training_loss: tensor(0.6749, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [16/1000]: training_loss: tensor(0.6471, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [17/1000]: training_loss: tensor(0.6301, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [18/1000]: training_loss: tensor(0.6906, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [19/1000]: training_loss: tensor(0.6580, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [20/1000]: training_loss: tensor(0.8071, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [21/1000]: training_loss: tensor(0.7529, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [22/1000]: training_loss: tensor(0.8503, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [23/1000]: training_loss: tensor(0.6333, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [24/1000]: training_loss: tensor(0.7189, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [25/1000]: training_loss: tensor(0.9215, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [26/1000]: training_loss: tensor(0.7399, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [27/1000]: training_loss: tensor(0.7461, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [28/1000]: training_loss: tensor(0.5872, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [29/1000]: training_loss: tensor(0.8558, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [30/1000]: training_loss: tensor(0.7380, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [31/1000]: training_loss: tensor(0.8035, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [32/1000]: training_loss: tensor(0.7803, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [33/1000]: training_loss: tensor(0.8425, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [34/1000]: training_loss: tensor(0.7113, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [35/1000]: training_loss: tensor(0.6561, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [36/1000]: training_loss: tensor(0.7358, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [37/1000]: training_loss: tensor(0.6878, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [38/1000]: training_loss: tensor(0.7106, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [39/1000]: training_loss: tensor(0.8311, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [40/1000]: training_loss: tensor(0.7485, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [41/1000]: training_loss: tensor(0.7596, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [42/1000]: training_loss: tensor(0.7573, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [43/1000]: training_loss: tensor(0.6989, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [44/1000]: training_loss: tensor(0.7708, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [45/1000]: training_loss: tensor(0.8642, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [46/1000]: training_loss: tensor(0.8523, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [47/1000]: training_loss: tensor(0.8505, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [48/1000]: training_loss: tensor(0.8233, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [49/1000]: training_loss: tensor(0.6936, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [50/1000]: training_loss: tensor(0.7332, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [51/1000]: training_loss: tensor(0.6502, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [52/1000]: training_loss: tensor(0.6495, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [53/1000]: training_loss: tensor(0.8706, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [54/1000]: training_loss: tensor(0.6354, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [55/1000]: training_loss: tensor(0.6885, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [56/1000]: training_loss: tensor(0.6164, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [57/1000]: training_loss: tensor(0.6628, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [58/1000]: training_loss: tensor(0.8985, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [59/1000]: training_loss: tensor(0.7520, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [60/1000]: training_loss: tensor(0.8802, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [61/1000]: training_loss: tensor(0.6237, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [62/1000]: training_loss: tensor(0.6925, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [63/1000]: training_loss: tensor(0.8134, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [64/1000]: training_loss: tensor(0.7133, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [65/1000]: training_loss: tensor(0.6761, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [66/1000]: training_loss: tensor(0.7481, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [67/1000]: training_loss: tensor(0.7065, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [68/1000]: training_loss: tensor(0.6867, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [69/1000]: training_loss: tensor(0.7869, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [70/1000]: training_loss: tensor(0.6989, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [71/1000]: training_loss: tensor(0.7398, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [72/1000]: training_loss: tensor(0.7687, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [73/1000]: training_loss: tensor(0.7172, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [74/1000]: training_loss: tensor(0.8051, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [75/1000]: training_loss: tensor(0.8034, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [76/1000]: training_loss: tensor(0.6227, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [77/1000]: training_loss: tensor(0.7441, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [78/1000]: training_loss: tensor(0.6970, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [79/1000]: training_loss: tensor(0.7571, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [80/1000]: training_loss: tensor(0.8434, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [81/1000]: training_loss: tensor(0.7084, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [82/1000]: training_loss: tensor(0.6988, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [83/1000]: training_loss: tensor(0.6937, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [84/1000]: training_loss: tensor(0.7946, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [85/1000]: training_loss: tensor(0.8076, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [86/1000]: training_loss: tensor(0.7233, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [87/1000]: training_loss: tensor(0.7494, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [88/1000]: training_loss: tensor(0.7798, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [89/1000]: training_loss: tensor(0.7382, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [90/1000]: training_loss: tensor(0.7410, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [91/1000]: training_loss: tensor(0.7933, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [92/1000]: training_loss: tensor(0.7538, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [93/1000]: training_loss: tensor(0.7074, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [94/1000]: training_loss: tensor(0.7079, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [95/1000]: training_loss: tensor(0.6999, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [96/1000]: training_loss: tensor(0.7860, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [97/1000]: training_loss: tensor(0.7698, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [98/1000]: training_loss: tensor(0.7444, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [99/1000]: training_loss: tensor(0.6978, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [100/1000]: training_loss: tensor(0.6872, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [101/1000]: training_loss: tensor(0.8068, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [0/1000]: training_loss: tensor(0.7130, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [1/1000]: training_loss: tensor(0.8002, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [2/1000]: training_loss: tensor(0.7102, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [3/1000]: training_loss: tensor(0.6586, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [4/1000]: training_loss: tensor(0.7707, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [5/1000]: training_loss: tensor(0.7029, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [6/1000]: training_loss: tensor(0.6621, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [7/1000]: training_loss: tensor(0.7408, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [8/1000]: training_loss: tensor(0.6722, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [9/1000]: training_loss: tensor(0.6868, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [10/1000]: training_loss: tensor(0.6626, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [11/1000]: training_loss: tensor(0.7516, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [12/1000]: training_loss: tensor(0.7211, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [13/1000]: training_loss: tensor(0.7572, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [14/1000]: training_loss: tensor(0.8665, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [15/1000]: training_loss: tensor(0.8058, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [16/1000]: training_loss: tensor(0.7679, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [17/1000]: training_loss: tensor(0.7912, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [18/1000]: training_loss: tensor(0.6700, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [19/1000]: training_loss: tensor(0.7708, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [20/1000]: training_loss: tensor(0.6621, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [21/1000]: training_loss: tensor(0.7450, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [22/1000]: training_loss: tensor(0.8130, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [23/1000]: training_loss: tensor(0.6962, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [24/1000]: training_loss: tensor(0.7281, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [25/1000]: training_loss: tensor(0.8245, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [26/1000]: training_loss: tensor(0.6511, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [27/1000]: training_loss: tensor(0.8402, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [28/1000]: training_loss: tensor(0.6971, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [29/1000]: training_loss: tensor(0.7080, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [30/1000]: training_loss: tensor(0.8260, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [31/1000]: training_loss: tensor(0.6996, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [32/1000]: training_loss: tensor(0.6694, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [33/1000]: training_loss: tensor(0.7635, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [34/1000]: training_loss: tensor(0.8138, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [35/1000]: training_loss: tensor(0.7004, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [36/1000]: training_loss: tensor(0.7738, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [37/1000]: training_loss: tensor(0.7268, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [38/1000]: training_loss: tensor(0.6103, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [39/1000]: training_loss: tensor(0.7433, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [40/1000]: training_loss: tensor(0.7490, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [41/1000]: training_loss: tensor(0.7165, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [42/1000]: training_loss: tensor(0.7554, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [43/1000]: training_loss: tensor(0.7311, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [44/1000]: training_loss: tensor(0.7530, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [45/1000]: training_loss: tensor(0.6709, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [46/1000]: training_loss: tensor(0.7296, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [47/1000]: training_loss: tensor(0.6795, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [48/1000]: training_loss: tensor(0.7925, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [49/1000]: training_loss: tensor(0.6996, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [50/1000]: training_loss: tensor(0.7709, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [51/1000]: training_loss: tensor(0.7589, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [52/1000]: training_loss: tensor(0.6802, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [53/1000]: training_loss: tensor(0.8411, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [54/1000]: training_loss: tensor(0.6480, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [55/1000]: training_loss: tensor(0.7821, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [56/1000]: training_loss: tensor(0.8145, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [57/1000]: training_loss: tensor(0.7232, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [58/1000]: training_loss: tensor(0.6840, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [59/1000]: training_loss: tensor(0.8391, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [60/1000]: training_loss: tensor(0.7783, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [61/1000]: training_loss: tensor(0.8674, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [62/1000]: training_loss: tensor(0.7189, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [63/1000]: training_loss: tensor(0.7135, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [64/1000]: training_loss: tensor(0.6585, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [65/1000]: training_loss: tensor(0.7616, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [66/1000]: training_loss: tensor(0.7393, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [67/1000]: training_loss: tensor(0.7101, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [68/1000]: training_loss: tensor(0.8017, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [69/1000]: training_loss: tensor(0.8691, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [70/1000]: training_loss: tensor(0.7701, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [71/1000]: training_loss: tensor(0.7340, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [72/1000]: training_loss: tensor(0.7061, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [73/1000]: training_loss: tensor(0.6914, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [74/1000]: training_loss: tensor(0.7327, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [75/1000]: training_loss: tensor(0.6240, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [76/1000]: training_loss: tensor(0.7478, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [77/1000]: training_loss: tensor(0.6913, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [78/1000]: training_loss: tensor(0.7237, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [79/1000]: training_loss: tensor(0.7018, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [80/1000]: training_loss: tensor(0.7400, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [81/1000]: training_loss: tensor(0.7539, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [82/1000]: training_loss: tensor(0.8549, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [83/1000]: training_loss: tensor(0.7182, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [84/1000]: training_loss: tensor(0.8394, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [85/1000]: training_loss: tensor(0.6302, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [86/1000]: training_loss: tensor(0.8486, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [87/1000]: training_loss: tensor(0.8028, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [88/1000]: training_loss: tensor(0.7611, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [89/1000]: training_loss: tensor(0.7830, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [90/1000]: training_loss: tensor(0.7410, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [91/1000]: training_loss: tensor(0.7391, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [92/1000]: training_loss: tensor(0.6578, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [93/1000]: training_loss: tensor(0.7024, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [94/1000]: training_loss: tensor(0.7736, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [95/1000]: training_loss: tensor(0.7211, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [96/1000]: training_loss: tensor(0.8160, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [97/1000]: training_loss: tensor(0.6712, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [98/1000]: training_loss: tensor(0.7735, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [99/1000]: training_loss: tensor(0.7266, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [100/1000]: training_loss: tensor(0.6773, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "batch_no [101/1000]: training_loss: tensor(0.6686, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "finetuning done!\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "steps_per_epoch = len(train_dataset)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=steps_per_epoch*1, \n",
    "                                          num_training_steps=steps_per_epoch*NUM_EPOCHS)\n",
    "print(\"======================= Start finetuning ==============================\")\n",
    "model_fintuned_2400 = Finetuning(model=model, \n",
    "      train_iter=training_loader_2400, \n",
    "      valid_iter=val_loader_2400, \n",
    "      optimizer=optimizer, \n",
    "      scheduler=scheduler, \n",
    "      num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [00:02, 26.36it/s]\n"
     ]
    }
   ],
   "source": [
    "fin_targets_2400_fine = []\n",
    "fin_outputs_2400_fine = []\n",
    "with torch.no_grad():                    \n",
    "    for _,data in tqdm(enumerate(test_loader_2400, 0)):\n",
    "        source = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        target = data['targets'].to(device, dtype = torch.float)\n",
    "        y_pred = model_fintuned_2400(input_ids=source, \n",
    "                       attention_mask=mask)\n",
    "        fin_targets_2400_fine.extend(target.cpu().detach().numpy().tolist())\n",
    "        fin_outputs_2400_fine.extend(torch.sigmoid(y_pred).cpu().detach().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_final_fine_2400 = []\n",
    "for i in fin_outputs_2400_fine:\n",
    "    temp = [0 for i in range(len(label_final))]\n",
    "    index = i.index(max(i))\n",
    "    temp[index] = 1\n",
    "    outputs_final_fine_2400.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_2400_final_fine = []\n",
    "prob_2400_fine = []\n",
    "for m in outputs_final_fine_2400:\n",
    "    if m[-1] == 1:\n",
    "        label_2400_final_fine.append(0)\n",
    "    elif m[1] == 1:\n",
    "        label_2400_final_fine.append(0)\n",
    "    else:\n",
    "        label_2400_final_fine.append(1)\n",
    "for m in fin_outputs_2400_fine:\n",
    "    prob_2400_fine.append((m[1]+m[2])/sum(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_2400_final_fine = []\n",
    "for m in fin_targets_2400_fine:\n",
    "    if m[-1] == 1:\n",
    "        true_2400_final_fine.append(0)\n",
    "    elif m[1] == 1:\n",
    "        true_2400_final_fine.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.5412    0.6174    0.5768       149\n",
      "           0     0.8202    0.7692    0.7939       338\n",
      "\n",
      "    accuracy                         0.7228       487\n",
      "   macro avg     0.6807    0.6933    0.6853       487\n",
      "weighted avg     0.7348    0.7228    0.7275       487\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Text(0, 0.5, 'Attack'), Text(0, 1.5, 'Not attack')]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test3 double text\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print('Classification Report:')\n",
    "print(classification_report(true_2400_final_fine, label_2400_final_fine, labels=[1,0], digits=4))\n",
    "\n",
    "cm = confusion_matrix(true_2400_final_fine, label_2400_final_fine, labels=[1,0])\n",
    "plt.figure(1, figsize=(20,8))\n",
    "\n",
    "ax= plt.subplot(121)\n",
    "sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "ax.xaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "ax.yaxis.set_ticklabels(['Attack', 'Not attack'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_true_2400, prob_2400)\n",
    "lw = 2\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(fpr_rt_lm, tpr_rt_lm, color='darkorange',\n",
    "         lw=lw, label='2400 data roc curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid()\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
