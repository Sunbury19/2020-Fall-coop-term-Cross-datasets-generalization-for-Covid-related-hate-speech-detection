{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack Comments Classification\n",
    "2020.09.12\n",
    "\n",
    "\n",
    "Student: Xuanyu Su                                                                 \n",
    "Supervisor: Isar Nejadgholi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3OBmC3UmYaRP"
   },
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aVCytM2-YWWR"
   },
   "outputs": [],
   "source": [
    "source_folder = 'Data'\n",
    "destination_folder = 'Model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "# Preliminaries\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n",
    "\n",
    "# Models\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv('attack_annotated_comments.tsv',sep='\\t',encoding = \"ISO-8859-1\")\n",
    "annotation = pd.read_csv('attack_annotations.tsv',sep='\\t',encoding = \"ISO-8859-1\")\n",
    "worker = pd.read_csv('attack_worker_demographics.tsv',sep='\\t',encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37675</td>\n",
       "      <td>`-NEWLINE_TOKENThis is not ``creative``.  Thos...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44816</td>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKEN:: the term ``stand...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49851</td>\n",
       "      <td>NEWLINE_TOKENNEWLINE_TOKENTrue or false, the s...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89320</td>\n",
       "      <td>Next, maybe you could work on being less cond...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93890</td>\n",
       "      <td>This page will need disambiguation.</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rev_id                                            comment  year  logged_in  \\\n",
       "0   37675  `-NEWLINE_TOKENThis is not ``creative``.  Thos...  2002      False   \n",
       "1   44816  `NEWLINE_TOKENNEWLINE_TOKEN:: the term ``stand...  2002      False   \n",
       "2   49851  NEWLINE_TOKENNEWLINE_TOKENTrue or false, the s...  2002      False   \n",
       "3   89320   Next, maybe you could work on being less cond...  2002       True   \n",
       "4   93890               This page will need disambiguation.   2002       True   \n",
       "\n",
       "        ns  sample  split  \n",
       "0  article  random  train  \n",
       "1  article  random  train  \n",
       "2  article  random  train  \n",
       "3  article  random    dev  \n",
       "4  article  random  train  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>quoting_attack</th>\n",
       "      <th>recipient_attack</th>\n",
       "      <th>third_party_attack</th>\n",
       "      <th>other_attack</th>\n",
       "      <th>attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37675</td>\n",
       "      <td>1362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37675</td>\n",
       "      <td>2408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37675</td>\n",
       "      <td>1493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37675</td>\n",
       "      <td>1439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37675</td>\n",
       "      <td>170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rev_id  worker_id  quoting_attack  recipient_attack  third_party_attack  \\\n",
       "0   37675       1362             0.0               0.0                 0.0   \n",
       "1   37675       2408             0.0               0.0                 0.0   \n",
       "2   37675       1493             0.0               0.0                 0.0   \n",
       "3   37675       1439             0.0               0.0                 0.0   \n",
       "4   37675        170             0.0               0.0                 0.0   \n",
       "\n",
       "   other_attack  attack  \n",
       "0           0.0     0.0  \n",
       "1           0.0     0.0  \n",
       "2           0.0     0.0  \n",
       "3           0.0     0.0  \n",
       "4           0.0     0.0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge comment and annotation tables \n",
    "result = pd.merge(comments, annotation, how='left', on=['rev_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates in comment\n",
    "result = result.drop_duplicates(subset=['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the special words with ''\n",
    "result['comment'] = result['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "result['comment'] = result['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>quoting_attack</th>\n",
       "      <th>recipient_attack</th>\n",
       "      <th>third_party_attack</th>\n",
       "      <th>other_attack</th>\n",
       "      <th>attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1365169</th>\n",
       "      <td>699848324</td>\n",
       "      <td>`   These sources don't exactly exude a sense ...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>1842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1365178</th>\n",
       "      <td>699851288</td>\n",
       "      <td>The Institute for Historical Review is a pee...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>test</td>\n",
       "      <td>337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1365188</th>\n",
       "      <td>699857133</td>\n",
       "      <td>:The way you're trying to describe it in this...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1365197</th>\n",
       "      <td>699891012</td>\n",
       "      <td>== Warning ==  There is clearly a protection...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>blocked</td>\n",
       "      <td>dev</td>\n",
       "      <td>2593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1365207</th>\n",
       "      <td>699897151</td>\n",
       "      <td>Alternate option=== Is there perhaps enough ne...</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>blocked</td>\n",
       "      <td>train</td>\n",
       "      <td>151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rev_id                                            comment  year  \\\n",
       "1365169  699848324  `   These sources don't exactly exude a sense ...  2016   \n",
       "1365178  699851288    The Institute for Historical Review is a pee...  2016   \n",
       "1365188  699857133   :The way you're trying to describe it in this...  2016   \n",
       "1365197  699891012    == Warning ==  There is clearly a protection...  2016   \n",
       "1365207  699897151  Alternate option=== Is there perhaps enough ne...  2016   \n",
       "\n",
       "         logged_in       ns   sample  split  worker_id  quoting_attack  \\\n",
       "1365169       True  article  blocked  train       1842             0.0   \n",
       "1365178       True  article  blocked   test        337             0.0   \n",
       "1365188       True  article  blocked  train        331             0.0   \n",
       "1365197       True     user  blocked    dev       2593             0.0   \n",
       "1365207       True  article  blocked  train        151             0.0   \n",
       "\n",
       "         recipient_attack  third_party_attack  other_attack  attack  \n",
       "1365169               0.0                 0.0           0.0     0.0  \n",
       "1365178               0.0                 0.0           0.0     0.0  \n",
       "1365188               0.0                 0.0           0.0     0.0  \n",
       "1365197               0.0                 0.0           0.0     0.0  \n",
       "1365207               0.0                 0.0           0.0     0.0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492         The references to the Congolese economy se...\n",
       "749             Iraq is not good  ===  ===  USA is bad   \n",
       "758       Perhaps it was possible to have dual citizen...\n",
       "1428    `GOD BLESS OUR FREEDOM FIGHTING TROOPS DEFEATI...\n",
       "1561      == Some thoughts ==  I am going to hit the p...\n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[result['attack'] == 1]['comment'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the irrelevant columns\n",
    "result2 = result.drop(columns = ['rev_id','year','logged_in','ns','sample','worker_id',\n",
    "                                 'quoting_attack','recipient_attack','third_party_attack','other_attack'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a400b23bb0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the comments in the train set and plot hisgram\n",
    "seq_len = [len(i.split()) for i in result2['comment']]\n",
    "pd.Series(seq_len).hist(bins = 50,range=[0,500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train, validation and test\n",
    "Train_comment = result2[result2['split']=='train']\n",
    "dec_comment = result2[result2['split']=='dev']\n",
    "test_comment = result2[result2['split']=='test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train set is 69426\n",
      "length of dev set is 23135\n",
      "length of test set is 23144\n"
     ]
    }
   ],
   "source": [
    "# get the length of each table\n",
    "print('length of train set is',len(Train_comment))\n",
    "print('length of dev set is',len(dec_comment))\n",
    "print('length of test set is',len(test_comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the duplicate column\n",
    "Train_comment = Train_comment.drop(columns = ['split'])\n",
    "dec_comment = dec_comment.drop(columns = ['split'])\n",
    "test_comment = test_comment.drop(columns = ['split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>`- This is not ``creative``.  Those are the di...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>`  :: the term ``standard model`` is itself le...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>True or false, the situation as of March 200...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>This page will need disambiguation.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-  Important note for all sysops: There is a ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              comment  attack\n",
       "0   `- This is not ``creative``.  Those are the di...     0.0\n",
       "10  `  :: the term ``standard model`` is itself le...     0.0\n",
       "19    True or false, the situation as of March 200...     0.0\n",
       "38               This page will need disambiguation.      0.0\n",
       "47   -  Important note for all sysops: There is a ...     0.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_comment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    0.830525\n",
       "1.0    0.169475\n",
       "Name: attack, dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check class distribution\n",
    "Train_comment['attack'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the splited tables into csv style\n",
    "Train_comment.to_csv('Train_comment.csv',index=False,header=True)\n",
    "dec_comment.to_csv('Val_comment.csv',index=False,header=True)\n",
    "test_comment.to_csv('Test_comment.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n",
      "3.1.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nqOJAYCiYlZs"
   },
   "source": [
    "## Define embedding methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fF1DCVrCh6_d"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "fdGccqrAZYjw",
    "outputId": "40c9d8a8-0c04-4759-e05e-bbd78f145d2b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (591 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (541 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1088 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (593 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1057 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (791 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (606 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (874 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (732 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1029 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1032 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (662 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (617 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1886 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (652 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (544 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (903 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (962 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1007 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (616 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (631 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1091 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1061 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1698 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1000 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1045 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (634 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (622 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (577 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1072 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1092 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (794 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (963 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (976 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (838 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (801 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1954 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (964 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (548 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (601 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (626 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (897 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (697 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2000 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (779 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (568 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (682 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (657 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (963 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (614 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1002 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1334 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2219 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2244 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1346 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1100 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (847 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (612 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1285 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (627 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (620 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1107 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2226 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2108 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (559 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1658 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (954 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (986 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (557 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (577 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (594 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1029 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1005 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (942 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2261 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (872 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (669 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1071 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1175 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1175 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1177 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (570 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (686 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (638 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1455 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (913 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (516 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (848 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1490 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1483 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1124 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (912 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1238 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1176 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1198 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2062 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1281 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1074 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1159 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1273 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1069 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (917 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (932 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1011 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (601 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (613 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1098 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1128 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (562 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (884 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (995 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (747 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (806 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1063 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1326 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2526 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1703 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1338 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (782 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (804 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (716 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (905 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1994 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (709 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (515 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1001 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1095 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (894 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (862 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (701 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (723 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1089 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1513 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (794 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (895 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3810 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (545 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1034 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (617 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (636 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (579 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (712 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1872 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (591 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (644 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1196 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (979 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (624 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1492 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1513 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3125 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1494 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1095 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (544 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (839 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3374 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (610 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1860 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (588 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (613 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (864 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1253 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (562 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1693 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (648 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (576 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (891 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1850 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (573 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1685 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2128 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (702 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (582 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (774 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (742 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (933 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (633 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (769 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (646 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (617 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (648 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1040 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2065 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (608 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1304 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2559 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (512 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1054 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (609 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (746 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (862 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2528 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1334 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (599 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1005 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1567 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (511 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (647 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1143 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (649 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (663 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2857 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (810 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1097 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1127 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (629 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1009 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2112 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (647 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1021 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (665 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (515 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (601 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1747 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1036 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1338 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (665 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (770 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1856 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (615 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (973 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (563 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1102 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (900 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (938 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1022 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (664 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (671 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (590 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (621 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (628 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (798 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1043 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2858 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (664 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (972 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (623 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (749 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (966 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (666 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (582 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (572 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (830 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (902 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (848 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1000 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (836 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (732 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (542 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (937 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (579 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1082 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2219 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2223 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (657 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (778 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (622 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9322 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3078 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1398 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1954 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (760 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (920 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1855 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1853 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1855 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (571 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (653 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (929 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1013 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (516 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3624 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (916 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1022 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (552 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (652 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2698 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (794 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (978 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2253 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1747 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1743 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (741 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (541 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1479 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (562 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1749 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1265 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (986 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (615 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (546 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (556 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1197 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (844 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (553 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (565 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1021 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1473 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (817 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (842 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1236 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (588 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1109 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (592 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (675 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (946 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (650 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (562 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (896 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1467 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (972 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (670 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (838 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1284 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1121 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1084 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (640 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (928 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1718 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (750 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1094 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1273 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1351 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (912 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (748 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (637 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (785 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (766 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1671 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (959 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (624 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (861 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (620 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (622 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1218 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1167 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1143 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (530 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (726 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (832 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (870 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (857 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (911 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (911 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1012 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1154 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (873 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (885 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1181 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (904 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (939 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (619 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1867 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2142 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (587 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1018 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2173 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (655 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (846 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1012 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (836 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1156 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1802 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (837 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (907 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (707 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1181 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (849 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (568 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (845 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1012 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (785 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (805 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1016 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (632 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (907 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (878 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3130 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (849 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2102 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (705 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (557 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (563 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (911 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (925 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (547 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (814 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1557 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1666 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1040 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1038 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1936 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (663 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2089 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (820 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (663 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (656 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (796 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1091 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1576 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (518 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (819 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (754 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (548 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (552 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (970 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (819 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1086 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1338 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2496 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4204 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4559 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (905 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (971 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1039 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (886 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1187 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1173 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (663 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (737 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (899 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1648 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1273 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (689 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (897 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (529 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (879 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1033 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (721 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (819 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (692 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1380 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (963 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1526 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1112 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (815 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (805 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (515 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1944 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (923 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2049 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (690 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (875 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1746 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (771 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (640 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1317 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (954 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (673 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (625 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (521 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (819 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1113 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (660 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (943 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (627 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1004 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2097 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1941 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (986 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (668 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (512 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2391 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (925 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1225 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2085 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (949 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (744 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (559 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1085 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (515 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (732 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (635 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1307 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1320 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (983 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1301 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2507 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1735 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1113 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (956 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (880 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1121 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (619 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (526 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1158 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2430 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1748 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (593 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (777 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1041 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (745 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (604 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1650 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (544 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (958 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (683 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (526 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1074 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1070 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (978 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (609 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1648 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (586 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (665 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (669 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1172 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (989 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (650 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1082 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (644 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (871 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (742 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (745 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (570 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (522 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (936 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (530 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1364 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1256 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (956 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (562 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1060 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (953 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (983 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1146 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (783 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1028 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1599 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (890 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1101 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (578 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (857 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1114 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (864 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (565 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (689 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1193 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (878 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1044 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (765 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (937 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (581 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (996 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (821 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1390 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1117 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1178 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (727 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (677 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (586 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (580 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1551 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1500 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (681 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (805 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (816 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (896 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1354 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (599 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1976 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (647 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2252 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1968 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (928 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1111 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1219 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2200 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1479 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (829 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (869 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3333 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2858 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9859 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (758 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (602 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (604 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (607 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (515 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1011 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1637 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (715 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1637 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (771 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4962 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (569 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (693 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (516 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (641 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (624 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (584 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (912 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1727 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2495 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (842 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (929 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1183 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (594 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (583 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3750 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1643 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1988 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1984 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (736 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (603 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (531 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (530 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1165 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (915 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (674 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (653 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1048 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1778 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1609 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1113 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1122 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (580 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (851 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (545 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (734 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (578 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1138 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (876 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (554 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (757 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1683 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (932 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1064 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (698 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (615 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (644 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (624 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (868 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (988 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1864 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (560 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (515 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (895 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3118 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (545 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2152 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (552 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1874 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1631 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (890 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1201 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (681 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3364 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3155 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1976 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2085 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1980 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (511 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (561 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (625 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (982 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (583 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (750 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (553 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1506 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1676 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (959 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1916 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1865 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (529 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (691 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1248 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (989 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (609 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (632 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (533 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1195 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1664 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (931 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (595 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (923 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1490 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (568 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1446 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (557 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (951 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1792 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (961 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (602 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (860 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (782 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (599 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (930 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (692 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (628 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (993 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2224 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2853 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (548 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2275 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2503 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2497 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1874 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2199 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (729 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2205 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (689 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (801 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3750 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1023 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2055 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1876 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (818 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1685 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1107 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (813 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (648 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (963 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (966 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (645 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (664 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (729 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1233 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1094 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (870 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (757 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (705 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2133 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1862 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (518 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1003 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (927 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2071 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (815 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1703 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (553 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (678 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (847 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (587 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (675 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (638 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (886 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (861 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (940 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (737 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (871 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (983 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (581 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1777 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (814 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (744 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3995 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (657 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2207 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1459 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (934 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (948 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (833 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (812 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (960 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (957 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1376 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1089 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1672 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (853 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1452 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1746 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1452 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1191 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (962 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (623 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1072 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (625 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (733 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2193 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (631 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (632 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (950 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (914 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (765 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (696 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1536 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1038 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1065 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (851 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (666 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (518 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2218 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1040 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (676 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2312 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1218 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (930 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1112 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1190 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1099 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (726 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1054 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1758 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1046 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1987 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (795 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (985 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1093 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2331 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (686 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (592 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1421 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (691 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (689 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1432 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (989 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1175 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (826 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (599 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1522 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1268 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1055 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1374 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (559 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1137 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2019 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3985 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (677 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (793 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (746 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (621 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1278 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1117 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (770 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (670 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (824 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (923 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (939 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (760 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2000 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3125 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (599 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (656 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1034 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3153 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2956 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1441 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1051 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (580 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (786 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1068 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (910 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1425 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (886 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (694 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (728 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (644 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (806 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3340 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1376 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (687 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1137 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (754 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (621 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (797 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2090 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1151 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1428 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (751 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (597 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (990 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1078 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (858 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (721 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (790 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (902 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1277 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (529 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1782 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (853 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (512 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1396 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (585 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (524 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (580 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (816 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2307 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (602 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (568 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1058 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (810 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1007 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1008 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4239 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (800 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1107 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1076 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1468 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1389 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (910 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1245 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (950 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (604 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (700 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (884 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1566 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (754 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (973 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1079 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (856 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (847 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2170 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (843 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (879 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (816 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (630 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (977 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (659 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (639 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (688 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (958 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (695 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (622 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (635 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (882 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (680 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1045 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (904 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (511 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (524 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (932 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2223 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (807 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (668 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2129 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (858 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (680 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (736 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (588 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (838 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (836 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (760 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (546 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (515 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2723 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (997 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1106 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (826 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (758 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (577 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (619 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (578 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (748 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (875 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2059 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1792 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (639 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1250 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1482 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (732 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (593 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (624 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (827 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (648 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (980 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1030 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4000 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3333 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1256 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (511 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2494 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1380 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2300 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1294 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2210 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (569 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (615 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1101 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1581 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1397 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1459 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (661 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (700 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (615 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (988 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1004 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (569 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1035 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (712 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (793 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (626 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (578 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (516 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2054 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2041 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2037 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2040 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1088 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (668 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (641 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (726 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1028 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1809 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (560 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (931 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1067 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1124 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (620 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (589 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1177 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (795 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (797 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (875 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (554 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (540 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (811 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (563 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (559 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (615 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (603 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2352 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (724 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (738 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (792 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (614 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (732 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2282 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (749 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (795 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (836 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (786 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (560 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1653 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (839 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (539 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1511 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1474 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (845 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (740 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2127 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (617 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1724 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1000 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1324 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1088 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1203 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (992 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (805 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (705 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (762 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (559 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (529 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (836 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4283 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (620 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (614 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (953 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (652 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (832 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (686 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (567 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (935 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1166 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (818 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (614 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (588 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (728 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1139 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (833 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (555 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (661 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (672 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (985 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2497 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4082 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (835 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (512 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (632 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (836 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (792 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (698 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (913 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (606 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (559 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (899 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1162 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (581 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1065 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (806 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (596 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (919 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1043 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (540 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3333 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3182 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (723 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1934 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1195 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (550 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (511 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1348 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (965 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (965 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (552 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (739 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (973 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1044 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (619 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (777 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1762 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (696 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (522 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1582 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (650 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (607 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (580 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (721 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (550 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (995 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (990 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1555 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (626 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (567 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (603 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (807 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (661 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (833 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1571 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (719 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1770 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1561 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (521 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (805 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (895 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1072 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1555 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1647 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (892 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (727 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (896 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (515 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (567 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (637 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (793 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1090 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (815 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (773 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1617 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (959 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1146 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (547 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1151 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1620 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (543 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1003 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1039 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2003 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (911 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (981 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (781 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1051 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1114 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1473 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (908 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1423 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (953 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2140 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (713 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (969 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2098 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1100 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1565 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (810 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (776 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1023 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (541 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (526 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (565 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (598 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2657 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2657 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1166 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (576 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (849 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1617 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (613 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (622 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (723 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (647 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (583 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1052 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1902 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1030 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (595 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (829 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2079 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (861 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1139 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (565 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (542 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2086 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (698 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1114 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (588 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2677 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (958 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (793 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (897 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1166 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (878 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (711 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (747 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (646 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (898 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (752 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (653 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (792 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (849 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (868 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1669 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (597 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (676 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (679 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (808 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (847 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (918 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (910 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (690 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1826 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (705 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (619 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (841 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2058 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2549 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1638 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (572 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (640 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1025 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (548 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (606 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (653 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (975 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (627 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (511 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (730 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (651 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1820 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (852 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (643 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1133 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (897 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2667 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (788 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (913 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2185 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (563 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (759 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (757 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1042 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (551 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (533 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1370 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (769 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (633 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (822 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1045 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (596 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (958 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (582 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (815 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1553 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (526 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (553 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1876 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1872 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (623 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (903 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1425 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (755 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (939 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (572 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (632 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (732 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (553 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (674 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (526 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (573 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (668 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1036 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (578 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (772 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (952 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1739 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (553 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (729 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (547 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1551 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (777 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2136 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (794 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (619 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (620 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (990 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (987 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (925 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (880 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (576 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (908 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1420 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1312 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (565 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (671 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (830 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1483 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (518 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (709 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2625 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (555 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1481 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (576 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (631 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (696 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (580 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (586 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (828 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (515 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1274 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (716 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1261 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (999 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1573 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (705 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1655 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (772 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1029 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1805 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1161 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (779 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1326 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (531 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1402 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (911 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (672 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (910 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (561 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (582 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (605 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1335 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (610 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (754 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1088 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (676 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (776 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (711 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1545 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (871 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (545 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (599 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (576 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1162 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1012 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (738 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (956 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1001 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (710 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (617 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (776 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (512 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (547 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (650 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1209 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (597 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (663 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (595 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (624 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1002 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1398 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (718 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (585 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1077 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (732 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (596 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (861 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (906 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (551 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (991 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (557 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (518 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2084 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1040 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2343 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (806 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (569 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (678 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (557 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (620 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (645 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (777 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (787 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (703 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (689 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (679 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (623 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (567 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (728 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (771 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1197 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1064 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2730 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2021 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1590 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1605 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (798 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (884 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (943 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (543 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (672 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1040 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (994 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (594 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1197 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1690 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1743 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1602 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (954 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1138 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (934 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (655 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (587 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1180 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (576 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (512 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (958 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1011 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (635 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1046 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (722 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (734 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (621 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (636 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (628 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (682 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (548 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (625 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (751 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (658 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (658 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (553 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (701 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (543 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (616 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1926 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (971 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (943 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (626 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (552 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1373 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2147 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1693 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1133 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (597 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (718 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (829 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (580 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (932 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (714 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (954 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (648 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1789 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (619 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (547 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2066 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2622 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (736 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (848 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (623 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (690 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (801 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (801 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (875 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (948 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (681 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (853 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (587 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (636 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2209 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2649 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (542 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (626 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (571 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (602 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (757 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (687 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1290 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1118 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1492 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (580 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (576 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (585 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (731 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (965 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (713 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (600 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1149 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (665 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (697 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (937 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (584 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (559 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (579 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1308 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (990 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (650 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (699 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (529 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (750 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (958 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1064 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1151 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (607 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (680 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (627 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (552 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (872 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1270 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (994 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (643 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (817 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (571 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (731 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (586 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1085 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (792 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (816 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (826 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (979 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (681 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (991 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1384 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (930 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1041 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (851 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (874 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (661 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (855 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1330 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1065 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1008 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (542 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (615 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (859 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (572 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2203 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (542 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1755 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1037 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (630 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (628 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (710 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (884 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (831 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (855 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1080 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1327 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (918 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (696 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (779 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (591 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (531 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (944 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1092 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (698 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1104 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (625 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (828 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (632 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (967 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (813 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (951 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (569 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (975 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (954 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1113 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2352 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (912 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (657 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2241 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1815 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (832 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (953 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1442 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (949 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2137 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (551 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (949 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2019 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2104 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2173 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (795 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (998 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1149 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (915 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (757 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1175 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1540 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1212 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1097 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (881 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (872 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (727 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1100 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (630 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (556 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1050 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (665 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (753 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (939 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1019 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1392 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (560 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1694 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2408 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (542 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (529 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (614 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1048 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (739 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (634 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (648 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (846 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1752 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (909 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1238 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1222 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (691 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2999 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (511 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1918 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1992 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (996 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1078 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1932 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1359 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (963 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (554 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (579 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1138 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1252 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1004 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (869 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (921 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (556 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (630 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1030 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (578 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (735 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1258 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (980 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2277 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (615 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2850 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (663 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1121 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1630 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (705 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1135 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (940 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (840 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1179 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (555 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (913 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (545 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (984 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (744 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (751 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1794 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (984 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1643 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (871 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (740 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1174 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1481 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1538 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1228 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (680 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (722 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (905 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (976 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (919 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (845 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (945 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (524 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (569 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (912 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1502 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (918 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (753 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (688 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1067 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (840 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1027 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (839 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (579 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (650 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1686 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (570 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (653 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (570 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (635 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1698 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (839 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (665 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (590 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (807 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (844 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1028 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (816 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (798 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (751 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1219 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1976 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (711 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (655 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (877 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (687 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1301 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (746 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (791 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (587 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (715 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (991 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (770 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (591 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (735 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (787 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1670 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1115 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (539 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (782 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1154 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (864 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (627 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (546 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (595 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (585 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1731 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (715 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (849 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1958 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (556 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1062 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1397 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2152 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1129 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (860 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (621 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (859 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1358 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (552 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (579 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (978 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (524 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (653 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1387 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (842 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1733 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (559 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1548 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (774 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1040 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (999 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (561 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (896 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (999 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (577 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (968 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (592 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1250 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1981 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1422 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1061 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (556 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (951 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1858 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (801 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (599 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1418 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (832 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (7829 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1604 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (601 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2238 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2238 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (672 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (810 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (686 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (654 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (682 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (968 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2000 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (882 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (664 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (568 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (905 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (684 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (814 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (559 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (879 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (916 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (681 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1518 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1709 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (809 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (671 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (832 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2187 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1689 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (548 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1113 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (559 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (806 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (572 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (803 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (952 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (798 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (786 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (810 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1070 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2582 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (860 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2174 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1150 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (636 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2238 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (638 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (562 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (919 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (728 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1323 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2858 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1046 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1408 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (999 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (885 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (585 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (719 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (819 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (902 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (884 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (573 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1030 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (530 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (591 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (530 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (555 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (943 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (837 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (888 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1040 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (522 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (526 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (613 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (651 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (660 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (815 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (803 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (751 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (778 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2131 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2422 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1589 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2279 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1668 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (652 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (809 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (714 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1173 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (840 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (672 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (567 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1012 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (847 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1158 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1362 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1135 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (512 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (589 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1272 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3989 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (654 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (707 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (640 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1791 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (841 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (555 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (623 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (777 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (662 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (7326 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (579 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (969 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (590 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (796 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (628 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1127 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (969 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (572 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (762 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1117 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1212 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1964 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (711 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (565 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (675 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (670 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (754 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (590 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (676 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (639 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (649 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1033 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2656 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3400 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3860 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (988 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9952 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1085 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (948 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (555 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1784 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (650 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (620 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1326 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (693 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (752 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (594 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (911 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1452 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (827 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1363 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (956 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (739 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (568 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1111 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (772 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (535 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (609 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (698 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (741 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (826 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (524 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (670 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (925 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (779 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (577 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (583 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (646 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (643 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1136 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (581 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2136 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (648 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (886 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (579 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (620 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (627 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (685 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (883 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (759 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1555 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (862 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (850 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (634 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3043 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1743 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1352 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (666 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (632 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (642 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (845 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (686 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1722 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1289 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (785 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (929 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (981 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1589 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (699 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (516 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1211 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (687 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1566 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (774 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1547 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (952 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1141 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (878 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1124 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1200 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1665 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1514 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1037 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (559 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (880 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2040 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (978 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (581 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (756 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (548 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (700 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1388 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (668 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1759 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (622 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (587 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (945 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (750 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (731 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (713 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1163 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (776 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1668 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (783 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (789 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (826 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (873 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (974 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (607 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (683 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1376 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (836 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1124 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1386 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (624 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (828 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1067 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (615 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (800 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (840 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (584 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (726 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (581 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1134 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1818 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1630 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (938 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1155 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1787 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (794 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (873 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (529 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (675 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (614 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (908 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (937 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (946 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (622 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (717 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (893 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (698 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (565 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (956 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (742 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1952 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1033 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (533 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (674 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (941 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (533 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (949 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (740 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (556 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (735 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (933 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1945 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1043 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2085 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1869 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (539 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (637 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1029 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (515 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (614 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (766 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (927 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (594 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (554 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1130 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (827 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1072 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (965 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (755 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (855 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1053 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (897 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1672 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (771 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4996 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (890 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (612 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (555 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2083 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (627 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (711 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (800 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1056 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (824 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (551 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (639 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1098 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1705 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2310 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (622 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (835 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (975 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (586 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (660 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2030 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (581 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (609 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1606 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1939 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1142 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (745 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (541 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (767 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2982 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1094 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (695 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (576 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (740 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1083 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2068 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1038 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (784 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1365 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (976 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (596 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (853 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (665 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1058 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (664 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (747 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (710 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (723 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (814 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (661 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3336 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1016 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2245 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (772 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (665 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5799 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1067 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (736 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (882 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2032 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (546 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1034 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2063 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1094 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (550 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1573 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (546 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (961 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (721 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (804 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (748 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (968 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (627 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (696 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (859 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (518 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (682 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (665 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1805 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2101 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (624 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (588 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (902 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1762 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (609 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (570 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (532 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (985 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (911 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (638 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (833 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (595 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (836 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1317 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3989 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (592 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (896 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (606 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2389 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (834 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (979 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (758 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (748 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (637 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1745 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (821 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (832 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (927 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (994 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1972 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1632 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (908 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (873 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (675 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (633 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (946 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1947 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (795 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (515 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (637 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (607 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1275 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (784 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (978 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1011 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1870 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (837 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (791 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2821 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (666 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1050 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (547 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (579 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2405 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1237 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1920 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (580 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (518 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1489 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (602 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (628 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (540 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2106 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1537 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (959 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1199 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1049 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1084 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1863 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (629 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1082 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (778 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2860 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2458 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (864 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (578 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (576 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4961 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (533 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1726 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (887 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (586 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (522 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1044 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1498 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1121 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (591 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (561 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (974 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1326 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (901 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (947 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2692 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (687 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (700 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2086 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (561 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1167 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (812 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (583 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (890 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (894 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1039 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1861 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (581 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (862 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1064 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1104 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (823 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1102 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (563 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (646 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (690 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (652 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (920 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (899 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (757 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (851 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (849 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1043 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (658 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (979 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (621 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (787 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2818 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1471 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (552 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (524 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (922 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1072 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (792 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (622 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (582 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (699 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (679 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1531 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (702 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (522 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (693 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2391 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (834 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3750 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1564 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (552 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (598 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1369 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (935 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (613 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1029 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (644 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (637 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4546 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4001 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (545 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1304 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (593 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1061 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1090 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (548 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (814 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (880 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (689 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1676 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (839 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (683 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2500 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2394 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2399 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2164 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2138 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1045 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1316 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (961 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (923 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (642 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1045 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (915 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (586 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (639 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (821 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (714 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (552 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4286 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (556 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (713 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1900 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1116 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (844 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2160 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1071 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (689 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1029 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1163 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1163 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (840 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (786 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1493 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (594 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1711 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (606 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (640 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1479 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1510 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1951 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2353 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2139 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2053 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1817 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (769 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (941 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1582 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1607 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (860 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (592 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (965 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (692 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (723 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (531 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2297 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (658 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (902 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (511 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1878 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (678 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (892 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (899 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1370 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (770 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1101 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2023 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (956 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (789 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2023 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (683 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1470 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1310 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1199 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (855 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1129 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5771 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (631 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (555 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (584 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (657 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (960 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (850 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (608 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (624 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (531 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (766 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (666 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (593 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1460 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1019 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1040 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (995 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1060 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1127 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (969 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (588 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1045 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (752 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (884 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (826 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1066 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (818 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2653 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (530 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (583 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (624 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1235 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (744 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (608 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1108 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (790 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (942 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1891 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (789 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (884 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (871 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (857 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2129 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1177 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1184 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (665 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (583 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1424 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (626 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1912 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1388 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (858 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (573 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2665 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (661 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1088 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (529 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (864 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (850 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (741 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1433 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (833 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (722 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (650 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (634 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (844 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (548 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (971 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (657 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1479 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (963 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (744 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (946 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (596 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (656 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (646 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (678 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (911 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (642 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1325 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1289 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (577 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (829 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (657 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (543 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1463 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (825 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1298 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (776 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1192 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (577 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (704 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (711 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (889 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1117 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (586 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (875 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1384 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (775 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (625 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1237 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (589 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1338 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (562 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (672 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (889 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1863 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1130 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (979 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (529 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (572 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (753 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (642 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (803 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (738 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1039 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (553 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (874 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (872 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1170 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1727 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (651 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (560 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1409 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (656 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (554 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (526 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (975 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (895 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2083 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1045 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1430 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (774 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (580 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (670 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (875 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1196 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (731 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (780 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (645 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1468 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1818 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (743 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (753 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1321 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (948 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (585 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1259 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (766 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (515 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (825 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (686 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (913 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1494 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (585 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (817 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (597 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (619 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (998 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (847 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (521 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1223 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (602 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (793 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (620 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# the maximum length of sentences\n",
    "MAX_SEQ_LEN = 128\n",
    "\n",
    "# pad the sentences with less words than 128\n",
    "PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n",
    "\n",
    "# Fields\n",
    "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
    "text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n",
    "                   fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\n",
    "fields = [('comment', text_field),('attack', label_field)]\n",
    "\n",
    "# TabularDataset\n",
    "train, valid, test = TabularDataset.splits(path=source_folder, train='Train_comment.csv', validation='Val_comment.csv',\n",
    "                                           test='Test_comment.csv', format='CSV', fields=fields, skip_header=True)\n",
    "\n",
    "\n",
    "train_iter = BucketIterator(train, batch_size=16, sort_key=lambda x: len(x.comment),\n",
    "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
    "valid_iter = BucketIterator(valid, batch_size=16, sort_key=lambda x: len(x.comment),\n",
    "                            device=device, train=True, sort=True, sort_within_batch=True)\n",
    "test_iter = Iterator(test, batch_size=16, device=device, train=False, shuffle=False, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IX-lWIMaYnsA"
   },
   "source": [
    "## Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2RkcXCHSph1_"
   },
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "\n",
    "        options_name = \"bert-base-uncased\"\n",
    "        self.encoder = BertForSequenceClassification.from_pretrained(options_name)\n",
    "\n",
    "    def forward(self, text, label):\n",
    "        loss, text_fea = self.encoder(text, labels=label)[:2]\n",
    "        \n",
    "\n",
    "        return loss, text_fea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z81slSELYqO1"
   },
   "source": [
    "## Model Training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JRpTJUGhklDv"
   },
   "outputs": [],
   "source": [
    "# Save and Load Functions\n",
    "\n",
    "def save_checkpoint(save_path, model, valid_loss):\n",
    "    if save_path == None:\n",
    "        return\n",
    "\n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'valid_loss': valid_loss}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "    \n",
    "    \n",
    "def load_checkpoint(load_path, model):\n",
    "    if load_path==None:\n",
    "        return\n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    return state_dict['valid_loss']\n",
    "\n",
    "\n",
    "\n",
    "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
    "    if save_path == None:\n",
    "        return\n",
    "    state_dict = {'train_loss_list': train_loss_list,\n",
    "                  'valid_loss_list': valid_loss_list,\n",
    "                  'global_steps_list': global_steps_list}\n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "    \n",
    "def load_metrics(load_path):\n",
    "    if load_path==None:\n",
    "        return\n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "81w1lahhkozO"
   },
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train(model,\n",
    "          optimizer,\n",
    "          criterion = nn.BCELoss(),\n",
    "          train_loader = train_iter,\n",
    "          valid_loader = valid_iter,\n",
    "          num_epochs = 5,\n",
    "          eval_every = len(train_iter) // 2,\n",
    "          file_path = destination_folder,\n",
    "          best_valid_loss = float(\"Inf\")):\n",
    "    \n",
    "    # initialize running values\n",
    "    running_loss = 0.0\n",
    "    valid_running_loss = 0.0\n",
    "    global_step = 0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    global_steps_list = []\n",
    "\n",
    "    # training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        batch_no = 1\n",
    "        for (comment, attack), _ in train_loader:\n",
    "            attack = attack.type(torch.LongTensor)           \n",
    "            attack = attack.to(device)\n",
    "            comment = comment.type(torch.LongTensor)  \n",
    "            comment = comment.to(device)\n",
    "            output = model(comment, attack)\n",
    "            loss, _ = output\n",
    "            print('batch_no [{}/{}]:'.format(batch_no, int(len(Train_comment)/16)),'training_loss:',loss)\n",
    "            batch_no+=1\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update running values\n",
    "            running_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            # evaluation step\n",
    "            if global_step % eval_every == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():                    \n",
    "\n",
    "                # validation loop\n",
    "                for (comment, attack), _ in valid_loader:\n",
    "                    attack = attack.type(torch.LongTensor)           \n",
    "                    attack = attack.to(device)\n",
    "                    comment = comment.type(torch.LongTensor)  \n",
    "                    comment = comment.to(device)\n",
    "                    output = model(comment, attack)\n",
    "                    loss, _ = output\n",
    "\n",
    "                    valid_running_loss += loss.item()\n",
    "\n",
    "                # evaluation\n",
    "                average_train_loss = running_loss / eval_every\n",
    "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
    "                train_loss_list.append(average_train_loss)\n",
    "                valid_loss_list.append(average_valid_loss)\n",
    "                global_steps_list.append(global_step)\n",
    "\n",
    "                # resetting running values\n",
    "                running_loss = 0.0                \n",
    "                valid_running_loss = 0.0\n",
    "                model.train()\n",
    "\n",
    "                # print progress\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
    "                              average_train_loss, average_valid_loss))\n",
    "                \n",
    "                # checkpoint\n",
    "                if best_valid_loss > average_valid_loss:\n",
    "                    best_valid_loss = average_valid_loss\n",
    "                    save_checkpoint(file_path + '/' + 'model.pt', model, best_valid_loss)\n",
    "                    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    \n",
    "    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    print('Finished Training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523,
     "referenced_widgets": [
      "78f713ef2a9b4d46a6f359dd151c8904",
      "db05a76085614d789d15b245fa83e73b",
      "0ff4905c17974a6092918a8d77b9d100",
      "07a9cf8752b743679a39fd412702301d",
      "8f673376fffb43a59f88fda6509e002a",
      "7cddb6e6e8c948d981adb8697a03ad9a",
      "23c685a4d97447ca95727f733d2c2c9d",
      "03029ccf12164dafbe4fd9bb6e3722cc",
      "7128ff90beac44faa7f71b36b1e56099",
      "8672b461409d421facbada337e8f273d",
      "1ffc393f193a4c3f8390dc8307df77fb",
      "af544453c65043b2a7f6fdb21d06a940",
      "e4a0002383884d32a62cb26f0db5276d",
      "7a01574d4d574cb9ac60e3d688125fae",
      "4f5be3cee49247c8973561ced8e5181d",
      "4a83eca6a3e84bb980085bd7ced5ced5"
     ]
    },
    "colab_type": "code",
    "id": "nHdi_cyEvC9K",
    "outputId": "da4003b8-bc21-4df9-94a8-63209dd1ec7d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1/4339]: training_loss: tensor(0.5370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2/4339]: training_loss: tensor(0.6460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3/4339]: training_loss: tensor(0.5828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4/4339]: training_loss: tensor(0.4385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [5/4339]: training_loss: tensor(0.4487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [6/4339]: training_loss: tensor(0.5640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [7/4339]: training_loss: tensor(0.4016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [8/4339]: training_loss: tensor(0.1764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [9/4339]: training_loss: tensor(0.3914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [10/4339]: training_loss: tensor(0.2528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [11/4339]: training_loss: tensor(0.2362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [12/4339]: training_loss: tensor(0.3853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [13/4339]: training_loss: tensor(0.2261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [14/4339]: training_loss: tensor(0.4037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [15/4339]: training_loss: tensor(0.2440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [16/4339]: training_loss: tensor(0.5413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [17/4339]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [18/4339]: training_loss: tensor(0.5467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [19/4339]: training_loss: tensor(0.9927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [20/4339]: training_loss: tensor(0.4117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [21/4339]: training_loss: tensor(0.4544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [22/4339]: training_loss: tensor(0.5441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [23/4339]: training_loss: tensor(0.6855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [24/4339]: training_loss: tensor(0.5710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [25/4339]: training_loss: tensor(0.4589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [26/4339]: training_loss: tensor(0.5063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [27/4339]: training_loss: tensor(0.5728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [28/4339]: training_loss: tensor(0.4163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [29/4339]: training_loss: tensor(0.4731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [30/4339]: training_loss: tensor(0.4660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [31/4339]: training_loss: tensor(0.5225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [32/4339]: training_loss: tensor(0.2587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [33/4339]: training_loss: tensor(0.3821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [34/4339]: training_loss: tensor(0.9194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [35/4339]: training_loss: tensor(0.5194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [36/4339]: training_loss: tensor(0.4653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [37/4339]: training_loss: tensor(0.3980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [38/4339]: training_loss: tensor(0.2971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [39/4339]: training_loss: tensor(0.6894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [40/4339]: training_loss: tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [41/4339]: training_loss: tensor(0.5835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [42/4339]: training_loss: tensor(0.5676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [43/4339]: training_loss: tensor(0.3589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [44/4339]: training_loss: tensor(0.4358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [45/4339]: training_loss: tensor(0.5464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [46/4339]: training_loss: tensor(0.3915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [47/4339]: training_loss: tensor(0.3279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [48/4339]: training_loss: tensor(0.2159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [49/4339]: training_loss: tensor(0.4704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [50/4339]: training_loss: tensor(0.6097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [51/4339]: training_loss: tensor(0.5600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [52/4339]: training_loss: tensor(0.3553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [53/4339]: training_loss: tensor(0.4763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [54/4339]: training_loss: tensor(0.5945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [55/4339]: training_loss: tensor(0.5818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [56/4339]: training_loss: tensor(0.7315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [57/4339]: training_loss: tensor(0.4122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [58/4339]: training_loss: tensor(0.7848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [59/4339]: training_loss: tensor(0.6400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [60/4339]: training_loss: tensor(0.4389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [61/4339]: training_loss: tensor(0.5497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [62/4339]: training_loss: tensor(0.4931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [63/4339]: training_loss: tensor(0.6547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [64/4339]: training_loss: tensor(0.5615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [65/4339]: training_loss: tensor(0.5664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [66/4339]: training_loss: tensor(0.3901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [67/4339]: training_loss: tensor(0.4457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [68/4339]: training_loss: tensor(0.6467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [69/4339]: training_loss: tensor(0.4900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [70/4339]: training_loss: tensor(0.5728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [71/4339]: training_loss: tensor(0.3515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [72/4339]: training_loss: tensor(0.3152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [73/4339]: training_loss: tensor(0.3536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [74/4339]: training_loss: tensor(0.1709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [75/4339]: training_loss: tensor(0.3752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [76/4339]: training_loss: tensor(0.8273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [77/4339]: training_loss: tensor(0.8516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [78/4339]: training_loss: tensor(0.2942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [79/4339]: training_loss: tensor(0.3949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [80/4339]: training_loss: tensor(0.5033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [81/4339]: training_loss: tensor(0.6524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [82/4339]: training_loss: tensor(0.6115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [83/4339]: training_loss: tensor(0.5032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [84/4339]: training_loss: tensor(0.6128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [85/4339]: training_loss: tensor(0.7364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [86/4339]: training_loss: tensor(0.5398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [87/4339]: training_loss: tensor(0.5554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [88/4339]: training_loss: tensor(0.5867, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [89/4339]: training_loss: tensor(0.4129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [90/4339]: training_loss: tensor(0.5085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [91/4339]: training_loss: tensor(0.6259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [92/4339]: training_loss: tensor(0.3924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [93/4339]: training_loss: tensor(0.4616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [94/4339]: training_loss: tensor(0.3791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [95/4339]: training_loss: tensor(0.3844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [96/4339]: training_loss: tensor(0.3285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [97/4339]: training_loss: tensor(0.7757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [98/4339]: training_loss: tensor(0.4017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [99/4339]: training_loss: tensor(0.8347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [100/4339]: training_loss: tensor(0.6487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [101/4339]: training_loss: tensor(0.7058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [102/4339]: training_loss: tensor(0.6138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [103/4339]: training_loss: tensor(0.5277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [104/4339]: training_loss: tensor(0.2863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [105/4339]: training_loss: tensor(0.5603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [106/4339]: training_loss: tensor(0.3435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [107/4339]: training_loss: tensor(0.4870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [108/4339]: training_loss: tensor(0.4483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [109/4339]: training_loss: tensor(0.4982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [110/4339]: training_loss: tensor(0.5597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [111/4339]: training_loss: tensor(0.4609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [112/4339]: training_loss: tensor(0.3772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [113/4339]: training_loss: tensor(0.6596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [114/4339]: training_loss: tensor(0.7606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [115/4339]: training_loss: tensor(0.3227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [116/4339]: training_loss: tensor(0.3045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [117/4339]: training_loss: tensor(0.3939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [118/4339]: training_loss: tensor(0.5227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [119/4339]: training_loss: tensor(0.4737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [120/4339]: training_loss: tensor(0.6009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [121/4339]: training_loss: tensor(0.3940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [122/4339]: training_loss: tensor(0.3973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [123/4339]: training_loss: tensor(0.5621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [124/4339]: training_loss: tensor(0.6003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [125/4339]: training_loss: tensor(0.6100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [126/4339]: training_loss: tensor(0.6656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [127/4339]: training_loss: tensor(0.5483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [128/4339]: training_loss: tensor(0.7585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [129/4339]: training_loss: tensor(0.4569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [130/4339]: training_loss: tensor(0.4151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [131/4339]: training_loss: tensor(0.4053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [132/4339]: training_loss: tensor(0.7170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [133/4339]: training_loss: tensor(0.3102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [134/4339]: training_loss: tensor(0.6571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [135/4339]: training_loss: tensor(0.4358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [136/4339]: training_loss: tensor(0.6406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [137/4339]: training_loss: tensor(0.5618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [138/4339]: training_loss: tensor(0.5010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [139/4339]: training_loss: tensor(0.5562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [140/4339]: training_loss: tensor(0.4163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [141/4339]: training_loss: tensor(0.4262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [142/4339]: training_loss: tensor(0.6678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [143/4339]: training_loss: tensor(0.4669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [144/4339]: training_loss: tensor(0.5106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [145/4339]: training_loss: tensor(0.4741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [146/4339]: training_loss: tensor(0.3255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [147/4339]: training_loss: tensor(0.3606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [148/4339]: training_loss: tensor(0.4729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [149/4339]: training_loss: tensor(0.6343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [150/4339]: training_loss: tensor(0.5943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [151/4339]: training_loss: tensor(0.5087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [152/4339]: training_loss: tensor(0.7787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [153/4339]: training_loss: tensor(0.5705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [154/4339]: training_loss: tensor(0.5093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [155/4339]: training_loss: tensor(0.7007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [156/4339]: training_loss: tensor(0.4080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [157/4339]: training_loss: tensor(0.4821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [158/4339]: training_loss: tensor(0.3949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [159/4339]: training_loss: tensor(0.5008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [160/4339]: training_loss: tensor(0.4920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [161/4339]: training_loss: tensor(0.4111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [162/4339]: training_loss: tensor(0.4673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [163/4339]: training_loss: tensor(0.4909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [164/4339]: training_loss: tensor(0.6774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [165/4339]: training_loss: tensor(0.4729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [166/4339]: training_loss: tensor(0.4674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [167/4339]: training_loss: tensor(0.2914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [168/4339]: training_loss: tensor(0.4654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [169/4339]: training_loss: tensor(0.5857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [170/4339]: training_loss: tensor(0.5927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [171/4339]: training_loss: tensor(0.4948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [172/4339]: training_loss: tensor(0.5397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [173/4339]: training_loss: tensor(0.4528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [174/4339]: training_loss: tensor(0.8469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [175/4339]: training_loss: tensor(0.3994, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [176/4339]: training_loss: tensor(0.3926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [177/4339]: training_loss: tensor(0.6025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [178/4339]: training_loss: tensor(0.3203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [179/4339]: training_loss: tensor(0.4494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [180/4339]: training_loss: tensor(0.3853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [181/4339]: training_loss: tensor(0.5682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [182/4339]: training_loss: tensor(0.5408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [183/4339]: training_loss: tensor(0.4475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [184/4339]: training_loss: tensor(0.5907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [185/4339]: training_loss: tensor(0.3686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [186/4339]: training_loss: tensor(0.4004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [187/4339]: training_loss: tensor(0.3685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [188/4339]: training_loss: tensor(0.6416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [189/4339]: training_loss: tensor(0.3996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [190/4339]: training_loss: tensor(0.6775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [191/4339]: training_loss: tensor(1.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [192/4339]: training_loss: tensor(0.3568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [193/4339]: training_loss: tensor(0.4513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [194/4339]: training_loss: tensor(0.8436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [195/4339]: training_loss: tensor(0.3471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [196/4339]: training_loss: tensor(0.6075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [197/4339]: training_loss: tensor(0.6997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [198/4339]: training_loss: tensor(0.6549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [199/4339]: training_loss: tensor(0.5156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [200/4339]: training_loss: tensor(0.3402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [201/4339]: training_loss: tensor(0.4335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [202/4339]: training_loss: tensor(0.4839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [203/4339]: training_loss: tensor(0.5698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [204/4339]: training_loss: tensor(0.6718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [205/4339]: training_loss: tensor(0.6109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [206/4339]: training_loss: tensor(0.6130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [207/4339]: training_loss: tensor(0.4198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [208/4339]: training_loss: tensor(0.4249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [209/4339]: training_loss: tensor(0.5705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [210/4339]: training_loss: tensor(0.5288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [211/4339]: training_loss: tensor(0.5566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [212/4339]: training_loss: tensor(0.3282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [213/4339]: training_loss: tensor(0.4059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [214/4339]: training_loss: tensor(0.4010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [215/4339]: training_loss: tensor(0.5981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [216/4339]: training_loss: tensor(0.3692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [217/4339]: training_loss: tensor(0.4550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [218/4339]: training_loss: tensor(0.5612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [219/4339]: training_loss: tensor(0.4073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [220/4339]: training_loss: tensor(0.3902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [221/4339]: training_loss: tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [222/4339]: training_loss: tensor(0.7206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [223/4339]: training_loss: tensor(0.3584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [224/4339]: training_loss: tensor(0.4175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [225/4339]: training_loss: tensor(0.5204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [226/4339]: training_loss: tensor(0.7422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [227/4339]: training_loss: tensor(0.6127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [228/4339]: training_loss: tensor(0.3368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [229/4339]: training_loss: tensor(0.5059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [230/4339]: training_loss: tensor(0.5752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [231/4339]: training_loss: tensor(0.5984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [232/4339]: training_loss: tensor(0.3389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [233/4339]: training_loss: tensor(0.5080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [234/4339]: training_loss: tensor(0.4809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [235/4339]: training_loss: tensor(0.6562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [236/4339]: training_loss: tensor(0.6211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [237/4339]: training_loss: tensor(0.6707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [238/4339]: training_loss: tensor(0.4256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [239/4339]: training_loss: tensor(0.3700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [240/4339]: training_loss: tensor(0.7184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [241/4339]: training_loss: tensor(0.4773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [242/4339]: training_loss: tensor(0.4094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [243/4339]: training_loss: tensor(0.4942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [244/4339]: training_loss: tensor(0.2376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [245/4339]: training_loss: tensor(0.5084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [246/4339]: training_loss: tensor(0.3877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [247/4339]: training_loss: tensor(0.5264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [248/4339]: training_loss: tensor(0.4861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [249/4339]: training_loss: tensor(0.2587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [250/4339]: training_loss: tensor(0.5746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [251/4339]: training_loss: tensor(0.3120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [252/4339]: training_loss: tensor(0.4241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [253/4339]: training_loss: tensor(0.2573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [254/4339]: training_loss: tensor(0.3896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [255/4339]: training_loss: tensor(0.1807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [256/4339]: training_loss: tensor(0.1593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [257/4339]: training_loss: tensor(0.4549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [258/4339]: training_loss: tensor(0.3883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [259/4339]: training_loss: tensor(0.3296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [260/4339]: training_loss: tensor(0.3889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [261/4339]: training_loss: tensor(0.3890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [262/4339]: training_loss: tensor(0.3126, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [263/4339]: training_loss: tensor(0.3745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [264/4339]: training_loss: tensor(0.4791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [265/4339]: training_loss: tensor(0.1976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [266/4339]: training_loss: tensor(0.5518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [267/4339]: training_loss: tensor(0.2383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [268/4339]: training_loss: tensor(0.3940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [269/4339]: training_loss: tensor(0.5246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [270/4339]: training_loss: tensor(0.2082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [271/4339]: training_loss: tensor(0.2762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [272/4339]: training_loss: tensor(0.3105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [273/4339]: training_loss: tensor(0.3511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [274/4339]: training_loss: tensor(0.1980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [275/4339]: training_loss: tensor(0.2235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [276/4339]: training_loss: tensor(0.4101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [277/4339]: training_loss: tensor(0.4020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [278/4339]: training_loss: tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [279/4339]: training_loss: tensor(0.6053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [280/4339]: training_loss: tensor(0.1593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [281/4339]: training_loss: tensor(0.4828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [282/4339]: training_loss: tensor(0.2772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [283/4339]: training_loss: tensor(0.3564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [284/4339]: training_loss: tensor(0.1808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [285/4339]: training_loss: tensor(0.1956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [286/4339]: training_loss: tensor(0.2143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [287/4339]: training_loss: tensor(0.5803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [288/4339]: training_loss: tensor(0.2444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [289/4339]: training_loss: tensor(0.4673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [290/4339]: training_loss: tensor(0.3516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [291/4339]: training_loss: tensor(0.4397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [292/4339]: training_loss: tensor(0.5141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [293/4339]: training_loss: tensor(0.3539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [294/4339]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [295/4339]: training_loss: tensor(0.2665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [296/4339]: training_loss: tensor(0.2036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [297/4339]: training_loss: tensor(0.5902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [298/4339]: training_loss: tensor(0.3631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [299/4339]: training_loss: tensor(0.4896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [300/4339]: training_loss: tensor(0.6970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [301/4339]: training_loss: tensor(0.4133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [302/4339]: training_loss: tensor(0.4535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [303/4339]: training_loss: tensor(0.5032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [304/4339]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [305/4339]: training_loss: tensor(0.3639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [306/4339]: training_loss: tensor(0.4168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [307/4339]: training_loss: tensor(0.2131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [308/4339]: training_loss: tensor(0.2016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [309/4339]: training_loss: tensor(0.2111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [310/4339]: training_loss: tensor(0.3274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [311/4339]: training_loss: tensor(0.3577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [312/4339]: training_loss: tensor(0.5329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [313/4339]: training_loss: tensor(0.5796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [314/4339]: training_loss: tensor(0.5688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [315/4339]: training_loss: tensor(0.3015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [316/4339]: training_loss: tensor(0.3586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [317/4339]: training_loss: tensor(0.3373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [318/4339]: training_loss: tensor(0.2786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [319/4339]: training_loss: tensor(0.2013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [320/4339]: training_loss: tensor(0.1701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [321/4339]: training_loss: tensor(0.6135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [322/4339]: training_loss: tensor(0.1568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [323/4339]: training_loss: tensor(0.1854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [324/4339]: training_loss: tensor(0.2580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [325/4339]: training_loss: tensor(0.2704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [326/4339]: training_loss: tensor(0.2389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [327/4339]: training_loss: tensor(0.3012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [328/4339]: training_loss: tensor(0.2553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [329/4339]: training_loss: tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [330/4339]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [331/4339]: training_loss: tensor(0.7373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [332/4339]: training_loss: tensor(0.3933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [333/4339]: training_loss: tensor(0.2656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [334/4339]: training_loss: tensor(0.3508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [335/4339]: training_loss: tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [336/4339]: training_loss: tensor(0.6746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [337/4339]: training_loss: tensor(0.5898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [338/4339]: training_loss: tensor(0.1984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [339/4339]: training_loss: tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [340/4339]: training_loss: tensor(0.1521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [341/4339]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [342/4339]: training_loss: tensor(0.1852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [343/4339]: training_loss: tensor(0.2735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [344/4339]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [345/4339]: training_loss: tensor(0.2983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [346/4339]: training_loss: tensor(0.2479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [347/4339]: training_loss: tensor(0.1614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [348/4339]: training_loss: tensor(0.1093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [349/4339]: training_loss: tensor(0.4860, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [350/4339]: training_loss: tensor(0.3757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [351/4339]: training_loss: tensor(0.2503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [352/4339]: training_loss: tensor(0.3311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [353/4339]: training_loss: tensor(0.2339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [354/4339]: training_loss: tensor(0.2052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [355/4339]: training_loss: tensor(0.3167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [356/4339]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [357/4339]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [358/4339]: training_loss: tensor(0.2121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [359/4339]: training_loss: tensor(0.1587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [360/4339]: training_loss: tensor(0.4266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [361/4339]: training_loss: tensor(0.3630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [362/4339]: training_loss: tensor(0.4183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [363/4339]: training_loss: tensor(0.2137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [364/4339]: training_loss: tensor(0.1759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [365/4339]: training_loss: tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [366/4339]: training_loss: tensor(0.2441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [367/4339]: training_loss: tensor(0.3790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [368/4339]: training_loss: tensor(0.8326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [369/4339]: training_loss: tensor(0.1881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [370/4339]: training_loss: tensor(0.1578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [371/4339]: training_loss: tensor(0.2090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [372/4339]: training_loss: tensor(0.3234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [373/4339]: training_loss: tensor(0.4805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [374/4339]: training_loss: tensor(0.2248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [375/4339]: training_loss: tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [376/4339]: training_loss: tensor(0.5455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [377/4339]: training_loss: tensor(0.1649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [378/4339]: training_loss: tensor(0.2505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [379/4339]: training_loss: tensor(0.2027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [380/4339]: training_loss: tensor(0.2370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [381/4339]: training_loss: tensor(0.3815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [382/4339]: training_loss: tensor(0.5743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [383/4339]: training_loss: tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [384/4339]: training_loss: tensor(0.5750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [385/4339]: training_loss: tensor(0.4190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [386/4339]: training_loss: tensor(0.2655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [387/4339]: training_loss: tensor(0.1810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [388/4339]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [389/4339]: training_loss: tensor(0.3563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [390/4339]: training_loss: tensor(0.3278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [391/4339]: training_loss: tensor(0.2262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [392/4339]: training_loss: tensor(0.2613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [393/4339]: training_loss: tensor(0.3134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [394/4339]: training_loss: tensor(0.7762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [395/4339]: training_loss: tensor(0.2204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [396/4339]: training_loss: tensor(0.3588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [397/4339]: training_loss: tensor(0.3686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [398/4339]: training_loss: tensor(0.1767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [399/4339]: training_loss: tensor(0.2649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [400/4339]: training_loss: tensor(0.4822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [401/4339]: training_loss: tensor(0.2042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [402/4339]: training_loss: tensor(0.4908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [403/4339]: training_loss: tensor(0.3016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [404/4339]: training_loss: tensor(0.1917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [405/4339]: training_loss: tensor(0.2062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [406/4339]: training_loss: tensor(0.2738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [407/4339]: training_loss: tensor(0.3495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [408/4339]: training_loss: tensor(0.1838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [409/4339]: training_loss: tensor(0.4976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [410/4339]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [411/4339]: training_loss: tensor(0.5451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [412/4339]: training_loss: tensor(0.2182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [413/4339]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [414/4339]: training_loss: tensor(0.4255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [415/4339]: training_loss: tensor(0.4202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [416/4339]: training_loss: tensor(0.2472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [417/4339]: training_loss: tensor(0.3238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [418/4339]: training_loss: tensor(0.1692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [419/4339]: training_loss: tensor(0.6441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [420/4339]: training_loss: tensor(0.1814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [421/4339]: training_loss: tensor(0.1198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [422/4339]: training_loss: tensor(0.4075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [423/4339]: training_loss: tensor(0.2132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [424/4339]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [425/4339]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [426/4339]: training_loss: tensor(0.4126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [427/4339]: training_loss: tensor(0.1902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [428/4339]: training_loss: tensor(0.5050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [429/4339]: training_loss: tensor(0.2076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [430/4339]: training_loss: tensor(0.1783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [431/4339]: training_loss: tensor(0.4272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [432/4339]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [433/4339]: training_loss: tensor(0.4960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [434/4339]: training_loss: tensor(0.1532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [435/4339]: training_loss: tensor(0.1720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [436/4339]: training_loss: tensor(0.3999, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [437/4339]: training_loss: tensor(0.4570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [438/4339]: training_loss: tensor(0.2424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [439/4339]: training_loss: tensor(0.2413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [440/4339]: training_loss: tensor(0.3633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [441/4339]: training_loss: tensor(0.2420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [442/4339]: training_loss: tensor(0.1951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [443/4339]: training_loss: tensor(0.1989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [444/4339]: training_loss: tensor(0.3391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [445/4339]: training_loss: tensor(0.4347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [446/4339]: training_loss: tensor(0.2879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [447/4339]: training_loss: tensor(0.4546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [448/4339]: training_loss: tensor(0.3616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [449/4339]: training_loss: tensor(0.1759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [450/4339]: training_loss: tensor(0.4446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [451/4339]: training_loss: tensor(0.1825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [452/4339]: training_loss: tensor(0.3083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [453/4339]: training_loss: tensor(0.2555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [454/4339]: training_loss: tensor(0.2370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [455/4339]: training_loss: tensor(0.2070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [456/4339]: training_loss: tensor(0.4030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [457/4339]: training_loss: tensor(0.2442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [458/4339]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [459/4339]: training_loss: tensor(0.2275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [460/4339]: training_loss: tensor(0.4047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [461/4339]: training_loss: tensor(0.4434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [462/4339]: training_loss: tensor(0.1645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [463/4339]: training_loss: tensor(0.1597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [464/4339]: training_loss: tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [465/4339]: training_loss: tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [466/4339]: training_loss: tensor(0.3880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [467/4339]: training_loss: tensor(0.6765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [468/4339]: training_loss: tensor(0.1194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [469/4339]: training_loss: tensor(0.1397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [470/4339]: training_loss: tensor(0.2956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [471/4339]: training_loss: tensor(0.1326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [472/4339]: training_loss: tensor(0.3238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [473/4339]: training_loss: tensor(0.5814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [474/4339]: training_loss: tensor(0.3335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [475/4339]: training_loss: tensor(0.3221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [476/4339]: training_loss: tensor(0.1103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [477/4339]: training_loss: tensor(0.1895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [478/4339]: training_loss: tensor(0.0615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [479/4339]: training_loss: tensor(0.1154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [480/4339]: training_loss: tensor(0.9229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [481/4339]: training_loss: tensor(0.4220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [482/4339]: training_loss: tensor(0.4451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [483/4339]: training_loss: tensor(0.1093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [484/4339]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [485/4339]: training_loss: tensor(0.4209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [486/4339]: training_loss: tensor(0.3184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [487/4339]: training_loss: tensor(0.3629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [488/4339]: training_loss: tensor(0.3036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [489/4339]: training_loss: tensor(0.2437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [490/4339]: training_loss: tensor(0.3487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [491/4339]: training_loss: tensor(0.2478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [492/4339]: training_loss: tensor(0.3218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [493/4339]: training_loss: tensor(0.4311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [494/4339]: training_loss: tensor(0.1847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [495/4339]: training_loss: tensor(0.4108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [496/4339]: training_loss: tensor(0.4974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [497/4339]: training_loss: tensor(0.2523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [498/4339]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [499/4339]: training_loss: tensor(0.1532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [500/4339]: training_loss: tensor(0.5705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [501/4339]: training_loss: tensor(0.3166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [502/4339]: training_loss: tensor(0.1729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [503/4339]: training_loss: tensor(0.3975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [504/4339]: training_loss: tensor(0.0616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [505/4339]: training_loss: tensor(0.2986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [506/4339]: training_loss: tensor(0.6171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [507/4339]: training_loss: tensor(0.3207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [508/4339]: training_loss: tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [509/4339]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [510/4339]: training_loss: tensor(0.4738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [511/4339]: training_loss: tensor(0.2143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [512/4339]: training_loss: tensor(0.2862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [513/4339]: training_loss: tensor(0.5002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [514/4339]: training_loss: tensor(0.1880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [515/4339]: training_loss: tensor(0.1810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [516/4339]: training_loss: tensor(0.2236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [517/4339]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [518/4339]: training_loss: tensor(0.2567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [519/4339]: training_loss: tensor(0.4401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [520/4339]: training_loss: tensor(0.4004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [521/4339]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [522/4339]: training_loss: tensor(0.2279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [523/4339]: training_loss: tensor(0.3225, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [524/4339]: training_loss: tensor(0.3272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [525/4339]: training_loss: tensor(0.7040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [526/4339]: training_loss: tensor(0.3534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [527/4339]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [528/4339]: training_loss: tensor(0.2899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [529/4339]: training_loss: tensor(0.3710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [530/4339]: training_loss: tensor(0.4401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [531/4339]: training_loss: tensor(0.1583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [532/4339]: training_loss: tensor(0.1682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [533/4339]: training_loss: tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [534/4339]: training_loss: tensor(0.1698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [535/4339]: training_loss: tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [536/4339]: training_loss: tensor(0.4938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [537/4339]: training_loss: tensor(0.2550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [538/4339]: training_loss: tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [539/4339]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [540/4339]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [541/4339]: training_loss: tensor(0.2888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [542/4339]: training_loss: tensor(0.5199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [543/4339]: training_loss: tensor(0.2632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [544/4339]: training_loss: tensor(0.3232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [545/4339]: training_loss: tensor(0.3718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [546/4339]: training_loss: tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [547/4339]: training_loss: tensor(0.5206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [548/4339]: training_loss: tensor(0.6093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [549/4339]: training_loss: tensor(0.1319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [550/4339]: training_loss: tensor(0.2520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [551/4339]: training_loss: tensor(0.5111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [552/4339]: training_loss: tensor(0.3269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [553/4339]: training_loss: tensor(0.1097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [554/4339]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [555/4339]: training_loss: tensor(0.4136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [556/4339]: training_loss: tensor(0.3602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [557/4339]: training_loss: tensor(0.3905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [558/4339]: training_loss: tensor(0.2858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [559/4339]: training_loss: tensor(0.2199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [560/4339]: training_loss: tensor(0.1728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [561/4339]: training_loss: tensor(0.4691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [562/4339]: training_loss: tensor(0.2834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [563/4339]: training_loss: tensor(0.3979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [564/4339]: training_loss: tensor(0.2357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [565/4339]: training_loss: tensor(0.2403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [566/4339]: training_loss: tensor(0.1765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [567/4339]: training_loss: tensor(0.3777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [568/4339]: training_loss: tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [569/4339]: training_loss: tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [570/4339]: training_loss: tensor(0.1434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [571/4339]: training_loss: tensor(0.1218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [572/4339]: training_loss: tensor(0.7075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [573/4339]: training_loss: tensor(0.1315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [574/4339]: training_loss: tensor(0.2341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [575/4339]: training_loss: tensor(0.3879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [576/4339]: training_loss: tensor(0.2307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [577/4339]: training_loss: tensor(0.3253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [578/4339]: training_loss: tensor(0.1933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [579/4339]: training_loss: tensor(0.2990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [580/4339]: training_loss: tensor(0.2996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [581/4339]: training_loss: tensor(0.3123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [582/4339]: training_loss: tensor(0.2445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [583/4339]: training_loss: tensor(0.1666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [584/4339]: training_loss: tensor(0.6121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [585/4339]: training_loss: tensor(0.3282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [586/4339]: training_loss: tensor(0.1537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [587/4339]: training_loss: tensor(0.2729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [588/4339]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [589/4339]: training_loss: tensor(0.2297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [590/4339]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [591/4339]: training_loss: tensor(0.1514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [592/4339]: training_loss: tensor(0.1962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [593/4339]: training_loss: tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [594/4339]: training_loss: tensor(0.4414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [595/4339]: training_loss: tensor(0.1756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [596/4339]: training_loss: tensor(0.3231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [597/4339]: training_loss: tensor(0.1763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [598/4339]: training_loss: tensor(0.2473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [599/4339]: training_loss: tensor(0.1618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [600/4339]: training_loss: tensor(0.3471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [601/4339]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [602/4339]: training_loss: tensor(0.4888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [603/4339]: training_loss: tensor(0.4104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [604/4339]: training_loss: tensor(0.5337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [605/4339]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [606/4339]: training_loss: tensor(0.5914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [607/4339]: training_loss: tensor(0.3391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [608/4339]: training_loss: tensor(0.5872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [609/4339]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [610/4339]: training_loss: tensor(0.1216, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [611/4339]: training_loss: tensor(0.5362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [612/4339]: training_loss: tensor(0.4133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [613/4339]: training_loss: tensor(0.1557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [614/4339]: training_loss: tensor(0.2303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [615/4339]: training_loss: tensor(0.2222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [616/4339]: training_loss: tensor(0.1988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [617/4339]: training_loss: tensor(0.2675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [618/4339]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [619/4339]: training_loss: tensor(0.3335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [620/4339]: training_loss: tensor(0.6558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [621/4339]: training_loss: tensor(0.2339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [622/4339]: training_loss: tensor(0.3766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [623/4339]: training_loss: tensor(0.1880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [624/4339]: training_loss: tensor(0.4331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [625/4339]: training_loss: tensor(0.4901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [626/4339]: training_loss: tensor(0.3840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [627/4339]: training_loss: tensor(0.1192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [628/4339]: training_loss: tensor(0.2755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [629/4339]: training_loss: tensor(0.1116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [630/4339]: training_loss: tensor(0.2029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [631/4339]: training_loss: tensor(0.4514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [632/4339]: training_loss: tensor(0.3152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [633/4339]: training_loss: tensor(0.3919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [634/4339]: training_loss: tensor(0.1980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [635/4339]: training_loss: tensor(0.3024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [636/4339]: training_loss: tensor(0.3133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [637/4339]: training_loss: tensor(0.2256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [638/4339]: training_loss: tensor(0.2039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [639/4339]: training_loss: tensor(0.3701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [640/4339]: training_loss: tensor(0.2950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [641/4339]: training_loss: tensor(0.7680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [642/4339]: training_loss: tensor(0.3322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [643/4339]: training_loss: tensor(0.1273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [644/4339]: training_loss: tensor(0.3388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [645/4339]: training_loss: tensor(0.1636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [646/4339]: training_loss: tensor(0.2294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [647/4339]: training_loss: tensor(0.1966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [648/4339]: training_loss: tensor(0.2294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [649/4339]: training_loss: tensor(0.2207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [650/4339]: training_loss: tensor(0.2756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [651/4339]: training_loss: tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [652/4339]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [653/4339]: training_loss: tensor(0.3683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [654/4339]: training_loss: tensor(0.2727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [655/4339]: training_loss: tensor(0.3980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [656/4339]: training_loss: tensor(0.5253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [657/4339]: training_loss: tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [658/4339]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [659/4339]: training_loss: tensor(0.1975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [660/4339]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [661/4339]: training_loss: tensor(0.4774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [662/4339]: training_loss: tensor(0.1555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [663/4339]: training_loss: tensor(0.3181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [664/4339]: training_loss: tensor(0.4834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [665/4339]: training_loss: tensor(0.3861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [666/4339]: training_loss: tensor(0.2195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [667/4339]: training_loss: tensor(0.3539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [668/4339]: training_loss: tensor(0.2020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [669/4339]: training_loss: tensor(0.2759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [670/4339]: training_loss: tensor(0.2706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [671/4339]: training_loss: tensor(0.3476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [672/4339]: training_loss: tensor(0.1364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [673/4339]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [674/4339]: training_loss: tensor(0.2812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [675/4339]: training_loss: tensor(0.1243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [676/4339]: training_loss: tensor(0.1312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [677/4339]: training_loss: tensor(0.2864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [678/4339]: training_loss: tensor(0.1860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [679/4339]: training_loss: tensor(0.1619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [680/4339]: training_loss: tensor(0.3943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [681/4339]: training_loss: tensor(0.6694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [682/4339]: training_loss: tensor(0.2068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [683/4339]: training_loss: tensor(0.2704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [684/4339]: training_loss: tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [685/4339]: training_loss: tensor(0.2870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [686/4339]: training_loss: tensor(0.6661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [687/4339]: training_loss: tensor(0.4539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [688/4339]: training_loss: tensor(0.1961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [689/4339]: training_loss: tensor(0.4574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [690/4339]: training_loss: tensor(0.3050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [691/4339]: training_loss: tensor(0.3232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [692/4339]: training_loss: tensor(0.1478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [693/4339]: training_loss: tensor(0.5483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [694/4339]: training_loss: tensor(0.4915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [695/4339]: training_loss: tensor(0.5344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [696/4339]: training_loss: tensor(0.4880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [697/4339]: training_loss: tensor(0.3116, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [698/4339]: training_loss: tensor(0.2455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [699/4339]: training_loss: tensor(0.5877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [700/4339]: training_loss: tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [701/4339]: training_loss: tensor(0.1931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [702/4339]: training_loss: tensor(0.3522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [703/4339]: training_loss: tensor(0.2081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [704/4339]: training_loss: tensor(0.2885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [705/4339]: training_loss: tensor(0.2562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [706/4339]: training_loss: tensor(0.1324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [707/4339]: training_loss: tensor(0.4459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [708/4339]: training_loss: tensor(0.3125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [709/4339]: training_loss: tensor(0.1965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [710/4339]: training_loss: tensor(0.4310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [711/4339]: training_loss: tensor(0.4909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [712/4339]: training_loss: tensor(0.3168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [713/4339]: training_loss: tensor(0.1269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [714/4339]: training_loss: tensor(0.6262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [715/4339]: training_loss: tensor(0.1403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [716/4339]: training_loss: tensor(0.1440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [717/4339]: training_loss: tensor(0.1926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [718/4339]: training_loss: tensor(0.2998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [719/4339]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [720/4339]: training_loss: tensor(0.4824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [721/4339]: training_loss: tensor(0.1761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [722/4339]: training_loss: tensor(0.6188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [723/4339]: training_loss: tensor(0.1832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [724/4339]: training_loss: tensor(0.2182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [725/4339]: training_loss: tensor(0.2254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [726/4339]: training_loss: tensor(0.3583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [727/4339]: training_loss: tensor(0.5249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [728/4339]: training_loss: tensor(0.2016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [729/4339]: training_loss: tensor(0.2497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [730/4339]: training_loss: tensor(0.1841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [731/4339]: training_loss: tensor(0.4790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [732/4339]: training_loss: tensor(0.2882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [733/4339]: training_loss: tensor(0.2259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [734/4339]: training_loss: tensor(0.4358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [735/4339]: training_loss: tensor(0.2323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [736/4339]: training_loss: tensor(0.2489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [737/4339]: training_loss: tensor(0.2719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [738/4339]: training_loss: tensor(0.1603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [739/4339]: training_loss: tensor(0.1959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [740/4339]: training_loss: tensor(0.2507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [741/4339]: training_loss: tensor(0.2083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [742/4339]: training_loss: tensor(0.5232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [743/4339]: training_loss: tensor(0.2815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [744/4339]: training_loss: tensor(0.3199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [745/4339]: training_loss: tensor(0.3312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [746/4339]: training_loss: tensor(0.1186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [747/4339]: training_loss: tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [748/4339]: training_loss: tensor(0.1412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [749/4339]: training_loss: tensor(0.4166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [750/4339]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [751/4339]: training_loss: tensor(0.4356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [752/4339]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [753/4339]: training_loss: tensor(0.3113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [754/4339]: training_loss: tensor(0.2806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [755/4339]: training_loss: tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [756/4339]: training_loss: tensor(0.1223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [757/4339]: training_loss: tensor(0.3529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [758/4339]: training_loss: tensor(0.3644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [759/4339]: training_loss: tensor(0.4927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [760/4339]: training_loss: tensor(0.1301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [761/4339]: training_loss: tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [762/4339]: training_loss: tensor(0.4643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [763/4339]: training_loss: tensor(0.3884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [764/4339]: training_loss: tensor(0.5956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [765/4339]: training_loss: tensor(0.5052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [766/4339]: training_loss: tensor(0.3649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [767/4339]: training_loss: tensor(0.3078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [768/4339]: training_loss: tensor(0.2177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [769/4339]: training_loss: tensor(0.1512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [770/4339]: training_loss: tensor(0.2473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [771/4339]: training_loss: tensor(0.1769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [772/4339]: training_loss: tensor(0.9613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [773/4339]: training_loss: tensor(0.2228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [774/4339]: training_loss: tensor(0.5202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [775/4339]: training_loss: tensor(0.2408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [776/4339]: training_loss: tensor(0.2221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [777/4339]: training_loss: tensor(0.3015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [778/4339]: training_loss: tensor(0.2407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [779/4339]: training_loss: tensor(0.2457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [780/4339]: training_loss: tensor(0.3483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [781/4339]: training_loss: tensor(0.2178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [782/4339]: training_loss: tensor(0.3417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [783/4339]: training_loss: tensor(0.3519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [784/4339]: training_loss: tensor(0.3032, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [785/4339]: training_loss: tensor(0.3736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [786/4339]: training_loss: tensor(0.4621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [787/4339]: training_loss: tensor(0.4075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [788/4339]: training_loss: tensor(0.2300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [789/4339]: training_loss: tensor(0.5364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [790/4339]: training_loss: tensor(0.4154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [791/4339]: training_loss: tensor(0.3839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [792/4339]: training_loss: tensor(0.3362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [793/4339]: training_loss: tensor(0.5323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [794/4339]: training_loss: tensor(0.2583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [795/4339]: training_loss: tensor(0.5448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [796/4339]: training_loss: tensor(0.3011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [797/4339]: training_loss: tensor(0.2670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [798/4339]: training_loss: tensor(0.2360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [799/4339]: training_loss: tensor(0.2049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [800/4339]: training_loss: tensor(0.2248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [801/4339]: training_loss: tensor(0.3038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [802/4339]: training_loss: tensor(0.2706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [803/4339]: training_loss: tensor(0.3706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [804/4339]: training_loss: tensor(0.2493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [805/4339]: training_loss: tensor(0.1970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [806/4339]: training_loss: tensor(0.2093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [807/4339]: training_loss: tensor(0.7785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [808/4339]: training_loss: tensor(0.3257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [809/4339]: training_loss: tensor(0.4418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [810/4339]: training_loss: tensor(0.4096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [811/4339]: training_loss: tensor(0.2779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [812/4339]: training_loss: tensor(0.3534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [813/4339]: training_loss: tensor(0.1412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [814/4339]: training_loss: tensor(0.1967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [815/4339]: training_loss: tensor(0.2769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [816/4339]: training_loss: tensor(0.1732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [817/4339]: training_loss: tensor(0.2574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [818/4339]: training_loss: tensor(0.2568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [819/4339]: training_loss: tensor(0.1702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [820/4339]: training_loss: tensor(0.5702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [821/4339]: training_loss: tensor(0.1172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [822/4339]: training_loss: tensor(0.1786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [823/4339]: training_loss: tensor(0.1350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [824/4339]: training_loss: tensor(0.2528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [825/4339]: training_loss: tensor(0.1562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [826/4339]: training_loss: tensor(0.2883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [827/4339]: training_loss: tensor(0.1493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [828/4339]: training_loss: tensor(0.1516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [829/4339]: training_loss: tensor(0.2190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [830/4339]: training_loss: tensor(0.2513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [831/4339]: training_loss: tensor(0.1426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [832/4339]: training_loss: tensor(0.4888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [833/4339]: training_loss: tensor(0.1801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [834/4339]: training_loss: tensor(0.1650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [835/4339]: training_loss: tensor(0.3357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [836/4339]: training_loss: tensor(0.3142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [837/4339]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [838/4339]: training_loss: tensor(0.1718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [839/4339]: training_loss: tensor(0.1626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [840/4339]: training_loss: tensor(0.2070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [841/4339]: training_loss: tensor(0.4262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [842/4339]: training_loss: tensor(0.2918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [843/4339]: training_loss: tensor(0.2970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [844/4339]: training_loss: tensor(0.3058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [845/4339]: training_loss: tensor(0.4831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [846/4339]: training_loss: tensor(0.1220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [847/4339]: training_loss: tensor(0.1442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [848/4339]: training_loss: tensor(0.4861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [849/4339]: training_loss: tensor(0.4495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [850/4339]: training_loss: tensor(0.5930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [851/4339]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [852/4339]: training_loss: tensor(0.7907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [853/4339]: training_loss: tensor(0.1465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [854/4339]: training_loss: tensor(0.1357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [855/4339]: training_loss: tensor(0.2527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [856/4339]: training_loss: tensor(0.2769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [857/4339]: training_loss: tensor(0.4092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [858/4339]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [859/4339]: training_loss: tensor(0.3654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [860/4339]: training_loss: tensor(0.2835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [861/4339]: training_loss: tensor(0.2264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [862/4339]: training_loss: tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [863/4339]: training_loss: tensor(0.2678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [864/4339]: training_loss: tensor(0.5597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [865/4339]: training_loss: tensor(0.2460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [866/4339]: training_loss: tensor(0.2598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [867/4339]: training_loss: tensor(0.4776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [868/4339]: training_loss: tensor(0.3076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [869/4339]: training_loss: tensor(0.2147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [870/4339]: training_loss: tensor(0.4320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [871/4339]: training_loss: tensor(0.3402, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [872/4339]: training_loss: tensor(0.1968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [873/4339]: training_loss: tensor(0.2093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [874/4339]: training_loss: tensor(0.2011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [875/4339]: training_loss: tensor(0.2901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [876/4339]: training_loss: tensor(0.5511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [877/4339]: training_loss: tensor(0.1729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [878/4339]: training_loss: tensor(0.3998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [879/4339]: training_loss: tensor(0.4005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [880/4339]: training_loss: tensor(0.3437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [881/4339]: training_loss: tensor(0.6088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [882/4339]: training_loss: tensor(0.2718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [883/4339]: training_loss: tensor(0.2492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [884/4339]: training_loss: tensor(0.1543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [885/4339]: training_loss: tensor(0.1352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [886/4339]: training_loss: tensor(0.3957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [887/4339]: training_loss: tensor(0.3057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [888/4339]: training_loss: tensor(0.3900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [889/4339]: training_loss: tensor(0.2870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [890/4339]: training_loss: tensor(0.2782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [891/4339]: training_loss: tensor(0.5882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [892/4339]: training_loss: tensor(0.2253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [893/4339]: training_loss: tensor(0.3615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [894/4339]: training_loss: tensor(0.1832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [895/4339]: training_loss: tensor(0.1958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [896/4339]: training_loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [897/4339]: training_loss: tensor(0.3356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [898/4339]: training_loss: tensor(0.1675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [899/4339]: training_loss: tensor(0.4419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [900/4339]: training_loss: tensor(0.6564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [901/4339]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [902/4339]: training_loss: tensor(0.3218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [903/4339]: training_loss: tensor(0.3756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [904/4339]: training_loss: tensor(0.2153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [905/4339]: training_loss: tensor(0.2438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [906/4339]: training_loss: tensor(0.1019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [907/4339]: training_loss: tensor(0.2973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [908/4339]: training_loss: tensor(0.1611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [909/4339]: training_loss: tensor(0.6115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [910/4339]: training_loss: tensor(0.2793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [911/4339]: training_loss: tensor(0.2284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [912/4339]: training_loss: tensor(0.0389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [913/4339]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [914/4339]: training_loss: tensor(0.2954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [915/4339]: training_loss: tensor(0.2257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [916/4339]: training_loss: tensor(0.2022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [917/4339]: training_loss: tensor(0.6465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [918/4339]: training_loss: tensor(0.3738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [919/4339]: training_loss: tensor(0.4536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [920/4339]: training_loss: tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [921/4339]: training_loss: tensor(0.1801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [922/4339]: training_loss: tensor(0.2905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [923/4339]: training_loss: tensor(0.2897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [924/4339]: training_loss: tensor(0.4006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [925/4339]: training_loss: tensor(0.3386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [926/4339]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [927/4339]: training_loss: tensor(0.2338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [928/4339]: training_loss: tensor(0.1265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [929/4339]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [930/4339]: training_loss: tensor(0.3589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [931/4339]: training_loss: tensor(0.2011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [932/4339]: training_loss: tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [933/4339]: training_loss: tensor(0.2110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [934/4339]: training_loss: tensor(0.3054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [935/4339]: training_loss: tensor(0.2877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [936/4339]: training_loss: tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [937/4339]: training_loss: tensor(0.6018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [938/4339]: training_loss: tensor(0.2233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [939/4339]: training_loss: tensor(0.0476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [940/4339]: training_loss: tensor(0.3669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [941/4339]: training_loss: tensor(0.2368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [942/4339]: training_loss: tensor(0.5705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [943/4339]: training_loss: tensor(0.2752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [944/4339]: training_loss: tensor(0.4020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [945/4339]: training_loss: tensor(0.0579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [946/4339]: training_loss: tensor(0.1162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [947/4339]: training_loss: tensor(0.1632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [948/4339]: training_loss: tensor(0.3583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [949/4339]: training_loss: tensor(0.2252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [950/4339]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [951/4339]: training_loss: tensor(0.1757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [952/4339]: training_loss: tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [953/4339]: training_loss: tensor(0.1180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [954/4339]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [955/4339]: training_loss: tensor(0.3111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [956/4339]: training_loss: tensor(0.2569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [957/4339]: training_loss: tensor(0.3070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [958/4339]: training_loss: tensor(0.3952, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [959/4339]: training_loss: tensor(0.1402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [960/4339]: training_loss: tensor(1.0419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [961/4339]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [962/4339]: training_loss: tensor(0.1975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [963/4339]: training_loss: tensor(0.1684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [964/4339]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [965/4339]: training_loss: tensor(0.5526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [966/4339]: training_loss: tensor(0.5300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [967/4339]: training_loss: tensor(0.3313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [968/4339]: training_loss: tensor(0.2612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [969/4339]: training_loss: tensor(0.1321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [970/4339]: training_loss: tensor(0.4299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [971/4339]: training_loss: tensor(0.3982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [972/4339]: training_loss: tensor(0.1258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [973/4339]: training_loss: tensor(0.3753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [974/4339]: training_loss: tensor(0.8419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [975/4339]: training_loss: tensor(0.2616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [976/4339]: training_loss: tensor(0.3806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [977/4339]: training_loss: tensor(0.1451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [978/4339]: training_loss: tensor(0.2004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [979/4339]: training_loss: tensor(0.4533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [980/4339]: training_loss: tensor(0.4523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [981/4339]: training_loss: tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [982/4339]: training_loss: tensor(0.1817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [983/4339]: training_loss: tensor(0.3006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [984/4339]: training_loss: tensor(0.4518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [985/4339]: training_loss: tensor(0.2766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [986/4339]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [987/4339]: training_loss: tensor(0.2963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [988/4339]: training_loss: tensor(0.1980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [989/4339]: training_loss: tensor(0.4945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [990/4339]: training_loss: tensor(0.2228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [991/4339]: training_loss: tensor(0.3245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [992/4339]: training_loss: tensor(0.2928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [993/4339]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [994/4339]: training_loss: tensor(0.3054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [995/4339]: training_loss: tensor(0.2479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [996/4339]: training_loss: tensor(0.2692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [997/4339]: training_loss: tensor(0.4276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [998/4339]: training_loss: tensor(0.1468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [999/4339]: training_loss: tensor(0.4157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1000/4339]: training_loss: tensor(0.2454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1001/4339]: training_loss: tensor(0.1662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1002/4339]: training_loss: tensor(0.2535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1003/4339]: training_loss: tensor(0.4955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1004/4339]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1005/4339]: training_loss: tensor(0.2220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/4339]: training_loss: tensor(0.7073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/4339]: training_loss: tensor(0.6852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/4339]: training_loss: tensor(0.4350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/4339]: training_loss: tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/4339]: training_loss: tensor(0.4733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/4339]: training_loss: tensor(0.2765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/4339]: training_loss: tensor(0.4206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/4339]: training_loss: tensor(0.3650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/4339]: training_loss: tensor(0.3506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/4339]: training_loss: tensor(0.3566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/4339]: training_loss: tensor(0.3263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/4339]: training_loss: tensor(0.2189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/4339]: training_loss: tensor(0.5464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1019/4339]: training_loss: tensor(0.3877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/4339]: training_loss: tensor(0.4862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/4339]: training_loss: tensor(0.3189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/4339]: training_loss: tensor(0.4111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1023/4339]: training_loss: tensor(0.4480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/4339]: training_loss: tensor(0.2747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/4339]: training_loss: tensor(0.2935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/4339]: training_loss: tensor(0.5171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/4339]: training_loss: tensor(0.3369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/4339]: training_loss: tensor(0.2366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/4339]: training_loss: tensor(0.2408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/4339]: training_loss: tensor(0.1478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/4339]: training_loss: tensor(0.1519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/4339]: training_loss: tensor(0.3110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/4339]: training_loss: tensor(0.2792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/4339]: training_loss: tensor(0.3097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/4339]: training_loss: tensor(0.2748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/4339]: training_loss: tensor(0.6498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/4339]: training_loss: tensor(0.1408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/4339]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/4339]: training_loss: tensor(0.3320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1040/4339]: training_loss: tensor(0.1390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/4339]: training_loss: tensor(0.4214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/4339]: training_loss: tensor(0.5070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1043/4339]: training_loss: tensor(0.2935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/4339]: training_loss: tensor(0.2455, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1045/4339]: training_loss: tensor(0.2824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/4339]: training_loss: tensor(0.2021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/4339]: training_loss: tensor(0.4598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/4339]: training_loss: tensor(0.4219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/4339]: training_loss: tensor(0.1940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/4339]: training_loss: tensor(0.1653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/4339]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/4339]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/4339]: training_loss: tensor(0.3101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/4339]: training_loss: tensor(0.4307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/4339]: training_loss: tensor(0.2987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/4339]: training_loss: tensor(0.4928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/4339]: training_loss: tensor(0.1164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/4339]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/4339]: training_loss: tensor(0.3278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/4339]: training_loss: tensor(0.2145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/4339]: training_loss: tensor(0.3993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/4339]: training_loss: tensor(0.2400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/4339]: training_loss: tensor(0.2185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/4339]: training_loss: tensor(0.4861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/4339]: training_loss: tensor(0.4012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/4339]: training_loss: tensor(0.3918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/4339]: training_loss: tensor(0.4648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/4339]: training_loss: tensor(0.2638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/4339]: training_loss: tensor(0.2508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/4339]: training_loss: tensor(0.5911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/4339]: training_loss: tensor(0.5149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/4339]: training_loss: tensor(0.4463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/4339]: training_loss: tensor(0.3994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/4339]: training_loss: tensor(0.1511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/4339]: training_loss: tensor(0.4027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/4339]: training_loss: tensor(0.3223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/4339]: training_loss: tensor(0.3539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/4339]: training_loss: tensor(0.3590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/4339]: training_loss: tensor(0.6812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/4339]: training_loss: tensor(0.3222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/4339]: training_loss: tensor(0.3335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/4339]: training_loss: tensor(0.2336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/4339]: training_loss: tensor(0.3179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/4339]: training_loss: tensor(0.4128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/4339]: training_loss: tensor(0.1442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/4339]: training_loss: tensor(0.4764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/4339]: training_loss: tensor(0.2595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/4339]: training_loss: tensor(0.3399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/4339]: training_loss: tensor(0.4034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/4339]: training_loss: tensor(0.3490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/4339]: training_loss: tensor(0.5311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/4339]: training_loss: tensor(0.2395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/4339]: training_loss: tensor(0.3782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/4339]: training_loss: tensor(0.5193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/4339]: training_loss: tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/4339]: training_loss: tensor(0.3416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/4339]: training_loss: tensor(0.3377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/4339]: training_loss: tensor(0.2211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/4339]: training_loss: tensor(0.3688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1100/4339]: training_loss: tensor(0.2229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/4339]: training_loss: tensor(0.4307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/4339]: training_loss: tensor(0.1365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/4339]: training_loss: tensor(0.5820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/4339]: training_loss: tensor(0.2854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/4339]: training_loss: tensor(0.2770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/4339]: training_loss: tensor(0.4006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/4339]: training_loss: tensor(0.2532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/4339]: training_loss: tensor(0.5257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/4339]: training_loss: tensor(0.2426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/4339]: training_loss: tensor(0.5626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/4339]: training_loss: tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/4339]: training_loss: tensor(0.2961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/4339]: training_loss: tensor(0.1514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/4339]: training_loss: tensor(0.2632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/4339]: training_loss: tensor(0.5067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/4339]: training_loss: tensor(0.1461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/4339]: training_loss: tensor(0.4347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/4339]: training_loss: tensor(0.4562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/4339]: training_loss: tensor(0.2477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/4339]: training_loss: tensor(0.1832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/4339]: training_loss: tensor(0.2213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/4339]: training_loss: tensor(0.1184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/4339]: training_loss: tensor(0.3176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/4339]: training_loss: tensor(0.3947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/4339]: training_loss: tensor(0.1642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/4339]: training_loss: tensor(0.5683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/4339]: training_loss: tensor(0.5668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/4339]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1129/4339]: training_loss: tensor(0.1601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/4339]: training_loss: tensor(0.3292, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1131/4339]: training_loss: tensor(0.8134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/4339]: training_loss: tensor(0.1214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/4339]: training_loss: tensor(0.4170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/4339]: training_loss: tensor(0.1948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/4339]: training_loss: tensor(0.3338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/4339]: training_loss: tensor(0.1963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/4339]: training_loss: tensor(0.3645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/4339]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/4339]: training_loss: tensor(0.4915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/4339]: training_loss: tensor(0.3353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/4339]: training_loss: tensor(0.2227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/4339]: training_loss: tensor(0.2513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/4339]: training_loss: tensor(0.2581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/4339]: training_loss: tensor(0.2351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/4339]: training_loss: tensor(0.2628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/4339]: training_loss: tensor(0.6021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/4339]: training_loss: tensor(0.2862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/4339]: training_loss: tensor(0.3356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/4339]: training_loss: tensor(0.6429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/4339]: training_loss: tensor(0.2334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/4339]: training_loss: tensor(0.3381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/4339]: training_loss: tensor(0.2294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/4339]: training_loss: tensor(0.1914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/4339]: training_loss: tensor(0.3326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/4339]: training_loss: tensor(0.2456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/4339]: training_loss: tensor(0.3560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/4339]: training_loss: tensor(0.4054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/4339]: training_loss: tensor(0.4285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/4339]: training_loss: tensor(0.3608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/4339]: training_loss: tensor(0.4226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/4339]: training_loss: tensor(0.4651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/4339]: training_loss: tensor(0.4833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/4339]: training_loss: tensor(0.3096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/4339]: training_loss: tensor(0.2927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/4339]: training_loss: tensor(0.6633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/4339]: training_loss: tensor(0.3965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/4339]: training_loss: tensor(0.3357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/4339]: training_loss: tensor(0.2004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/4339]: training_loss: tensor(0.4001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/4339]: training_loss: tensor(0.2263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/4339]: training_loss: tensor(0.4047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/4339]: training_loss: tensor(0.2226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/4339]: training_loss: tensor(0.2611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/4339]: training_loss: tensor(0.4303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/4339]: training_loss: tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/4339]: training_loss: tensor(0.4588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/4339]: training_loss: tensor(0.2331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/4339]: training_loss: tensor(0.4480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/4339]: training_loss: tensor(0.2467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/4339]: training_loss: tensor(0.5262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/4339]: training_loss: tensor(0.2200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/4339]: training_loss: tensor(0.2547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/4339]: training_loss: tensor(0.1567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/4339]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/4339]: training_loss: tensor(0.1592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/4339]: training_loss: tensor(0.4008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/4339]: training_loss: tensor(0.2207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/4339]: training_loss: tensor(0.3370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/4339]: training_loss: tensor(0.2036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/4339]: training_loss: tensor(0.1294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/4339]: training_loss: tensor(0.1386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/4339]: training_loss: tensor(0.3768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/4339]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/4339]: training_loss: tensor(0.3449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/4339]: training_loss: tensor(0.3348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/4339]: training_loss: tensor(0.1799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/4339]: training_loss: tensor(0.1628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/4339]: training_loss: tensor(0.2466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/4339]: training_loss: tensor(0.2907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/4339]: training_loss: tensor(0.3595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/4339]: training_loss: tensor(0.3543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/4339]: training_loss: tensor(0.3453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/4339]: training_loss: tensor(0.2166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/4339]: training_loss: tensor(0.2991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/4339]: training_loss: tensor(0.1295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/4339]: training_loss: tensor(0.3760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/4339]: training_loss: tensor(0.9700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/4339]: training_loss: tensor(0.3944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/4339]: training_loss: tensor(0.1512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/4339]: training_loss: tensor(0.2584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/4339]: training_loss: tensor(0.1884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/4339]: training_loss: tensor(0.3577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1213/4339]: training_loss: tensor(0.4236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/4339]: training_loss: tensor(0.3573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1215/4339]: training_loss: tensor(0.1236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/4339]: training_loss: tensor(0.3462, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1217/4339]: training_loss: tensor(0.2490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/4339]: training_loss: tensor(0.2283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/4339]: training_loss: tensor(0.1307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/4339]: training_loss: tensor(0.2957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/4339]: training_loss: tensor(0.5124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/4339]: training_loss: tensor(0.5620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/4339]: training_loss: tensor(0.5653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/4339]: training_loss: tensor(0.3399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/4339]: training_loss: tensor(0.2194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/4339]: training_loss: tensor(0.1777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/4339]: training_loss: tensor(0.1940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/4339]: training_loss: tensor(0.1635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/4339]: training_loss: tensor(0.4107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/4339]: training_loss: tensor(0.3359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/4339]: training_loss: tensor(0.2684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/4339]: training_loss: tensor(0.2770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/4339]: training_loss: tensor(0.4453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/4339]: training_loss: tensor(0.3284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/4339]: training_loss: tensor(0.1729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/4339]: training_loss: tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/4339]: training_loss: tensor(0.2736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/4339]: training_loss: tensor(0.4250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/4339]: training_loss: tensor(0.1725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/4339]: training_loss: tensor(0.2393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/4339]: training_loss: tensor(0.2904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/4339]: training_loss: tensor(0.3743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/4339]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/4339]: training_loss: tensor(0.3103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/4339]: training_loss: tensor(0.2924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/4339]: training_loss: tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/4339]: training_loss: tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/4339]: training_loss: tensor(0.2485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/4339]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/4339]: training_loss: tensor(0.3541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/4339]: training_loss: tensor(0.3764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/4339]: training_loss: tensor(0.2277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/4339]: training_loss: tensor(0.5637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/4339]: training_loss: tensor(0.1358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/4339]: training_loss: tensor(0.3174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/4339]: training_loss: tensor(0.2914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/4339]: training_loss: tensor(0.4803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/4339]: training_loss: tensor(0.4425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/4339]: training_loss: tensor(0.6285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/4339]: training_loss: tensor(0.1526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/4339]: training_loss: tensor(0.5121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/4339]: training_loss: tensor(0.1935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/4339]: training_loss: tensor(0.2319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/4339]: training_loss: tensor(0.2323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/4339]: training_loss: tensor(0.2613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/4339]: training_loss: tensor(0.6108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/4339]: training_loss: tensor(0.4056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/4339]: training_loss: tensor(0.4908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/4339]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/4339]: training_loss: tensor(0.1687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/4339]: training_loss: tensor(0.1374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/4339]: training_loss: tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/4339]: training_loss: tensor(0.1636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/4339]: training_loss: tensor(0.2936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/4339]: training_loss: tensor(0.1160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/4339]: training_loss: tensor(0.2208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/4339]: training_loss: tensor(0.3705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/4339]: training_loss: tensor(0.1818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/4339]: training_loss: tensor(0.4356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/4339]: training_loss: tensor(0.2462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/4339]: training_loss: tensor(0.1145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/4339]: training_loss: tensor(0.2967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/4339]: training_loss: tensor(0.3193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/4339]: training_loss: tensor(0.3902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/4339]: training_loss: tensor(0.3312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/4339]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/4339]: training_loss: tensor(0.2237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/4339]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/4339]: training_loss: tensor(0.1369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/4339]: training_loss: tensor(0.3289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/4339]: training_loss: tensor(0.2407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/4339]: training_loss: tensor(0.3747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/4339]: training_loss: tensor(0.1754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/4339]: training_loss: tensor(0.1784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/4339]: training_loss: tensor(0.2992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/4339]: training_loss: tensor(0.1998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/4339]: training_loss: tensor(0.5197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/4339]: training_loss: tensor(0.4029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1299/4339]: training_loss: tensor(0.3141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/4339]: training_loss: tensor(0.5960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/4339]: training_loss: tensor(0.3822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/4339]: training_loss: tensor(0.5643, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1303/4339]: training_loss: tensor(0.2157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/4339]: training_loss: tensor(0.2932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/4339]: training_loss: tensor(0.3950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/4339]: training_loss: tensor(0.3417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/4339]: training_loss: tensor(0.4411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/4339]: training_loss: tensor(0.6076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/4339]: training_loss: tensor(0.1550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/4339]: training_loss: tensor(0.3425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/4339]: training_loss: tensor(0.3676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/4339]: training_loss: tensor(0.4008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/4339]: training_loss: tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/4339]: training_loss: tensor(0.4768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/4339]: training_loss: tensor(0.3313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/4339]: training_loss: tensor(0.4200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/4339]: training_loss: tensor(0.3094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/4339]: training_loss: tensor(0.4591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/4339]: training_loss: tensor(0.3110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/4339]: training_loss: tensor(0.1264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/4339]: training_loss: tensor(0.3790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/4339]: training_loss: tensor(0.1772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/4339]: training_loss: tensor(0.3426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/4339]: training_loss: tensor(0.2830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/4339]: training_loss: tensor(0.4003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/4339]: training_loss: tensor(0.3054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/4339]: training_loss: tensor(0.3582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/4339]: training_loss: tensor(0.2480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/4339]: training_loss: tensor(0.5424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/4339]: training_loss: tensor(0.2233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/4339]: training_loss: tensor(0.1844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/4339]: training_loss: tensor(0.2036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/4339]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/4339]: training_loss: tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/4339]: training_loss: tensor(0.1919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/4339]: training_loss: tensor(0.1737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/4339]: training_loss: tensor(0.2097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/4339]: training_loss: tensor(0.6291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/4339]: training_loss: tensor(0.2621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/4339]: training_loss: tensor(0.2695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/4339]: training_loss: tensor(0.3623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/4339]: training_loss: tensor(0.4044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/4339]: training_loss: tensor(0.7804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/4339]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/4339]: training_loss: tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/4339]: training_loss: tensor(0.3003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/4339]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/4339]: training_loss: tensor(0.3469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/4339]: training_loss: tensor(0.5977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/4339]: training_loss: tensor(0.2604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/4339]: training_loss: tensor(0.2527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/4339]: training_loss: tensor(0.1963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/4339]: training_loss: tensor(0.4677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/4339]: training_loss: tensor(0.1353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/4339]: training_loss: tensor(0.1266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/4339]: training_loss: tensor(0.2885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/4339]: training_loss: tensor(0.6329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/4339]: training_loss: tensor(0.2475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/4339]: training_loss: tensor(0.4255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/4339]: training_loss: tensor(0.1978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1361/4339]: training_loss: tensor(0.2705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/4339]: training_loss: tensor(0.3166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1363/4339]: training_loss: tensor(0.3833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/4339]: training_loss: tensor(0.3372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/4339]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/4339]: training_loss: tensor(0.7055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/4339]: training_loss: tensor(0.2441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/4339]: training_loss: tensor(0.7317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/4339]: training_loss: tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/4339]: training_loss: tensor(0.1078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/4339]: training_loss: tensor(0.3397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/4339]: training_loss: tensor(0.4576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/4339]: training_loss: tensor(0.2886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/4339]: training_loss: tensor(0.3779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/4339]: training_loss: tensor(0.3722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/4339]: training_loss: tensor(0.2550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/4339]: training_loss: tensor(0.4457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/4339]: training_loss: tensor(0.2236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/4339]: training_loss: tensor(0.3011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/4339]: training_loss: tensor(0.1878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/4339]: training_loss: tensor(0.5010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/4339]: training_loss: tensor(0.2180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/4339]: training_loss: tensor(0.4386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/4339]: training_loss: tensor(0.2837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1385/4339]: training_loss: tensor(0.4539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1386/4339]: training_loss: tensor(0.1949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1387/4339]: training_loss: tensor(0.2459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1388/4339]: training_loss: tensor(0.1270, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1389/4339]: training_loss: tensor(0.3402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1390/4339]: training_loss: tensor(0.3583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1391/4339]: training_loss: tensor(0.4133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1392/4339]: training_loss: tensor(0.1814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1393/4339]: training_loss: tensor(0.2091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1394/4339]: training_loss: tensor(0.2437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1395/4339]: training_loss: tensor(0.2129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1396/4339]: training_loss: tensor(0.2285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1397/4339]: training_loss: tensor(0.2766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1398/4339]: training_loss: tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1399/4339]: training_loss: tensor(0.5269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1400/4339]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1401/4339]: training_loss: tensor(0.1518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1402/4339]: training_loss: tensor(0.1590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1403/4339]: training_loss: tensor(0.4283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1404/4339]: training_loss: tensor(0.2661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1405/4339]: training_loss: tensor(0.2108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1406/4339]: training_loss: tensor(0.3162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1407/4339]: training_loss: tensor(0.4091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1408/4339]: training_loss: tensor(0.3242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1409/4339]: training_loss: tensor(0.4989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1410/4339]: training_loss: tensor(0.3483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1411/4339]: training_loss: tensor(0.1218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1412/4339]: training_loss: tensor(0.3004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1413/4339]: training_loss: tensor(0.9415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1414/4339]: training_loss: tensor(0.8430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1415/4339]: training_loss: tensor(0.2512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1416/4339]: training_loss: tensor(0.2903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1417/4339]: training_loss: tensor(0.2220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1418/4339]: training_loss: tensor(0.5054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1419/4339]: training_loss: tensor(0.4241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1420/4339]: training_loss: tensor(0.1800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1421/4339]: training_loss: tensor(0.1813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1422/4339]: training_loss: tensor(0.3283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1423/4339]: training_loss: tensor(0.3453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1424/4339]: training_loss: tensor(0.5368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1425/4339]: training_loss: tensor(0.3799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1426/4339]: training_loss: tensor(0.3615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1427/4339]: training_loss: tensor(0.3879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1428/4339]: training_loss: tensor(0.2460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1429/4339]: training_loss: tensor(0.2923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1430/4339]: training_loss: tensor(0.1768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1431/4339]: training_loss: tensor(0.2226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1432/4339]: training_loss: tensor(0.3942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1433/4339]: training_loss: tensor(0.1381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1434/4339]: training_loss: tensor(0.3225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1435/4339]: training_loss: tensor(0.4657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1436/4339]: training_loss: tensor(0.2006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1437/4339]: training_loss: tensor(0.3337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1438/4339]: training_loss: tensor(0.2798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1439/4339]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1440/4339]: training_loss: tensor(0.6257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1441/4339]: training_loss: tensor(0.1159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1442/4339]: training_loss: tensor(0.5641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1443/4339]: training_loss: tensor(0.0576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1444/4339]: training_loss: tensor(0.3027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1445/4339]: training_loss: tensor(0.2240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1446/4339]: training_loss: tensor(0.3210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1447/4339]: training_loss: tensor(0.3052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1448/4339]: training_loss: tensor(0.3313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1449/4339]: training_loss: tensor(0.2248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1450/4339]: training_loss: tensor(0.4087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1451/4339]: training_loss: tensor(0.2962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1452/4339]: training_loss: tensor(0.4887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1453/4339]: training_loss: tensor(0.3539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1454/4339]: training_loss: tensor(0.5321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1455/4339]: training_loss: tensor(0.3233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1456/4339]: training_loss: tensor(0.4128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1457/4339]: training_loss: tensor(0.1686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1458/4339]: training_loss: tensor(0.2725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1459/4339]: training_loss: tensor(0.3540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1460/4339]: training_loss: tensor(0.1660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1461/4339]: training_loss: tensor(0.2305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1462/4339]: training_loss: tensor(0.2708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1463/4339]: training_loss: tensor(0.1833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1464/4339]: training_loss: tensor(0.4323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1465/4339]: training_loss: tensor(0.1453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1466/4339]: training_loss: tensor(0.1915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1467/4339]: training_loss: tensor(0.1097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1468/4339]: training_loss: tensor(0.3162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1469/4339]: training_loss: tensor(0.2713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1470/4339]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1471/4339]: training_loss: tensor(0.3668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1472/4339]: training_loss: tensor(0.3818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1473/4339]: training_loss: tensor(0.4780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1474/4339]: training_loss: tensor(0.2704, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1475/4339]: training_loss: tensor(0.3649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1476/4339]: training_loss: tensor(0.3322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1477/4339]: training_loss: tensor(0.4865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1478/4339]: training_loss: tensor(0.3690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1479/4339]: training_loss: tensor(0.3725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1480/4339]: training_loss: tensor(0.2614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1481/4339]: training_loss: tensor(0.1564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1482/4339]: training_loss: tensor(0.3633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1483/4339]: training_loss: tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1484/4339]: training_loss: tensor(0.1404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1485/4339]: training_loss: tensor(0.5594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1486/4339]: training_loss: tensor(0.1399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1487/4339]: training_loss: tensor(0.3147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1488/4339]: training_loss: tensor(0.6792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1489/4339]: training_loss: tensor(0.3317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1490/4339]: training_loss: tensor(0.1688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1491/4339]: training_loss: tensor(0.2679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1492/4339]: training_loss: tensor(0.4685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1493/4339]: training_loss: tensor(0.2730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1494/4339]: training_loss: tensor(0.2940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1495/4339]: training_loss: tensor(0.2169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1496/4339]: training_loss: tensor(0.2726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1497/4339]: training_loss: tensor(0.2781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1498/4339]: training_loss: tensor(0.4083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1499/4339]: training_loss: tensor(0.1144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1500/4339]: training_loss: tensor(0.1702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1501/4339]: training_loss: tensor(0.2444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1502/4339]: training_loss: tensor(0.1106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1503/4339]: training_loss: tensor(0.2377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1504/4339]: training_loss: tensor(0.3719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1505/4339]: training_loss: tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1506/4339]: training_loss: tensor(0.1199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1507/4339]: training_loss: tensor(0.3737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1508/4339]: training_loss: tensor(0.3364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1509/4339]: training_loss: tensor(0.5352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1510/4339]: training_loss: tensor(0.3928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1511/4339]: training_loss: tensor(0.4557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1512/4339]: training_loss: tensor(0.4734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1513/4339]: training_loss: tensor(0.3087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1514/4339]: training_loss: tensor(0.2727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1515/4339]: training_loss: tensor(0.2990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1516/4339]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1517/4339]: training_loss: tensor(0.4321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1518/4339]: training_loss: tensor(0.4132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1519/4339]: training_loss: tensor(0.3126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1520/4339]: training_loss: tensor(0.1866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1521/4339]: training_loss: tensor(0.3504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1522/4339]: training_loss: tensor(0.1183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1523/4339]: training_loss: tensor(0.2243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1524/4339]: training_loss: tensor(0.3073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1525/4339]: training_loss: tensor(0.6529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1526/4339]: training_loss: tensor(0.3015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1527/4339]: training_loss: tensor(0.2496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1528/4339]: training_loss: tensor(0.2236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1529/4339]: training_loss: tensor(0.4700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1530/4339]: training_loss: tensor(0.5007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1531/4339]: training_loss: tensor(0.2099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1532/4339]: training_loss: tensor(0.1459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1533/4339]: training_loss: tensor(0.1482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1534/4339]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1535/4339]: training_loss: tensor(0.4024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1536/4339]: training_loss: tensor(0.1340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1537/4339]: training_loss: tensor(0.4873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1538/4339]: training_loss: tensor(0.3315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1539/4339]: training_loss: tensor(0.5369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1540/4339]: training_loss: tensor(0.2677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1541/4339]: training_loss: tensor(0.1763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1542/4339]: training_loss: tensor(0.3088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1543/4339]: training_loss: tensor(0.1320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1544/4339]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1545/4339]: training_loss: tensor(0.3088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1546/4339]: training_loss: tensor(0.3747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1547/4339]: training_loss: tensor(0.4030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1548/4339]: training_loss: tensor(0.3605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1549/4339]: training_loss: tensor(0.4248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1550/4339]: training_loss: tensor(0.3946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1551/4339]: training_loss: tensor(0.2403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1552/4339]: training_loss: tensor(0.1357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1553/4339]: training_loss: tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1554/4339]: training_loss: tensor(0.4096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1555/4339]: training_loss: tensor(0.3326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1556/4339]: training_loss: tensor(0.5443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1557/4339]: training_loss: tensor(0.6131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1558/4339]: training_loss: tensor(0.3583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1559/4339]: training_loss: tensor(0.1947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1560/4339]: training_loss: tensor(0.5096, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1561/4339]: training_loss: tensor(0.1194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1562/4339]: training_loss: tensor(0.1241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1563/4339]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1564/4339]: training_loss: tensor(0.1949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1565/4339]: training_loss: tensor(0.2809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1566/4339]: training_loss: tensor(0.2526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1567/4339]: training_loss: tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1568/4339]: training_loss: tensor(0.2641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1569/4339]: training_loss: tensor(0.1356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1570/4339]: training_loss: tensor(0.1597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1571/4339]: training_loss: tensor(0.1387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1572/4339]: training_loss: tensor(0.2397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1573/4339]: training_loss: tensor(0.8564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1574/4339]: training_loss: tensor(0.2055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1575/4339]: training_loss: tensor(0.2980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1576/4339]: training_loss: tensor(0.4631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1577/4339]: training_loss: tensor(0.1636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1578/4339]: training_loss: tensor(0.8404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1579/4339]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1580/4339]: training_loss: tensor(0.3460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1581/4339]: training_loss: tensor(0.1979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1582/4339]: training_loss: tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1583/4339]: training_loss: tensor(0.2549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1584/4339]: training_loss: tensor(0.2372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1585/4339]: training_loss: tensor(0.2018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1586/4339]: training_loss: tensor(0.1487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1587/4339]: training_loss: tensor(0.2275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1588/4339]: training_loss: tensor(0.2962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1589/4339]: training_loss: tensor(0.2917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1590/4339]: training_loss: tensor(0.3173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1591/4339]: training_loss: tensor(0.2688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1592/4339]: training_loss: tensor(0.3515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1593/4339]: training_loss: tensor(0.3350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1594/4339]: training_loss: tensor(0.4514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1595/4339]: training_loss: tensor(0.4322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1596/4339]: training_loss: tensor(0.3287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1597/4339]: training_loss: tensor(0.1726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1598/4339]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1599/4339]: training_loss: tensor(0.1753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1600/4339]: training_loss: tensor(0.2567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1601/4339]: training_loss: tensor(0.1737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1602/4339]: training_loss: tensor(0.1678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1603/4339]: training_loss: tensor(0.1891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1604/4339]: training_loss: tensor(0.1885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1605/4339]: training_loss: tensor(0.2187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1606/4339]: training_loss: tensor(0.2012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1607/4339]: training_loss: tensor(0.1176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1608/4339]: training_loss: tensor(0.2119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1609/4339]: training_loss: tensor(0.4835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1610/4339]: training_loss: tensor(0.3127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1611/4339]: training_loss: tensor(0.2095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1612/4339]: training_loss: tensor(0.1896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1613/4339]: training_loss: tensor(0.6384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1614/4339]: training_loss: tensor(0.3498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1615/4339]: training_loss: tensor(0.2558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1616/4339]: training_loss: tensor(0.6245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1617/4339]: training_loss: tensor(0.2034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1618/4339]: training_loss: tensor(0.2069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1619/4339]: training_loss: tensor(0.2354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1620/4339]: training_loss: tensor(0.1504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1621/4339]: training_loss: tensor(0.2452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1622/4339]: training_loss: tensor(0.3988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1623/4339]: training_loss: tensor(0.2647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1624/4339]: training_loss: tensor(0.4374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1625/4339]: training_loss: tensor(0.6398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1626/4339]: training_loss: tensor(0.1205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1627/4339]: training_loss: tensor(0.2314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1628/4339]: training_loss: tensor(0.4925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1629/4339]: training_loss: tensor(0.4343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1630/4339]: training_loss: tensor(0.2066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1631/4339]: training_loss: tensor(0.2559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1632/4339]: training_loss: tensor(0.4299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1633/4339]: training_loss: tensor(0.6723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1634/4339]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1635/4339]: training_loss: tensor(0.2034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1636/4339]: training_loss: tensor(0.1202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1637/4339]: training_loss: tensor(0.3061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1638/4339]: training_loss: tensor(0.5736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1639/4339]: training_loss: tensor(0.5881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1640/4339]: training_loss: tensor(0.2374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1641/4339]: training_loss: tensor(0.2623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1642/4339]: training_loss: tensor(0.2949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1643/4339]: training_loss: tensor(0.2013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1644/4339]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1645/4339]: training_loss: tensor(0.2399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1646/4339]: training_loss: tensor(0.3391, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1647/4339]: training_loss: tensor(0.1933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1648/4339]: training_loss: tensor(0.4466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1649/4339]: training_loss: tensor(0.1267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1650/4339]: training_loss: tensor(0.3563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1651/4339]: training_loss: tensor(0.4506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1652/4339]: training_loss: tensor(0.3673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1653/4339]: training_loss: tensor(0.2239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1654/4339]: training_loss: tensor(0.5038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1655/4339]: training_loss: tensor(0.3528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1656/4339]: training_loss: tensor(0.1644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1657/4339]: training_loss: tensor(0.1511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1658/4339]: training_loss: tensor(0.3105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1659/4339]: training_loss: tensor(0.4898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1660/4339]: training_loss: tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1661/4339]: training_loss: tensor(0.2170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1662/4339]: training_loss: tensor(0.5336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1663/4339]: training_loss: tensor(0.4947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1664/4339]: training_loss: tensor(0.2663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1665/4339]: training_loss: tensor(0.3330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1666/4339]: training_loss: tensor(0.1961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1667/4339]: training_loss: tensor(0.2010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1668/4339]: training_loss: tensor(0.2850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1669/4339]: training_loss: tensor(0.2215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1670/4339]: training_loss: tensor(0.2124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1671/4339]: training_loss: tensor(0.1071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1672/4339]: training_loss: tensor(0.4499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1673/4339]: training_loss: tensor(0.5701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1674/4339]: training_loss: tensor(0.3604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1675/4339]: training_loss: tensor(0.2622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1676/4339]: training_loss: tensor(0.2667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1677/4339]: training_loss: tensor(0.4057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1678/4339]: training_loss: tensor(0.1655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1679/4339]: training_loss: tensor(0.1971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1680/4339]: training_loss: tensor(0.1926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1681/4339]: training_loss: tensor(0.3559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1682/4339]: training_loss: tensor(0.2731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1683/4339]: training_loss: tensor(0.2815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1684/4339]: training_loss: tensor(0.6087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1685/4339]: training_loss: tensor(0.1896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1686/4339]: training_loss: tensor(0.4321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1687/4339]: training_loss: tensor(0.2822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1688/4339]: training_loss: tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1689/4339]: training_loss: tensor(0.5136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1690/4339]: training_loss: tensor(0.2918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1691/4339]: training_loss: tensor(0.3409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1692/4339]: training_loss: tensor(0.7879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1693/4339]: training_loss: tensor(0.4630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1694/4339]: training_loss: tensor(0.1649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1695/4339]: training_loss: tensor(0.4866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1696/4339]: training_loss: tensor(0.2742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1697/4339]: training_loss: tensor(0.2072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1698/4339]: training_loss: tensor(0.3682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1699/4339]: training_loss: tensor(0.2060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1700/4339]: training_loss: tensor(0.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1701/4339]: training_loss: tensor(0.3434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1702/4339]: training_loss: tensor(0.4258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1703/4339]: training_loss: tensor(0.1946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1704/4339]: training_loss: tensor(0.3118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1705/4339]: training_loss: tensor(0.5417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1706/4339]: training_loss: tensor(0.1910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1707/4339]: training_loss: tensor(0.1693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1708/4339]: training_loss: tensor(0.4515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1709/4339]: training_loss: tensor(0.1546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1710/4339]: training_loss: tensor(0.3426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1711/4339]: training_loss: tensor(0.2430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1712/4339]: training_loss: tensor(0.2423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1713/4339]: training_loss: tensor(0.1774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1714/4339]: training_loss: tensor(0.1744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1715/4339]: training_loss: tensor(0.5366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1716/4339]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1717/4339]: training_loss: tensor(0.5511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1718/4339]: training_loss: tensor(0.5535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1719/4339]: training_loss: tensor(0.2280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1720/4339]: training_loss: tensor(0.5421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1721/4339]: training_loss: tensor(0.1992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1722/4339]: training_loss: tensor(0.1503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1723/4339]: training_loss: tensor(0.2090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1724/4339]: training_loss: tensor(0.5792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1725/4339]: training_loss: tensor(0.1470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1726/4339]: training_loss: tensor(0.1872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1727/4339]: training_loss: tensor(0.2687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1728/4339]: training_loss: tensor(0.5060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1729/4339]: training_loss: tensor(0.7359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1730/4339]: training_loss: tensor(0.4094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1731/4339]: training_loss: tensor(0.3925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1732/4339]: training_loss: tensor(0.2360, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1733/4339]: training_loss: tensor(0.3696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1734/4339]: training_loss: tensor(0.1577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1735/4339]: training_loss: tensor(0.1644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1736/4339]: training_loss: tensor(0.2278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1737/4339]: training_loss: tensor(0.3750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1738/4339]: training_loss: tensor(0.1280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1739/4339]: training_loss: tensor(0.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1740/4339]: training_loss: tensor(0.3613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1741/4339]: training_loss: tensor(0.4670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1742/4339]: training_loss: tensor(0.1891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1743/4339]: training_loss: tensor(0.2805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1744/4339]: training_loss: tensor(0.3943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1745/4339]: training_loss: tensor(0.3930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1746/4339]: training_loss: tensor(0.1216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1747/4339]: training_loss: tensor(0.3696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1748/4339]: training_loss: tensor(0.1958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1749/4339]: training_loss: tensor(0.2541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1750/4339]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1751/4339]: training_loss: tensor(0.3846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1752/4339]: training_loss: tensor(0.2657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1753/4339]: training_loss: tensor(0.4641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1754/4339]: training_loss: tensor(0.5225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1755/4339]: training_loss: tensor(0.1710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1756/4339]: training_loss: tensor(0.2618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1757/4339]: training_loss: tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1758/4339]: training_loss: tensor(0.2460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1759/4339]: training_loss: tensor(0.2991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1760/4339]: training_loss: tensor(0.1668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1761/4339]: training_loss: tensor(0.1596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1762/4339]: training_loss: tensor(0.2813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1763/4339]: training_loss: tensor(0.4309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1764/4339]: training_loss: tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1765/4339]: training_loss: tensor(0.5717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1766/4339]: training_loss: tensor(0.1548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1767/4339]: training_loss: tensor(0.5956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1768/4339]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1769/4339]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1770/4339]: training_loss: tensor(0.2242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1771/4339]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1772/4339]: training_loss: tensor(0.5897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1773/4339]: training_loss: tensor(0.1616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1774/4339]: training_loss: tensor(0.2554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1775/4339]: training_loss: tensor(0.4302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1776/4339]: training_loss: tensor(0.2709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1777/4339]: training_loss: tensor(0.5803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1778/4339]: training_loss: tensor(0.1274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1779/4339]: training_loss: tensor(0.4721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1780/4339]: training_loss: tensor(0.3238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1781/4339]: training_loss: tensor(0.4591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1782/4339]: training_loss: tensor(0.2159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1783/4339]: training_loss: tensor(0.4137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1784/4339]: training_loss: tensor(0.1400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1785/4339]: training_loss: tensor(0.4215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1786/4339]: training_loss: tensor(0.1428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1787/4339]: training_loss: tensor(0.2916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1788/4339]: training_loss: tensor(0.3293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1789/4339]: training_loss: tensor(0.2225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1790/4339]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1791/4339]: training_loss: tensor(0.6064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1792/4339]: training_loss: tensor(0.1504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1793/4339]: training_loss: tensor(0.3320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1794/4339]: training_loss: tensor(0.2948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1795/4339]: training_loss: tensor(0.2141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1796/4339]: training_loss: tensor(0.3304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1797/4339]: training_loss: tensor(0.3723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1798/4339]: training_loss: tensor(0.3271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1799/4339]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1800/4339]: training_loss: tensor(0.4150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1801/4339]: training_loss: tensor(0.1492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1802/4339]: training_loss: tensor(0.3623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1803/4339]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1804/4339]: training_loss: tensor(0.2831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1805/4339]: training_loss: tensor(0.3776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1806/4339]: training_loss: tensor(0.2154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1807/4339]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1808/4339]: training_loss: tensor(0.3860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1809/4339]: training_loss: tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1810/4339]: training_loss: tensor(0.1407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1811/4339]: training_loss: tensor(0.4492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1812/4339]: training_loss: tensor(0.5844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1813/4339]: training_loss: tensor(0.1353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1814/4339]: training_loss: tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1815/4339]: training_loss: tensor(0.4497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1816/4339]: training_loss: tensor(0.1723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1817/4339]: training_loss: tensor(0.3594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1818/4339]: training_loss: tensor(0.3867, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1819/4339]: training_loss: tensor(0.5740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1820/4339]: training_loss: tensor(0.4332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1821/4339]: training_loss: tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1822/4339]: training_loss: tensor(0.3961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1823/4339]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1824/4339]: training_loss: tensor(0.2779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1825/4339]: training_loss: tensor(0.3035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1826/4339]: training_loss: tensor(0.7149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1827/4339]: training_loss: tensor(0.1275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1828/4339]: training_loss: tensor(0.3111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1829/4339]: training_loss: tensor(0.2434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1830/4339]: training_loss: tensor(0.3022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1831/4339]: training_loss: tensor(0.3131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1832/4339]: training_loss: tensor(0.1342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1833/4339]: training_loss: tensor(0.3363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1834/4339]: training_loss: tensor(0.3755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1835/4339]: training_loss: tensor(0.3608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1836/4339]: training_loss: tensor(0.1845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1837/4339]: training_loss: tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1838/4339]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1839/4339]: training_loss: tensor(0.2170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1840/4339]: training_loss: tensor(0.2776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1841/4339]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1842/4339]: training_loss: tensor(0.5230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1843/4339]: training_loss: tensor(0.5224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1844/4339]: training_loss: tensor(0.2605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1845/4339]: training_loss: tensor(0.1897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1846/4339]: training_loss: tensor(0.3696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1847/4339]: training_loss: tensor(0.3656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1848/4339]: training_loss: tensor(0.2323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1849/4339]: training_loss: tensor(0.2942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1850/4339]: training_loss: tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1851/4339]: training_loss: tensor(0.5742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1852/4339]: training_loss: tensor(0.5175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1853/4339]: training_loss: tensor(0.2809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1854/4339]: training_loss: tensor(0.3877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1855/4339]: training_loss: tensor(0.3101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1856/4339]: training_loss: tensor(0.7119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1857/4339]: training_loss: tensor(0.3658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1858/4339]: training_loss: tensor(0.3833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1859/4339]: training_loss: tensor(0.2822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1860/4339]: training_loss: tensor(0.4374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1861/4339]: training_loss: tensor(0.2821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1862/4339]: training_loss: tensor(0.4219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1863/4339]: training_loss: tensor(0.2452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1864/4339]: training_loss: tensor(0.2978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1865/4339]: training_loss: tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1866/4339]: training_loss: tensor(0.2232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1867/4339]: training_loss: tensor(0.1715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1868/4339]: training_loss: tensor(0.6461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1869/4339]: training_loss: tensor(0.3004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1870/4339]: training_loss: tensor(0.2240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1871/4339]: training_loss: tensor(0.2513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1872/4339]: training_loss: tensor(0.2279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1873/4339]: training_loss: tensor(0.3802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1874/4339]: training_loss: tensor(0.2467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1875/4339]: training_loss: tensor(0.5681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1876/4339]: training_loss: tensor(0.2274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1877/4339]: training_loss: tensor(0.1526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1878/4339]: training_loss: tensor(0.5084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1879/4339]: training_loss: tensor(0.1538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1880/4339]: training_loss: tensor(0.2459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1881/4339]: training_loss: tensor(0.2296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1882/4339]: training_loss: tensor(0.2882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1883/4339]: training_loss: tensor(0.6851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1884/4339]: training_loss: tensor(0.3527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1885/4339]: training_loss: tensor(0.2901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1886/4339]: training_loss: tensor(0.1168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1887/4339]: training_loss: tensor(0.4190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1888/4339]: training_loss: tensor(0.2377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1889/4339]: training_loss: tensor(0.4023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1890/4339]: training_loss: tensor(0.3375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1891/4339]: training_loss: tensor(0.2184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1892/4339]: training_loss: tensor(0.2774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1893/4339]: training_loss: tensor(0.2654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1894/4339]: training_loss: tensor(0.2532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1895/4339]: training_loss: tensor(0.2568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1896/4339]: training_loss: tensor(0.3141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1897/4339]: training_loss: tensor(0.1663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1898/4339]: training_loss: tensor(0.4406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1899/4339]: training_loss: tensor(0.2057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1900/4339]: training_loss: tensor(0.4212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1901/4339]: training_loss: tensor(0.2635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1902/4339]: training_loss: tensor(0.1645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1903/4339]: training_loss: tensor(0.2658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1904/4339]: training_loss: tensor(0.1577, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1905/4339]: training_loss: tensor(0.2163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1906/4339]: training_loss: tensor(0.2831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1907/4339]: training_loss: tensor(0.4091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1908/4339]: training_loss: tensor(0.2024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1909/4339]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1910/4339]: training_loss: tensor(0.4419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1911/4339]: training_loss: tensor(0.1972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1912/4339]: training_loss: tensor(0.3490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1913/4339]: training_loss: tensor(0.1449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1914/4339]: training_loss: tensor(0.2117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1915/4339]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1916/4339]: training_loss: tensor(0.4152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1917/4339]: training_loss: tensor(0.2724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1918/4339]: training_loss: tensor(0.7843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1919/4339]: training_loss: tensor(0.3851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1920/4339]: training_loss: tensor(0.1598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1921/4339]: training_loss: tensor(0.3265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1922/4339]: training_loss: tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1923/4339]: training_loss: tensor(0.3666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1924/4339]: training_loss: tensor(0.4179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1925/4339]: training_loss: tensor(0.5890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1926/4339]: training_loss: tensor(0.2284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1927/4339]: training_loss: tensor(0.2432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1928/4339]: training_loss: tensor(0.4220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1929/4339]: training_loss: tensor(0.2489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1930/4339]: training_loss: tensor(0.2247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1931/4339]: training_loss: tensor(0.3768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1932/4339]: training_loss: tensor(0.3138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1933/4339]: training_loss: tensor(0.3252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1934/4339]: training_loss: tensor(0.3119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1935/4339]: training_loss: tensor(0.3128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1936/4339]: training_loss: tensor(0.2051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1937/4339]: training_loss: tensor(0.5420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1938/4339]: training_loss: tensor(0.6460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1939/4339]: training_loss: tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1940/4339]: training_loss: tensor(0.2714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1941/4339]: training_loss: tensor(0.4822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1942/4339]: training_loss: tensor(0.3279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1943/4339]: training_loss: tensor(0.2019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1944/4339]: training_loss: tensor(0.2184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1945/4339]: training_loss: tensor(0.3241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1946/4339]: training_loss: tensor(0.2214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1947/4339]: training_loss: tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1948/4339]: training_loss: tensor(0.5493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1949/4339]: training_loss: tensor(0.4395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1950/4339]: training_loss: tensor(0.2436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1951/4339]: training_loss: tensor(0.2977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1952/4339]: training_loss: tensor(0.3838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1953/4339]: training_loss: tensor(0.3431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1954/4339]: training_loss: tensor(0.2421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1955/4339]: training_loss: tensor(0.5194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1956/4339]: training_loss: tensor(0.4196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1957/4339]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1958/4339]: training_loss: tensor(0.2013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1959/4339]: training_loss: tensor(0.4206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1960/4339]: training_loss: tensor(0.3122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1961/4339]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1962/4339]: training_loss: tensor(0.2476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1963/4339]: training_loss: tensor(0.2918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1964/4339]: training_loss: tensor(0.2695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1965/4339]: training_loss: tensor(0.2089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1966/4339]: training_loss: tensor(0.6033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1967/4339]: training_loss: tensor(0.4779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1968/4339]: training_loss: tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1969/4339]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1970/4339]: training_loss: tensor(0.2132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1971/4339]: training_loss: tensor(0.2423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1972/4339]: training_loss: tensor(0.6943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1973/4339]: training_loss: tensor(0.3042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1974/4339]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1975/4339]: training_loss: tensor(0.2720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1976/4339]: training_loss: tensor(0.3830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1977/4339]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1978/4339]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1979/4339]: training_loss: tensor(0.2990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1980/4339]: training_loss: tensor(0.1322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1981/4339]: training_loss: tensor(0.5680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1982/4339]: training_loss: tensor(0.1593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1983/4339]: training_loss: tensor(0.2087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1984/4339]: training_loss: tensor(0.3324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1985/4339]: training_loss: tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1986/4339]: training_loss: tensor(0.3661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1987/4339]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1988/4339]: training_loss: tensor(0.1587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1989/4339]: training_loss: tensor(0.3886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1990/4339]: training_loss: tensor(0.5795, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1991/4339]: training_loss: tensor(0.3728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1992/4339]: training_loss: tensor(0.3269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1993/4339]: training_loss: tensor(0.2923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1994/4339]: training_loss: tensor(0.2383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1995/4339]: training_loss: tensor(0.6782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1996/4339]: training_loss: tensor(0.2137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1997/4339]: training_loss: tensor(0.3456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1998/4339]: training_loss: tensor(0.1865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1999/4339]: training_loss: tensor(0.4142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2000/4339]: training_loss: tensor(0.3498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2001/4339]: training_loss: tensor(0.1618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2002/4339]: training_loss: tensor(0.3187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2003/4339]: training_loss: tensor(0.3259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2004/4339]: training_loss: tensor(0.2591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2005/4339]: training_loss: tensor(0.1721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2006/4339]: training_loss: tensor(0.2611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2007/4339]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2008/4339]: training_loss: tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2009/4339]: training_loss: tensor(0.3338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2010/4339]: training_loss: tensor(0.3482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2011/4339]: training_loss: tensor(0.3256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2012/4339]: training_loss: tensor(0.5530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2013/4339]: training_loss: tensor(0.1192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2014/4339]: training_loss: tensor(0.2611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2015/4339]: training_loss: tensor(0.1479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2016/4339]: training_loss: tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2017/4339]: training_loss: tensor(0.4951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2018/4339]: training_loss: tensor(0.1559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2019/4339]: training_loss: tensor(0.1674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2020/4339]: training_loss: tensor(0.3691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2021/4339]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2022/4339]: training_loss: tensor(0.5915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2023/4339]: training_loss: tensor(0.1873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2024/4339]: training_loss: tensor(0.3355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2025/4339]: training_loss: tensor(0.5491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2026/4339]: training_loss: tensor(0.2620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2027/4339]: training_loss: tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2028/4339]: training_loss: tensor(0.2883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2029/4339]: training_loss: tensor(0.1164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2030/4339]: training_loss: tensor(0.1642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2031/4339]: training_loss: tensor(0.4075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2032/4339]: training_loss: tensor(0.4206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2033/4339]: training_loss: tensor(0.1959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2034/4339]: training_loss: tensor(0.2361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2035/4339]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2036/4339]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2037/4339]: training_loss: tensor(0.6151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2038/4339]: training_loss: tensor(0.2061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2039/4339]: training_loss: tensor(0.2730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2040/4339]: training_loss: tensor(0.2909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2041/4339]: training_loss: tensor(0.4662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2042/4339]: training_loss: tensor(0.3756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2043/4339]: training_loss: tensor(0.2806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2044/4339]: training_loss: tensor(0.2981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2045/4339]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2046/4339]: training_loss: tensor(0.2453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2047/4339]: training_loss: tensor(0.5056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2048/4339]: training_loss: tensor(0.2904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2049/4339]: training_loss: tensor(0.4877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2050/4339]: training_loss: tensor(0.1640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2051/4339]: training_loss: tensor(0.4099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2052/4339]: training_loss: tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2053/4339]: training_loss: tensor(0.5336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2054/4339]: training_loss: tensor(0.1881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2055/4339]: training_loss: tensor(0.4017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2056/4339]: training_loss: tensor(0.5684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2057/4339]: training_loss: tensor(0.2399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2058/4339]: training_loss: tensor(0.3291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2059/4339]: training_loss: tensor(0.2929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2060/4339]: training_loss: tensor(0.1557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2061/4339]: training_loss: tensor(0.4322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2062/4339]: training_loss: tensor(0.4120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2063/4339]: training_loss: tensor(0.2800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2064/4339]: training_loss: tensor(0.4059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2065/4339]: training_loss: tensor(0.4464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2066/4339]: training_loss: tensor(0.5437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2067/4339]: training_loss: tensor(0.2329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2068/4339]: training_loss: tensor(0.2177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2069/4339]: training_loss: tensor(0.1919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2070/4339]: training_loss: tensor(0.2957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2071/4339]: training_loss: tensor(0.2148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2072/4339]: training_loss: tensor(0.2757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2073/4339]: training_loss: tensor(0.3474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2074/4339]: training_loss: tensor(0.4368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2075/4339]: training_loss: tensor(0.4052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2076/4339]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2077/4339]: training_loss: tensor(0.2997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2078/4339]: training_loss: tensor(0.2393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2079/4339]: training_loss: tensor(0.1733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2080/4339]: training_loss: tensor(0.2314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2081/4339]: training_loss: tensor(0.3392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2082/4339]: training_loss: tensor(0.5441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2083/4339]: training_loss: tensor(0.5929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2084/4339]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2085/4339]: training_loss: tensor(0.3369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2086/4339]: training_loss: tensor(0.0683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2087/4339]: training_loss: tensor(0.3703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2088/4339]: training_loss: tensor(0.2452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2089/4339]: training_loss: tensor(0.4283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2090/4339]: training_loss: tensor(0.1217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2091/4339]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2092/4339]: training_loss: tensor(0.1978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2093/4339]: training_loss: tensor(0.5052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2094/4339]: training_loss: tensor(0.3814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2095/4339]: training_loss: tensor(0.1281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2096/4339]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2097/4339]: training_loss: tensor(0.2865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2098/4339]: training_loss: tensor(0.1527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2099/4339]: training_loss: tensor(0.2939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2100/4339]: training_loss: tensor(0.3231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2101/4339]: training_loss: tensor(0.4022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2102/4339]: training_loss: tensor(0.1571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2103/4339]: training_loss: tensor(0.3011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2104/4339]: training_loss: tensor(0.3084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2105/4339]: training_loss: tensor(0.3651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2106/4339]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2107/4339]: training_loss: tensor(0.2525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2108/4339]: training_loss: tensor(0.4661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2109/4339]: training_loss: tensor(0.4087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2110/4339]: training_loss: tensor(0.2735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2111/4339]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2112/4339]: training_loss: tensor(0.8084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2113/4339]: training_loss: tensor(0.4150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2114/4339]: training_loss: tensor(0.3350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2115/4339]: training_loss: tensor(0.3322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2116/4339]: training_loss: tensor(0.4504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2117/4339]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2118/4339]: training_loss: tensor(0.3648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2119/4339]: training_loss: tensor(0.1892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2120/4339]: training_loss: tensor(0.2995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2121/4339]: training_loss: tensor(0.4090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2122/4339]: training_loss: tensor(0.2679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2123/4339]: training_loss: tensor(0.4006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2124/4339]: training_loss: tensor(0.3076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2125/4339]: training_loss: tensor(0.1849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2126/4339]: training_loss: tensor(0.5180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2127/4339]: training_loss: tensor(0.6330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2128/4339]: training_loss: tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2129/4339]: training_loss: tensor(0.2916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2130/4339]: training_loss: tensor(0.5142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2131/4339]: training_loss: tensor(0.2682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2132/4339]: training_loss: tensor(0.5209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2133/4339]: training_loss: tensor(0.2268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2134/4339]: training_loss: tensor(0.1898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2135/4339]: training_loss: tensor(0.1474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2136/4339]: training_loss: tensor(0.4974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2137/4339]: training_loss: tensor(0.3903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2138/4339]: training_loss: tensor(0.4472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2139/4339]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2140/4339]: training_loss: tensor(0.2248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2141/4339]: training_loss: tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2142/4339]: training_loss: tensor(0.3820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2143/4339]: training_loss: tensor(0.2487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2144/4339]: training_loss: tensor(0.2090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2145/4339]: training_loss: tensor(0.3166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2146/4339]: training_loss: tensor(0.3932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2147/4339]: training_loss: tensor(0.3489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2148/4339]: training_loss: tensor(0.1517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2149/4339]: training_loss: tensor(0.1185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2150/4339]: training_loss: tensor(0.2379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2151/4339]: training_loss: tensor(0.2798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2152/4339]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2153/4339]: training_loss: tensor(0.9889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2154/4339]: training_loss: tensor(0.5178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2155/4339]: training_loss: tensor(0.1300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2156/4339]: training_loss: tensor(0.2822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2157/4339]: training_loss: tensor(0.3744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2158/4339]: training_loss: tensor(0.1401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2159/4339]: training_loss: tensor(0.4388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2160/4339]: training_loss: tensor(0.6194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2161/4339]: training_loss: tensor(0.6053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2162/4339]: training_loss: tensor(0.3854, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2163/4339]: training_loss: tensor(0.3576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2164/4339]: training_loss: tensor(0.6483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2165/4339]: training_loss: tensor(0.2721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2166/4339]: training_loss: tensor(0.2089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2167/4339]: training_loss: tensor(0.2934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2168/4339]: training_loss: tensor(0.3180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2169/4339]: training_loss: tensor(0.2180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2170/4339]: training_loss: tensor(0.2115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [1/5], Step [2170/21700], Train Loss: 0.3253, Valid Loss: 0.3248\n",
      "Model saved to ==> Model/model.pt\n",
      "Model saved to ==> Model/metrics.pt\n",
      "batch_no [2171/4339]: training_loss: tensor(0.2462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2172/4339]: training_loss: tensor(0.3455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2173/4339]: training_loss: tensor(0.3882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2174/4339]: training_loss: tensor(0.2854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2175/4339]: training_loss: tensor(0.2413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2176/4339]: training_loss: tensor(0.2970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2177/4339]: training_loss: tensor(0.1131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2178/4339]: training_loss: tensor(0.4463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2179/4339]: training_loss: tensor(0.1601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2180/4339]: training_loss: tensor(0.3884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2181/4339]: training_loss: tensor(0.2156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2182/4339]: training_loss: tensor(0.1085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2183/4339]: training_loss: tensor(0.5331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2184/4339]: training_loss: tensor(0.4751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2185/4339]: training_loss: tensor(0.1938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2186/4339]: training_loss: tensor(0.5357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2187/4339]: training_loss: tensor(0.3916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2188/4339]: training_loss: tensor(0.6588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2189/4339]: training_loss: tensor(0.1810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2190/4339]: training_loss: tensor(0.7362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2191/4339]: training_loss: tensor(0.2647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2192/4339]: training_loss: tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2193/4339]: training_loss: tensor(0.4044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2194/4339]: training_loss: tensor(0.4109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2195/4339]: training_loss: tensor(0.2612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2196/4339]: training_loss: tensor(0.4590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2197/4339]: training_loss: tensor(0.3244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2198/4339]: training_loss: tensor(0.2986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2199/4339]: training_loss: tensor(0.2173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2200/4339]: training_loss: tensor(0.2609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2201/4339]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2202/4339]: training_loss: tensor(0.1229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2203/4339]: training_loss: tensor(0.8596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2204/4339]: training_loss: tensor(0.3111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2205/4339]: training_loss: tensor(0.3140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2206/4339]: training_loss: tensor(0.3339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2207/4339]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2208/4339]: training_loss: tensor(0.4160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2209/4339]: training_loss: tensor(0.3633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2210/4339]: training_loss: tensor(0.1969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2211/4339]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2212/4339]: training_loss: tensor(0.4713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2213/4339]: training_loss: tensor(0.2312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2214/4339]: training_loss: tensor(0.1197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2215/4339]: training_loss: tensor(0.6112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2216/4339]: training_loss: tensor(0.3817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2217/4339]: training_loss: tensor(0.1482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2218/4339]: training_loss: tensor(0.2146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2219/4339]: training_loss: tensor(0.4690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2220/4339]: training_loss: tensor(0.5162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2221/4339]: training_loss: tensor(0.5261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2222/4339]: training_loss: tensor(0.1428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2223/4339]: training_loss: tensor(0.5286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2224/4339]: training_loss: tensor(0.2580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2225/4339]: training_loss: tensor(0.3074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2226/4339]: training_loss: tensor(0.5272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2227/4339]: training_loss: tensor(0.4059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2228/4339]: training_loss: tensor(0.1677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2229/4339]: training_loss: tensor(0.3130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2230/4339]: training_loss: tensor(0.2615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2231/4339]: training_loss: tensor(0.1920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2232/4339]: training_loss: tensor(0.3691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2233/4339]: training_loss: tensor(0.3143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2234/4339]: training_loss: tensor(0.4204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2235/4339]: training_loss: tensor(0.4422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2236/4339]: training_loss: tensor(0.5350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2237/4339]: training_loss: tensor(0.1173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2238/4339]: training_loss: tensor(0.3574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2239/4339]: training_loss: tensor(0.2474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2240/4339]: training_loss: tensor(0.1603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2241/4339]: training_loss: tensor(0.4558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2242/4339]: training_loss: tensor(0.4011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2243/4339]: training_loss: tensor(0.3673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2244/4339]: training_loss: tensor(0.3502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2245/4339]: training_loss: tensor(0.1431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2246/4339]: training_loss: tensor(0.3725, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2247/4339]: training_loss: tensor(0.4556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2248/4339]: training_loss: tensor(0.4472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2249/4339]: training_loss: tensor(0.2898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2250/4339]: training_loss: tensor(0.6993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2251/4339]: training_loss: tensor(0.2949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2252/4339]: training_loss: tensor(0.2648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2253/4339]: training_loss: tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2254/4339]: training_loss: tensor(0.3225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2255/4339]: training_loss: tensor(0.2788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2256/4339]: training_loss: tensor(0.4884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2257/4339]: training_loss: tensor(0.1617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2258/4339]: training_loss: tensor(0.1510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2259/4339]: training_loss: tensor(0.3820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2260/4339]: training_loss: tensor(0.2747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2261/4339]: training_loss: tensor(0.3991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2262/4339]: training_loss: tensor(0.2706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2263/4339]: training_loss: tensor(0.2042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2264/4339]: training_loss: tensor(0.1633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2265/4339]: training_loss: tensor(0.1435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2266/4339]: training_loss: tensor(0.3152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2267/4339]: training_loss: tensor(0.3071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2268/4339]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2269/4339]: training_loss: tensor(0.2476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2270/4339]: training_loss: tensor(0.4892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2271/4339]: training_loss: tensor(0.4933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2272/4339]: training_loss: tensor(0.5485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2273/4339]: training_loss: tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2274/4339]: training_loss: tensor(0.6750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2275/4339]: training_loss: tensor(0.3458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2276/4339]: training_loss: tensor(0.1299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2277/4339]: training_loss: tensor(0.3183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2278/4339]: training_loss: tensor(0.3050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2279/4339]: training_loss: tensor(0.2638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2280/4339]: training_loss: tensor(0.2820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2281/4339]: training_loss: tensor(0.2373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2282/4339]: training_loss: tensor(0.2221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2283/4339]: training_loss: tensor(0.3791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2284/4339]: training_loss: tensor(0.3349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2285/4339]: training_loss: tensor(0.3443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2286/4339]: training_loss: tensor(0.2143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2287/4339]: training_loss: tensor(0.1572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2288/4339]: training_loss: tensor(0.3553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2289/4339]: training_loss: tensor(0.6374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2290/4339]: training_loss: tensor(0.3059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2291/4339]: training_loss: tensor(0.2710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2292/4339]: training_loss: tensor(0.2972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2293/4339]: training_loss: tensor(0.4073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2294/4339]: training_loss: tensor(0.2578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2295/4339]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2296/4339]: training_loss: tensor(0.2799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2297/4339]: training_loss: tensor(0.2394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2298/4339]: training_loss: tensor(0.5241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2299/4339]: training_loss: tensor(0.2616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2300/4339]: training_loss: tensor(0.4227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2301/4339]: training_loss: tensor(0.2984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2302/4339]: training_loss: tensor(0.1468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2303/4339]: training_loss: tensor(0.4559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2304/4339]: training_loss: tensor(0.5443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2305/4339]: training_loss: tensor(0.4530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2306/4339]: training_loss: tensor(0.2963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2307/4339]: training_loss: tensor(0.3368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2308/4339]: training_loss: tensor(0.6254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2309/4339]: training_loss: tensor(0.5180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2310/4339]: training_loss: tensor(0.2148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2311/4339]: training_loss: tensor(0.5543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2312/4339]: training_loss: tensor(0.6329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2313/4339]: training_loss: tensor(0.2317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2314/4339]: training_loss: tensor(0.2654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2315/4339]: training_loss: tensor(0.2457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2316/4339]: training_loss: tensor(0.4057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2317/4339]: training_loss: tensor(0.1749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2318/4339]: training_loss: tensor(0.3795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2319/4339]: training_loss: tensor(0.1966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2320/4339]: training_loss: tensor(0.3029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2321/4339]: training_loss: tensor(0.4457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2322/4339]: training_loss: tensor(0.3263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2323/4339]: training_loss: tensor(0.3343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2324/4339]: training_loss: tensor(0.2442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2325/4339]: training_loss: tensor(0.2758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2326/4339]: training_loss: tensor(0.3627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2327/4339]: training_loss: tensor(0.2841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2328/4339]: training_loss: tensor(0.5214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2329/4339]: training_loss: tensor(0.5324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2330/4339]: training_loss: tensor(0.4203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2331/4339]: training_loss: tensor(0.2538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2332/4339]: training_loss: tensor(0.3615, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2333/4339]: training_loss: tensor(0.3046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2334/4339]: training_loss: tensor(0.1564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2335/4339]: training_loss: tensor(0.1674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2336/4339]: training_loss: tensor(0.4378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2337/4339]: training_loss: tensor(0.2998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2338/4339]: training_loss: tensor(0.3065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2339/4339]: training_loss: tensor(0.5611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2340/4339]: training_loss: tensor(0.5240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2341/4339]: training_loss: tensor(0.2665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2342/4339]: training_loss: tensor(0.4308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2343/4339]: training_loss: tensor(0.4237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2344/4339]: training_loss: tensor(0.2240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2345/4339]: training_loss: tensor(0.2833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2346/4339]: training_loss: tensor(0.3747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2347/4339]: training_loss: tensor(0.5728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2348/4339]: training_loss: tensor(0.2930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2349/4339]: training_loss: tensor(0.3275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2350/4339]: training_loss: tensor(0.1223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2351/4339]: training_loss: tensor(0.3406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2352/4339]: training_loss: tensor(0.2764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2353/4339]: training_loss: tensor(0.2267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2354/4339]: training_loss: tensor(0.2078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2355/4339]: training_loss: tensor(0.4039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2356/4339]: training_loss: tensor(0.2190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2357/4339]: training_loss: tensor(0.6708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2358/4339]: training_loss: tensor(0.6086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2359/4339]: training_loss: tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2360/4339]: training_loss: tensor(0.2574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2361/4339]: training_loss: tensor(0.1644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2362/4339]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2363/4339]: training_loss: tensor(0.3301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2364/4339]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2365/4339]: training_loss: tensor(0.2622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2366/4339]: training_loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2367/4339]: training_loss: tensor(0.1105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2368/4339]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2369/4339]: training_loss: tensor(0.2340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2370/4339]: training_loss: tensor(0.1263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2371/4339]: training_loss: tensor(0.3892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2372/4339]: training_loss: tensor(0.8007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2373/4339]: training_loss: tensor(0.6282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2374/4339]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2375/4339]: training_loss: tensor(0.3127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2376/4339]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2377/4339]: training_loss: tensor(0.5031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2378/4339]: training_loss: tensor(0.7557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2379/4339]: training_loss: tensor(0.2129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2380/4339]: training_loss: tensor(0.1591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2381/4339]: training_loss: tensor(0.1506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2382/4339]: training_loss: tensor(0.3892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2383/4339]: training_loss: tensor(0.3710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2384/4339]: training_loss: tensor(0.5463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2385/4339]: training_loss: tensor(0.1765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2386/4339]: training_loss: tensor(0.4211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2387/4339]: training_loss: tensor(0.1182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2388/4339]: training_loss: tensor(0.2419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2389/4339]: training_loss: tensor(0.1940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2390/4339]: training_loss: tensor(0.2335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2391/4339]: training_loss: tensor(0.3355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2392/4339]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2393/4339]: training_loss: tensor(0.1262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2394/4339]: training_loss: tensor(0.7228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2395/4339]: training_loss: tensor(0.1951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2396/4339]: training_loss: tensor(0.2760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2397/4339]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2398/4339]: training_loss: tensor(0.4004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2399/4339]: training_loss: tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2400/4339]: training_loss: tensor(0.1836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2401/4339]: training_loss: tensor(0.3789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2402/4339]: training_loss: tensor(0.1825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2403/4339]: training_loss: tensor(0.5644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2404/4339]: training_loss: tensor(0.2063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2405/4339]: training_loss: tensor(0.2826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2406/4339]: training_loss: tensor(0.2440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2407/4339]: training_loss: tensor(0.1398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2408/4339]: training_loss: tensor(0.4880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2409/4339]: training_loss: tensor(0.6170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2410/4339]: training_loss: tensor(0.2410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2411/4339]: training_loss: tensor(0.5303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2412/4339]: training_loss: tensor(0.1282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2413/4339]: training_loss: tensor(0.3705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2414/4339]: training_loss: tensor(0.1854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2415/4339]: training_loss: tensor(0.4202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2416/4339]: training_loss: tensor(0.2077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2417/4339]: training_loss: tensor(0.4454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2418/4339]: training_loss: tensor(0.3589, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2419/4339]: training_loss: tensor(0.3119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2420/4339]: training_loss: tensor(0.2386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2421/4339]: training_loss: tensor(0.2403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2422/4339]: training_loss: tensor(0.2651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2423/4339]: training_loss: tensor(0.2003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2424/4339]: training_loss: tensor(0.3730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2425/4339]: training_loss: tensor(0.6185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2426/4339]: training_loss: tensor(0.3029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2427/4339]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2428/4339]: training_loss: tensor(0.1716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2429/4339]: training_loss: tensor(0.2665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2430/4339]: training_loss: tensor(0.3842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2431/4339]: training_loss: tensor(0.1475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2432/4339]: training_loss: tensor(0.2223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2433/4339]: training_loss: tensor(0.3438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2434/4339]: training_loss: tensor(0.4168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2435/4339]: training_loss: tensor(0.3220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2436/4339]: training_loss: tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2437/4339]: training_loss: tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2438/4339]: training_loss: tensor(0.4324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2439/4339]: training_loss: tensor(0.7082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2440/4339]: training_loss: tensor(0.8339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2441/4339]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2442/4339]: training_loss: tensor(0.3500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2443/4339]: training_loss: tensor(0.3355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2444/4339]: training_loss: tensor(0.4197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2445/4339]: training_loss: tensor(0.5591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2446/4339]: training_loss: tensor(0.2737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2447/4339]: training_loss: tensor(0.1675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2448/4339]: training_loss: tensor(0.2743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2449/4339]: training_loss: tensor(0.4865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2450/4339]: training_loss: tensor(0.4931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2451/4339]: training_loss: tensor(0.2792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2452/4339]: training_loss: tensor(0.3070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2453/4339]: training_loss: tensor(0.3564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2454/4339]: training_loss: tensor(0.2106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2455/4339]: training_loss: tensor(0.4297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2456/4339]: training_loss: tensor(0.3413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2457/4339]: training_loss: tensor(0.3615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2458/4339]: training_loss: tensor(0.4095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2459/4339]: training_loss: tensor(0.2482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2460/4339]: training_loss: tensor(0.1968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2461/4339]: training_loss: tensor(0.2338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2462/4339]: training_loss: tensor(0.1653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2463/4339]: training_loss: tensor(0.2963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2464/4339]: training_loss: tensor(0.3034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2465/4339]: training_loss: tensor(0.1410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2466/4339]: training_loss: tensor(0.2562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2467/4339]: training_loss: tensor(0.6342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2468/4339]: training_loss: tensor(0.4419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2469/4339]: training_loss: tensor(0.4111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2470/4339]: training_loss: tensor(0.7360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2471/4339]: training_loss: tensor(0.3953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2472/4339]: training_loss: tensor(0.1832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2473/4339]: training_loss: tensor(0.2897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2474/4339]: training_loss: tensor(0.3169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2475/4339]: training_loss: tensor(0.4769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2476/4339]: training_loss: tensor(0.3020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2477/4339]: training_loss: tensor(0.3824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2478/4339]: training_loss: tensor(0.2263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2479/4339]: training_loss: tensor(0.2175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2480/4339]: training_loss: tensor(0.4985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2481/4339]: training_loss: tensor(0.2913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2482/4339]: training_loss: tensor(0.2300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2483/4339]: training_loss: tensor(0.2882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2484/4339]: training_loss: tensor(0.3360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2485/4339]: training_loss: tensor(0.4856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2486/4339]: training_loss: tensor(0.2803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2487/4339]: training_loss: tensor(0.4540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2488/4339]: training_loss: tensor(0.2906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2489/4339]: training_loss: tensor(0.3928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2490/4339]: training_loss: tensor(0.2037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2491/4339]: training_loss: tensor(0.2180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2492/4339]: training_loss: tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2493/4339]: training_loss: tensor(0.4830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2494/4339]: training_loss: tensor(0.4213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2495/4339]: training_loss: tensor(0.2410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2496/4339]: training_loss: tensor(0.1927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2497/4339]: training_loss: tensor(0.6081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2498/4339]: training_loss: tensor(0.4161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2499/4339]: training_loss: tensor(0.2541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2500/4339]: training_loss: tensor(0.1794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2501/4339]: training_loss: tensor(0.1465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2502/4339]: training_loss: tensor(0.2373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2503/4339]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2504/4339]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2505/4339]: training_loss: tensor(0.2501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2506/4339]: training_loss: tensor(0.1341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2507/4339]: training_loss: tensor(0.3277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2508/4339]: training_loss: tensor(0.3460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2509/4339]: training_loss: tensor(0.2113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2510/4339]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2511/4339]: training_loss: tensor(0.1283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2512/4339]: training_loss: tensor(0.2541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2513/4339]: training_loss: tensor(0.1290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2514/4339]: training_loss: tensor(0.2162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2515/4339]: training_loss: tensor(0.1644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2516/4339]: training_loss: tensor(0.5885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2517/4339]: training_loss: tensor(0.2315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2518/4339]: training_loss: tensor(0.2133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2519/4339]: training_loss: tensor(0.1645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2520/4339]: training_loss: tensor(0.1177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2521/4339]: training_loss: tensor(0.2530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2522/4339]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2523/4339]: training_loss: tensor(0.3718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2524/4339]: training_loss: tensor(0.3740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2525/4339]: training_loss: tensor(0.3467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2526/4339]: training_loss: tensor(0.2837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2527/4339]: training_loss: tensor(0.2614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2528/4339]: training_loss: tensor(0.1291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2529/4339]: training_loss: tensor(0.2074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2530/4339]: training_loss: tensor(0.4987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2531/4339]: training_loss: tensor(0.4823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2532/4339]: training_loss: tensor(0.6570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2533/4339]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2534/4339]: training_loss: tensor(0.1450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2535/4339]: training_loss: tensor(0.6245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2536/4339]: training_loss: tensor(0.1635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2537/4339]: training_loss: tensor(0.3188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2538/4339]: training_loss: tensor(0.1326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2539/4339]: training_loss: tensor(0.2801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2540/4339]: training_loss: tensor(0.2471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2541/4339]: training_loss: tensor(0.1924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2542/4339]: training_loss: tensor(0.3375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2543/4339]: training_loss: tensor(0.5886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2544/4339]: training_loss: tensor(0.1304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2545/4339]: training_loss: tensor(0.2135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2546/4339]: training_loss: tensor(0.1867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2547/4339]: training_loss: tensor(0.3221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2548/4339]: training_loss: tensor(0.2456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2549/4339]: training_loss: tensor(0.2815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2550/4339]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2551/4339]: training_loss: tensor(0.4979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2552/4339]: training_loss: tensor(0.2310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2553/4339]: training_loss: tensor(0.3330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2554/4339]: training_loss: tensor(0.6159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2555/4339]: training_loss: tensor(0.1622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2556/4339]: training_loss: tensor(0.1780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2557/4339]: training_loss: tensor(0.1180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2558/4339]: training_loss: tensor(0.5302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2559/4339]: training_loss: tensor(0.3482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2560/4339]: training_loss: tensor(0.3653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2561/4339]: training_loss: tensor(0.3276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2562/4339]: training_loss: tensor(0.4589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2563/4339]: training_loss: tensor(0.2621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2564/4339]: training_loss: tensor(0.2509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2565/4339]: training_loss: tensor(0.2513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2566/4339]: training_loss: tensor(0.5318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2567/4339]: training_loss: tensor(0.2256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2568/4339]: training_loss: tensor(0.3321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2569/4339]: training_loss: tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2570/4339]: training_loss: tensor(0.4867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2571/4339]: training_loss: tensor(0.3186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2572/4339]: training_loss: tensor(0.3097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2573/4339]: training_loss: tensor(0.1394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2574/4339]: training_loss: tensor(0.5470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2575/4339]: training_loss: tensor(0.3295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2576/4339]: training_loss: tensor(0.2681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2577/4339]: training_loss: tensor(0.2426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2578/4339]: training_loss: tensor(0.5770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2579/4339]: training_loss: tensor(0.2067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2580/4339]: training_loss: tensor(0.3975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2581/4339]: training_loss: tensor(0.1924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2582/4339]: training_loss: tensor(0.1219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2583/4339]: training_loss: tensor(0.2578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2584/4339]: training_loss: tensor(0.1608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2585/4339]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2586/4339]: training_loss: tensor(0.6491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2587/4339]: training_loss: tensor(0.0543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2588/4339]: training_loss: tensor(0.2046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2589/4339]: training_loss: tensor(0.5734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2590/4339]: training_loss: tensor(0.3073, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2591/4339]: training_loss: tensor(0.1190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2592/4339]: training_loss: tensor(0.3483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2593/4339]: training_loss: tensor(0.1970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2594/4339]: training_loss: tensor(0.2295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2595/4339]: training_loss: tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2596/4339]: training_loss: tensor(0.5326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2597/4339]: training_loss: tensor(0.1560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2598/4339]: training_loss: tensor(0.3306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2599/4339]: training_loss: tensor(0.1276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2600/4339]: training_loss: tensor(0.7024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2601/4339]: training_loss: tensor(0.1220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2602/4339]: training_loss: tensor(0.5606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2603/4339]: training_loss: tensor(0.3788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2604/4339]: training_loss: tensor(0.2031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2605/4339]: training_loss: tensor(0.3830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2606/4339]: training_loss: tensor(0.2387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2607/4339]: training_loss: tensor(0.4627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2608/4339]: training_loss: tensor(0.4405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2609/4339]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2610/4339]: training_loss: tensor(0.3377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2611/4339]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2612/4339]: training_loss: tensor(0.1844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2613/4339]: training_loss: tensor(0.2760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2614/4339]: training_loss: tensor(0.6309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2615/4339]: training_loss: tensor(0.1766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2616/4339]: training_loss: tensor(0.1423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2617/4339]: training_loss: tensor(0.2702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2618/4339]: training_loss: tensor(0.3110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2619/4339]: training_loss: tensor(0.2738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2620/4339]: training_loss: tensor(0.1563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2621/4339]: training_loss: tensor(0.5545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2622/4339]: training_loss: tensor(0.1896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2623/4339]: training_loss: tensor(0.1657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2624/4339]: training_loss: tensor(0.4327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2625/4339]: training_loss: tensor(0.1994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2626/4339]: training_loss: tensor(0.2460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2627/4339]: training_loss: tensor(0.1178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2628/4339]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2629/4339]: training_loss: tensor(0.4625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2630/4339]: training_loss: tensor(0.2956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2631/4339]: training_loss: tensor(0.6862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2632/4339]: training_loss: tensor(0.1765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2633/4339]: training_loss: tensor(0.1966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2634/4339]: training_loss: tensor(0.1600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2635/4339]: training_loss: tensor(0.3807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2636/4339]: training_loss: tensor(0.3950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2637/4339]: training_loss: tensor(0.2213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2638/4339]: training_loss: tensor(0.2753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2639/4339]: training_loss: tensor(0.2171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2640/4339]: training_loss: tensor(0.1983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2641/4339]: training_loss: tensor(0.2059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2642/4339]: training_loss: tensor(0.3279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2643/4339]: training_loss: tensor(0.4026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2644/4339]: training_loss: tensor(0.3634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2645/4339]: training_loss: tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2646/4339]: training_loss: tensor(0.1618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2647/4339]: training_loss: tensor(0.2134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2648/4339]: training_loss: tensor(0.4384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2649/4339]: training_loss: tensor(0.3127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2650/4339]: training_loss: tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2651/4339]: training_loss: tensor(0.1343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2652/4339]: training_loss: tensor(0.2313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2653/4339]: training_loss: tensor(0.2476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2654/4339]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2655/4339]: training_loss: tensor(0.0442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2656/4339]: training_loss: tensor(0.2606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2657/4339]: training_loss: tensor(0.1838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2658/4339]: training_loss: tensor(0.3091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2659/4339]: training_loss: tensor(0.1867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2660/4339]: training_loss: tensor(0.1646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2661/4339]: training_loss: tensor(0.1438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2662/4339]: training_loss: tensor(0.1401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2663/4339]: training_loss: tensor(0.3623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2664/4339]: training_loss: tensor(0.1748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2665/4339]: training_loss: tensor(0.3560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2666/4339]: training_loss: tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2667/4339]: training_loss: tensor(0.3954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2668/4339]: training_loss: tensor(0.4920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2669/4339]: training_loss: tensor(0.6612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2670/4339]: training_loss: tensor(0.2037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2671/4339]: training_loss: tensor(0.4608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2672/4339]: training_loss: tensor(0.5498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2673/4339]: training_loss: tensor(0.6366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2674/4339]: training_loss: tensor(0.4029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2675/4339]: training_loss: tensor(0.2845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2676/4339]: training_loss: tensor(0.3175, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2677/4339]: training_loss: tensor(0.3268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2678/4339]: training_loss: tensor(0.3026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2679/4339]: training_loss: tensor(0.2055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2680/4339]: training_loss: tensor(0.1381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2681/4339]: training_loss: tensor(0.2665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2682/4339]: training_loss: tensor(0.2762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2683/4339]: training_loss: tensor(0.4961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2684/4339]: training_loss: tensor(0.1721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2685/4339]: training_loss: tensor(0.0640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2686/4339]: training_loss: tensor(0.2494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2687/4339]: training_loss: tensor(0.2202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2688/4339]: training_loss: tensor(0.1974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2689/4339]: training_loss: tensor(0.1837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2690/4339]: training_loss: tensor(0.3348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2691/4339]: training_loss: tensor(0.2219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2692/4339]: training_loss: tensor(0.1787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2693/4339]: training_loss: tensor(0.4632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2694/4339]: training_loss: tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2695/4339]: training_loss: tensor(0.1681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2696/4339]: training_loss: tensor(0.1583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2697/4339]: training_loss: tensor(0.2998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2698/4339]: training_loss: tensor(0.3811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2699/4339]: training_loss: tensor(0.2054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2700/4339]: training_loss: tensor(0.3579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2701/4339]: training_loss: tensor(0.4756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2702/4339]: training_loss: tensor(0.4469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2703/4339]: training_loss: tensor(0.3762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2704/4339]: training_loss: tensor(0.2971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2705/4339]: training_loss: tensor(0.3817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2706/4339]: training_loss: tensor(0.2106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2707/4339]: training_loss: tensor(0.9489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2708/4339]: training_loss: tensor(0.1725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2709/4339]: training_loss: tensor(0.2286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2710/4339]: training_loss: tensor(0.3774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2711/4339]: training_loss: tensor(0.3958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2712/4339]: training_loss: tensor(0.3561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2713/4339]: training_loss: tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2714/4339]: training_loss: tensor(0.2191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2715/4339]: training_loss: tensor(0.1489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2716/4339]: training_loss: tensor(0.2752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2717/4339]: training_loss: tensor(0.1131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2718/4339]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2719/4339]: training_loss: tensor(0.3029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2720/4339]: training_loss: tensor(0.7252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2721/4339]: training_loss: tensor(0.3752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2722/4339]: training_loss: tensor(0.2209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2723/4339]: training_loss: tensor(0.3629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2724/4339]: training_loss: tensor(0.2094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2725/4339]: training_loss: tensor(0.1417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2726/4339]: training_loss: tensor(0.5886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2727/4339]: training_loss: tensor(0.5118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2728/4339]: training_loss: tensor(0.1834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2729/4339]: training_loss: tensor(0.1703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2730/4339]: training_loss: tensor(0.2020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2731/4339]: training_loss: tensor(0.5042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2732/4339]: training_loss: tensor(0.2032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2733/4339]: training_loss: tensor(0.5965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2734/4339]: training_loss: tensor(0.6730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2735/4339]: training_loss: tensor(0.4276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2736/4339]: training_loss: tensor(0.5940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2737/4339]: training_loss: tensor(0.4357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2738/4339]: training_loss: tensor(0.1902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2739/4339]: training_loss: tensor(0.2732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2740/4339]: training_loss: tensor(0.4764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2741/4339]: training_loss: tensor(0.3763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2742/4339]: training_loss: tensor(0.3883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2743/4339]: training_loss: tensor(0.6154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2744/4339]: training_loss: tensor(0.1798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2745/4339]: training_loss: tensor(0.2780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2746/4339]: training_loss: tensor(0.2206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2747/4339]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2748/4339]: training_loss: tensor(0.1967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2749/4339]: training_loss: tensor(0.1090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2750/4339]: training_loss: tensor(0.4864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2751/4339]: training_loss: tensor(0.3360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2752/4339]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2753/4339]: training_loss: tensor(0.1607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2754/4339]: training_loss: tensor(0.5452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2755/4339]: training_loss: tensor(0.2593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2756/4339]: training_loss: tensor(0.1331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2757/4339]: training_loss: tensor(0.1497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2758/4339]: training_loss: tensor(0.2666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2759/4339]: training_loss: tensor(0.3820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2760/4339]: training_loss: tensor(0.2589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2761/4339]: training_loss: tensor(0.2558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2762/4339]: training_loss: tensor(0.1679, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2763/4339]: training_loss: tensor(0.2645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2764/4339]: training_loss: tensor(0.2639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2765/4339]: training_loss: tensor(0.2275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2766/4339]: training_loss: tensor(0.2056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2767/4339]: training_loss: tensor(0.2677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2768/4339]: training_loss: tensor(0.5216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2769/4339]: training_loss: tensor(0.3027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2770/4339]: training_loss: tensor(0.1301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2771/4339]: training_loss: tensor(0.3270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2772/4339]: training_loss: tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2773/4339]: training_loss: tensor(0.3528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2774/4339]: training_loss: tensor(0.4332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2775/4339]: training_loss: tensor(0.4155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2776/4339]: training_loss: tensor(0.4565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2777/4339]: training_loss: tensor(0.4668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2778/4339]: training_loss: tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2779/4339]: training_loss: tensor(0.5338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2780/4339]: training_loss: tensor(0.2898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2781/4339]: training_loss: tensor(0.4732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2782/4339]: training_loss: tensor(0.4692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2783/4339]: training_loss: tensor(0.3476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2784/4339]: training_loss: tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2785/4339]: training_loss: tensor(0.2576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2786/4339]: training_loss: tensor(0.3435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2787/4339]: training_loss: tensor(0.2572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2788/4339]: training_loss: tensor(0.2794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2789/4339]: training_loss: tensor(0.2215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2790/4339]: training_loss: tensor(0.1436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2791/4339]: training_loss: tensor(0.3657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2792/4339]: training_loss: tensor(0.2756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2793/4339]: training_loss: tensor(0.4622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2794/4339]: training_loss: tensor(0.3666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2795/4339]: training_loss: tensor(0.2279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2796/4339]: training_loss: tensor(0.1604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2797/4339]: training_loss: tensor(0.5337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2798/4339]: training_loss: tensor(0.5120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2799/4339]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2800/4339]: training_loss: tensor(0.2016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2801/4339]: training_loss: tensor(0.7135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2802/4339]: training_loss: tensor(0.3581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2803/4339]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2804/4339]: training_loss: tensor(0.5585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2805/4339]: training_loss: tensor(0.2147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2806/4339]: training_loss: tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2807/4339]: training_loss: tensor(0.3567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2808/4339]: training_loss: tensor(0.4910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2809/4339]: training_loss: tensor(0.1828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2810/4339]: training_loss: tensor(0.9445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2811/4339]: training_loss: tensor(0.2170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2812/4339]: training_loss: tensor(0.5481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2813/4339]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2814/4339]: training_loss: tensor(0.2526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2815/4339]: training_loss: tensor(0.3298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2816/4339]: training_loss: tensor(0.1980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2817/4339]: training_loss: tensor(0.1979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2818/4339]: training_loss: tensor(0.4128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2819/4339]: training_loss: tensor(0.2979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2820/4339]: training_loss: tensor(0.1862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2821/4339]: training_loss: tensor(0.1154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2822/4339]: training_loss: tensor(0.2028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2823/4339]: training_loss: tensor(0.2660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2824/4339]: training_loss: tensor(0.1737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2825/4339]: training_loss: tensor(0.4235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2826/4339]: training_loss: tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2827/4339]: training_loss: tensor(0.4228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2828/4339]: training_loss: tensor(0.4094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2829/4339]: training_loss: tensor(0.5601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2830/4339]: training_loss: tensor(0.2771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2831/4339]: training_loss: tensor(0.1956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2832/4339]: training_loss: tensor(0.3427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2833/4339]: training_loss: tensor(0.2063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2834/4339]: training_loss: tensor(0.1349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2835/4339]: training_loss: tensor(0.5964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2836/4339]: training_loss: tensor(0.4406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2837/4339]: training_loss: tensor(0.2607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2838/4339]: training_loss: tensor(0.2354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2839/4339]: training_loss: tensor(0.2126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2840/4339]: training_loss: tensor(0.3047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2841/4339]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2842/4339]: training_loss: tensor(0.3122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2843/4339]: training_loss: tensor(0.3135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2844/4339]: training_loss: tensor(0.6768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2845/4339]: training_loss: tensor(0.3175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2846/4339]: training_loss: tensor(0.3390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2847/4339]: training_loss: tensor(0.2997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2848/4339]: training_loss: tensor(0.3600, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2849/4339]: training_loss: tensor(0.3307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2850/4339]: training_loss: tensor(0.2794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2851/4339]: training_loss: tensor(0.1722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2852/4339]: training_loss: tensor(0.3723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2853/4339]: training_loss: tensor(0.1820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2854/4339]: training_loss: tensor(0.6689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2855/4339]: training_loss: tensor(0.1924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2856/4339]: training_loss: tensor(0.4112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2857/4339]: training_loss: tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2858/4339]: training_loss: tensor(0.2470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2859/4339]: training_loss: tensor(0.3534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2860/4339]: training_loss: tensor(0.2061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2861/4339]: training_loss: tensor(0.2132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2862/4339]: training_loss: tensor(0.5894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2863/4339]: training_loss: tensor(0.2738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2864/4339]: training_loss: tensor(0.7121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2865/4339]: training_loss: tensor(0.2403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2866/4339]: training_loss: tensor(0.1541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2867/4339]: training_loss: tensor(0.4157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2868/4339]: training_loss: tensor(0.3182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2869/4339]: training_loss: tensor(0.5245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2870/4339]: training_loss: tensor(0.2903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2871/4339]: training_loss: tensor(0.3967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2872/4339]: training_loss: tensor(0.5922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2873/4339]: training_loss: tensor(0.2999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2874/4339]: training_loss: tensor(0.1880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2875/4339]: training_loss: tensor(0.2483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2876/4339]: training_loss: tensor(0.2863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2877/4339]: training_loss: tensor(0.3413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2878/4339]: training_loss: tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2879/4339]: training_loss: tensor(0.2566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2880/4339]: training_loss: tensor(0.2103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2881/4339]: training_loss: tensor(0.1950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2882/4339]: training_loss: tensor(0.3781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2883/4339]: training_loss: tensor(0.1932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2884/4339]: training_loss: tensor(0.5259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2885/4339]: training_loss: tensor(0.3761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2886/4339]: training_loss: tensor(0.2716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2887/4339]: training_loss: tensor(0.2928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2888/4339]: training_loss: tensor(0.6191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2889/4339]: training_loss: tensor(0.1791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2890/4339]: training_loss: tensor(0.3854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2891/4339]: training_loss: tensor(0.3522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2892/4339]: training_loss: tensor(0.7122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2893/4339]: training_loss: tensor(0.5431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2894/4339]: training_loss: tensor(0.3318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2895/4339]: training_loss: tensor(0.4057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2896/4339]: training_loss: tensor(0.6156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2897/4339]: training_loss: tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2898/4339]: training_loss: tensor(0.2852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2899/4339]: training_loss: tensor(0.4446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2900/4339]: training_loss: tensor(0.4789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2901/4339]: training_loss: tensor(0.2792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2902/4339]: training_loss: tensor(0.3200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2903/4339]: training_loss: tensor(0.3525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2904/4339]: training_loss: tensor(0.4367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2905/4339]: training_loss: tensor(0.3797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2906/4339]: training_loss: tensor(0.3084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2907/4339]: training_loss: tensor(0.2157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2908/4339]: training_loss: tensor(0.3221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2909/4339]: training_loss: tensor(0.3419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2910/4339]: training_loss: tensor(0.2926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2911/4339]: training_loss: tensor(0.1892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2912/4339]: training_loss: tensor(0.3391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2913/4339]: training_loss: tensor(0.5397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2914/4339]: training_loss: tensor(0.1167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2915/4339]: training_loss: tensor(0.4792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2916/4339]: training_loss: tensor(0.1270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2917/4339]: training_loss: tensor(0.2744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2918/4339]: training_loss: tensor(0.1681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2919/4339]: training_loss: tensor(0.1668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2920/4339]: training_loss: tensor(0.1459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2921/4339]: training_loss: tensor(0.2511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2922/4339]: training_loss: tensor(0.3914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2923/4339]: training_loss: tensor(0.2209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2924/4339]: training_loss: tensor(0.4479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2925/4339]: training_loss: tensor(0.6718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2926/4339]: training_loss: tensor(0.2808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2927/4339]: training_loss: tensor(0.2253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2928/4339]: training_loss: tensor(0.1688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2929/4339]: training_loss: tensor(0.2173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2930/4339]: training_loss: tensor(0.2125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2931/4339]: training_loss: tensor(0.4833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2932/4339]: training_loss: tensor(0.2739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2933/4339]: training_loss: tensor(0.1715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2934/4339]: training_loss: tensor(0.1559, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2935/4339]: training_loss: tensor(0.5180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2936/4339]: training_loss: tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2937/4339]: training_loss: tensor(0.3388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2938/4339]: training_loss: tensor(0.5845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2939/4339]: training_loss: tensor(0.0616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2940/4339]: training_loss: tensor(0.4164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2941/4339]: training_loss: tensor(0.5241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2942/4339]: training_loss: tensor(0.2541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2943/4339]: training_loss: tensor(0.2252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2944/4339]: training_loss: tensor(0.3192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2945/4339]: training_loss: tensor(0.2488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2946/4339]: training_loss: tensor(0.3631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2947/4339]: training_loss: tensor(0.2069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2948/4339]: training_loss: tensor(0.2936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2949/4339]: training_loss: tensor(0.3591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2950/4339]: training_loss: tensor(0.1612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2951/4339]: training_loss: tensor(0.2246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2952/4339]: training_loss: tensor(0.5937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2953/4339]: training_loss: tensor(0.4211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2954/4339]: training_loss: tensor(0.1451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2955/4339]: training_loss: tensor(0.4860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2956/4339]: training_loss: tensor(0.2592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2957/4339]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2958/4339]: training_loss: tensor(0.4044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2959/4339]: training_loss: tensor(0.4543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2960/4339]: training_loss: tensor(0.2544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2961/4339]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2962/4339]: training_loss: tensor(0.1267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2963/4339]: training_loss: tensor(0.1785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2964/4339]: training_loss: tensor(0.2109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2965/4339]: training_loss: tensor(0.1910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2966/4339]: training_loss: tensor(0.2558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2967/4339]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2968/4339]: training_loss: tensor(0.2832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2969/4339]: training_loss: tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2970/4339]: training_loss: tensor(0.3315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2971/4339]: training_loss: tensor(0.3691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2972/4339]: training_loss: tensor(0.4296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2973/4339]: training_loss: tensor(0.2143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2974/4339]: training_loss: tensor(0.2904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2975/4339]: training_loss: tensor(0.5463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2976/4339]: training_loss: tensor(0.2077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2977/4339]: training_loss: tensor(0.2058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2978/4339]: training_loss: tensor(0.2252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2979/4339]: training_loss: tensor(0.3628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2980/4339]: training_loss: tensor(0.3355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2981/4339]: training_loss: tensor(0.3757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2982/4339]: training_loss: tensor(0.2348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2983/4339]: training_loss: tensor(0.6803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2984/4339]: training_loss: tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2985/4339]: training_loss: tensor(0.1645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2986/4339]: training_loss: tensor(0.2177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2987/4339]: training_loss: tensor(0.3474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2988/4339]: training_loss: tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2989/4339]: training_loss: tensor(0.2205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2990/4339]: training_loss: tensor(0.1169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2991/4339]: training_loss: tensor(0.1912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2992/4339]: training_loss: tensor(0.4489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2993/4339]: training_loss: tensor(0.1925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2994/4339]: training_loss: tensor(0.2834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2995/4339]: training_loss: tensor(0.4909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2996/4339]: training_loss: tensor(0.4926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2997/4339]: training_loss: tensor(0.2874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2998/4339]: training_loss: tensor(0.2962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2999/4339]: training_loss: tensor(0.1361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3000/4339]: training_loss: tensor(0.3762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3001/4339]: training_loss: tensor(0.1964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3002/4339]: training_loss: tensor(0.4743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3003/4339]: training_loss: tensor(0.2881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3004/4339]: training_loss: tensor(0.3278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3005/4339]: training_loss: tensor(0.2455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3006/4339]: training_loss: tensor(0.2965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3007/4339]: training_loss: tensor(0.2256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3008/4339]: training_loss: tensor(0.1461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3009/4339]: training_loss: tensor(0.4372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3010/4339]: training_loss: tensor(0.3963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3011/4339]: training_loss: tensor(0.5453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3012/4339]: training_loss: tensor(0.2307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3013/4339]: training_loss: tensor(0.5418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3014/4339]: training_loss: tensor(0.3863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3015/4339]: training_loss: tensor(0.3289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3016/4339]: training_loss: tensor(0.4164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3017/4339]: training_loss: tensor(0.1403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3018/4339]: training_loss: tensor(0.6306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3019/4339]: training_loss: tensor(0.1710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3020/4339]: training_loss: tensor(0.5041, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3021/4339]: training_loss: tensor(0.6427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3022/4339]: training_loss: tensor(0.5208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3023/4339]: training_loss: tensor(0.4560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3024/4339]: training_loss: tensor(0.2396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3025/4339]: training_loss: tensor(0.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3026/4339]: training_loss: tensor(0.1284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3027/4339]: training_loss: tensor(0.1944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3028/4339]: training_loss: tensor(0.5530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3029/4339]: training_loss: tensor(0.3218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3030/4339]: training_loss: tensor(0.1645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3031/4339]: training_loss: tensor(0.2288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3032/4339]: training_loss: tensor(0.2533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3033/4339]: training_loss: tensor(0.1626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3034/4339]: training_loss: tensor(0.3574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3035/4339]: training_loss: tensor(0.2853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3036/4339]: training_loss: tensor(0.3257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3037/4339]: training_loss: tensor(0.2787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3038/4339]: training_loss: tensor(0.2663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3039/4339]: training_loss: tensor(0.3489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3040/4339]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3041/4339]: training_loss: tensor(0.3768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3042/4339]: training_loss: tensor(0.1530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3043/4339]: training_loss: tensor(0.0492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3044/4339]: training_loss: tensor(0.2320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3045/4339]: training_loss: tensor(0.4208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3046/4339]: training_loss: tensor(0.5103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3047/4339]: training_loss: tensor(0.1865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3048/4339]: training_loss: tensor(0.1581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3049/4339]: training_loss: tensor(0.5681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3050/4339]: training_loss: tensor(0.7571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3051/4339]: training_loss: tensor(0.5347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3052/4339]: training_loss: tensor(0.2426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3053/4339]: training_loss: tensor(0.4132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3054/4339]: training_loss: tensor(0.6456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3055/4339]: training_loss: tensor(0.4505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3056/4339]: training_loss: tensor(0.1409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3057/4339]: training_loss: tensor(0.2786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3058/4339]: training_loss: tensor(0.1684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3059/4339]: training_loss: tensor(0.2943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3060/4339]: training_loss: tensor(0.1599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3061/4339]: training_loss: tensor(0.6212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3062/4339]: training_loss: tensor(0.1924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3063/4339]: training_loss: tensor(0.3527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3064/4339]: training_loss: tensor(0.3263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3065/4339]: training_loss: tensor(0.1679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3066/4339]: training_loss: tensor(0.2784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3067/4339]: training_loss: tensor(0.3462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3068/4339]: training_loss: tensor(0.2039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3069/4339]: training_loss: tensor(0.1269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3070/4339]: training_loss: tensor(0.1289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3071/4339]: training_loss: tensor(0.3044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3072/4339]: training_loss: tensor(0.3756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3073/4339]: training_loss: tensor(0.2699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3074/4339]: training_loss: tensor(0.4499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3075/4339]: training_loss: tensor(0.2595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3076/4339]: training_loss: tensor(0.3450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3077/4339]: training_loss: tensor(0.2308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3078/4339]: training_loss: tensor(0.2899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3079/4339]: training_loss: tensor(0.2073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3080/4339]: training_loss: tensor(0.2970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3081/4339]: training_loss: tensor(0.1449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3082/4339]: training_loss: tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3083/4339]: training_loss: tensor(0.1553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3084/4339]: training_loss: tensor(0.1251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3085/4339]: training_loss: tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3086/4339]: training_loss: tensor(0.3485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3087/4339]: training_loss: tensor(0.6052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3088/4339]: training_loss: tensor(0.4202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3089/4339]: training_loss: tensor(0.7291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3090/4339]: training_loss: tensor(0.2701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3091/4339]: training_loss: tensor(0.5102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3092/4339]: training_loss: tensor(0.3006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3093/4339]: training_loss: tensor(0.2381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3094/4339]: training_loss: tensor(0.3965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3095/4339]: training_loss: tensor(0.3170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3096/4339]: training_loss: tensor(0.2940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3097/4339]: training_loss: tensor(0.3570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3098/4339]: training_loss: tensor(0.1571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3099/4339]: training_loss: tensor(0.4625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3100/4339]: training_loss: tensor(0.2969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3101/4339]: training_loss: tensor(0.2516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3102/4339]: training_loss: tensor(0.2457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3103/4339]: training_loss: tensor(0.5749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3104/4339]: training_loss: tensor(0.1967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3105/4339]: training_loss: tensor(0.2111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3106/4339]: training_loss: tensor(0.1796, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3107/4339]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3108/4339]: training_loss: tensor(0.2776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3109/4339]: training_loss: tensor(0.1487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3110/4339]: training_loss: tensor(0.4623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3111/4339]: training_loss: tensor(0.1612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3112/4339]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3113/4339]: training_loss: tensor(0.1125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3114/4339]: training_loss: tensor(0.2588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3115/4339]: training_loss: tensor(0.5700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3116/4339]: training_loss: tensor(0.3733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3117/4339]: training_loss: tensor(0.2253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3118/4339]: training_loss: tensor(0.3411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3119/4339]: training_loss: tensor(0.4012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3120/4339]: training_loss: tensor(0.2781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3121/4339]: training_loss: tensor(0.3172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3122/4339]: training_loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3123/4339]: training_loss: tensor(0.6706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3124/4339]: training_loss: tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3125/4339]: training_loss: tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3126/4339]: training_loss: tensor(0.2788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3127/4339]: training_loss: tensor(0.1437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3128/4339]: training_loss: tensor(0.3012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3129/4339]: training_loss: tensor(0.5196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3130/4339]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3131/4339]: training_loss: tensor(0.6945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3132/4339]: training_loss: tensor(0.3500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3133/4339]: training_loss: tensor(0.4428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3134/4339]: training_loss: tensor(0.4220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3135/4339]: training_loss: tensor(0.5941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3136/4339]: training_loss: tensor(0.1266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3137/4339]: training_loss: tensor(0.4389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3138/4339]: training_loss: tensor(0.1608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3139/4339]: training_loss: tensor(0.2807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3140/4339]: training_loss: tensor(0.1741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3141/4339]: training_loss: tensor(0.2646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3142/4339]: training_loss: tensor(0.2052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3143/4339]: training_loss: tensor(0.5616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3144/4339]: training_loss: tensor(0.2792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3145/4339]: training_loss: tensor(0.3927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3146/4339]: training_loss: tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3147/4339]: training_loss: tensor(0.4561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3148/4339]: training_loss: tensor(0.1834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3149/4339]: training_loss: tensor(0.4364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3150/4339]: training_loss: tensor(0.3873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3151/4339]: training_loss: tensor(0.3835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3152/4339]: training_loss: tensor(0.1810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3153/4339]: training_loss: tensor(0.4017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3154/4339]: training_loss: tensor(0.4807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3155/4339]: training_loss: tensor(0.2564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3156/4339]: training_loss: tensor(0.3209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3157/4339]: training_loss: tensor(0.3840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3158/4339]: training_loss: tensor(0.4281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3159/4339]: training_loss: tensor(0.1601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3160/4339]: training_loss: tensor(0.2912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3161/4339]: training_loss: tensor(0.3718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3162/4339]: training_loss: tensor(0.2754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3163/4339]: training_loss: tensor(0.4275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3164/4339]: training_loss: tensor(0.4380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3165/4339]: training_loss: tensor(0.3814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3166/4339]: training_loss: tensor(0.2172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3167/4339]: training_loss: tensor(0.4876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3168/4339]: training_loss: tensor(0.2267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3169/4339]: training_loss: tensor(0.6741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3170/4339]: training_loss: tensor(0.3358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3171/4339]: training_loss: tensor(0.1457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3172/4339]: training_loss: tensor(0.3256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3173/4339]: training_loss: tensor(0.2305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3174/4339]: training_loss: tensor(0.1544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3175/4339]: training_loss: tensor(0.1577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3176/4339]: training_loss: tensor(0.2700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3177/4339]: training_loss: tensor(0.3830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3178/4339]: training_loss: tensor(0.6352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3179/4339]: training_loss: tensor(0.1134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3180/4339]: training_loss: tensor(0.1234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3181/4339]: training_loss: tensor(0.1307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3182/4339]: training_loss: tensor(0.2680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3183/4339]: training_loss: tensor(0.2379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3184/4339]: training_loss: tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3185/4339]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3186/4339]: training_loss: tensor(0.6086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3187/4339]: training_loss: tensor(0.1553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3188/4339]: training_loss: tensor(0.3040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3189/4339]: training_loss: tensor(0.2756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3190/4339]: training_loss: tensor(0.2577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3191/4339]: training_loss: tensor(0.3283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3192/4339]: training_loss: tensor(0.2889, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3193/4339]: training_loss: tensor(0.2917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3194/4339]: training_loss: tensor(0.1897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3195/4339]: training_loss: tensor(0.5691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3196/4339]: training_loss: tensor(0.1386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3197/4339]: training_loss: tensor(0.3781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3198/4339]: training_loss: tensor(0.1850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3199/4339]: training_loss: tensor(0.3207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3200/4339]: training_loss: tensor(0.3531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3201/4339]: training_loss: tensor(0.2192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3202/4339]: training_loss: tensor(0.1139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3203/4339]: training_loss: tensor(0.1396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3204/4339]: training_loss: tensor(0.4998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3205/4339]: training_loss: tensor(0.3487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3206/4339]: training_loss: tensor(0.2060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3207/4339]: training_loss: tensor(0.6325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3208/4339]: training_loss: tensor(0.4512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3209/4339]: training_loss: tensor(0.2053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3210/4339]: training_loss: tensor(0.1696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3211/4339]: training_loss: tensor(0.3832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3212/4339]: training_loss: tensor(0.4294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3213/4339]: training_loss: tensor(0.2235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3214/4339]: training_loss: tensor(0.3756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3215/4339]: training_loss: tensor(0.3646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3216/4339]: training_loss: tensor(0.2233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3217/4339]: training_loss: tensor(0.3647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3218/4339]: training_loss: tensor(0.2771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3219/4339]: training_loss: tensor(0.3078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3220/4339]: training_loss: tensor(0.2243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3221/4339]: training_loss: tensor(0.2231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3222/4339]: training_loss: tensor(0.3454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3223/4339]: training_loss: tensor(0.3454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3224/4339]: training_loss: tensor(0.1691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3225/4339]: training_loss: tensor(0.3234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3226/4339]: training_loss: tensor(0.2012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3227/4339]: training_loss: tensor(0.2059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3228/4339]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3229/4339]: training_loss: tensor(0.3526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3230/4339]: training_loss: tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3231/4339]: training_loss: tensor(0.1059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3232/4339]: training_loss: tensor(0.4662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3233/4339]: training_loss: tensor(0.7749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3234/4339]: training_loss: tensor(0.1873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3235/4339]: training_loss: tensor(0.2193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3236/4339]: training_loss: tensor(0.4377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3237/4339]: training_loss: tensor(0.1491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3238/4339]: training_loss: tensor(0.3095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3239/4339]: training_loss: tensor(0.1572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3240/4339]: training_loss: tensor(0.3490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3241/4339]: training_loss: tensor(0.4976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3242/4339]: training_loss: tensor(0.3872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3243/4339]: training_loss: tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3244/4339]: training_loss: tensor(0.9272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3245/4339]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3246/4339]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3247/4339]: training_loss: tensor(0.3660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3248/4339]: training_loss: tensor(0.1312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3249/4339]: training_loss: tensor(0.1891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3250/4339]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3251/4339]: training_loss: tensor(0.2391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3252/4339]: training_loss: tensor(0.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3253/4339]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3254/4339]: training_loss: tensor(0.1926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3255/4339]: training_loss: tensor(0.3702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3256/4339]: training_loss: tensor(0.2843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3257/4339]: training_loss: tensor(0.1845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3258/4339]: training_loss: tensor(0.6075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3259/4339]: training_loss: tensor(0.2186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3260/4339]: training_loss: tensor(0.5377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3261/4339]: training_loss: tensor(0.2069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3262/4339]: training_loss: tensor(0.4039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3263/4339]: training_loss: tensor(0.1547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3264/4339]: training_loss: tensor(0.3735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3265/4339]: training_loss: tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3266/4339]: training_loss: tensor(0.2038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3267/4339]: training_loss: tensor(0.2611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3268/4339]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3269/4339]: training_loss: tensor(0.2034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3270/4339]: training_loss: tensor(0.2654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3271/4339]: training_loss: tensor(0.2084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3272/4339]: training_loss: tensor(0.3185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3273/4339]: training_loss: tensor(0.3312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3274/4339]: training_loss: tensor(0.2999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3275/4339]: training_loss: tensor(0.6590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3276/4339]: training_loss: tensor(0.5923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3277/4339]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3278/4339]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3279/4339]: training_loss: tensor(0.4956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3280/4339]: training_loss: tensor(0.3190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3281/4339]: training_loss: tensor(0.1862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3282/4339]: training_loss: tensor(0.2959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3283/4339]: training_loss: tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3284/4339]: training_loss: tensor(0.4109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3285/4339]: training_loss: tensor(0.3447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3286/4339]: training_loss: tensor(0.3178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3287/4339]: training_loss: tensor(0.1906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3288/4339]: training_loss: tensor(0.4214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3289/4339]: training_loss: tensor(0.1990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3290/4339]: training_loss: tensor(0.1927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3291/4339]: training_loss: tensor(0.1559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3292/4339]: training_loss: tensor(0.3176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3293/4339]: training_loss: tensor(0.4134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3294/4339]: training_loss: tensor(0.1605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3295/4339]: training_loss: tensor(0.2820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3296/4339]: training_loss: tensor(0.1919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3297/4339]: training_loss: tensor(0.1628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3298/4339]: training_loss: tensor(0.2639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3299/4339]: training_loss: tensor(0.4354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3300/4339]: training_loss: tensor(0.3859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3301/4339]: training_loss: tensor(0.2818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3302/4339]: training_loss: tensor(0.2208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3303/4339]: training_loss: tensor(0.1539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3304/4339]: training_loss: tensor(0.3991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3305/4339]: training_loss: tensor(0.3199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3306/4339]: training_loss: tensor(0.3646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3307/4339]: training_loss: tensor(0.5331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3308/4339]: training_loss: tensor(0.2009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3309/4339]: training_loss: tensor(0.3886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3310/4339]: training_loss: tensor(0.3277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3311/4339]: training_loss: tensor(0.2547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3312/4339]: training_loss: tensor(0.3855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3313/4339]: training_loss: tensor(0.1974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3314/4339]: training_loss: tensor(0.4901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3315/4339]: training_loss: tensor(0.1678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3316/4339]: training_loss: tensor(0.5316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3317/4339]: training_loss: tensor(0.6305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3318/4339]: training_loss: tensor(0.3429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3319/4339]: training_loss: tensor(0.1704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3320/4339]: training_loss: tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3321/4339]: training_loss: tensor(0.7209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3322/4339]: training_loss: tensor(0.2569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3323/4339]: training_loss: tensor(0.3855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3324/4339]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3325/4339]: training_loss: tensor(0.2523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3326/4339]: training_loss: tensor(0.4911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3327/4339]: training_loss: tensor(0.2328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3328/4339]: training_loss: tensor(0.2883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3329/4339]: training_loss: tensor(0.6217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3330/4339]: training_loss: tensor(0.2486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3331/4339]: training_loss: tensor(0.1993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3332/4339]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3333/4339]: training_loss: tensor(0.1753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3334/4339]: training_loss: tensor(0.4742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3335/4339]: training_loss: tensor(0.3417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3336/4339]: training_loss: tensor(0.2854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3337/4339]: training_loss: tensor(0.1925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3338/4339]: training_loss: tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3339/4339]: training_loss: tensor(0.3778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3340/4339]: training_loss: tensor(0.3593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3341/4339]: training_loss: tensor(0.2070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3342/4339]: training_loss: tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3343/4339]: training_loss: tensor(0.2284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3344/4339]: training_loss: tensor(0.1308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3345/4339]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3346/4339]: training_loss: tensor(0.4539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3347/4339]: training_loss: tensor(0.3233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3348/4339]: training_loss: tensor(0.3709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3349/4339]: training_loss: tensor(0.2176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3350/4339]: training_loss: tensor(0.1233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3351/4339]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3352/4339]: training_loss: tensor(0.8810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3353/4339]: training_loss: tensor(0.3904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3354/4339]: training_loss: tensor(0.2288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3355/4339]: training_loss: tensor(0.3331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3356/4339]: training_loss: tensor(0.3512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3357/4339]: training_loss: tensor(0.3319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3358/4339]: training_loss: tensor(0.3774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3359/4339]: training_loss: tensor(0.3150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3360/4339]: training_loss: tensor(0.2712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3361/4339]: training_loss: tensor(0.3308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3362/4339]: training_loss: tensor(0.2765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3363/4339]: training_loss: tensor(0.2648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3364/4339]: training_loss: tensor(0.3541, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3365/4339]: training_loss: tensor(0.3505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3366/4339]: training_loss: tensor(0.1593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3367/4339]: training_loss: tensor(0.2249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3368/4339]: training_loss: tensor(0.3139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3369/4339]: training_loss: tensor(0.1530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3370/4339]: training_loss: tensor(0.2773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3371/4339]: training_loss: tensor(0.3783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3372/4339]: training_loss: tensor(0.4053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3373/4339]: training_loss: tensor(0.3592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3374/4339]: training_loss: tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3375/4339]: training_loss: tensor(0.2306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3376/4339]: training_loss: tensor(0.2109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3377/4339]: training_loss: tensor(0.3053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3378/4339]: training_loss: tensor(0.3985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3379/4339]: training_loss: tensor(0.3680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3380/4339]: training_loss: tensor(0.2675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3381/4339]: training_loss: tensor(0.4095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3382/4339]: training_loss: tensor(0.5194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3383/4339]: training_loss: tensor(0.2819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3384/4339]: training_loss: tensor(0.4679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3385/4339]: training_loss: tensor(0.2442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3386/4339]: training_loss: tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3387/4339]: training_loss: tensor(0.4011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3388/4339]: training_loss: tensor(0.2761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3389/4339]: training_loss: tensor(0.2653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3390/4339]: training_loss: tensor(0.2259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3391/4339]: training_loss: tensor(0.1821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3392/4339]: training_loss: tensor(0.5399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3393/4339]: training_loss: tensor(0.2086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3394/4339]: training_loss: tensor(0.2413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3395/4339]: training_loss: tensor(0.3846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3396/4339]: training_loss: tensor(0.1835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3397/4339]: training_loss: tensor(0.3425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3398/4339]: training_loss: tensor(0.2927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3399/4339]: training_loss: tensor(0.1783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3400/4339]: training_loss: tensor(0.1132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3401/4339]: training_loss: tensor(0.2590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3402/4339]: training_loss: tensor(0.6342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3403/4339]: training_loss: tensor(0.4366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3404/4339]: training_loss: tensor(0.3178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3405/4339]: training_loss: tensor(0.1289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3406/4339]: training_loss: tensor(0.1885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3407/4339]: training_loss: tensor(0.4252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3408/4339]: training_loss: tensor(0.4755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3409/4339]: training_loss: tensor(0.2655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3410/4339]: training_loss: tensor(0.4290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3411/4339]: training_loss: tensor(0.3089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3412/4339]: training_loss: tensor(0.5256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3413/4339]: training_loss: tensor(0.5466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3414/4339]: training_loss: tensor(0.1549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3415/4339]: training_loss: tensor(0.3081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3416/4339]: training_loss: tensor(0.2549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3417/4339]: training_loss: tensor(0.2529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3418/4339]: training_loss: tensor(0.4037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3419/4339]: training_loss: tensor(0.2396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3420/4339]: training_loss: tensor(0.2276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3421/4339]: training_loss: tensor(0.2331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3422/4339]: training_loss: tensor(0.9289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3423/4339]: training_loss: tensor(0.1456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3424/4339]: training_loss: tensor(0.1720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3425/4339]: training_loss: tensor(0.4114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3426/4339]: training_loss: tensor(0.1641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3427/4339]: training_loss: tensor(0.2974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3428/4339]: training_loss: tensor(0.4377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3429/4339]: training_loss: tensor(0.3242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3430/4339]: training_loss: tensor(0.2827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3431/4339]: training_loss: tensor(0.1558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3432/4339]: training_loss: tensor(0.2746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3433/4339]: training_loss: tensor(0.3098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3434/4339]: training_loss: tensor(0.2629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3435/4339]: training_loss: tensor(0.3582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3436/4339]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3437/4339]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3438/4339]: training_loss: tensor(0.4505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3439/4339]: training_loss: tensor(0.2455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3440/4339]: training_loss: tensor(0.3063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3441/4339]: training_loss: tensor(0.2184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3442/4339]: training_loss: tensor(0.2580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3443/4339]: training_loss: tensor(0.5148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3444/4339]: training_loss: tensor(0.2731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3445/4339]: training_loss: tensor(0.3236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3446/4339]: training_loss: tensor(0.3415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3447/4339]: training_loss: tensor(0.2488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3448/4339]: training_loss: tensor(0.3527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3449/4339]: training_loss: tensor(0.2196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3450/4339]: training_loss: tensor(0.1543, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3451/4339]: training_loss: tensor(0.2349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3452/4339]: training_loss: tensor(0.2332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3453/4339]: training_loss: tensor(0.3963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3454/4339]: training_loss: tensor(0.4604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3455/4339]: training_loss: tensor(0.1859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3456/4339]: training_loss: tensor(0.1816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3457/4339]: training_loss: tensor(0.1187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3458/4339]: training_loss: tensor(0.3924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3459/4339]: training_loss: tensor(0.1429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3460/4339]: training_loss: tensor(0.1724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3461/4339]: training_loss: tensor(0.2817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3462/4339]: training_loss: tensor(0.1448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3463/4339]: training_loss: tensor(0.2339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3464/4339]: training_loss: tensor(0.4426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3465/4339]: training_loss: tensor(0.2508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3466/4339]: training_loss: tensor(0.2007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3467/4339]: training_loss: tensor(0.4356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3468/4339]: training_loss: tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3469/4339]: training_loss: tensor(0.5135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3470/4339]: training_loss: tensor(0.4629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3471/4339]: training_loss: tensor(0.2136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3472/4339]: training_loss: tensor(0.3829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3473/4339]: training_loss: tensor(0.2936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3474/4339]: training_loss: tensor(0.2742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3475/4339]: training_loss: tensor(0.3876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3476/4339]: training_loss: tensor(0.2764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3477/4339]: training_loss: tensor(0.4113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3478/4339]: training_loss: tensor(0.2924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3479/4339]: training_loss: tensor(0.1877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3480/4339]: training_loss: tensor(0.2646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3481/4339]: training_loss: tensor(0.1447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3482/4339]: training_loss: tensor(0.2183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3483/4339]: training_loss: tensor(0.1555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3484/4339]: training_loss: tensor(0.1684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3485/4339]: training_loss: tensor(0.1846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3486/4339]: training_loss: tensor(0.3848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3487/4339]: training_loss: tensor(0.3794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3488/4339]: training_loss: tensor(0.1962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3489/4339]: training_loss: tensor(0.1939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3490/4339]: training_loss: tensor(0.5027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3491/4339]: training_loss: tensor(0.8173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3492/4339]: training_loss: tensor(0.3464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3493/4339]: training_loss: tensor(0.3167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3494/4339]: training_loss: tensor(0.8669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3495/4339]: training_loss: tensor(0.3173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3496/4339]: training_loss: tensor(0.3968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3497/4339]: training_loss: tensor(0.4652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3498/4339]: training_loss: tensor(0.2622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3499/4339]: training_loss: tensor(0.4091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3500/4339]: training_loss: tensor(0.3360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3501/4339]: training_loss: tensor(0.3579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3502/4339]: training_loss: tensor(0.5552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3503/4339]: training_loss: tensor(0.2775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3504/4339]: training_loss: tensor(0.6199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3505/4339]: training_loss: tensor(0.2130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3506/4339]: training_loss: tensor(0.3689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3507/4339]: training_loss: tensor(0.2656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3508/4339]: training_loss: tensor(0.3198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3509/4339]: training_loss: tensor(0.4322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3510/4339]: training_loss: tensor(0.3228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3511/4339]: training_loss: tensor(0.1597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3512/4339]: training_loss: tensor(0.6084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3513/4339]: training_loss: tensor(0.2492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3514/4339]: training_loss: tensor(0.1683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3515/4339]: training_loss: tensor(0.2088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3516/4339]: training_loss: tensor(0.5678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3517/4339]: training_loss: tensor(0.1934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3518/4339]: training_loss: tensor(0.2250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3519/4339]: training_loss: tensor(0.2780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3520/4339]: training_loss: tensor(0.6043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3521/4339]: training_loss: tensor(0.4121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3522/4339]: training_loss: tensor(0.3041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3523/4339]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3524/4339]: training_loss: tensor(0.5186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3525/4339]: training_loss: tensor(0.3350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3526/4339]: training_loss: tensor(0.2829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3527/4339]: training_loss: tensor(0.2807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3528/4339]: training_loss: tensor(0.3717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3529/4339]: training_loss: tensor(0.2743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3530/4339]: training_loss: tensor(0.1575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3531/4339]: training_loss: tensor(0.2643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3532/4339]: training_loss: tensor(0.3841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3533/4339]: training_loss: tensor(0.4586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3534/4339]: training_loss: tensor(0.4842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3535/4339]: training_loss: tensor(0.1679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3536/4339]: training_loss: tensor(0.1888, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3537/4339]: training_loss: tensor(0.2338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3538/4339]: training_loss: tensor(0.1918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3539/4339]: training_loss: tensor(0.4399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3540/4339]: training_loss: tensor(0.2894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3541/4339]: training_loss: tensor(0.2763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3542/4339]: training_loss: tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3543/4339]: training_loss: tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3544/4339]: training_loss: tensor(0.3334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3545/4339]: training_loss: tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3546/4339]: training_loss: tensor(0.2815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3547/4339]: training_loss: tensor(0.4834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3548/4339]: training_loss: tensor(0.5915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3549/4339]: training_loss: tensor(0.3737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3550/4339]: training_loss: tensor(0.4885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3551/4339]: training_loss: tensor(0.3361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3552/4339]: training_loss: tensor(0.1322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3553/4339]: training_loss: tensor(0.2903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3554/4339]: training_loss: tensor(0.3159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3555/4339]: training_loss: tensor(0.3023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3556/4339]: training_loss: tensor(0.3618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3557/4339]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3558/4339]: training_loss: tensor(0.1836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3559/4339]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3560/4339]: training_loss: tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3561/4339]: training_loss: tensor(0.2289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3562/4339]: training_loss: tensor(0.5332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3563/4339]: training_loss: tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3564/4339]: training_loss: tensor(0.5210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3565/4339]: training_loss: tensor(0.2638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3566/4339]: training_loss: tensor(0.5146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3567/4339]: training_loss: tensor(0.4630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3568/4339]: training_loss: tensor(0.2514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3569/4339]: training_loss: tensor(0.3912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3570/4339]: training_loss: tensor(0.3461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3571/4339]: training_loss: tensor(0.4062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3572/4339]: training_loss: tensor(0.4364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3573/4339]: training_loss: tensor(0.1824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3574/4339]: training_loss: tensor(0.4652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3575/4339]: training_loss: tensor(0.3477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3576/4339]: training_loss: tensor(0.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3577/4339]: training_loss: tensor(0.5080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3578/4339]: training_loss: tensor(0.2371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3579/4339]: training_loss: tensor(0.4549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3580/4339]: training_loss: tensor(0.2303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3581/4339]: training_loss: tensor(0.3601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3582/4339]: training_loss: tensor(0.3871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3583/4339]: training_loss: tensor(0.4408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3584/4339]: training_loss: tensor(0.3555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3585/4339]: training_loss: tensor(0.4626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3586/4339]: training_loss: tensor(0.4536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3587/4339]: training_loss: tensor(0.5041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3588/4339]: training_loss: tensor(0.5460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3589/4339]: training_loss: tensor(0.2948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3590/4339]: training_loss: tensor(0.2386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3591/4339]: training_loss: tensor(0.3169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3592/4339]: training_loss: tensor(0.2587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3593/4339]: training_loss: tensor(0.5413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3594/4339]: training_loss: tensor(0.3144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3595/4339]: training_loss: tensor(0.2228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3596/4339]: training_loss: tensor(0.6072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3597/4339]: training_loss: tensor(0.1217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3598/4339]: training_loss: tensor(0.3795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3599/4339]: training_loss: tensor(0.1525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3600/4339]: training_loss: tensor(0.3366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3601/4339]: training_loss: tensor(0.2533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3602/4339]: training_loss: tensor(0.2855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3603/4339]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3604/4339]: training_loss: tensor(0.2318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3605/4339]: training_loss: tensor(0.4450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3606/4339]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3607/4339]: training_loss: tensor(0.5628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3608/4339]: training_loss: tensor(0.1279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3609/4339]: training_loss: tensor(0.1615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3610/4339]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3611/4339]: training_loss: tensor(0.4758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3612/4339]: training_loss: tensor(0.3357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3613/4339]: training_loss: tensor(0.3595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3614/4339]: training_loss: tensor(0.2075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3615/4339]: training_loss: tensor(0.3942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3616/4339]: training_loss: tensor(0.2865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3617/4339]: training_loss: tensor(0.3619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3618/4339]: training_loss: tensor(0.4748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3619/4339]: training_loss: tensor(0.3523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3620/4339]: training_loss: tensor(0.7458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3621/4339]: training_loss: tensor(0.3029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3622/4339]: training_loss: tensor(0.3675, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3623/4339]: training_loss: tensor(0.4017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3624/4339]: training_loss: tensor(0.3773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3625/4339]: training_loss: tensor(0.3550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3626/4339]: training_loss: tensor(0.3886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3627/4339]: training_loss: tensor(0.3883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3628/4339]: training_loss: tensor(0.5866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3629/4339]: training_loss: tensor(0.3769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3630/4339]: training_loss: tensor(0.2077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3631/4339]: training_loss: tensor(0.3611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3632/4339]: training_loss: tensor(0.3989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3633/4339]: training_loss: tensor(0.2565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3634/4339]: training_loss: tensor(0.2204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3635/4339]: training_loss: tensor(0.4919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3636/4339]: training_loss: tensor(0.1891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3637/4339]: training_loss: tensor(0.2611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3638/4339]: training_loss: tensor(0.3863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3639/4339]: training_loss: tensor(0.2878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3640/4339]: training_loss: tensor(0.4495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3641/4339]: training_loss: tensor(0.3570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3642/4339]: training_loss: tensor(0.2325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3643/4339]: training_loss: tensor(0.2197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3644/4339]: training_loss: tensor(0.3186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3645/4339]: training_loss: tensor(0.7377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3646/4339]: training_loss: tensor(0.2375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3647/4339]: training_loss: tensor(0.3819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3648/4339]: training_loss: tensor(0.2264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3649/4339]: training_loss: tensor(0.4347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3650/4339]: training_loss: tensor(0.2361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3651/4339]: training_loss: tensor(0.1132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3652/4339]: training_loss: tensor(0.3723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3653/4339]: training_loss: tensor(0.1578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3654/4339]: training_loss: tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3655/4339]: training_loss: tensor(0.1624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3656/4339]: training_loss: tensor(0.3323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3657/4339]: training_loss: tensor(0.2065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3658/4339]: training_loss: tensor(0.2642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3659/4339]: training_loss: tensor(0.2921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3660/4339]: training_loss: tensor(0.1633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3661/4339]: training_loss: tensor(0.4330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3662/4339]: training_loss: tensor(0.2459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3663/4339]: training_loss: tensor(0.3318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3664/4339]: training_loss: tensor(0.2347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3665/4339]: training_loss: tensor(0.4921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3666/4339]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3667/4339]: training_loss: tensor(0.7335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3668/4339]: training_loss: tensor(0.4588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3669/4339]: training_loss: tensor(0.4715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3670/4339]: training_loss: tensor(0.3183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3671/4339]: training_loss: tensor(0.2390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3672/4339]: training_loss: tensor(0.3600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3673/4339]: training_loss: tensor(0.3553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3674/4339]: training_loss: tensor(0.6019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3675/4339]: training_loss: tensor(0.4494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3676/4339]: training_loss: tensor(0.5075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3677/4339]: training_loss: tensor(0.4366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3678/4339]: training_loss: tensor(0.2302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3679/4339]: training_loss: tensor(0.6190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3680/4339]: training_loss: tensor(0.3535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3681/4339]: training_loss: tensor(0.5775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3682/4339]: training_loss: tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3683/4339]: training_loss: tensor(0.3475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3684/4339]: training_loss: tensor(0.5644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3685/4339]: training_loss: tensor(0.3814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3686/4339]: training_loss: tensor(0.3772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3687/4339]: training_loss: tensor(0.3415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3688/4339]: training_loss: tensor(0.4204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3689/4339]: training_loss: tensor(0.1581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3690/4339]: training_loss: tensor(0.4705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3691/4339]: training_loss: tensor(0.3250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3692/4339]: training_loss: tensor(0.1813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3693/4339]: training_loss: tensor(0.3335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3694/4339]: training_loss: tensor(0.2288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3695/4339]: training_loss: tensor(0.3654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3696/4339]: training_loss: tensor(0.1853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3697/4339]: training_loss: tensor(0.2688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3698/4339]: training_loss: tensor(0.3261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3699/4339]: training_loss: tensor(0.3657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3700/4339]: training_loss: tensor(0.2854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3701/4339]: training_loss: tensor(0.4203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3702/4339]: training_loss: tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3703/4339]: training_loss: tensor(0.4394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3704/4339]: training_loss: tensor(0.2635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3705/4339]: training_loss: tensor(0.2868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3706/4339]: training_loss: tensor(0.1985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3707/4339]: training_loss: tensor(0.4389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3708/4339]: training_loss: tensor(0.3788, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3709/4339]: training_loss: tensor(0.7542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3710/4339]: training_loss: tensor(0.4792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3711/4339]: training_loss: tensor(0.1327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3712/4339]: training_loss: tensor(0.1268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3713/4339]: training_loss: tensor(0.2852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3714/4339]: training_loss: tensor(0.4486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3715/4339]: training_loss: tensor(0.3772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3716/4339]: training_loss: tensor(0.3405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3717/4339]: training_loss: tensor(0.3435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3718/4339]: training_loss: tensor(0.2939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3719/4339]: training_loss: tensor(0.2030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3720/4339]: training_loss: tensor(0.1553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3721/4339]: training_loss: tensor(0.2090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3722/4339]: training_loss: tensor(0.2868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3723/4339]: training_loss: tensor(0.2653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3724/4339]: training_loss: tensor(0.3300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3725/4339]: training_loss: tensor(0.4930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3726/4339]: training_loss: tensor(0.3973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3727/4339]: training_loss: tensor(0.1634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3728/4339]: training_loss: tensor(0.3387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3729/4339]: training_loss: tensor(0.1869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3730/4339]: training_loss: tensor(0.5230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3731/4339]: training_loss: tensor(0.7409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3732/4339]: training_loss: tensor(0.1839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3733/4339]: training_loss: tensor(0.1672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3734/4339]: training_loss: tensor(0.3519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3735/4339]: training_loss: tensor(0.2864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3736/4339]: training_loss: tensor(0.3210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3737/4339]: training_loss: tensor(0.1172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3738/4339]: training_loss: tensor(0.4051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3739/4339]: training_loss: tensor(0.2196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3740/4339]: training_loss: tensor(0.5546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3741/4339]: training_loss: tensor(0.2084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3742/4339]: training_loss: tensor(0.2673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3743/4339]: training_loss: tensor(0.5335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3744/4339]: training_loss: tensor(0.5153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3745/4339]: training_loss: tensor(0.1774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3746/4339]: training_loss: tensor(0.5609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3747/4339]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3748/4339]: training_loss: tensor(0.3587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3749/4339]: training_loss: tensor(0.5644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3750/4339]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3751/4339]: training_loss: tensor(0.2735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3752/4339]: training_loss: tensor(0.1797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3753/4339]: training_loss: tensor(0.3735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3754/4339]: training_loss: tensor(0.4126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3755/4339]: training_loss: tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3756/4339]: training_loss: tensor(0.7096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3757/4339]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3758/4339]: training_loss: tensor(0.2360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3759/4339]: training_loss: tensor(0.4338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3760/4339]: training_loss: tensor(0.5049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3761/4339]: training_loss: tensor(0.5524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3762/4339]: training_loss: tensor(0.3511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3763/4339]: training_loss: tensor(0.5232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3764/4339]: training_loss: tensor(0.3100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3765/4339]: training_loss: tensor(0.2494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3766/4339]: training_loss: tensor(0.3490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3767/4339]: training_loss: tensor(0.4209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3768/4339]: training_loss: tensor(0.1821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3769/4339]: training_loss: tensor(0.1762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3770/4339]: training_loss: tensor(0.1374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3771/4339]: training_loss: tensor(0.2515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3772/4339]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3773/4339]: training_loss: tensor(0.3210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3774/4339]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3775/4339]: training_loss: tensor(0.2853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3776/4339]: training_loss: tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3777/4339]: training_loss: tensor(0.1191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3778/4339]: training_loss: tensor(0.1539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3779/4339]: training_loss: tensor(0.8364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3780/4339]: training_loss: tensor(0.2953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3781/4339]: training_loss: tensor(0.4041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3782/4339]: training_loss: tensor(0.2008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3783/4339]: training_loss: tensor(0.2558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3784/4339]: training_loss: tensor(0.4501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3785/4339]: training_loss: tensor(0.5265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3786/4339]: training_loss: tensor(0.1887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3787/4339]: training_loss: tensor(0.1117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3788/4339]: training_loss: tensor(0.1987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3789/4339]: training_loss: tensor(0.1149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3790/4339]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3791/4339]: training_loss: tensor(0.5687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3792/4339]: training_loss: tensor(0.3676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3793/4339]: training_loss: tensor(0.3050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3794/4339]: training_loss: tensor(0.6005, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3795/4339]: training_loss: tensor(0.1532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3796/4339]: training_loss: tensor(0.2552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3797/4339]: training_loss: tensor(0.3767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3798/4339]: training_loss: tensor(0.4027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3799/4339]: training_loss: tensor(0.3570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3800/4339]: training_loss: tensor(0.3604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3801/4339]: training_loss: tensor(0.3581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3802/4339]: training_loss: tensor(0.3888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3803/4339]: training_loss: tensor(0.5135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3804/4339]: training_loss: tensor(0.2032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3805/4339]: training_loss: tensor(0.6777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3806/4339]: training_loss: tensor(0.1716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3807/4339]: training_loss: tensor(0.4165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3808/4339]: training_loss: tensor(0.3720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3809/4339]: training_loss: tensor(0.2452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3810/4339]: training_loss: tensor(0.2225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3811/4339]: training_loss: tensor(0.3806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3812/4339]: training_loss: tensor(0.4646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3813/4339]: training_loss: tensor(0.5907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3814/4339]: training_loss: tensor(0.4302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3815/4339]: training_loss: tensor(0.2860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3816/4339]: training_loss: tensor(0.2649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3817/4339]: training_loss: tensor(0.2060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3818/4339]: training_loss: tensor(0.2013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3819/4339]: training_loss: tensor(0.3730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3820/4339]: training_loss: tensor(0.4030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3821/4339]: training_loss: tensor(0.3174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3822/4339]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3823/4339]: training_loss: tensor(0.2843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3824/4339]: training_loss: tensor(0.1949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3825/4339]: training_loss: tensor(0.7048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3826/4339]: training_loss: tensor(0.1128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3827/4339]: training_loss: tensor(0.2736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3828/4339]: training_loss: tensor(0.2698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3829/4339]: training_loss: tensor(0.4585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3830/4339]: training_loss: tensor(0.3399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3831/4339]: training_loss: tensor(0.5891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3832/4339]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3833/4339]: training_loss: tensor(0.2296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3834/4339]: training_loss: tensor(0.4464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3835/4339]: training_loss: tensor(0.2744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3836/4339]: training_loss: tensor(0.2755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3837/4339]: training_loss: tensor(0.4175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3838/4339]: training_loss: tensor(0.4629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3839/4339]: training_loss: tensor(0.4598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3840/4339]: training_loss: tensor(0.3246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3841/4339]: training_loss: tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3842/4339]: training_loss: tensor(0.3250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3843/4339]: training_loss: tensor(0.3893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3844/4339]: training_loss: tensor(0.1495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3845/4339]: training_loss: tensor(0.3060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3846/4339]: training_loss: tensor(0.2580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3847/4339]: training_loss: tensor(0.3161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3848/4339]: training_loss: tensor(0.1393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3849/4339]: training_loss: tensor(0.4798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3850/4339]: training_loss: tensor(0.1768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3851/4339]: training_loss: tensor(0.2525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3852/4339]: training_loss: tensor(0.4041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3853/4339]: training_loss: tensor(0.1420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3854/4339]: training_loss: tensor(0.2552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3855/4339]: training_loss: tensor(0.5944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3856/4339]: training_loss: tensor(0.2202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3857/4339]: training_loss: tensor(0.3957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3858/4339]: training_loss: tensor(0.1244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3859/4339]: training_loss: tensor(0.4660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3860/4339]: training_loss: tensor(0.2505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3861/4339]: training_loss: tensor(0.1909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3862/4339]: training_loss: tensor(0.3218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3863/4339]: training_loss: tensor(0.0566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3864/4339]: training_loss: tensor(0.3270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3865/4339]: training_loss: tensor(0.1926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3866/4339]: training_loss: tensor(0.4690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3867/4339]: training_loss: tensor(0.2167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3868/4339]: training_loss: tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3869/4339]: training_loss: tensor(0.2940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3870/4339]: training_loss: tensor(0.2851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3871/4339]: training_loss: tensor(0.3832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3872/4339]: training_loss: tensor(0.1368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3873/4339]: training_loss: tensor(0.1198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3874/4339]: training_loss: tensor(0.0543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3875/4339]: training_loss: tensor(0.3370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3876/4339]: training_loss: tensor(0.2119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3877/4339]: training_loss: tensor(0.3700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3878/4339]: training_loss: tensor(0.8561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3879/4339]: training_loss: tensor(0.3282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3880/4339]: training_loss: tensor(0.7878, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3881/4339]: training_loss: tensor(0.2510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3882/4339]: training_loss: tensor(0.3129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3883/4339]: training_loss: tensor(0.6193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3884/4339]: training_loss: tensor(0.6259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3885/4339]: training_loss: tensor(0.3835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3886/4339]: training_loss: tensor(0.2413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3887/4339]: training_loss: tensor(0.3512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3888/4339]: training_loss: tensor(0.5335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3889/4339]: training_loss: tensor(0.5407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3890/4339]: training_loss: tensor(0.4996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3891/4339]: training_loss: tensor(0.1576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3892/4339]: training_loss: tensor(0.4316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3893/4339]: training_loss: tensor(0.3977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3894/4339]: training_loss: tensor(0.2952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3895/4339]: training_loss: tensor(0.2515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3896/4339]: training_loss: tensor(0.4253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3897/4339]: training_loss: tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3898/4339]: training_loss: tensor(0.5353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3899/4339]: training_loss: tensor(0.4223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3900/4339]: training_loss: tensor(0.3363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3901/4339]: training_loss: tensor(0.3245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3902/4339]: training_loss: tensor(0.6764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3903/4339]: training_loss: tensor(0.3450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3904/4339]: training_loss: tensor(0.2353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3905/4339]: training_loss: tensor(0.2615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3906/4339]: training_loss: tensor(0.3280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3907/4339]: training_loss: tensor(0.5630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3908/4339]: training_loss: tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3909/4339]: training_loss: tensor(0.2721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3910/4339]: training_loss: tensor(0.6667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3911/4339]: training_loss: tensor(0.4090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3912/4339]: training_loss: tensor(0.3625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3913/4339]: training_loss: tensor(0.4615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3914/4339]: training_loss: tensor(0.3851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3915/4339]: training_loss: tensor(0.3631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3916/4339]: training_loss: tensor(0.3154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3917/4339]: training_loss: tensor(0.2754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3918/4339]: training_loss: tensor(0.3638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3919/4339]: training_loss: tensor(0.4423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3920/4339]: training_loss: tensor(0.2331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3921/4339]: training_loss: tensor(0.4548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3922/4339]: training_loss: tensor(0.3173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3923/4339]: training_loss: tensor(0.2517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3924/4339]: training_loss: tensor(0.5103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3925/4339]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3926/4339]: training_loss: tensor(0.5358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3927/4339]: training_loss: tensor(0.3115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3928/4339]: training_loss: tensor(0.6349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3929/4339]: training_loss: tensor(0.6058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3930/4339]: training_loss: tensor(0.3116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3931/4339]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3932/4339]: training_loss: tensor(0.2615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3933/4339]: training_loss: tensor(0.6391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3934/4339]: training_loss: tensor(0.3225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3935/4339]: training_loss: tensor(0.3021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3936/4339]: training_loss: tensor(0.3855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3937/4339]: training_loss: tensor(0.5674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3938/4339]: training_loss: tensor(0.3439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3939/4339]: training_loss: tensor(0.2775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3940/4339]: training_loss: tensor(0.4038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3941/4339]: training_loss: tensor(0.1705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3942/4339]: training_loss: tensor(0.2230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3943/4339]: training_loss: tensor(0.1321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3944/4339]: training_loss: tensor(0.3144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3945/4339]: training_loss: tensor(0.2601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3946/4339]: training_loss: tensor(0.1196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3947/4339]: training_loss: tensor(0.2756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3948/4339]: training_loss: tensor(0.2941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3949/4339]: training_loss: tensor(0.2769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3950/4339]: training_loss: tensor(0.4346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3951/4339]: training_loss: tensor(0.3377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3952/4339]: training_loss: tensor(0.1120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3953/4339]: training_loss: tensor(0.3190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3954/4339]: training_loss: tensor(0.4011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3955/4339]: training_loss: tensor(0.2351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3956/4339]: training_loss: tensor(0.1936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3957/4339]: training_loss: tensor(0.2517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3958/4339]: training_loss: tensor(0.1649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3959/4339]: training_loss: tensor(0.3673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3960/4339]: training_loss: tensor(0.1095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3961/4339]: training_loss: tensor(0.4282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3962/4339]: training_loss: tensor(0.4843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3963/4339]: training_loss: tensor(0.1199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3964/4339]: training_loss: tensor(0.3634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3965/4339]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3966/4339]: training_loss: tensor(0.1999, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3967/4339]: training_loss: tensor(0.1284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3968/4339]: training_loss: tensor(0.2200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3969/4339]: training_loss: tensor(0.3935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3970/4339]: training_loss: tensor(0.3692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3971/4339]: training_loss: tensor(0.2699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3972/4339]: training_loss: tensor(0.1839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3973/4339]: training_loss: tensor(0.2023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3974/4339]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3975/4339]: training_loss: tensor(0.1781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3976/4339]: training_loss: tensor(0.4788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3977/4339]: training_loss: tensor(0.3460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3978/4339]: training_loss: tensor(0.5344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3979/4339]: training_loss: tensor(0.2407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3980/4339]: training_loss: tensor(0.2146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3981/4339]: training_loss: tensor(0.2703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3982/4339]: training_loss: tensor(0.2226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3983/4339]: training_loss: tensor(0.2367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3984/4339]: training_loss: tensor(0.6843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3985/4339]: training_loss: tensor(0.2289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3986/4339]: training_loss: tensor(0.4207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3987/4339]: training_loss: tensor(0.6345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3988/4339]: training_loss: tensor(0.5501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3989/4339]: training_loss: tensor(0.2643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3990/4339]: training_loss: tensor(0.2247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3991/4339]: training_loss: tensor(0.4214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3992/4339]: training_loss: tensor(0.1775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3993/4339]: training_loss: tensor(0.2367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3994/4339]: training_loss: tensor(0.3330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3995/4339]: training_loss: tensor(0.4681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3996/4339]: training_loss: tensor(0.2517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3997/4339]: training_loss: tensor(0.1693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3998/4339]: training_loss: tensor(0.1701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3999/4339]: training_loss: tensor(0.2116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4000/4339]: training_loss: tensor(0.2634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4001/4339]: training_loss: tensor(0.2624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4002/4339]: training_loss: tensor(0.1807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4003/4339]: training_loss: tensor(0.3202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4004/4339]: training_loss: tensor(0.2069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4005/4339]: training_loss: tensor(0.2434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4006/4339]: training_loss: tensor(0.6944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4007/4339]: training_loss: tensor(0.2987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4008/4339]: training_loss: tensor(0.5639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4009/4339]: training_loss: tensor(0.3556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4010/4339]: training_loss: tensor(0.3896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4011/4339]: training_loss: tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4012/4339]: training_loss: tensor(0.4898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4013/4339]: training_loss: tensor(0.3400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4014/4339]: training_loss: tensor(0.1223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4015/4339]: training_loss: tensor(0.2242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4016/4339]: training_loss: tensor(0.3852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4017/4339]: training_loss: tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4018/4339]: training_loss: tensor(0.4024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4019/4339]: training_loss: tensor(0.2307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4020/4339]: training_loss: tensor(0.1535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4021/4339]: training_loss: tensor(0.2064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4022/4339]: training_loss: tensor(0.4245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4023/4339]: training_loss: tensor(0.1729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4024/4339]: training_loss: tensor(0.6762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4025/4339]: training_loss: tensor(0.3580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4026/4339]: training_loss: tensor(0.4664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4027/4339]: training_loss: tensor(0.3028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4028/4339]: training_loss: tensor(0.2537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4029/4339]: training_loss: tensor(0.6172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4030/4339]: training_loss: tensor(0.4471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4031/4339]: training_loss: tensor(0.4924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4032/4339]: training_loss: tensor(0.6231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4033/4339]: training_loss: tensor(0.4375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4034/4339]: training_loss: tensor(0.1781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4035/4339]: training_loss: tensor(0.4303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4036/4339]: training_loss: tensor(0.1701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4037/4339]: training_loss: tensor(0.8328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4038/4339]: training_loss: tensor(0.2961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4039/4339]: training_loss: tensor(0.4665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4040/4339]: training_loss: tensor(0.4560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4041/4339]: training_loss: tensor(0.3564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4042/4339]: training_loss: tensor(0.2116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4043/4339]: training_loss: tensor(0.2980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4044/4339]: training_loss: tensor(0.3244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4045/4339]: training_loss: tensor(0.2887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4046/4339]: training_loss: tensor(0.4301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4047/4339]: training_loss: tensor(0.1950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4048/4339]: training_loss: tensor(0.3982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4049/4339]: training_loss: tensor(0.3524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4050/4339]: training_loss: tensor(0.5597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4051/4339]: training_loss: tensor(0.3824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4052/4339]: training_loss: tensor(0.4312, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4053/4339]: training_loss: tensor(0.9185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4054/4339]: training_loss: tensor(0.3469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4055/4339]: training_loss: tensor(0.3331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4056/4339]: training_loss: tensor(0.4462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4057/4339]: training_loss: tensor(0.3608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4058/4339]: training_loss: tensor(0.4237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4059/4339]: training_loss: tensor(0.2481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4060/4339]: training_loss: tensor(0.2588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4061/4339]: training_loss: tensor(0.1436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4062/4339]: training_loss: tensor(0.2513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4063/4339]: training_loss: tensor(0.4698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4064/4339]: training_loss: tensor(0.2297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4065/4339]: training_loss: tensor(0.1986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4066/4339]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4067/4339]: training_loss: tensor(0.1387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4068/4339]: training_loss: tensor(0.2066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4069/4339]: training_loss: tensor(0.1544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4070/4339]: training_loss: tensor(0.9059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4071/4339]: training_loss: tensor(0.2172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4072/4339]: training_loss: tensor(0.3918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4073/4339]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4074/4339]: training_loss: tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4075/4339]: training_loss: tensor(0.6525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4076/4339]: training_loss: tensor(0.4113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4077/4339]: training_loss: tensor(0.2239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4078/4339]: training_loss: tensor(0.3029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4079/4339]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4080/4339]: training_loss: tensor(0.3708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4081/4339]: training_loss: tensor(0.4149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4082/4339]: training_loss: tensor(0.2774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4083/4339]: training_loss: tensor(0.5769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4084/4339]: training_loss: tensor(0.2511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4085/4339]: training_loss: tensor(0.3828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4086/4339]: training_loss: tensor(0.3876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4087/4339]: training_loss: tensor(0.3313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4088/4339]: training_loss: tensor(0.3218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4089/4339]: training_loss: tensor(0.1756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4090/4339]: training_loss: tensor(0.3450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4091/4339]: training_loss: tensor(0.5027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4092/4339]: training_loss: tensor(0.3405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4093/4339]: training_loss: tensor(0.5593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4094/4339]: training_loss: tensor(0.3100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4095/4339]: training_loss: tensor(0.3488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4096/4339]: training_loss: tensor(0.2146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4097/4339]: training_loss: tensor(0.3559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4098/4339]: training_loss: tensor(0.1533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4099/4339]: training_loss: tensor(0.2459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4100/4339]: training_loss: tensor(0.4256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4101/4339]: training_loss: tensor(0.5707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4102/4339]: training_loss: tensor(0.5918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4103/4339]: training_loss: tensor(0.1529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4104/4339]: training_loss: tensor(0.1297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4105/4339]: training_loss: tensor(0.6066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4106/4339]: training_loss: tensor(0.3791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4107/4339]: training_loss: tensor(0.4165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4108/4339]: training_loss: tensor(0.2934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4109/4339]: training_loss: tensor(0.6299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4110/4339]: training_loss: tensor(0.2397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4111/4339]: training_loss: tensor(0.1798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4112/4339]: training_loss: tensor(0.3800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4113/4339]: training_loss: tensor(0.3866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4114/4339]: training_loss: tensor(0.2334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4115/4339]: training_loss: tensor(0.2995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4116/4339]: training_loss: tensor(0.1642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4117/4339]: training_loss: tensor(0.2532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4118/4339]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4119/4339]: training_loss: tensor(0.2804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4120/4339]: training_loss: tensor(0.6410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4121/4339]: training_loss: tensor(0.2950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4122/4339]: training_loss: tensor(0.3924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4123/4339]: training_loss: tensor(0.2830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4124/4339]: training_loss: tensor(0.4257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4125/4339]: training_loss: tensor(0.2832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4126/4339]: training_loss: tensor(0.3817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4127/4339]: training_loss: tensor(0.5447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4128/4339]: training_loss: tensor(0.3626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4129/4339]: training_loss: tensor(0.3266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4130/4339]: training_loss: tensor(0.3923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4131/4339]: training_loss: tensor(0.2494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4132/4339]: training_loss: tensor(0.7278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4133/4339]: training_loss: tensor(0.5866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4134/4339]: training_loss: tensor(0.2179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4135/4339]: training_loss: tensor(0.5593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4136/4339]: training_loss: tensor(0.2281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4137/4339]: training_loss: tensor(0.3152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4138/4339]: training_loss: tensor(0.2956, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4139/4339]: training_loss: tensor(0.4579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4140/4339]: training_loss: tensor(0.1373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4141/4339]: training_loss: tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4142/4339]: training_loss: tensor(0.2556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4143/4339]: training_loss: tensor(0.3926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4144/4339]: training_loss: tensor(0.3274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4145/4339]: training_loss: tensor(0.2465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4146/4339]: training_loss: tensor(0.2054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4147/4339]: training_loss: tensor(0.2534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4148/4339]: training_loss: tensor(0.3503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4149/4339]: training_loss: tensor(0.4435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4150/4339]: training_loss: tensor(0.3942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4151/4339]: training_loss: tensor(0.1241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4152/4339]: training_loss: tensor(0.4720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4153/4339]: training_loss: tensor(0.2369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4154/4339]: training_loss: tensor(0.1310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4155/4339]: training_loss: tensor(0.2167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4156/4339]: training_loss: tensor(0.5995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4157/4339]: training_loss: tensor(0.1823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4158/4339]: training_loss: tensor(0.1742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4159/4339]: training_loss: tensor(0.7343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4160/4339]: training_loss: tensor(0.4049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4161/4339]: training_loss: tensor(0.4438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4162/4339]: training_loss: tensor(0.3022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4163/4339]: training_loss: tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4164/4339]: training_loss: tensor(0.5511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4165/4339]: training_loss: tensor(0.3256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4166/4339]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4167/4339]: training_loss: tensor(0.1996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4168/4339]: training_loss: tensor(0.4024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4169/4339]: training_loss: tensor(0.3012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4170/4339]: training_loss: tensor(0.2777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4171/4339]: training_loss: tensor(0.1816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4172/4339]: training_loss: tensor(0.2290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4173/4339]: training_loss: tensor(0.5829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4174/4339]: training_loss: tensor(0.2116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4175/4339]: training_loss: tensor(0.3391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4176/4339]: training_loss: tensor(0.1775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4177/4339]: training_loss: tensor(0.6598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4178/4339]: training_loss: tensor(0.4060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4179/4339]: training_loss: tensor(0.5778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4180/4339]: training_loss: tensor(0.4597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4181/4339]: training_loss: tensor(0.2534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4182/4339]: training_loss: tensor(0.2938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4183/4339]: training_loss: tensor(0.1827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4184/4339]: training_loss: tensor(0.3938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4185/4339]: training_loss: tensor(0.2803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4186/4339]: training_loss: tensor(0.2983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4187/4339]: training_loss: tensor(0.2940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4188/4339]: training_loss: tensor(0.3693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4189/4339]: training_loss: tensor(0.1861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4190/4339]: training_loss: tensor(0.2450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4191/4339]: training_loss: tensor(0.3706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4192/4339]: training_loss: tensor(0.4016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4193/4339]: training_loss: tensor(0.4298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4194/4339]: training_loss: tensor(0.1231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4195/4339]: training_loss: tensor(0.1918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4196/4339]: training_loss: tensor(0.2623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4197/4339]: training_loss: tensor(0.4194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4198/4339]: training_loss: tensor(0.6028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4199/4339]: training_loss: tensor(0.1858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4200/4339]: training_loss: tensor(0.3124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4201/4339]: training_loss: tensor(0.2594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4202/4339]: training_loss: tensor(0.3114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4203/4339]: training_loss: tensor(0.7004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4204/4339]: training_loss: tensor(0.3210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4205/4339]: training_loss: tensor(0.2915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4206/4339]: training_loss: tensor(0.2278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4207/4339]: training_loss: tensor(0.2373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4208/4339]: training_loss: tensor(0.4290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4209/4339]: training_loss: tensor(0.3216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4210/4339]: training_loss: tensor(0.7155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4211/4339]: training_loss: tensor(0.1829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4212/4339]: training_loss: tensor(0.3009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4213/4339]: training_loss: tensor(0.5108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4214/4339]: training_loss: tensor(0.3501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4215/4339]: training_loss: tensor(0.5042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4216/4339]: training_loss: tensor(0.2266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4217/4339]: training_loss: tensor(0.5118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4218/4339]: training_loss: tensor(0.3357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4219/4339]: training_loss: tensor(0.2514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4220/4339]: training_loss: tensor(0.4218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4221/4339]: training_loss: tensor(0.4657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4222/4339]: training_loss: tensor(0.3492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4223/4339]: training_loss: tensor(0.7112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4224/4339]: training_loss: tensor(0.6303, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4225/4339]: training_loss: tensor(0.3084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4226/4339]: training_loss: tensor(0.3604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4227/4339]: training_loss: tensor(0.3168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4228/4339]: training_loss: tensor(0.3561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4229/4339]: training_loss: tensor(0.3783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4230/4339]: training_loss: tensor(0.2774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4231/4339]: training_loss: tensor(0.4959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4232/4339]: training_loss: tensor(0.1808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4233/4339]: training_loss: tensor(0.3392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4234/4339]: training_loss: tensor(0.4399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4235/4339]: training_loss: tensor(0.2089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4236/4339]: training_loss: tensor(0.5139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4237/4339]: training_loss: tensor(0.2709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4238/4339]: training_loss: tensor(0.1802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4239/4339]: training_loss: tensor(0.1091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4240/4339]: training_loss: tensor(0.4446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4241/4339]: training_loss: tensor(0.2290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4242/4339]: training_loss: tensor(0.7102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4243/4339]: training_loss: tensor(0.4588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4244/4339]: training_loss: tensor(0.3754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4245/4339]: training_loss: tensor(0.4179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4246/4339]: training_loss: tensor(0.4431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4247/4339]: training_loss: tensor(0.5639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4248/4339]: training_loss: tensor(0.6089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4249/4339]: training_loss: tensor(0.1963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4250/4339]: training_loss: tensor(0.4212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4251/4339]: training_loss: tensor(0.4213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4252/4339]: training_loss: tensor(0.2581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4253/4339]: training_loss: tensor(0.1251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4254/4339]: training_loss: tensor(0.4109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4255/4339]: training_loss: tensor(0.4206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4256/4339]: training_loss: tensor(0.4208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4257/4339]: training_loss: tensor(0.3053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4258/4339]: training_loss: tensor(0.2164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4259/4339]: training_loss: tensor(0.3518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4260/4339]: training_loss: tensor(0.5524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4261/4339]: training_loss: tensor(0.7956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4262/4339]: training_loss: tensor(0.5447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4263/4339]: training_loss: tensor(0.2684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4264/4339]: training_loss: tensor(0.3546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4265/4339]: training_loss: tensor(0.3061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4266/4339]: training_loss: tensor(0.2216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4267/4339]: training_loss: tensor(0.4790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4268/4339]: training_loss: tensor(0.3637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4269/4339]: training_loss: tensor(0.4003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4270/4339]: training_loss: tensor(0.4511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4271/4339]: training_loss: tensor(0.4912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4272/4339]: training_loss: tensor(0.3138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4273/4339]: training_loss: tensor(0.4155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4274/4339]: training_loss: tensor(0.3043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4275/4339]: training_loss: tensor(0.3708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4276/4339]: training_loss: tensor(0.3588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4277/4339]: training_loss: tensor(0.3160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4278/4339]: training_loss: tensor(0.2000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4279/4339]: training_loss: tensor(0.6347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4280/4339]: training_loss: tensor(0.2543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4281/4339]: training_loss: tensor(0.2869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4282/4339]: training_loss: tensor(0.6841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4283/4339]: training_loss: tensor(0.2346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4284/4339]: training_loss: tensor(0.3811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4285/4339]: training_loss: tensor(0.1863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4286/4339]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4287/4339]: training_loss: tensor(0.1229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4288/4339]: training_loss: tensor(0.1501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4289/4339]: training_loss: tensor(0.5370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4290/4339]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4291/4339]: training_loss: tensor(0.1538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4292/4339]: training_loss: tensor(0.3522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4293/4339]: training_loss: tensor(0.3556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4294/4339]: training_loss: tensor(0.3576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4295/4339]: training_loss: tensor(0.1478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4296/4339]: training_loss: tensor(0.4301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4297/4339]: training_loss: tensor(0.3135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4298/4339]: training_loss: tensor(0.2711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4299/4339]: training_loss: tensor(0.3528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4300/4339]: training_loss: tensor(0.2415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4301/4339]: training_loss: tensor(0.3188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4302/4339]: training_loss: tensor(0.3077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4303/4339]: training_loss: tensor(0.1528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4304/4339]: training_loss: tensor(0.3094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4305/4339]: training_loss: tensor(0.1824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4306/4339]: training_loss: tensor(0.5353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4307/4339]: training_loss: tensor(0.2980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4308/4339]: training_loss: tensor(0.3438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4309/4339]: training_loss: tensor(0.1881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4310/4339]: training_loss: tensor(0.4017, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4311/4339]: training_loss: tensor(0.4796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4312/4339]: training_loss: tensor(0.3859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4313/4339]: training_loss: tensor(0.2879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4314/4339]: training_loss: tensor(0.3803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4315/4339]: training_loss: tensor(0.2695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4316/4339]: training_loss: tensor(0.3202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4317/4339]: training_loss: tensor(0.8095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4318/4339]: training_loss: tensor(0.3662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4319/4339]: training_loss: tensor(0.3583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4320/4339]: training_loss: tensor(0.4723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4321/4339]: training_loss: tensor(0.3758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4322/4339]: training_loss: tensor(0.5036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4323/4339]: training_loss: tensor(0.4919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4324/4339]: training_loss: tensor(0.3247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4325/4339]: training_loss: tensor(0.4305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4326/4339]: training_loss: tensor(0.3370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4327/4339]: training_loss: tensor(0.5645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4328/4339]: training_loss: tensor(0.4087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4329/4339]: training_loss: tensor(0.3497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4330/4339]: training_loss: tensor(0.4141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4331/4339]: training_loss: tensor(0.4366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4332/4339]: training_loss: tensor(0.3325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4333/4339]: training_loss: tensor(0.4054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4334/4339]: training_loss: tensor(0.2731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4335/4339]: training_loss: tensor(0.3047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4336/4339]: training_loss: tensor(0.3861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4337/4339]: training_loss: tensor(0.4542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4338/4339]: training_loss: tensor(0.4683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4339/4339]: training_loss: tensor(0.8124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4340/4339]: training_loss: tensor(0.2036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [1/5], Step [4340/21700], Train Loss: 0.3196, Valid Loss: 0.3018\n",
      "Model saved to ==> Model/model.pt\n",
      "Model saved to ==> Model/metrics.pt\n",
      "batch_no [1/4339]: training_loss: tensor(0.2950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2/4339]: training_loss: tensor(0.3896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3/4339]: training_loss: tensor(0.3859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4/4339]: training_loss: tensor(0.1939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [5/4339]: training_loss: tensor(0.3963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [6/4339]: training_loss: tensor(0.4723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [7/4339]: training_loss: tensor(0.3497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [8/4339]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [9/4339]: training_loss: tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [10/4339]: training_loss: tensor(0.1735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [11/4339]: training_loss: tensor(0.2216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [12/4339]: training_loss: tensor(0.2397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [13/4339]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [14/4339]: training_loss: tensor(0.2618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [15/4339]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [16/4339]: training_loss: tensor(0.1605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [17/4339]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [18/4339]: training_loss: tensor(0.1835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [19/4339]: training_loss: tensor(0.4867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [20/4339]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [21/4339]: training_loss: tensor(0.1506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [22/4339]: training_loss: tensor(0.4215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [23/4339]: training_loss: tensor(0.6524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [24/4339]: training_loss: tensor(0.1220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [25/4339]: training_loss: tensor(0.1525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [26/4339]: training_loss: tensor(0.1459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [27/4339]: training_loss: tensor(0.1631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [28/4339]: training_loss: tensor(0.1393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [29/4339]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [30/4339]: training_loss: tensor(0.5070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [31/4339]: training_loss: tensor(0.3139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [32/4339]: training_loss: tensor(0.0447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [33/4339]: training_loss: tensor(0.2525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [34/4339]: training_loss: tensor(0.8302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [35/4339]: training_loss: tensor(0.3996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [36/4339]: training_loss: tensor(0.3366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [37/4339]: training_loss: tensor(0.2232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [38/4339]: training_loss: tensor(0.2179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [39/4339]: training_loss: tensor(0.2886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [40/4339]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [41/4339]: training_loss: tensor(0.3293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [42/4339]: training_loss: tensor(0.1799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [43/4339]: training_loss: tensor(0.1351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [44/4339]: training_loss: tensor(0.3447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [45/4339]: training_loss: tensor(0.3360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [46/4339]: training_loss: tensor(0.3083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [47/4339]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [48/4339]: training_loss: tensor(0.1723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [49/4339]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [50/4339]: training_loss: tensor(0.3938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [51/4339]: training_loss: tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [52/4339]: training_loss: tensor(0.4120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [53/4339]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [54/4339]: training_loss: tensor(0.4208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [55/4339]: training_loss: tensor(0.3115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [56/4339]: training_loss: tensor(0.5380, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [57/4339]: training_loss: tensor(0.1671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [58/4339]: training_loss: tensor(0.6098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [59/4339]: training_loss: tensor(0.3908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [60/4339]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [61/4339]: training_loss: tensor(0.2864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [62/4339]: training_loss: tensor(0.3260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [63/4339]: training_loss: tensor(0.3605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [64/4339]: training_loss: tensor(0.4742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [65/4339]: training_loss: tensor(0.1962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [66/4339]: training_loss: tensor(0.1402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [67/4339]: training_loss: tensor(0.1743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [68/4339]: training_loss: tensor(0.3376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [69/4339]: training_loss: tensor(0.3978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [70/4339]: training_loss: tensor(0.3274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [71/4339]: training_loss: tensor(0.1621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [72/4339]: training_loss: tensor(0.5468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [73/4339]: training_loss: tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [74/4339]: training_loss: tensor(0.1127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [75/4339]: training_loss: tensor(0.1211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [76/4339]: training_loss: tensor(0.1398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [77/4339]: training_loss: tensor(0.5232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [78/4339]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [79/4339]: training_loss: tensor(0.1932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [80/4339]: training_loss: tensor(0.4528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [81/4339]: training_loss: tensor(0.3350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [82/4339]: training_loss: tensor(0.4347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [83/4339]: training_loss: tensor(0.6561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [84/4339]: training_loss: tensor(0.3269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [85/4339]: training_loss: tensor(0.7961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [86/4339]: training_loss: tensor(0.1876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [87/4339]: training_loss: tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [88/4339]: training_loss: tensor(0.3856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [89/4339]: training_loss: tensor(0.1903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [90/4339]: training_loss: tensor(0.3528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [91/4339]: training_loss: tensor(0.2893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [92/4339]: training_loss: tensor(0.3351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [93/4339]: training_loss: tensor(0.2369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [94/4339]: training_loss: tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [95/4339]: training_loss: tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [96/4339]: training_loss: tensor(0.1363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [97/4339]: training_loss: tensor(0.1553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [98/4339]: training_loss: tensor(0.1255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [99/4339]: training_loss: tensor(0.1993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [100/4339]: training_loss: tensor(0.2482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [101/4339]: training_loss: tensor(0.2660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [102/4339]: training_loss: tensor(0.1715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [103/4339]: training_loss: tensor(0.4333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [104/4339]: training_loss: tensor(0.1509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [105/4339]: training_loss: tensor(0.3091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [106/4339]: training_loss: tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [107/4339]: training_loss: tensor(0.2301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [108/4339]: training_loss: tensor(0.1147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [109/4339]: training_loss: tensor(0.5750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [110/4339]: training_loss: tensor(0.3777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [111/4339]: training_loss: tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [112/4339]: training_loss: tensor(0.2571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [113/4339]: training_loss: tensor(0.4736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [114/4339]: training_loss: tensor(0.4839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [115/4339]: training_loss: tensor(0.1514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [116/4339]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [117/4339]: training_loss: tensor(0.3467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [118/4339]: training_loss: tensor(0.3556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [119/4339]: training_loss: tensor(0.2609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [120/4339]: training_loss: tensor(0.4519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [121/4339]: training_loss: tensor(0.1698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [122/4339]: training_loss: tensor(0.1619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [123/4339]: training_loss: tensor(0.8116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [124/4339]: training_loss: tensor(0.5209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [125/4339]: training_loss: tensor(0.6047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [126/4339]: training_loss: tensor(0.4224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [127/4339]: training_loss: tensor(0.1602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [128/4339]: training_loss: tensor(0.3908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [129/4339]: training_loss: tensor(0.1200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [130/4339]: training_loss: tensor(0.2620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [131/4339]: training_loss: tensor(0.1595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [132/4339]: training_loss: tensor(0.2186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [133/4339]: training_loss: tensor(0.1274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [134/4339]: training_loss: tensor(0.3808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [135/4339]: training_loss: tensor(0.1592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [136/4339]: training_loss: tensor(0.1814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [137/4339]: training_loss: tensor(0.2127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [138/4339]: training_loss: tensor(0.1596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [139/4339]: training_loss: tensor(0.3664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [140/4339]: training_loss: tensor(0.2201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [141/4339]: training_loss: tensor(0.5608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [142/4339]: training_loss: tensor(0.5378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [143/4339]: training_loss: tensor(0.3617, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [144/4339]: training_loss: tensor(0.3256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [145/4339]: training_loss: tensor(0.1406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [146/4339]: training_loss: tensor(0.4179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [147/4339]: training_loss: tensor(0.2409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [148/4339]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [149/4339]: training_loss: tensor(0.2592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [150/4339]: training_loss: tensor(0.3518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [151/4339]: training_loss: tensor(0.2288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [152/4339]: training_loss: tensor(0.2718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [153/4339]: training_loss: tensor(0.2009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [154/4339]: training_loss: tensor(0.1504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [155/4339]: training_loss: tensor(0.1526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [156/4339]: training_loss: tensor(0.1303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [157/4339]: training_loss: tensor(0.2290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [158/4339]: training_loss: tensor(0.1325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [159/4339]: training_loss: tensor(0.2163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [160/4339]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [161/4339]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [162/4339]: training_loss: tensor(0.0462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [163/4339]: training_loss: tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [164/4339]: training_loss: tensor(0.3194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [165/4339]: training_loss: tensor(0.3538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [166/4339]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [167/4339]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [168/4339]: training_loss: tensor(0.4627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [169/4339]: training_loss: tensor(0.5477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [170/4339]: training_loss: tensor(0.5631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [171/4339]: training_loss: tensor(0.3388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [172/4339]: training_loss: tensor(0.4682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [173/4339]: training_loss: tensor(0.5763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [174/4339]: training_loss: tensor(0.5942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [175/4339]: training_loss: tensor(0.1912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [176/4339]: training_loss: tensor(0.2133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [177/4339]: training_loss: tensor(0.2705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [178/4339]: training_loss: tensor(0.1713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [179/4339]: training_loss: tensor(0.3509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [180/4339]: training_loss: tensor(0.2536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [181/4339]: training_loss: tensor(0.3109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [182/4339]: training_loss: tensor(0.1676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [183/4339]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [184/4339]: training_loss: tensor(0.1494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [185/4339]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [186/4339]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [187/4339]: training_loss: tensor(0.1858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [188/4339]: training_loss: tensor(0.2612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [189/4339]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [190/4339]: training_loss: tensor(0.4965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [191/4339]: training_loss: tensor(0.4191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [192/4339]: training_loss: tensor(0.2500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [193/4339]: training_loss: tensor(0.1242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [194/4339]: training_loss: tensor(0.2919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [195/4339]: training_loss: tensor(0.2133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [196/4339]: training_loss: tensor(0.3929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [197/4339]: training_loss: tensor(0.3515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [198/4339]: training_loss: tensor(0.3114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [199/4339]: training_loss: tensor(0.1635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [200/4339]: training_loss: tensor(0.0413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [201/4339]: training_loss: tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [202/4339]: training_loss: tensor(0.4072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [203/4339]: training_loss: tensor(0.5741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [204/4339]: training_loss: tensor(0.4745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [205/4339]: training_loss: tensor(0.2099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [206/4339]: training_loss: tensor(0.1921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [207/4339]: training_loss: tensor(0.0594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [208/4339]: training_loss: tensor(0.1859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [209/4339]: training_loss: tensor(0.2471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [210/4339]: training_loss: tensor(0.5730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [211/4339]: training_loss: tensor(0.3974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [212/4339]: training_loss: tensor(0.0686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [213/4339]: training_loss: tensor(0.2774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [214/4339]: training_loss: tensor(0.1327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [215/4339]: training_loss: tensor(0.1279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [216/4339]: training_loss: tensor(0.1860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [217/4339]: training_loss: tensor(0.1801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [218/4339]: training_loss: tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [219/4339]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [220/4339]: training_loss: tensor(0.3683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [221/4339]: training_loss: tensor(0.3407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [222/4339]: training_loss: tensor(0.5270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [223/4339]: training_loss: tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [224/4339]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [225/4339]: training_loss: tensor(0.1759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [226/4339]: training_loss: tensor(0.2048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [227/4339]: training_loss: tensor(0.2291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [228/4339]: training_loss: tensor(0.0429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [229/4339]: training_loss: tensor(0.2627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [230/4339]: training_loss: tensor(0.1558, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [231/4339]: training_loss: tensor(0.3857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [232/4339]: training_loss: tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [233/4339]: training_loss: tensor(0.3558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [234/4339]: training_loss: tensor(0.2726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [235/4339]: training_loss: tensor(0.5738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [236/4339]: training_loss: tensor(0.1761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [237/4339]: training_loss: tensor(0.6119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [238/4339]: training_loss: tensor(0.2225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [239/4339]: training_loss: tensor(0.1191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [240/4339]: training_loss: tensor(0.1922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [241/4339]: training_loss: tensor(0.2655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [242/4339]: training_loss: tensor(0.1292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [243/4339]: training_loss: tensor(0.1338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [244/4339]: training_loss: tensor(0.1612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [245/4339]: training_loss: tensor(0.2411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [246/4339]: training_loss: tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [247/4339]: training_loss: tensor(0.6186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [248/4339]: training_loss: tensor(0.5201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [249/4339]: training_loss: tensor(0.1103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [250/4339]: training_loss: tensor(0.3105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [251/4339]: training_loss: tensor(0.1069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [252/4339]: training_loss: tensor(0.1832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [253/4339]: training_loss: tensor(0.2417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [254/4339]: training_loss: tensor(0.1460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [255/4339]: training_loss: tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [256/4339]: training_loss: tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [257/4339]: training_loss: tensor(0.2263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [258/4339]: training_loss: tensor(0.4470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [259/4339]: training_loss: tensor(0.2317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [260/4339]: training_loss: tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [261/4339]: training_loss: tensor(0.2660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [262/4339]: training_loss: tensor(0.1463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [263/4339]: training_loss: tensor(0.2101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [264/4339]: training_loss: tensor(0.3825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [265/4339]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [266/4339]: training_loss: tensor(0.4395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [267/4339]: training_loss: tensor(0.1883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [268/4339]: training_loss: tensor(0.3137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [269/4339]: training_loss: tensor(0.4404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [270/4339]: training_loss: tensor(0.2099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [271/4339]: training_loss: tensor(0.2294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [272/4339]: training_loss: tensor(0.1359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [273/4339]: training_loss: tensor(0.3045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [274/4339]: training_loss: tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [275/4339]: training_loss: tensor(0.1333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [276/4339]: training_loss: tensor(0.6148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [277/4339]: training_loss: tensor(0.3502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [278/4339]: training_loss: tensor(0.0587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [279/4339]: training_loss: tensor(0.3668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [280/4339]: training_loss: tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [281/4339]: training_loss: tensor(0.1624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [282/4339]: training_loss: tensor(0.1634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [283/4339]: training_loss: tensor(0.4010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [284/4339]: training_loss: tensor(0.1261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [285/4339]: training_loss: tensor(0.1392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [286/4339]: training_loss: tensor(0.1355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [287/4339]: training_loss: tensor(0.3328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [288/4339]: training_loss: tensor(0.1984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [289/4339]: training_loss: tensor(0.4546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [290/4339]: training_loss: tensor(0.2974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [291/4339]: training_loss: tensor(0.2287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [292/4339]: training_loss: tensor(0.2983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [293/4339]: training_loss: tensor(0.2658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [294/4339]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [295/4339]: training_loss: tensor(0.1757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [296/4339]: training_loss: tensor(0.1695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [297/4339]: training_loss: tensor(0.3844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [298/4339]: training_loss: tensor(0.2598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [299/4339]: training_loss: tensor(0.3022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [300/4339]: training_loss: tensor(0.3173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [301/4339]: training_loss: tensor(0.3836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [302/4339]: training_loss: tensor(0.2335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [303/4339]: training_loss: tensor(0.4229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [304/4339]: training_loss: tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [305/4339]: training_loss: tensor(0.4273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [306/4339]: training_loss: tensor(0.4108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [307/4339]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [308/4339]: training_loss: tensor(0.1225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [309/4339]: training_loss: tensor(0.1636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [310/4339]: training_loss: tensor(0.1256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [311/4339]: training_loss: tensor(0.1618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [312/4339]: training_loss: tensor(0.4891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [313/4339]: training_loss: tensor(0.4832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [314/4339]: training_loss: tensor(0.5043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [315/4339]: training_loss: tensor(0.3045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [316/4339]: training_loss: tensor(0.2160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [317/4339]: training_loss: tensor(0.1409, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [318/4339]: training_loss: tensor(0.1906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [319/4339]: training_loss: tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [320/4339]: training_loss: tensor(0.1181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [321/4339]: training_loss: tensor(0.5478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [322/4339]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [323/4339]: training_loss: tensor(0.1105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [324/4339]: training_loss: tensor(0.0629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [325/4339]: training_loss: tensor(0.3591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [326/4339]: training_loss: tensor(0.1753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [327/4339]: training_loss: tensor(0.1813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [328/4339]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [329/4339]: training_loss: tensor(0.0495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [330/4339]: training_loss: tensor(0.1628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [331/4339]: training_loss: tensor(0.4569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [332/4339]: training_loss: tensor(0.4212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [333/4339]: training_loss: tensor(0.1921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [334/4339]: training_loss: tensor(0.3890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [335/4339]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [336/4339]: training_loss: tensor(0.5770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [337/4339]: training_loss: tensor(0.4266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [338/4339]: training_loss: tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [339/4339]: training_loss: tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [340/4339]: training_loss: tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [341/4339]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [342/4339]: training_loss: tensor(0.1503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [343/4339]: training_loss: tensor(0.2045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [344/4339]: training_loss: tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [345/4339]: training_loss: tensor(0.1295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [346/4339]: training_loss: tensor(0.1673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [347/4339]: training_loss: tensor(0.1260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [348/4339]: training_loss: tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [349/4339]: training_loss: tensor(0.5614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [350/4339]: training_loss: tensor(0.2398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [351/4339]: training_loss: tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [352/4339]: training_loss: tensor(0.3457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [353/4339]: training_loss: tensor(0.5200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [354/4339]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [355/4339]: training_loss: tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [356/4339]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [357/4339]: training_loss: tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [358/4339]: training_loss: tensor(0.2301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [359/4339]: training_loss: tensor(0.1284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [360/4339]: training_loss: tensor(0.4074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [361/4339]: training_loss: tensor(0.3153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [362/4339]: training_loss: tensor(0.2528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [363/4339]: training_loss: tensor(0.1290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [364/4339]: training_loss: tensor(0.1971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [365/4339]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [366/4339]: training_loss: tensor(0.2187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [367/4339]: training_loss: tensor(0.3560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [368/4339]: training_loss: tensor(0.9522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [369/4339]: training_loss: tensor(0.1078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [370/4339]: training_loss: tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [371/4339]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [372/4339]: training_loss: tensor(0.1740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [373/4339]: training_loss: tensor(0.5032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [374/4339]: training_loss: tensor(0.2448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [375/4339]: training_loss: tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [376/4339]: training_loss: tensor(0.4754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [377/4339]: training_loss: tensor(0.1355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [378/4339]: training_loss: tensor(0.1953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [379/4339]: training_loss: tensor(0.2381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [380/4339]: training_loss: tensor(0.1992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [381/4339]: training_loss: tensor(0.3934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [382/4339]: training_loss: tensor(0.6845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [383/4339]: training_loss: tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [384/4339]: training_loss: tensor(0.5752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [385/4339]: training_loss: tensor(0.5082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [386/4339]: training_loss: tensor(0.1678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [387/4339]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [388/4339]: training_loss: tensor(0.1254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [389/4339]: training_loss: tensor(0.3034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [390/4339]: training_loss: tensor(0.3019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [391/4339]: training_loss: tensor(0.2063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [392/4339]: training_loss: tensor(0.2448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [393/4339]: training_loss: tensor(0.2655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [394/4339]: training_loss: tensor(0.5361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [395/4339]: training_loss: tensor(0.1546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [396/4339]: training_loss: tensor(0.1584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [397/4339]: training_loss: tensor(0.1908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [398/4339]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [399/4339]: training_loss: tensor(0.2919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [400/4339]: training_loss: tensor(0.3894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [401/4339]: training_loss: tensor(0.1768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [402/4339]: training_loss: tensor(0.4187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [403/4339]: training_loss: tensor(0.2019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [404/4339]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [405/4339]: training_loss: tensor(0.1786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [406/4339]: training_loss: tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [407/4339]: training_loss: tensor(0.2840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [408/4339]: training_loss: tensor(0.1615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [409/4339]: training_loss: tensor(0.5200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [410/4339]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [411/4339]: training_loss: tensor(0.5704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [412/4339]: training_loss: tensor(0.1399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [413/4339]: training_loss: tensor(0.1204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [414/4339]: training_loss: tensor(0.3728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [415/4339]: training_loss: tensor(0.3898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [416/4339]: training_loss: tensor(0.1346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [417/4339]: training_loss: tensor(0.3905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [418/4339]: training_loss: tensor(0.2370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [419/4339]: training_loss: tensor(0.4609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [420/4339]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [421/4339]: training_loss: tensor(0.1108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [422/4339]: training_loss: tensor(0.3048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [423/4339]: training_loss: tensor(0.1695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [424/4339]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [425/4339]: training_loss: tensor(0.1925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [426/4339]: training_loss: tensor(0.4797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [427/4339]: training_loss: tensor(0.2018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [428/4339]: training_loss: tensor(0.3319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [429/4339]: training_loss: tensor(0.1523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [430/4339]: training_loss: tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [431/4339]: training_loss: tensor(0.3763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [432/4339]: training_loss: tensor(0.0439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [433/4339]: training_loss: tensor(0.3451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [434/4339]: training_loss: tensor(0.1489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [435/4339]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [436/4339]: training_loss: tensor(0.4426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [437/4339]: training_loss: tensor(0.4308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [438/4339]: training_loss: tensor(0.1704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [439/4339]: training_loss: tensor(0.2662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [440/4339]: training_loss: tensor(0.3387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [441/4339]: training_loss: tensor(0.2367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [442/4339]: training_loss: tensor(0.1953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [443/4339]: training_loss: tensor(0.1309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [444/4339]: training_loss: tensor(0.4111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [445/4339]: training_loss: tensor(0.3790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [446/4339]: training_loss: tensor(0.2242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [447/4339]: training_loss: tensor(0.4000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [448/4339]: training_loss: tensor(0.1783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [449/4339]: training_loss: tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [450/4339]: training_loss: tensor(0.3919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [451/4339]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [452/4339]: training_loss: tensor(0.1503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [453/4339]: training_loss: tensor(0.2753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [454/4339]: training_loss: tensor(0.1655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [455/4339]: training_loss: tensor(0.1659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [456/4339]: training_loss: tensor(0.3475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [457/4339]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [458/4339]: training_loss: tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [459/4339]: training_loss: tensor(0.2597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [460/4339]: training_loss: tensor(0.4122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [461/4339]: training_loss: tensor(0.4028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [462/4339]: training_loss: tensor(0.1649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [463/4339]: training_loss: tensor(0.1487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [464/4339]: training_loss: tensor(0.0594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [465/4339]: training_loss: tensor(0.1449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [466/4339]: training_loss: tensor(0.2465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [467/4339]: training_loss: tensor(0.5445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [468/4339]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [469/4339]: training_loss: tensor(0.1341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [470/4339]: training_loss: tensor(0.2305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [471/4339]: training_loss: tensor(0.1212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [472/4339]: training_loss: tensor(0.2396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [473/4339]: training_loss: tensor(0.4904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [474/4339]: training_loss: tensor(0.3251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [475/4339]: training_loss: tensor(0.3217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [476/4339]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [477/4339]: training_loss: tensor(0.2706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [478/4339]: training_loss: tensor(0.1358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [479/4339]: training_loss: tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [480/4339]: training_loss: tensor(0.6661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [481/4339]: training_loss: tensor(0.4101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [482/4339]: training_loss: tensor(0.5151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [483/4339]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [484/4339]: training_loss: tensor(0.0507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [485/4339]: training_loss: tensor(0.2066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [486/4339]: training_loss: tensor(0.4136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [487/4339]: training_loss: tensor(0.1769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [488/4339]: training_loss: tensor(0.2394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [489/4339]: training_loss: tensor(0.1617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [490/4339]: training_loss: tensor(0.2520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [491/4339]: training_loss: tensor(0.2075, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [492/4339]: training_loss: tensor(0.2627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [493/4339]: training_loss: tensor(0.4351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [494/4339]: training_loss: tensor(0.1374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [495/4339]: training_loss: tensor(0.3746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [496/4339]: training_loss: tensor(0.4537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [497/4339]: training_loss: tensor(0.1452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [498/4339]: training_loss: tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [499/4339]: training_loss: tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [500/4339]: training_loss: tensor(0.5768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [501/4339]: training_loss: tensor(0.2128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [502/4339]: training_loss: tensor(0.2127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [503/4339]: training_loss: tensor(0.3624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [504/4339]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [505/4339]: training_loss: tensor(0.2523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [506/4339]: training_loss: tensor(0.4910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [507/4339]: training_loss: tensor(0.3068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [508/4339]: training_loss: tensor(0.1384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [509/4339]: training_loss: tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [510/4339]: training_loss: tensor(0.3623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [511/4339]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [512/4339]: training_loss: tensor(0.3802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [513/4339]: training_loss: tensor(0.2565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [514/4339]: training_loss: tensor(0.1789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [515/4339]: training_loss: tensor(0.1858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [516/4339]: training_loss: tensor(0.2173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [517/4339]: training_loss: tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [518/4339]: training_loss: tensor(0.2541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [519/4339]: training_loss: tensor(0.4707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [520/4339]: training_loss: tensor(0.2575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [521/4339]: training_loss: tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [522/4339]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [523/4339]: training_loss: tensor(0.3237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [524/4339]: training_loss: tensor(0.1593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [525/4339]: training_loss: tensor(0.6099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [526/4339]: training_loss: tensor(0.2514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [527/4339]: training_loss: tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [528/4339]: training_loss: tensor(0.1893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [529/4339]: training_loss: tensor(0.1332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [530/4339]: training_loss: tensor(0.3795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [531/4339]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [532/4339]: training_loss: tensor(0.1566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [533/4339]: training_loss: tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [534/4339]: training_loss: tensor(0.1378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [535/4339]: training_loss: tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [536/4339]: training_loss: tensor(0.5106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [537/4339]: training_loss: tensor(0.1361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [538/4339]: training_loss: tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [539/4339]: training_loss: tensor(0.0543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [540/4339]: training_loss: tensor(0.1966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [541/4339]: training_loss: tensor(0.1218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [542/4339]: training_loss: tensor(0.4135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [543/4339]: training_loss: tensor(0.2932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [544/4339]: training_loss: tensor(0.2334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [545/4339]: training_loss: tensor(0.2964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [546/4339]: training_loss: tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [547/4339]: training_loss: tensor(0.4596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [548/4339]: training_loss: tensor(0.5976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [549/4339]: training_loss: tensor(0.1697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [550/4339]: training_loss: tensor(0.1771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [551/4339]: training_loss: tensor(0.3792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [552/4339]: training_loss: tensor(0.2300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [553/4339]: training_loss: tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [554/4339]: training_loss: tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [555/4339]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [556/4339]: training_loss: tensor(0.2367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [557/4339]: training_loss: tensor(0.4035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [558/4339]: training_loss: tensor(0.2353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [559/4339]: training_loss: tensor(0.1614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [560/4339]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [561/4339]: training_loss: tensor(0.3862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [562/4339]: training_loss: tensor(0.2949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [563/4339]: training_loss: tensor(0.2633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [564/4339]: training_loss: tensor(0.1805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [565/4339]: training_loss: tensor(0.1805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [566/4339]: training_loss: tensor(0.1224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [567/4339]: training_loss: tensor(0.3288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [568/4339]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [569/4339]: training_loss: tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [570/4339]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [571/4339]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [572/4339]: training_loss: tensor(0.5559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [573/4339]: training_loss: tensor(0.1214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [574/4339]: training_loss: tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [575/4339]: training_loss: tensor(0.3553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [576/4339]: training_loss: tensor(0.1846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [577/4339]: training_loss: tensor(0.2241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [578/4339]: training_loss: tensor(0.1368, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [579/4339]: training_loss: tensor(0.2024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [580/4339]: training_loss: tensor(0.1838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [581/4339]: training_loss: tensor(0.2018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [582/4339]: training_loss: tensor(0.2088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [583/4339]: training_loss: tensor(0.1850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [584/4339]: training_loss: tensor(0.5737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [585/4339]: training_loss: tensor(0.2173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [586/4339]: training_loss: tensor(0.1381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [587/4339]: training_loss: tensor(0.2301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [588/4339]: training_loss: tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [589/4339]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [590/4339]: training_loss: tensor(0.0650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [591/4339]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [592/4339]: training_loss: tensor(0.1309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [593/4339]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [594/4339]: training_loss: tensor(0.3820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [595/4339]: training_loss: tensor(0.1298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [596/4339]: training_loss: tensor(0.3873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [597/4339]: training_loss: tensor(0.1289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [598/4339]: training_loss: tensor(0.2212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [599/4339]: training_loss: tensor(0.1175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [600/4339]: training_loss: tensor(0.2882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [601/4339]: training_loss: tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [602/4339]: training_loss: tensor(0.4298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [603/4339]: training_loss: tensor(0.2185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [604/4339]: training_loss: tensor(0.3936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [605/4339]: training_loss: tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [606/4339]: training_loss: tensor(0.6747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [607/4339]: training_loss: tensor(0.1825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [608/4339]: training_loss: tensor(0.3873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [609/4339]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [610/4339]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [611/4339]: training_loss: tensor(0.2818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [612/4339]: training_loss: tensor(0.3446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [613/4339]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [614/4339]: training_loss: tensor(0.1224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [615/4339]: training_loss: tensor(0.0945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [616/4339]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [617/4339]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [618/4339]: training_loss: tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [619/4339]: training_loss: tensor(0.3426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [620/4339]: training_loss: tensor(0.6535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [621/4339]: training_loss: tensor(0.1517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [622/4339]: training_loss: tensor(0.4211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [623/4339]: training_loss: tensor(0.1209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [624/4339]: training_loss: tensor(0.4473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [625/4339]: training_loss: tensor(0.4129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [626/4339]: training_loss: tensor(0.3005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [627/4339]: training_loss: tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [628/4339]: training_loss: tensor(0.1754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [629/4339]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [630/4339]: training_loss: tensor(0.1598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [631/4339]: training_loss: tensor(0.4660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [632/4339]: training_loss: tensor(0.3085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [633/4339]: training_loss: tensor(0.3321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [634/4339]: training_loss: tensor(0.1451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [635/4339]: training_loss: tensor(0.3162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [636/4339]: training_loss: tensor(0.2632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [637/4339]: training_loss: tensor(0.2303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [638/4339]: training_loss: tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [639/4339]: training_loss: tensor(0.2968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [640/4339]: training_loss: tensor(0.2653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [641/4339]: training_loss: tensor(0.5501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [642/4339]: training_loss: tensor(0.2067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [643/4339]: training_loss: tensor(0.1780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [644/4339]: training_loss: tensor(0.3958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [645/4339]: training_loss: tensor(0.1348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [646/4339]: training_loss: tensor(0.2164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [647/4339]: training_loss: tensor(0.2139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [648/4339]: training_loss: tensor(0.1231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [649/4339]: training_loss: tensor(0.2489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [650/4339]: training_loss: tensor(0.2004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [651/4339]: training_loss: tensor(0.1142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [652/4339]: training_loss: tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [653/4339]: training_loss: tensor(0.3256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [654/4339]: training_loss: tensor(0.3288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [655/4339]: training_loss: tensor(0.3799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [656/4339]: training_loss: tensor(0.3284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [657/4339]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [658/4339]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [659/4339]: training_loss: tensor(0.1606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [660/4339]: training_loss: tensor(0.1150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [661/4339]: training_loss: tensor(0.4811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [662/4339]: training_loss: tensor(0.1408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [663/4339]: training_loss: tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [664/4339]: training_loss: tensor(0.3775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [665/4339]: training_loss: tensor(0.2670, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [666/4339]: training_loss: tensor(0.1436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [667/4339]: training_loss: tensor(0.3458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [668/4339]: training_loss: tensor(0.1712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [669/4339]: training_loss: tensor(0.2482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [670/4339]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [671/4339]: training_loss: tensor(0.2701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [672/4339]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [673/4339]: training_loss: tensor(0.1453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [674/4339]: training_loss: tensor(0.2420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [675/4339]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [676/4339]: training_loss: tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [677/4339]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [678/4339]: training_loss: tensor(0.1675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [679/4339]: training_loss: tensor(0.1112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [680/4339]: training_loss: tensor(0.3142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [681/4339]: training_loss: tensor(0.7143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [682/4339]: training_loss: tensor(0.1327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [683/4339]: training_loss: tensor(0.1820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [684/4339]: training_loss: tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [685/4339]: training_loss: tensor(0.2760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [686/4339]: training_loss: tensor(0.6017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [687/4339]: training_loss: tensor(0.4357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [688/4339]: training_loss: tensor(0.1602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [689/4339]: training_loss: tensor(0.3885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [690/4339]: training_loss: tensor(0.1760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [691/4339]: training_loss: tensor(0.1963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [692/4339]: training_loss: tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [693/4339]: training_loss: tensor(0.5397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [694/4339]: training_loss: tensor(0.5085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [695/4339]: training_loss: tensor(0.5629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [696/4339]: training_loss: tensor(0.2820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [697/4339]: training_loss: tensor(0.2206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [698/4339]: training_loss: tensor(0.2567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [699/4339]: training_loss: tensor(0.5322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [700/4339]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [701/4339]: training_loss: tensor(0.1578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [702/4339]: training_loss: tensor(0.3938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [703/4339]: training_loss: tensor(0.1491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [704/4339]: training_loss: tensor(0.2460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [705/4339]: training_loss: tensor(0.2865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [706/4339]: training_loss: tensor(0.1252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [707/4339]: training_loss: tensor(0.4435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [708/4339]: training_loss: tensor(0.2244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [709/4339]: training_loss: tensor(0.1978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [710/4339]: training_loss: tensor(0.4274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [711/4339]: training_loss: tensor(0.5166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [712/4339]: training_loss: tensor(0.3079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [713/4339]: training_loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [714/4339]: training_loss: tensor(0.4967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [715/4339]: training_loss: tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [716/4339]: training_loss: tensor(0.1308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [717/4339]: training_loss: tensor(0.1574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [718/4339]: training_loss: tensor(0.2274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [719/4339]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [720/4339]: training_loss: tensor(0.4665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [721/4339]: training_loss: tensor(0.1774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [722/4339]: training_loss: tensor(0.5858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [723/4339]: training_loss: tensor(0.2097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [724/4339]: training_loss: tensor(0.2203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [725/4339]: training_loss: tensor(0.1294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [726/4339]: training_loss: tensor(0.3355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [727/4339]: training_loss: tensor(0.4092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [728/4339]: training_loss: tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [729/4339]: training_loss: tensor(0.1934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [730/4339]: training_loss: tensor(0.2034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [731/4339]: training_loss: tensor(0.4162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [732/4339]: training_loss: tensor(0.1612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [733/4339]: training_loss: tensor(0.1313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [734/4339]: training_loss: tensor(0.3172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [735/4339]: training_loss: tensor(0.1657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [736/4339]: training_loss: tensor(0.1859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [737/4339]: training_loss: tensor(0.1857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [738/4339]: training_loss: tensor(0.1374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [739/4339]: training_loss: tensor(0.1808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [740/4339]: training_loss: tensor(0.1504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [741/4339]: training_loss: tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [742/4339]: training_loss: tensor(0.3181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [743/4339]: training_loss: tensor(0.2596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [744/4339]: training_loss: tensor(0.1590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [745/4339]: training_loss: tensor(0.3251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [746/4339]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [747/4339]: training_loss: tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [748/4339]: training_loss: tensor(0.1419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [749/4339]: training_loss: tensor(0.2001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [750/4339]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [751/4339]: training_loss: tensor(0.3876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [752/4339]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [753/4339]: training_loss: tensor(0.1814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [754/4339]: training_loss: tensor(0.2099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [755/4339]: training_loss: tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [756/4339]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [757/4339]: training_loss: tensor(0.1905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [758/4339]: training_loss: tensor(0.3089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [759/4339]: training_loss: tensor(0.5882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [760/4339]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [761/4339]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [762/4339]: training_loss: tensor(0.3607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [763/4339]: training_loss: tensor(0.2865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [764/4339]: training_loss: tensor(0.4821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [765/4339]: training_loss: tensor(0.5318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [766/4339]: training_loss: tensor(0.1600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [767/4339]: training_loss: tensor(0.3900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [768/4339]: training_loss: tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [769/4339]: training_loss: tensor(0.1401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [770/4339]: training_loss: tensor(0.2620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [771/4339]: training_loss: tensor(0.2187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [772/4339]: training_loss: tensor(0.7747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [773/4339]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [774/4339]: training_loss: tensor(0.5587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [775/4339]: training_loss: tensor(0.2682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [776/4339]: training_loss: tensor(0.2272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [777/4339]: training_loss: tensor(0.2232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [778/4339]: training_loss: tensor(0.2199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [779/4339]: training_loss: tensor(0.1707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [780/4339]: training_loss: tensor(0.3583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [781/4339]: training_loss: tensor(0.1768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [782/4339]: training_loss: tensor(0.3192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [783/4339]: training_loss: tensor(0.2131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [784/4339]: training_loss: tensor(0.2588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [785/4339]: training_loss: tensor(0.5247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [786/4339]: training_loss: tensor(0.3369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [787/4339]: training_loss: tensor(0.5007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [788/4339]: training_loss: tensor(0.1795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [789/4339]: training_loss: tensor(0.6592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [790/4339]: training_loss: tensor(0.3995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [791/4339]: training_loss: tensor(0.2376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [792/4339]: training_loss: tensor(0.3731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [793/4339]: training_loss: tensor(0.5557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [794/4339]: training_loss: tensor(0.2747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [795/4339]: training_loss: tensor(0.5775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [796/4339]: training_loss: tensor(0.2203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [797/4339]: training_loss: tensor(0.1656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [798/4339]: training_loss: tensor(0.2248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [799/4339]: training_loss: tensor(0.1648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [800/4339]: training_loss: tensor(0.2175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [801/4339]: training_loss: tensor(0.3307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [802/4339]: training_loss: tensor(0.2751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [803/4339]: training_loss: tensor(0.3080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [804/4339]: training_loss: tensor(0.2370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [805/4339]: training_loss: tensor(0.1975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [806/4339]: training_loss: tensor(0.2228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [807/4339]: training_loss: tensor(0.5281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [808/4339]: training_loss: tensor(0.2178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [809/4339]: training_loss: tensor(0.3881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [810/4339]: training_loss: tensor(0.3878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [811/4339]: training_loss: tensor(0.3048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [812/4339]: training_loss: tensor(0.2444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [813/4339]: training_loss: tensor(0.1589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [814/4339]: training_loss: tensor(0.1180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [815/4339]: training_loss: tensor(0.2973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [816/4339]: training_loss: tensor(0.1302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [817/4339]: training_loss: tensor(0.1881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [818/4339]: training_loss: tensor(0.1819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [819/4339]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [820/4339]: training_loss: tensor(0.5585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [821/4339]: training_loss: tensor(0.1180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [822/4339]: training_loss: tensor(0.1632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [823/4339]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [824/4339]: training_loss: tensor(0.1735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [825/4339]: training_loss: tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [826/4339]: training_loss: tensor(0.2763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [827/4339]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [828/4339]: training_loss: tensor(0.1536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [829/4339]: training_loss: tensor(0.2639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [830/4339]: training_loss: tensor(0.1873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [831/4339]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [832/4339]: training_loss: tensor(0.3117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [833/4339]: training_loss: tensor(0.1808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [834/4339]: training_loss: tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [835/4339]: training_loss: tensor(0.3220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [836/4339]: training_loss: tensor(0.2730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [837/4339]: training_loss: tensor(0.0494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [838/4339]: training_loss: tensor(0.1693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [839/4339]: training_loss: tensor(0.1431, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [840/4339]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [841/4339]: training_loss: tensor(0.4021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [842/4339]: training_loss: tensor(0.3187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [843/4339]: training_loss: tensor(0.1120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [844/4339]: training_loss: tensor(0.2663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [845/4339]: training_loss: tensor(0.5195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [846/4339]: training_loss: tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [847/4339]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [848/4339]: training_loss: tensor(0.2330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [849/4339]: training_loss: tensor(0.3416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [850/4339]: training_loss: tensor(0.4484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [851/4339]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [852/4339]: training_loss: tensor(0.7780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [853/4339]: training_loss: tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [854/4339]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [855/4339]: training_loss: tensor(0.1399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [856/4339]: training_loss: tensor(0.1204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [857/4339]: training_loss: tensor(0.5284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [858/4339]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [859/4339]: training_loss: tensor(0.2910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [860/4339]: training_loss: tensor(0.1911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [861/4339]: training_loss: tensor(0.2072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [862/4339]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [863/4339]: training_loss: tensor(0.1928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [864/4339]: training_loss: tensor(0.3976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [865/4339]: training_loss: tensor(0.1602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [866/4339]: training_loss: tensor(0.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [867/4339]: training_loss: tensor(0.3932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [868/4339]: training_loss: tensor(0.3240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [869/4339]: training_loss: tensor(0.1770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [870/4339]: training_loss: tensor(0.5229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [871/4339]: training_loss: tensor(0.2820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [872/4339]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [873/4339]: training_loss: tensor(0.1652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [874/4339]: training_loss: tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [875/4339]: training_loss: tensor(0.2534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [876/4339]: training_loss: tensor(0.6690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [877/4339]: training_loss: tensor(0.1616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [878/4339]: training_loss: tensor(0.3234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [879/4339]: training_loss: tensor(0.4038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [880/4339]: training_loss: tensor(0.3089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [881/4339]: training_loss: tensor(0.4566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [882/4339]: training_loss: tensor(0.1521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [883/4339]: training_loss: tensor(0.2305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [884/4339]: training_loss: tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [885/4339]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [886/4339]: training_loss: tensor(0.2031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [887/4339]: training_loss: tensor(0.3085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [888/4339]: training_loss: tensor(0.3896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [889/4339]: training_loss: tensor(0.1646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [890/4339]: training_loss: tensor(0.2129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [891/4339]: training_loss: tensor(0.5835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [892/4339]: training_loss: tensor(0.2134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [893/4339]: training_loss: tensor(0.3506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [894/4339]: training_loss: tensor(0.1876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [895/4339]: training_loss: tensor(0.1990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [896/4339]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [897/4339]: training_loss: tensor(0.1721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [898/4339]: training_loss: tensor(0.1582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [899/4339]: training_loss: tensor(0.4428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [900/4339]: training_loss: tensor(0.5056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [901/4339]: training_loss: tensor(0.0686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [902/4339]: training_loss: tensor(0.2920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [903/4339]: training_loss: tensor(0.3280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [904/4339]: training_loss: tensor(0.1605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [905/4339]: training_loss: tensor(0.1934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [906/4339]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [907/4339]: training_loss: tensor(0.3214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [908/4339]: training_loss: tensor(0.1765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [909/4339]: training_loss: tensor(0.5803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [910/4339]: training_loss: tensor(0.2035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [911/4339]: training_loss: tensor(0.1708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [912/4339]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [913/4339]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [914/4339]: training_loss: tensor(0.2315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [915/4339]: training_loss: tensor(0.1783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [916/4339]: training_loss: tensor(0.1581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [917/4339]: training_loss: tensor(0.5617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [918/4339]: training_loss: tensor(0.1705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [919/4339]: training_loss: tensor(0.4631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [920/4339]: training_loss: tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [921/4339]: training_loss: tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [922/4339]: training_loss: tensor(0.2630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [923/4339]: training_loss: tensor(0.2203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [924/4339]: training_loss: tensor(0.5367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [925/4339]: training_loss: tensor(0.3272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [926/4339]: training_loss: tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [927/4339]: training_loss: tensor(0.1581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [928/4339]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [929/4339]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [930/4339]: training_loss: tensor(0.3379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [931/4339]: training_loss: tensor(0.1941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [932/4339]: training_loss: tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [933/4339]: training_loss: tensor(0.1737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [934/4339]: training_loss: tensor(0.3115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [935/4339]: training_loss: tensor(0.2693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [936/4339]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [937/4339]: training_loss: tensor(0.5722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [938/4339]: training_loss: tensor(0.1628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [939/4339]: training_loss: tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [940/4339]: training_loss: tensor(0.3852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [941/4339]: training_loss: tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [942/4339]: training_loss: tensor(0.5160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [943/4339]: training_loss: tensor(0.1582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [944/4339]: training_loss: tensor(0.3476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [945/4339]: training_loss: tensor(0.0541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [946/4339]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [947/4339]: training_loss: tensor(0.1984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [948/4339]: training_loss: tensor(0.2357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [949/4339]: training_loss: tensor(0.2097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [950/4339]: training_loss: tensor(0.0645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [951/4339]: training_loss: tensor(0.1517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [952/4339]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [953/4339]: training_loss: tensor(0.1360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [954/4339]: training_loss: tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [955/4339]: training_loss: tensor(0.2723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [956/4339]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [957/4339]: training_loss: tensor(0.2125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [958/4339]: training_loss: tensor(0.3783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [959/4339]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [960/4339]: training_loss: tensor(0.8185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [961/4339]: training_loss: tensor(0.0587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [962/4339]: training_loss: tensor(0.1805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [963/4339]: training_loss: tensor(0.0603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [964/4339]: training_loss: tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [965/4339]: training_loss: tensor(0.4251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [966/4339]: training_loss: tensor(0.3895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [967/4339]: training_loss: tensor(0.2893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [968/4339]: training_loss: tensor(0.2218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [969/4339]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [970/4339]: training_loss: tensor(0.3720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [971/4339]: training_loss: tensor(0.3293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [972/4339]: training_loss: tensor(0.1910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [973/4339]: training_loss: tensor(0.3355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [974/4339]: training_loss: tensor(0.7880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [975/4339]: training_loss: tensor(0.2385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [976/4339]: training_loss: tensor(0.3909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [977/4339]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [978/4339]: training_loss: tensor(0.2237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [979/4339]: training_loss: tensor(0.5333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [980/4339]: training_loss: tensor(0.4572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [981/4339]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [982/4339]: training_loss: tensor(0.1351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [983/4339]: training_loss: tensor(0.2627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [984/4339]: training_loss: tensor(0.4403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [985/4339]: training_loss: tensor(0.1660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [986/4339]: training_loss: tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [987/4339]: training_loss: tensor(0.3311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [988/4339]: training_loss: tensor(0.2346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [989/4339]: training_loss: tensor(0.4526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [990/4339]: training_loss: tensor(0.1822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [991/4339]: training_loss: tensor(0.3181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [992/4339]: training_loss: tensor(0.2296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [993/4339]: training_loss: tensor(0.0611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [994/4339]: training_loss: tensor(0.2714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [995/4339]: training_loss: tensor(0.2662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [996/4339]: training_loss: tensor(0.2864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [997/4339]: training_loss: tensor(0.4457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [998/4339]: training_loss: tensor(0.1542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [999/4339]: training_loss: tensor(0.3622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1000/4339]: training_loss: tensor(0.1697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1001/4339]: training_loss: tensor(0.1535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1002/4339]: training_loss: tensor(0.1985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1003/4339]: training_loss: tensor(0.3164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1004/4339]: training_loss: tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1005/4339]: training_loss: tensor(0.1351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/4339]: training_loss: tensor(0.5581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/4339]: training_loss: tensor(0.4790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/4339]: training_loss: tensor(0.4716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/4339]: training_loss: tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/4339]: training_loss: tensor(0.3880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/4339]: training_loss: tensor(0.2437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/4339]: training_loss: tensor(0.3836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/4339]: training_loss: tensor(0.3596, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1014/4339]: training_loss: tensor(0.3204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/4339]: training_loss: tensor(0.2659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/4339]: training_loss: tensor(0.2835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/4339]: training_loss: tensor(0.2168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/4339]: training_loss: tensor(0.5689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1019/4339]: training_loss: tensor(0.3811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/4339]: training_loss: tensor(0.5263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/4339]: training_loss: tensor(0.2988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/4339]: training_loss: tensor(0.4059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1023/4339]: training_loss: tensor(0.4302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/4339]: training_loss: tensor(0.2192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/4339]: training_loss: tensor(0.2556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/4339]: training_loss: tensor(0.4262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/4339]: training_loss: tensor(0.3536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/4339]: training_loss: tensor(0.2295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/4339]: training_loss: tensor(0.2331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/4339]: training_loss: tensor(0.1353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/4339]: training_loss: tensor(0.1481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/4339]: training_loss: tensor(0.2642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/4339]: training_loss: tensor(0.2413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/4339]: training_loss: tensor(0.2933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/4339]: training_loss: tensor(0.2247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/4339]: training_loss: tensor(0.5000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/4339]: training_loss: tensor(0.1307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/4339]: training_loss: tensor(0.1371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/4339]: training_loss: tensor(0.3414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1040/4339]: training_loss: tensor(0.1629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/4339]: training_loss: tensor(0.4347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/4339]: training_loss: tensor(0.4267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1043/4339]: training_loss: tensor(0.3101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/4339]: training_loss: tensor(0.2015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/4339]: training_loss: tensor(0.2911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/4339]: training_loss: tensor(0.1766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/4339]: training_loss: tensor(0.3648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/4339]: training_loss: tensor(0.3883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/4339]: training_loss: tensor(0.1220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/4339]: training_loss: tensor(0.1725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/4339]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/4339]: training_loss: tensor(0.0683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/4339]: training_loss: tensor(0.3100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/4339]: training_loss: tensor(0.5573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/4339]: training_loss: tensor(0.2602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/4339]: training_loss: tensor(0.3365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/4339]: training_loss: tensor(0.1400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/4339]: training_loss: tensor(0.2121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/4339]: training_loss: tensor(0.3002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/4339]: training_loss: tensor(0.1695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/4339]: training_loss: tensor(0.3736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/4339]: training_loss: tensor(0.1958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/4339]: training_loss: tensor(0.1488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/4339]: training_loss: tensor(0.5006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/4339]: training_loss: tensor(0.3332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/4339]: training_loss: tensor(0.3716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/4339]: training_loss: tensor(0.4714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/4339]: training_loss: tensor(0.3098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/4339]: training_loss: tensor(0.2803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/4339]: training_loss: tensor(0.5377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/4339]: training_loss: tensor(0.4876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/4339]: training_loss: tensor(0.4834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/4339]: training_loss: tensor(0.3014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/4339]: training_loss: tensor(0.1557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/4339]: training_loss: tensor(0.3728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/4339]: training_loss: tensor(0.3339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/4339]: training_loss: tensor(0.3931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/4339]: training_loss: tensor(0.2707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/4339]: training_loss: tensor(0.5572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/4339]: training_loss: tensor(0.3166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/4339]: training_loss: tensor(0.3458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/4339]: training_loss: tensor(0.1609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/4339]: training_loss: tensor(0.2836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/4339]: training_loss: tensor(0.3886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/4339]: training_loss: tensor(0.1451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/4339]: training_loss: tensor(0.3635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/4339]: training_loss: tensor(0.1941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/4339]: training_loss: tensor(0.3415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/4339]: training_loss: tensor(0.4024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/4339]: training_loss: tensor(0.3067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/4339]: training_loss: tensor(0.4059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/4339]: training_loss: tensor(0.2609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/4339]: training_loss: tensor(0.3383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/4339]: training_loss: tensor(0.4506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/4339]: training_loss: tensor(0.0616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/4339]: training_loss: tensor(0.2741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/4339]: training_loss: tensor(0.3937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/4339]: training_loss: tensor(0.1763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/4339]: training_loss: tensor(0.2937, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1100/4339]: training_loss: tensor(0.1691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/4339]: training_loss: tensor(0.4365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/4339]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/4339]: training_loss: tensor(0.5189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/4339]: training_loss: tensor(0.2507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/4339]: training_loss: tensor(0.1798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/4339]: training_loss: tensor(0.3607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/4339]: training_loss: tensor(0.2262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/4339]: training_loss: tensor(0.4955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/4339]: training_loss: tensor(0.2170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/4339]: training_loss: tensor(0.5527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/4339]: training_loss: tensor(0.2152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/4339]: training_loss: tensor(0.2332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/4339]: training_loss: tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/4339]: training_loss: tensor(0.2632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/4339]: training_loss: tensor(0.4546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/4339]: training_loss: tensor(0.1128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/4339]: training_loss: tensor(0.3077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/4339]: training_loss: tensor(0.4707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/4339]: training_loss: tensor(0.2032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/4339]: training_loss: tensor(0.1131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/4339]: training_loss: tensor(0.2006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/4339]: training_loss: tensor(0.1175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/4339]: training_loss: tensor(0.3226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/4339]: training_loss: tensor(0.3608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/4339]: training_loss: tensor(0.1516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/4339]: training_loss: tensor(0.3709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/4339]: training_loss: tensor(0.5911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/4339]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1129/4339]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/4339]: training_loss: tensor(0.2537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/4339]: training_loss: tensor(0.7005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/4339]: training_loss: tensor(0.1292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/4339]: training_loss: tensor(0.3290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/4339]: training_loss: tensor(0.1186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/4339]: training_loss: tensor(0.3346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/4339]: training_loss: tensor(0.2083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/4339]: training_loss: tensor(0.3265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/4339]: training_loss: tensor(0.1098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/4339]: training_loss: tensor(0.4424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/4339]: training_loss: tensor(0.2276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/4339]: training_loss: tensor(0.1491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/4339]: training_loss: tensor(0.1960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/4339]: training_loss: tensor(0.2551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/4339]: training_loss: tensor(0.2498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/4339]: training_loss: tensor(0.2457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/4339]: training_loss: tensor(0.6146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/4339]: training_loss: tensor(0.1618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/4339]: training_loss: tensor(0.2713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/4339]: training_loss: tensor(0.6220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/4339]: training_loss: tensor(0.1559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/4339]: training_loss: tensor(0.3423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/4339]: training_loss: tensor(0.1491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/4339]: training_loss: tensor(0.1817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/4339]: training_loss: tensor(0.3230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/4339]: training_loss: tensor(0.1994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/4339]: training_loss: tensor(0.3331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/4339]: training_loss: tensor(0.4639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/4339]: training_loss: tensor(0.4127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/4339]: training_loss: tensor(0.3173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/4339]: training_loss: tensor(0.4085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/4339]: training_loss: tensor(0.3757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/4339]: training_loss: tensor(0.4122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/4339]: training_loss: tensor(0.3166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/4339]: training_loss: tensor(0.3193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/4339]: training_loss: tensor(0.5424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/4339]: training_loss: tensor(0.3353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/4339]: training_loss: tensor(0.2455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/4339]: training_loss: tensor(0.1862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/4339]: training_loss: tensor(0.3380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/4339]: training_loss: tensor(0.2430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/4339]: training_loss: tensor(0.3828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/4339]: training_loss: tensor(0.2406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/4339]: training_loss: tensor(0.3233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/4339]: training_loss: tensor(0.3957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/4339]: training_loss: tensor(0.1181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/4339]: training_loss: tensor(0.5153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/4339]: training_loss: tensor(0.2405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/4339]: training_loss: tensor(0.2806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/4339]: training_loss: tensor(0.2748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/4339]: training_loss: tensor(0.3975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/4339]: training_loss: tensor(0.1699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/4339]: training_loss: tensor(0.1785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/4339]: training_loss: tensor(0.1278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/4339]: training_loss: tensor(0.0573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/4339]: training_loss: tensor(0.1238, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1186/4339]: training_loss: tensor(0.3016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/4339]: training_loss: tensor(0.1404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/4339]: training_loss: tensor(0.3790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/4339]: training_loss: tensor(0.2030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/4339]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/4339]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/4339]: training_loss: tensor(0.2977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/4339]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/4339]: training_loss: tensor(0.2967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/4339]: training_loss: tensor(0.3090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/4339]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/4339]: training_loss: tensor(0.1495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/4339]: training_loss: tensor(0.1661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/4339]: training_loss: tensor(0.2164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/4339]: training_loss: tensor(0.2915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/4339]: training_loss: tensor(0.1224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/4339]: training_loss: tensor(0.2362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/4339]: training_loss: tensor(0.1657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/4339]: training_loss: tensor(0.2795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/4339]: training_loss: tensor(0.1177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/4339]: training_loss: tensor(0.4255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/4339]: training_loss: tensor(0.6977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/4339]: training_loss: tensor(0.4187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/4339]: training_loss: tensor(0.1544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/4339]: training_loss: tensor(0.1688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/4339]: training_loss: tensor(0.1133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/4339]: training_loss: tensor(0.2612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1213/4339]: training_loss: tensor(0.3383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/4339]: training_loss: tensor(0.3690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1215/4339]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/4339]: training_loss: tensor(0.2160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/4339]: training_loss: tensor(0.2328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/4339]: training_loss: tensor(0.1765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/4339]: training_loss: tensor(0.1964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/4339]: training_loss: tensor(0.2721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/4339]: training_loss: tensor(0.2236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/4339]: training_loss: tensor(0.5848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/4339]: training_loss: tensor(0.3838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/4339]: training_loss: tensor(0.2474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/4339]: training_loss: tensor(0.1542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/4339]: training_loss: tensor(0.1277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/4339]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/4339]: training_loss: tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/4339]: training_loss: tensor(0.4034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/4339]: training_loss: tensor(0.2927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/4339]: training_loss: tensor(0.1791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/4339]: training_loss: tensor(0.1537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/4339]: training_loss: tensor(0.3704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/4339]: training_loss: tensor(0.2825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/4339]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/4339]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/4339]: training_loss: tensor(0.1897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/4339]: training_loss: tensor(0.5126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/4339]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/4339]: training_loss: tensor(0.1814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/4339]: training_loss: tensor(0.3411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/4339]: training_loss: tensor(0.3185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/4339]: training_loss: tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/4339]: training_loss: tensor(0.2038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/4339]: training_loss: tensor(0.2398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/4339]: training_loss: tensor(0.1792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/4339]: training_loss: tensor(0.1405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/4339]: training_loss: tensor(0.1764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/4339]: training_loss: tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/4339]: training_loss: tensor(0.3207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/4339]: training_loss: tensor(0.3410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/4339]: training_loss: tensor(0.2485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/4339]: training_loss: tensor(0.6534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/4339]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/4339]: training_loss: tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/4339]: training_loss: tensor(0.2996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/4339]: training_loss: tensor(0.4398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/4339]: training_loss: tensor(0.2841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/4339]: training_loss: tensor(0.5649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/4339]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/4339]: training_loss: tensor(0.6160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/4339]: training_loss: tensor(0.1277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/4339]: training_loss: tensor(0.2203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/4339]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/4339]: training_loss: tensor(0.3003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/4339]: training_loss: tensor(0.5393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/4339]: training_loss: tensor(0.2468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/4339]: training_loss: tensor(0.4237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/4339]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/4339]: training_loss: tensor(0.1456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/4339]: training_loss: tensor(0.1769, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1272/4339]: training_loss: tensor(0.1133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/4339]: training_loss: tensor(0.1446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/4339]: training_loss: tensor(0.2639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/4339]: training_loss: tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/4339]: training_loss: tensor(0.1536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/4339]: training_loss: tensor(0.2619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/4339]: training_loss: tensor(0.1490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/4339]: training_loss: tensor(0.3263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/4339]: training_loss: tensor(0.2029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/4339]: training_loss: tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/4339]: training_loss: tensor(0.2164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/4339]: training_loss: tensor(0.2906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/4339]: training_loss: tensor(0.2821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/4339]: training_loss: tensor(0.3442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/4339]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/4339]: training_loss: tensor(0.2360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/4339]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/4339]: training_loss: tensor(0.1400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/4339]: training_loss: tensor(0.2688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/4339]: training_loss: tensor(0.2129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/4339]: training_loss: tensor(0.2144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/4339]: training_loss: tensor(0.1588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/4339]: training_loss: tensor(0.1399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/4339]: training_loss: tensor(0.2018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/4339]: training_loss: tensor(0.1959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/4339]: training_loss: tensor(0.4150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/4339]: training_loss: tensor(0.2942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1299/4339]: training_loss: tensor(0.1439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/4339]: training_loss: tensor(0.4210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/4339]: training_loss: tensor(0.2817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/4339]: training_loss: tensor(0.6295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/4339]: training_loss: tensor(0.1233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/4339]: training_loss: tensor(0.3151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/4339]: training_loss: tensor(0.4165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/4339]: training_loss: tensor(0.3590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/4339]: training_loss: tensor(0.4920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/4339]: training_loss: tensor(0.6833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/4339]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/4339]: training_loss: tensor(0.2342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/4339]: training_loss: tensor(0.3340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/4339]: training_loss: tensor(0.3513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/4339]: training_loss: tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/4339]: training_loss: tensor(0.3658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/4339]: training_loss: tensor(0.2265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/4339]: training_loss: tensor(0.3772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/4339]: training_loss: tensor(0.2105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/4339]: training_loss: tensor(0.3377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/4339]: training_loss: tensor(0.2662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/4339]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/4339]: training_loss: tensor(0.3533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/4339]: training_loss: tensor(0.1670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/4339]: training_loss: tensor(0.2948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/4339]: training_loss: tensor(0.1893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/4339]: training_loss: tensor(0.3329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/4339]: training_loss: tensor(0.1851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/4339]: training_loss: tensor(0.3430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/4339]: training_loss: tensor(0.1933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/4339]: training_loss: tensor(0.4422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/4339]: training_loss: tensor(0.1893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/4339]: training_loss: tensor(0.1509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/4339]: training_loss: tensor(0.1916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/4339]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/4339]: training_loss: tensor(0.0579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/4339]: training_loss: tensor(0.1617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/4339]: training_loss: tensor(0.1265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/4339]: training_loss: tensor(0.2070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/4339]: training_loss: tensor(0.5220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/4339]: training_loss: tensor(0.2222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/4339]: training_loss: tensor(0.3432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/4339]: training_loss: tensor(0.2991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/4339]: training_loss: tensor(0.2687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/4339]: training_loss: tensor(0.5351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/4339]: training_loss: tensor(0.1112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/4339]: training_loss: tensor(0.0384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/4339]: training_loss: tensor(0.2786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/4339]: training_loss: tensor(0.1318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/4339]: training_loss: tensor(0.3178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/4339]: training_loss: tensor(0.4636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/4339]: training_loss: tensor(0.1771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/4339]: training_loss: tensor(0.3050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/4339]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/4339]: training_loss: tensor(0.5064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/4339]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/4339]: training_loss: tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/4339]: training_loss: tensor(0.2890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/4339]: training_loss: tensor(0.6702, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1358/4339]: training_loss: tensor(0.1928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/4339]: training_loss: tensor(0.3811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/4339]: training_loss: tensor(0.1306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1361/4339]: training_loss: tensor(0.2626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/4339]: training_loss: tensor(0.3153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1363/4339]: training_loss: tensor(0.3400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/4339]: training_loss: tensor(0.2563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/4339]: training_loss: tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/4339]: training_loss: tensor(0.6156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/4339]: training_loss: tensor(0.1776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/4339]: training_loss: tensor(0.7546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/4339]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/4339]: training_loss: tensor(0.1154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/4339]: training_loss: tensor(0.2375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/4339]: training_loss: tensor(0.3469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/4339]: training_loss: tensor(0.2751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/4339]: training_loss: tensor(0.3992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/4339]: training_loss: tensor(0.2167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/4339]: training_loss: tensor(0.2165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/4339]: training_loss: tensor(0.3966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/4339]: training_loss: tensor(0.2101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/4339]: training_loss: tensor(0.2399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/4339]: training_loss: tensor(0.1878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/4339]: training_loss: tensor(0.4566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/4339]: training_loss: tensor(0.1430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/4339]: training_loss: tensor(0.4498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/4339]: training_loss: tensor(0.2600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1385/4339]: training_loss: tensor(0.3491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1386/4339]: training_loss: tensor(0.2020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1387/4339]: training_loss: tensor(0.1684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1388/4339]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1389/4339]: training_loss: tensor(0.2888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1390/4339]: training_loss: tensor(0.2943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1391/4339]: training_loss: tensor(0.3249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1392/4339]: training_loss: tensor(0.1630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1393/4339]: training_loss: tensor(0.1652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1394/4339]: training_loss: tensor(0.1487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1395/4339]: training_loss: tensor(0.1369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1396/4339]: training_loss: tensor(0.1473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1397/4339]: training_loss: tensor(0.2472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1398/4339]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1399/4339]: training_loss: tensor(0.5348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1400/4339]: training_loss: tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1401/4339]: training_loss: tensor(0.1223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1402/4339]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1403/4339]: training_loss: tensor(0.3838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1404/4339]: training_loss: tensor(0.1278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1405/4339]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1406/4339]: training_loss: tensor(0.2427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1407/4339]: training_loss: tensor(0.4734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1408/4339]: training_loss: tensor(0.2436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1409/4339]: training_loss: tensor(0.5275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1410/4339]: training_loss: tensor(0.3024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1411/4339]: training_loss: tensor(0.1453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1412/4339]: training_loss: tensor(0.2577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1413/4339]: training_loss: tensor(1.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1414/4339]: training_loss: tensor(0.7471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1415/4339]: training_loss: tensor(0.2128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1416/4339]: training_loss: tensor(0.3224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1417/4339]: training_loss: tensor(0.1736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1418/4339]: training_loss: tensor(0.4987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1419/4339]: training_loss: tensor(0.2943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1420/4339]: training_loss: tensor(0.1365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1421/4339]: training_loss: tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1422/4339]: training_loss: tensor(0.2909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1423/4339]: training_loss: tensor(0.2130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1424/4339]: training_loss: tensor(0.5484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1425/4339]: training_loss: tensor(0.3464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1426/4339]: training_loss: tensor(0.3323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1427/4339]: training_loss: tensor(0.4231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1428/4339]: training_loss: tensor(0.1852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1429/4339]: training_loss: tensor(0.2328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1430/4339]: training_loss: tensor(0.1509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1431/4339]: training_loss: tensor(0.2135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1432/4339]: training_loss: tensor(0.3872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1433/4339]: training_loss: tensor(0.1489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1434/4339]: training_loss: tensor(0.2417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1435/4339]: training_loss: tensor(0.4157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1436/4339]: training_loss: tensor(0.1906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1437/4339]: training_loss: tensor(0.3090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1438/4339]: training_loss: tensor(0.1804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1439/4339]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1440/4339]: training_loss: tensor(0.5090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1441/4339]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1442/4339]: training_loss: tensor(0.5221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1443/4339]: training_loss: tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1444/4339]: training_loss: tensor(0.2147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1445/4339]: training_loss: tensor(0.1964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1446/4339]: training_loss: tensor(0.2589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1447/4339]: training_loss: tensor(0.2989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1448/4339]: training_loss: tensor(0.3210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1449/4339]: training_loss: tensor(0.1553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1450/4339]: training_loss: tensor(0.3307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1451/4339]: training_loss: tensor(0.2743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1452/4339]: training_loss: tensor(0.4545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1453/4339]: training_loss: tensor(0.3290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1454/4339]: training_loss: tensor(0.4350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1455/4339]: training_loss: tensor(0.2692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1456/4339]: training_loss: tensor(0.3801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1457/4339]: training_loss: tensor(0.1508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1458/4339]: training_loss: tensor(0.2725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1459/4339]: training_loss: tensor(0.3225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1460/4339]: training_loss: tensor(0.1377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1461/4339]: training_loss: tensor(0.2095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1462/4339]: training_loss: tensor(0.2200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1463/4339]: training_loss: tensor(0.1646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1464/4339]: training_loss: tensor(0.2701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1465/4339]: training_loss: tensor(0.1184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1466/4339]: training_loss: tensor(0.1405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1467/4339]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1468/4339]: training_loss: tensor(0.2793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1469/4339]: training_loss: tensor(0.1732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1470/4339]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1471/4339]: training_loss: tensor(0.3549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1472/4339]: training_loss: tensor(0.3283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1473/4339]: training_loss: tensor(0.3429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1474/4339]: training_loss: tensor(0.2300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1475/4339]: training_loss: tensor(0.3826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1476/4339]: training_loss: tensor(0.2605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1477/4339]: training_loss: tensor(0.4809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1478/4339]: training_loss: tensor(0.3607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1479/4339]: training_loss: tensor(0.4363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1480/4339]: training_loss: tensor(0.1962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1481/4339]: training_loss: tensor(0.1841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1482/4339]: training_loss: tensor(0.2228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1483/4339]: training_loss: tensor(0.1809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1484/4339]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1485/4339]: training_loss: tensor(0.5119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1486/4339]: training_loss: tensor(0.1265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1487/4339]: training_loss: tensor(0.2494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1488/4339]: training_loss: tensor(0.6237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1489/4339]: training_loss: tensor(0.3221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1490/4339]: training_loss: tensor(0.1381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1491/4339]: training_loss: tensor(0.1976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1492/4339]: training_loss: tensor(0.4201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1493/4339]: training_loss: tensor(0.2178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1494/4339]: training_loss: tensor(0.2358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1495/4339]: training_loss: tensor(0.1600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1496/4339]: training_loss: tensor(0.1720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1497/4339]: training_loss: tensor(0.2610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1498/4339]: training_loss: tensor(0.2702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1499/4339]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1500/4339]: training_loss: tensor(0.1745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1501/4339]: training_loss: tensor(0.2392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1502/4339]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1503/4339]: training_loss: tensor(0.2517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1504/4339]: training_loss: tensor(0.3410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1505/4339]: training_loss: tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1506/4339]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1507/4339]: training_loss: tensor(0.2428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1508/4339]: training_loss: tensor(0.2383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1509/4339]: training_loss: tensor(0.5626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1510/4339]: training_loss: tensor(0.4054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1511/4339]: training_loss: tensor(0.3274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1512/4339]: training_loss: tensor(0.4416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1513/4339]: training_loss: tensor(0.2211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1514/4339]: training_loss: tensor(0.2163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1515/4339]: training_loss: tensor(0.1870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1516/4339]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1517/4339]: training_loss: tensor(0.3449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1518/4339]: training_loss: tensor(0.2169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1519/4339]: training_loss: tensor(0.3143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1520/4339]: training_loss: tensor(0.1465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1521/4339]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1522/4339]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1523/4339]: training_loss: tensor(0.1548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1524/4339]: training_loss: tensor(0.2773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1525/4339]: training_loss: tensor(0.6758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1526/4339]: training_loss: tensor(0.3192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1527/4339]: training_loss: tensor(0.1648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1528/4339]: training_loss: tensor(0.1893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1529/4339]: training_loss: tensor(0.4122, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1530/4339]: training_loss: tensor(0.4695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1531/4339]: training_loss: tensor(0.2303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1532/4339]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1533/4339]: training_loss: tensor(0.1303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1534/4339]: training_loss: tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1535/4339]: training_loss: tensor(0.3340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1536/4339]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1537/4339]: training_loss: tensor(0.4800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1538/4339]: training_loss: tensor(0.2519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1539/4339]: training_loss: tensor(0.5872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1540/4339]: training_loss: tensor(0.1947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1541/4339]: training_loss: tensor(0.1792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1542/4339]: training_loss: tensor(0.2419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1543/4339]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1544/4339]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1545/4339]: training_loss: tensor(0.2757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1546/4339]: training_loss: tensor(0.3175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1547/4339]: training_loss: tensor(0.2944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1548/4339]: training_loss: tensor(0.2862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1549/4339]: training_loss: tensor(0.3969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1550/4339]: training_loss: tensor(0.3146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1551/4339]: training_loss: tensor(0.1821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1552/4339]: training_loss: tensor(0.1321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1553/4339]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1554/4339]: training_loss: tensor(0.4639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1555/4339]: training_loss: tensor(0.2927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1556/4339]: training_loss: tensor(0.4589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1557/4339]: training_loss: tensor(0.7139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1558/4339]: training_loss: tensor(0.2743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1559/4339]: training_loss: tensor(0.1865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1560/4339]: training_loss: tensor(0.5183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1561/4339]: training_loss: tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1562/4339]: training_loss: tensor(0.1391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1563/4339]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1564/4339]: training_loss: tensor(0.1712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1565/4339]: training_loss: tensor(0.1843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1566/4339]: training_loss: tensor(0.2115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1567/4339]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1568/4339]: training_loss: tensor(0.2360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1569/4339]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1570/4339]: training_loss: tensor(0.1418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1571/4339]: training_loss: tensor(0.1547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1572/4339]: training_loss: tensor(0.2252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1573/4339]: training_loss: tensor(0.8224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1574/4339]: training_loss: tensor(0.1196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1575/4339]: training_loss: tensor(0.3007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1576/4339]: training_loss: tensor(0.2878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1577/4339]: training_loss: tensor(0.1283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1578/4339]: training_loss: tensor(0.6841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1579/4339]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1580/4339]: training_loss: tensor(0.3380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1581/4339]: training_loss: tensor(0.1818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1582/4339]: training_loss: tensor(0.5805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1583/4339]: training_loss: tensor(0.2612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1584/4339]: training_loss: tensor(0.1809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1585/4339]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1586/4339]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1587/4339]: training_loss: tensor(0.3094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1588/4339]: training_loss: tensor(0.3192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1589/4339]: training_loss: tensor(0.1616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1590/4339]: training_loss: tensor(0.1984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1591/4339]: training_loss: tensor(0.2144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1592/4339]: training_loss: tensor(0.2369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1593/4339]: training_loss: tensor(0.3089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1594/4339]: training_loss: tensor(0.4419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1595/4339]: training_loss: tensor(0.3806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1596/4339]: training_loss: tensor(0.2920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1597/4339]: training_loss: tensor(0.2201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1598/4339]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1599/4339]: training_loss: tensor(0.1369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1600/4339]: training_loss: tensor(0.1979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1601/4339]: training_loss: tensor(0.1630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1602/4339]: training_loss: tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1603/4339]: training_loss: tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1604/4339]: training_loss: tensor(0.1541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1605/4339]: training_loss: tensor(0.1503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1606/4339]: training_loss: tensor(0.1411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1607/4339]: training_loss: tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1608/4339]: training_loss: tensor(0.2488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1609/4339]: training_loss: tensor(0.5181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1610/4339]: training_loss: tensor(0.3036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1611/4339]: training_loss: tensor(0.1877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1612/4339]: training_loss: tensor(0.1781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1613/4339]: training_loss: tensor(0.4026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1614/4339]: training_loss: tensor(0.3972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1615/4339]: training_loss: tensor(0.1460, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1616/4339]: training_loss: tensor(0.5549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1617/4339]: training_loss: tensor(0.2267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1618/4339]: training_loss: tensor(0.1958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1619/4339]: training_loss: tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1620/4339]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1621/4339]: training_loss: tensor(0.1643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1622/4339]: training_loss: tensor(0.2487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1623/4339]: training_loss: tensor(0.2456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1624/4339]: training_loss: tensor(0.3207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1625/4339]: training_loss: tensor(0.5511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1626/4339]: training_loss: tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1627/4339]: training_loss: tensor(0.1994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1628/4339]: training_loss: tensor(0.4752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1629/4339]: training_loss: tensor(0.3850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1630/4339]: training_loss: tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1631/4339]: training_loss: tensor(0.1759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1632/4339]: training_loss: tensor(0.3955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1633/4339]: training_loss: tensor(0.5585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1634/4339]: training_loss: tensor(0.1663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1635/4339]: training_loss: tensor(0.1842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1636/4339]: training_loss: tensor(0.1175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1637/4339]: training_loss: tensor(0.3914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1638/4339]: training_loss: tensor(0.5112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1639/4339]: training_loss: tensor(0.3284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1640/4339]: training_loss: tensor(0.2025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1641/4339]: training_loss: tensor(0.1752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1642/4339]: training_loss: tensor(0.3099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1643/4339]: training_loss: tensor(0.1269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1644/4339]: training_loss: tensor(0.0586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1645/4339]: training_loss: tensor(0.1869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1646/4339]: training_loss: tensor(0.3432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1647/4339]: training_loss: tensor(0.1893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1648/4339]: training_loss: tensor(0.3984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1649/4339]: training_loss: tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1650/4339]: training_loss: tensor(0.2725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1651/4339]: training_loss: tensor(0.3402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1652/4339]: training_loss: tensor(0.2880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1653/4339]: training_loss: tensor(0.1665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1654/4339]: training_loss: tensor(0.4661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1655/4339]: training_loss: tensor(0.3199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1656/4339]: training_loss: tensor(0.1936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1657/4339]: training_loss: tensor(0.1415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1658/4339]: training_loss: tensor(0.2608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1659/4339]: training_loss: tensor(0.4487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1660/4339]: training_loss: tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1661/4339]: training_loss: tensor(0.1926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1662/4339]: training_loss: tensor(0.4306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1663/4339]: training_loss: tensor(0.4955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1664/4339]: training_loss: tensor(0.2344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1665/4339]: training_loss: tensor(0.2374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1666/4339]: training_loss: tensor(0.1822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1667/4339]: training_loss: tensor(0.1716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1668/4339]: training_loss: tensor(0.1123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1669/4339]: training_loss: tensor(0.2391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1670/4339]: training_loss: tensor(0.1657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1671/4339]: training_loss: tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1672/4339]: training_loss: tensor(0.2993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1673/4339]: training_loss: tensor(0.3186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1674/4339]: training_loss: tensor(0.3724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1675/4339]: training_loss: tensor(0.2484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1676/4339]: training_loss: tensor(0.2490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1677/4339]: training_loss: tensor(0.4069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1678/4339]: training_loss: tensor(0.0882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1679/4339]: training_loss: tensor(0.1431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1680/4339]: training_loss: tensor(0.1263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1681/4339]: training_loss: tensor(0.3011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1682/4339]: training_loss: tensor(0.2235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1683/4339]: training_loss: tensor(0.1760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1684/4339]: training_loss: tensor(0.4756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1685/4339]: training_loss: tensor(0.1444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1686/4339]: training_loss: tensor(0.4041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1687/4339]: training_loss: tensor(0.2615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1688/4339]: training_loss: tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1689/4339]: training_loss: tensor(0.3324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1690/4339]: training_loss: tensor(0.2952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1691/4339]: training_loss: tensor(0.2666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1692/4339]: training_loss: tensor(0.8150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1693/4339]: training_loss: tensor(0.3578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1694/4339]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1695/4339]: training_loss: tensor(0.4824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1696/4339]: training_loss: tensor(0.1754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1697/4339]: training_loss: tensor(0.1262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1698/4339]: training_loss: tensor(0.2426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1699/4339]: training_loss: tensor(0.1553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1700/4339]: training_loss: tensor(0.2134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1701/4339]: training_loss: tensor(0.3115, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1702/4339]: training_loss: tensor(0.4816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1703/4339]: training_loss: tensor(0.1958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1704/4339]: training_loss: tensor(0.3097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1705/4339]: training_loss: tensor(0.4410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1706/4339]: training_loss: tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1707/4339]: training_loss: tensor(0.1212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1708/4339]: training_loss: tensor(0.2795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1709/4339]: training_loss: tensor(0.1307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1710/4339]: training_loss: tensor(0.3128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1711/4339]: training_loss: tensor(0.2757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1712/4339]: training_loss: tensor(0.1578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1713/4339]: training_loss: tensor(0.1705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1714/4339]: training_loss: tensor(0.1706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1715/4339]: training_loss: tensor(0.5076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1716/4339]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1717/4339]: training_loss: tensor(0.3816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1718/4339]: training_loss: tensor(0.5053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1719/4339]: training_loss: tensor(0.2085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1720/4339]: training_loss: tensor(0.3435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1721/4339]: training_loss: tensor(0.1355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1722/4339]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1723/4339]: training_loss: tensor(0.1575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1724/4339]: training_loss: tensor(0.4081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1725/4339]: training_loss: tensor(0.1103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1726/4339]: training_loss: tensor(0.1706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1727/4339]: training_loss: tensor(0.2516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1728/4339]: training_loss: tensor(0.5701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1729/4339]: training_loss: tensor(0.7859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1730/4339]: training_loss: tensor(0.2776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1731/4339]: training_loss: tensor(0.3450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1732/4339]: training_loss: tensor(0.1925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1733/4339]: training_loss: tensor(0.3614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1734/4339]: training_loss: tensor(0.1319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1735/4339]: training_loss: tensor(0.1512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1736/4339]: training_loss: tensor(0.1641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1737/4339]: training_loss: tensor(0.4398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1738/4339]: training_loss: tensor(0.1814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1739/4339]: training_loss: tensor(0.2390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1740/4339]: training_loss: tensor(0.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1741/4339]: training_loss: tensor(0.3827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1742/4339]: training_loss: tensor(0.1589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1743/4339]: training_loss: tensor(0.2603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1744/4339]: training_loss: tensor(0.3503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1745/4339]: training_loss: tensor(0.2845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1746/4339]: training_loss: tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1747/4339]: training_loss: tensor(0.3431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1748/4339]: training_loss: tensor(0.2113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1749/4339]: training_loss: tensor(0.1226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1750/4339]: training_loss: tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1751/4339]: training_loss: tensor(0.3938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1752/4339]: training_loss: tensor(0.2838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1753/4339]: training_loss: tensor(0.3858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1754/4339]: training_loss: tensor(0.4862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1755/4339]: training_loss: tensor(0.1169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1756/4339]: training_loss: tensor(0.2615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1757/4339]: training_loss: tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1758/4339]: training_loss: tensor(0.1997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1759/4339]: training_loss: tensor(0.2067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1760/4339]: training_loss: tensor(0.1679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1761/4339]: training_loss: tensor(0.1483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1762/4339]: training_loss: tensor(0.2107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1763/4339]: training_loss: tensor(0.3576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1764/4339]: training_loss: tensor(0.0566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1765/4339]: training_loss: tensor(0.5700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1766/4339]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1767/4339]: training_loss: tensor(0.4417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1768/4339]: training_loss: tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1769/4339]: training_loss: tensor(0.0629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1770/4339]: training_loss: tensor(0.1854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1771/4339]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1772/4339]: training_loss: tensor(0.4734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1773/4339]: training_loss: tensor(0.1282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1774/4339]: training_loss: tensor(0.2047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1775/4339]: training_loss: tensor(0.3272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1776/4339]: training_loss: tensor(0.2135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1777/4339]: training_loss: tensor(0.4606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1778/4339]: training_loss: tensor(0.1278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1779/4339]: training_loss: tensor(0.4053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1780/4339]: training_loss: tensor(0.2606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1781/4339]: training_loss: tensor(0.3480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1782/4339]: training_loss: tensor(0.1887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1783/4339]: training_loss: tensor(0.3619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1784/4339]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1785/4339]: training_loss: tensor(0.2908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1786/4339]: training_loss: tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1787/4339]: training_loss: tensor(0.2407, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1788/4339]: training_loss: tensor(0.2685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1789/4339]: training_loss: tensor(0.1695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1790/4339]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1791/4339]: training_loss: tensor(0.6070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1792/4339]: training_loss: tensor(0.1372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1793/4339]: training_loss: tensor(0.2422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1794/4339]: training_loss: tensor(0.1993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1795/4339]: training_loss: tensor(0.1772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1796/4339]: training_loss: tensor(0.3491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1797/4339]: training_loss: tensor(0.3401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1798/4339]: training_loss: tensor(0.3792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1799/4339]: training_loss: tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1800/4339]: training_loss: tensor(0.2935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1801/4339]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1802/4339]: training_loss: tensor(0.2555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1803/4339]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1804/4339]: training_loss: tensor(0.2931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1805/4339]: training_loss: tensor(0.4275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1806/4339]: training_loss: tensor(0.2020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1807/4339]: training_loss: tensor(0.1898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1808/4339]: training_loss: tensor(0.3059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1809/4339]: training_loss: tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1810/4339]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1811/4339]: training_loss: tensor(0.3953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1812/4339]: training_loss: tensor(0.4269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1813/4339]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1814/4339]: training_loss: tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1815/4339]: training_loss: tensor(0.5455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1816/4339]: training_loss: tensor(0.1284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1817/4339]: training_loss: tensor(0.3164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1818/4339]: training_loss: tensor(0.2668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1819/4339]: training_loss: tensor(0.6158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1820/4339]: training_loss: tensor(0.2251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1821/4339]: training_loss: tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1822/4339]: training_loss: tensor(0.4043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1823/4339]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1824/4339]: training_loss: tensor(0.2535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1825/4339]: training_loss: tensor(0.2591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1826/4339]: training_loss: tensor(0.6118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1827/4339]: training_loss: tensor(0.1306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1828/4339]: training_loss: tensor(0.2723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1829/4339]: training_loss: tensor(0.2082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1830/4339]: training_loss: tensor(0.2334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1831/4339]: training_loss: tensor(0.1789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1832/4339]: training_loss: tensor(0.1347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1833/4339]: training_loss: tensor(0.2812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1834/4339]: training_loss: tensor(0.2809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1835/4339]: training_loss: tensor(0.2761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1836/4339]: training_loss: tensor(0.1880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1837/4339]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1838/4339]: training_loss: tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1839/4339]: training_loss: tensor(0.1969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1840/4339]: training_loss: tensor(0.2054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1841/4339]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1842/4339]: training_loss: tensor(0.4750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1843/4339]: training_loss: tensor(0.5148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1844/4339]: training_loss: tensor(0.1866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1845/4339]: training_loss: tensor(0.1316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1846/4339]: training_loss: tensor(0.2355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1847/4339]: training_loss: tensor(0.2830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1848/4339]: training_loss: tensor(0.2265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1849/4339]: training_loss: tensor(0.2169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1850/4339]: training_loss: tensor(0.1741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1851/4339]: training_loss: tensor(0.3984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1852/4339]: training_loss: tensor(0.4883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1853/4339]: training_loss: tensor(0.1472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1854/4339]: training_loss: tensor(0.2911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1855/4339]: training_loss: tensor(0.1896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1856/4339]: training_loss: tensor(0.7737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1857/4339]: training_loss: tensor(0.3352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1858/4339]: training_loss: tensor(0.2757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1859/4339]: training_loss: tensor(0.2058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1860/4339]: training_loss: tensor(0.2711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1861/4339]: training_loss: tensor(0.2149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1862/4339]: training_loss: tensor(0.3752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1863/4339]: training_loss: tensor(0.2053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1864/4339]: training_loss: tensor(0.2361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1865/4339]: training_loss: tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1866/4339]: training_loss: tensor(0.1715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1867/4339]: training_loss: tensor(0.1106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1868/4339]: training_loss: tensor(0.6478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1869/4339]: training_loss: tensor(0.3219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1870/4339]: training_loss: tensor(0.2225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1871/4339]: training_loss: tensor(0.2339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1872/4339]: training_loss: tensor(0.2334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1873/4339]: training_loss: tensor(0.3342, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1874/4339]: training_loss: tensor(0.1767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1875/4339]: training_loss: tensor(0.4786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1876/4339]: training_loss: tensor(0.1996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1877/4339]: training_loss: tensor(0.1654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1878/4339]: training_loss: tensor(0.5447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1879/4339]: training_loss: tensor(0.1289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1880/4339]: training_loss: tensor(0.2316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1881/4339]: training_loss: tensor(0.2536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1882/4339]: training_loss: tensor(0.2666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1883/4339]: training_loss: tensor(0.6646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1884/4339]: training_loss: tensor(0.2895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1885/4339]: training_loss: tensor(0.2228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1886/4339]: training_loss: tensor(0.1172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1887/4339]: training_loss: tensor(0.3303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1888/4339]: training_loss: tensor(0.1809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1889/4339]: training_loss: tensor(0.2458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1890/4339]: training_loss: tensor(0.2628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1891/4339]: training_loss: tensor(0.1792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1892/4339]: training_loss: tensor(0.2506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1893/4339]: training_loss: tensor(0.2540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1894/4339]: training_loss: tensor(0.2023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1895/4339]: training_loss: tensor(0.2206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1896/4339]: training_loss: tensor(0.3135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1897/4339]: training_loss: tensor(0.1426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1898/4339]: training_loss: tensor(0.3824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1899/4339]: training_loss: tensor(0.1562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1900/4339]: training_loss: tensor(0.3845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1901/4339]: training_loss: tensor(0.2084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1902/4339]: training_loss: tensor(0.1352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1903/4339]: training_loss: tensor(0.2376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1904/4339]: training_loss: tensor(0.1175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1905/4339]: training_loss: tensor(0.1105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1906/4339]: training_loss: tensor(0.1896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1907/4339]: training_loss: tensor(0.3821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1908/4339]: training_loss: tensor(0.2434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1909/4339]: training_loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1910/4339]: training_loss: tensor(0.2734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1911/4339]: training_loss: tensor(0.1523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1912/4339]: training_loss: tensor(0.2776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1913/4339]: training_loss: tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1914/4339]: training_loss: tensor(0.1736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1915/4339]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1916/4339]: training_loss: tensor(0.3584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1917/4339]: training_loss: tensor(0.1844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1918/4339]: training_loss: tensor(0.5866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1919/4339]: training_loss: tensor(0.4307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1920/4339]: training_loss: tensor(0.1662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1921/4339]: training_loss: tensor(0.3206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1922/4339]: training_loss: tensor(0.2062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1923/4339]: training_loss: tensor(0.3080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1924/4339]: training_loss: tensor(0.3419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1925/4339]: training_loss: tensor(0.5490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1926/4339]: training_loss: tensor(0.1319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1927/4339]: training_loss: tensor(0.2477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1928/4339]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1929/4339]: training_loss: tensor(0.1888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1930/4339]: training_loss: tensor(0.1875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1931/4339]: training_loss: tensor(0.3103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1932/4339]: training_loss: tensor(0.1922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1933/4339]: training_loss: tensor(0.2557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1934/4339]: training_loss: tensor(0.1551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1935/4339]: training_loss: tensor(0.1903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1936/4339]: training_loss: tensor(0.1546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1937/4339]: training_loss: tensor(0.5389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1938/4339]: training_loss: tensor(0.6071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1939/4339]: training_loss: tensor(0.1805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1940/4339]: training_loss: tensor(0.2882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1941/4339]: training_loss: tensor(0.5426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1942/4339]: training_loss: tensor(0.3283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1943/4339]: training_loss: tensor(0.1799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1944/4339]: training_loss: tensor(0.1237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1945/4339]: training_loss: tensor(0.2859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1946/4339]: training_loss: tensor(0.1459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1947/4339]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1948/4339]: training_loss: tensor(0.4697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1949/4339]: training_loss: tensor(0.4104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1950/4339]: training_loss: tensor(0.3292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1951/4339]: training_loss: tensor(0.2781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1952/4339]: training_loss: tensor(0.3259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1953/4339]: training_loss: tensor(0.2878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1954/4339]: training_loss: tensor(0.2343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1955/4339]: training_loss: tensor(0.4444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1956/4339]: training_loss: tensor(0.3116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1957/4339]: training_loss: tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1958/4339]: training_loss: tensor(0.1416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1959/4339]: training_loss: tensor(0.2672, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1960/4339]: training_loss: tensor(0.2013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1961/4339]: training_loss: tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1962/4339]: training_loss: tensor(0.2572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1963/4339]: training_loss: tensor(0.2994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1964/4339]: training_loss: tensor(0.1944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1965/4339]: training_loss: tensor(0.1687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1966/4339]: training_loss: tensor(0.5567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1967/4339]: training_loss: tensor(0.3112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1968/4339]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1969/4339]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1970/4339]: training_loss: tensor(0.1478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1971/4339]: training_loss: tensor(0.1773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1972/4339]: training_loss: tensor(0.6445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1973/4339]: training_loss: tensor(0.2679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1974/4339]: training_loss: tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1975/4339]: training_loss: tensor(0.1625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1976/4339]: training_loss: tensor(0.3181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1977/4339]: training_loss: tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1978/4339]: training_loss: tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1979/4339]: training_loss: tensor(0.2967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1980/4339]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1981/4339]: training_loss: tensor(0.5351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1982/4339]: training_loss: tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1983/4339]: training_loss: tensor(0.1706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1984/4339]: training_loss: tensor(0.2433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1985/4339]: training_loss: tensor(0.0629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1986/4339]: training_loss: tensor(0.2539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1987/4339]: training_loss: tensor(0.1315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1988/4339]: training_loss: tensor(0.1442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1989/4339]: training_loss: tensor(0.2349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1990/4339]: training_loss: tensor(0.3528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1991/4339]: training_loss: tensor(0.2262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1992/4339]: training_loss: tensor(0.2384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1993/4339]: training_loss: tensor(0.2039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1994/4339]: training_loss: tensor(0.1704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1995/4339]: training_loss: tensor(0.6761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1996/4339]: training_loss: tensor(0.1521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1997/4339]: training_loss: tensor(0.3170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1998/4339]: training_loss: tensor(0.1979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1999/4339]: training_loss: tensor(0.3258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2000/4339]: training_loss: tensor(0.2723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2001/4339]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2002/4339]: training_loss: tensor(0.2514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2003/4339]: training_loss: tensor(0.2620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2004/4339]: training_loss: tensor(0.1721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2005/4339]: training_loss: tensor(0.1548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2006/4339]: training_loss: tensor(0.1719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2007/4339]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2008/4339]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2009/4339]: training_loss: tensor(0.2522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2010/4339]: training_loss: tensor(0.2519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2011/4339]: training_loss: tensor(0.2948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2012/4339]: training_loss: tensor(0.4907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2013/4339]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2014/4339]: training_loss: tensor(0.1809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2015/4339]: training_loss: tensor(0.1385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2016/4339]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2017/4339]: training_loss: tensor(0.4377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2018/4339]: training_loss: tensor(0.1532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2019/4339]: training_loss: tensor(0.1622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2020/4339]: training_loss: tensor(0.2952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2021/4339]: training_loss: tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2022/4339]: training_loss: tensor(0.5012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2023/4339]: training_loss: tensor(0.1238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2024/4339]: training_loss: tensor(0.4086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2025/4339]: training_loss: tensor(0.4853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2026/4339]: training_loss: tensor(0.2966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2027/4339]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2028/4339]: training_loss: tensor(0.2706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2029/4339]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2030/4339]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2031/4339]: training_loss: tensor(0.3339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2032/4339]: training_loss: tensor(0.1684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2033/4339]: training_loss: tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2034/4339]: training_loss: tensor(0.1601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2035/4339]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2036/4339]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2037/4339]: training_loss: tensor(0.5611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2038/4339]: training_loss: tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2039/4339]: training_loss: tensor(0.2115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2040/4339]: training_loss: tensor(0.2774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2041/4339]: training_loss: tensor(0.4413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2042/4339]: training_loss: tensor(0.3350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2043/4339]: training_loss: tensor(0.2140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2044/4339]: training_loss: tensor(0.2858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2045/4339]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2046/4339]: training_loss: tensor(0.2043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2047/4339]: training_loss: tensor(0.4934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2048/4339]: training_loss: tensor(0.2109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2049/4339]: training_loss: tensor(0.2979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2050/4339]: training_loss: tensor(0.1583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2051/4339]: training_loss: tensor(0.2833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2052/4339]: training_loss: tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2053/4339]: training_loss: tensor(0.4318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2054/4339]: training_loss: tensor(0.2315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2055/4339]: training_loss: tensor(0.2787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2056/4339]: training_loss: tensor(0.5618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2057/4339]: training_loss: tensor(0.2833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2058/4339]: training_loss: tensor(0.2159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2059/4339]: training_loss: tensor(0.1723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2060/4339]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2061/4339]: training_loss: tensor(0.3537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2062/4339]: training_loss: tensor(0.3598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2063/4339]: training_loss: tensor(0.3189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2064/4339]: training_loss: tensor(0.2679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2065/4339]: training_loss: tensor(0.3968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2066/4339]: training_loss: tensor(0.3806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2067/4339]: training_loss: tensor(0.1891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2068/4339]: training_loss: tensor(0.1473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2069/4339]: training_loss: tensor(0.2278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2070/4339]: training_loss: tensor(0.2932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2071/4339]: training_loss: tensor(0.1939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2072/4339]: training_loss: tensor(0.1554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2073/4339]: training_loss: tensor(0.2541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2074/4339]: training_loss: tensor(0.3327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2075/4339]: training_loss: tensor(0.3539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2076/4339]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2077/4339]: training_loss: tensor(0.3914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2078/4339]: training_loss: tensor(0.1843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2079/4339]: training_loss: tensor(0.1336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2080/4339]: training_loss: tensor(0.1569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2081/4339]: training_loss: tensor(0.3018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2082/4339]: training_loss: tensor(0.5996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2083/4339]: training_loss: tensor(0.5832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2084/4339]: training_loss: tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2085/4339]: training_loss: tensor(0.2382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2086/4339]: training_loss: tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2087/4339]: training_loss: tensor(0.3565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2088/4339]: training_loss: tensor(0.2199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2089/4339]: training_loss: tensor(0.4709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2090/4339]: training_loss: tensor(0.1098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2091/4339]: training_loss: tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2092/4339]: training_loss: tensor(0.1414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2093/4339]: training_loss: tensor(0.3505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2094/4339]: training_loss: tensor(0.2107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2095/4339]: training_loss: tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2096/4339]: training_loss: tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2097/4339]: training_loss: tensor(0.2188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2098/4339]: training_loss: tensor(0.1749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2099/4339]: training_loss: tensor(0.2128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2100/4339]: training_loss: tensor(0.2535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2101/4339]: training_loss: tensor(0.2421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2102/4339]: training_loss: tensor(0.1129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2103/4339]: training_loss: tensor(0.2223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2104/4339]: training_loss: tensor(0.2673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2105/4339]: training_loss: tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2106/4339]: training_loss: tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2107/4339]: training_loss: tensor(0.3333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2108/4339]: training_loss: tensor(0.3222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2109/4339]: training_loss: tensor(0.1774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2110/4339]: training_loss: tensor(0.1540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2111/4339]: training_loss: tensor(0.1328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2112/4339]: training_loss: tensor(0.7984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2113/4339]: training_loss: tensor(0.2309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2114/4339]: training_loss: tensor(0.2433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2115/4339]: training_loss: tensor(0.2812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2116/4339]: training_loss: tensor(0.4460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2117/4339]: training_loss: tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2118/4339]: training_loss: tensor(0.2117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2119/4339]: training_loss: tensor(0.2066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2120/4339]: training_loss: tensor(0.1837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2121/4339]: training_loss: tensor(0.4177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2122/4339]: training_loss: tensor(0.1766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2123/4339]: training_loss: tensor(0.3556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2124/4339]: training_loss: tensor(0.2976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2125/4339]: training_loss: tensor(0.1465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2126/4339]: training_loss: tensor(0.4365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2127/4339]: training_loss: tensor(0.7282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2128/4339]: training_loss: tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2129/4339]: training_loss: tensor(0.1958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2130/4339]: training_loss: tensor(0.5989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2131/4339]: training_loss: tensor(0.2448, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2132/4339]: training_loss: tensor(0.4259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2133/4339]: training_loss: tensor(0.1980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2134/4339]: training_loss: tensor(0.1896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2135/4339]: training_loss: tensor(0.1952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2136/4339]: training_loss: tensor(0.5028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2137/4339]: training_loss: tensor(0.3240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2138/4339]: training_loss: tensor(0.3967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2139/4339]: training_loss: tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2140/4339]: training_loss: tensor(0.1690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2141/4339]: training_loss: tensor(0.1346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2142/4339]: training_loss: tensor(0.3209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2143/4339]: training_loss: tensor(0.2147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2144/4339]: training_loss: tensor(0.1972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2145/4339]: training_loss: tensor(0.2737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2146/4339]: training_loss: tensor(0.3387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2147/4339]: training_loss: tensor(0.3179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2148/4339]: training_loss: tensor(0.1409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2149/4339]: training_loss: tensor(0.1259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2150/4339]: training_loss: tensor(0.2160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2151/4339]: training_loss: tensor(0.1818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2152/4339]: training_loss: tensor(0.1601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2153/4339]: training_loss: tensor(0.9413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2154/4339]: training_loss: tensor(0.4652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2155/4339]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2156/4339]: training_loss: tensor(0.2092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2157/4339]: training_loss: tensor(0.3680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2158/4339]: training_loss: tensor(0.1360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2159/4339]: training_loss: tensor(0.4335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2160/4339]: training_loss: tensor(0.5248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2161/4339]: training_loss: tensor(0.6013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2162/4339]: training_loss: tensor(0.3249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2163/4339]: training_loss: tensor(0.2304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2164/4339]: training_loss: tensor(0.5219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2165/4339]: training_loss: tensor(0.2153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2166/4339]: training_loss: tensor(0.2332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2167/4339]: training_loss: tensor(0.2137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2168/4339]: training_loss: tensor(0.2960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2169/4339]: training_loss: tensor(0.2024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2170/4339]: training_loss: tensor(0.2245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [2/5], Step [6510/21700], Train Loss: 0.2559, Valid Loss: 0.3327\n",
      "batch_no [2171/4339]: training_loss: tensor(0.1828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2172/4339]: training_loss: tensor(0.2923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2173/4339]: training_loss: tensor(0.4567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2174/4339]: training_loss: tensor(0.2342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2175/4339]: training_loss: tensor(0.2845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2176/4339]: training_loss: tensor(0.2546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2177/4339]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2178/4339]: training_loss: tensor(0.3943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2179/4339]: training_loss: tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2180/4339]: training_loss: tensor(0.3462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2181/4339]: training_loss: tensor(0.1535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2182/4339]: training_loss: tensor(0.0621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2183/4339]: training_loss: tensor(0.5438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2184/4339]: training_loss: tensor(0.5185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2185/4339]: training_loss: tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2186/4339]: training_loss: tensor(0.3697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2187/4339]: training_loss: tensor(0.2751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2188/4339]: training_loss: tensor(0.5880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2189/4339]: training_loss: tensor(0.1722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2190/4339]: training_loss: tensor(0.7281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2191/4339]: training_loss: tensor(0.2533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2192/4339]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2193/4339]: training_loss: tensor(0.3711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2194/4339]: training_loss: tensor(0.3271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2195/4339]: training_loss: tensor(0.1677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2196/4339]: training_loss: tensor(0.4487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2197/4339]: training_loss: tensor(0.3121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2198/4339]: training_loss: tensor(0.2660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2199/4339]: training_loss: tensor(0.2401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2200/4339]: training_loss: tensor(0.2471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2201/4339]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2202/4339]: training_loss: tensor(0.1365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2203/4339]: training_loss: tensor(0.7700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2204/4339]: training_loss: tensor(0.2903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2205/4339]: training_loss: tensor(0.3790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2206/4339]: training_loss: tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2207/4339]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2208/4339]: training_loss: tensor(0.3506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2209/4339]: training_loss: tensor(0.3585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2210/4339]: training_loss: tensor(0.1570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2211/4339]: training_loss: tensor(0.1133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2212/4339]: training_loss: tensor(0.4716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2213/4339]: training_loss: tensor(0.2609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2214/4339]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2215/4339]: training_loss: tensor(0.4971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2216/4339]: training_loss: tensor(0.2842, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2217/4339]: training_loss: tensor(0.1205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2218/4339]: training_loss: tensor(0.2162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2219/4339]: training_loss: tensor(0.4135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2220/4339]: training_loss: tensor(0.4681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2221/4339]: training_loss: tensor(0.4433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2222/4339]: training_loss: tensor(0.1753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2223/4339]: training_loss: tensor(0.5580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2224/4339]: training_loss: tensor(0.1997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2225/4339]: training_loss: tensor(0.3364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2226/4339]: training_loss: tensor(0.5722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2227/4339]: training_loss: tensor(0.3232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2228/4339]: training_loss: tensor(0.1545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2229/4339]: training_loss: tensor(0.2500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2230/4339]: training_loss: tensor(0.2546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2231/4339]: training_loss: tensor(0.1721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2232/4339]: training_loss: tensor(0.3233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2233/4339]: training_loss: tensor(0.2737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2234/4339]: training_loss: tensor(0.3818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2235/4339]: training_loss: tensor(0.4066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2236/4339]: training_loss: tensor(0.5913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2237/4339]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2238/4339]: training_loss: tensor(0.2968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2239/4339]: training_loss: tensor(0.1498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2240/4339]: training_loss: tensor(0.1257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2241/4339]: training_loss: tensor(0.4303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2242/4339]: training_loss: tensor(0.3729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2243/4339]: training_loss: tensor(0.3011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2244/4339]: training_loss: tensor(0.2889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2245/4339]: training_loss: tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2246/4339]: training_loss: tensor(0.3490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2247/4339]: training_loss: tensor(0.4746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2248/4339]: training_loss: tensor(0.4856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2249/4339]: training_loss: tensor(0.2495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2250/4339]: training_loss: tensor(0.6850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2251/4339]: training_loss: tensor(0.2497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2252/4339]: training_loss: tensor(0.1993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2253/4339]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2254/4339]: training_loss: tensor(0.3558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2255/4339]: training_loss: tensor(0.2153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2256/4339]: training_loss: tensor(0.4227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2257/4339]: training_loss: tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2258/4339]: training_loss: tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2259/4339]: training_loss: tensor(0.3716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2260/4339]: training_loss: tensor(0.2344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2261/4339]: training_loss: tensor(0.4182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2262/4339]: training_loss: tensor(0.1935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2263/4339]: training_loss: tensor(0.1548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2264/4339]: training_loss: tensor(0.1324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2265/4339]: training_loss: tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2266/4339]: training_loss: tensor(0.3066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2267/4339]: training_loss: tensor(0.2318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2268/4339]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2269/4339]: training_loss: tensor(0.1992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2270/4339]: training_loss: tensor(0.4466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2271/4339]: training_loss: tensor(0.5203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2272/4339]: training_loss: tensor(0.4659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2273/4339]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2274/4339]: training_loss: tensor(0.5348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2275/4339]: training_loss: tensor(0.2571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2276/4339]: training_loss: tensor(0.2555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2277/4339]: training_loss: tensor(0.2012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2278/4339]: training_loss: tensor(0.2712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2279/4339]: training_loss: tensor(0.2938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2280/4339]: training_loss: tensor(0.2627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2281/4339]: training_loss: tensor(0.1529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2282/4339]: training_loss: tensor(0.2514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2283/4339]: training_loss: tensor(0.4207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2284/4339]: training_loss: tensor(0.2664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2285/4339]: training_loss: tensor(0.2812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2286/4339]: training_loss: tensor(0.2285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2287/4339]: training_loss: tensor(0.1655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2288/4339]: training_loss: tensor(0.3579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2289/4339]: training_loss: tensor(0.4206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2290/4339]: training_loss: tensor(0.2535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2291/4339]: training_loss: tensor(0.2422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2292/4339]: training_loss: tensor(0.2623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2293/4339]: training_loss: tensor(0.3401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2294/4339]: training_loss: tensor(0.2445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2295/4339]: training_loss: tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2296/4339]: training_loss: tensor(0.1280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2297/4339]: training_loss: tensor(0.1975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2298/4339]: training_loss: tensor(0.4181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2299/4339]: training_loss: tensor(0.2772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2300/4339]: training_loss: tensor(0.2323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2301/4339]: training_loss: tensor(0.3370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2302/4339]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2303/4339]: training_loss: tensor(0.4546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2304/4339]: training_loss: tensor(0.5152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2305/4339]: training_loss: tensor(0.3084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2306/4339]: training_loss: tensor(0.1418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2307/4339]: training_loss: tensor(0.2761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2308/4339]: training_loss: tensor(0.5154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2309/4339]: training_loss: tensor(0.5155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2310/4339]: training_loss: tensor(0.1348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2311/4339]: training_loss: tensor(0.4852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2312/4339]: training_loss: tensor(0.6690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2313/4339]: training_loss: tensor(0.2850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2314/4339]: training_loss: tensor(0.2596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2315/4339]: training_loss: tensor(0.2480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2316/4339]: training_loss: tensor(0.4081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2317/4339]: training_loss: tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2318/4339]: training_loss: tensor(0.2450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2319/4339]: training_loss: tensor(0.1465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2320/4339]: training_loss: tensor(0.3014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2321/4339]: training_loss: tensor(0.2504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2322/4339]: training_loss: tensor(0.2845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2323/4339]: training_loss: tensor(0.2092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2324/4339]: training_loss: tensor(0.2420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2325/4339]: training_loss: tensor(0.2855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2326/4339]: training_loss: tensor(0.3089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2327/4339]: training_loss: tensor(0.2108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2328/4339]: training_loss: tensor(0.5855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2329/4339]: training_loss: tensor(0.4700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2330/4339]: training_loss: tensor(0.3246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2331/4339]: training_loss: tensor(0.2421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2332/4339]: training_loss: tensor(0.3944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2333/4339]: training_loss: tensor(0.2781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2334/4339]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2335/4339]: training_loss: tensor(0.1532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2336/4339]: training_loss: tensor(0.4081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2337/4339]: training_loss: tensor(0.2340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2338/4339]: training_loss: tensor(0.3460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2339/4339]: training_loss: tensor(0.4425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2340/4339]: training_loss: tensor(0.4463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2341/4339]: training_loss: tensor(0.2233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2342/4339]: training_loss: tensor(0.3580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2343/4339]: training_loss: tensor(0.3826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2344/4339]: training_loss: tensor(0.1391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2345/4339]: training_loss: tensor(0.1956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2346/4339]: training_loss: tensor(0.3030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2347/4339]: training_loss: tensor(0.4738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2348/4339]: training_loss: tensor(0.2848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2349/4339]: training_loss: tensor(0.2340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2350/4339]: training_loss: tensor(0.1667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2351/4339]: training_loss: tensor(0.2847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2352/4339]: training_loss: tensor(0.2847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2353/4339]: training_loss: tensor(0.2408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2354/4339]: training_loss: tensor(0.1928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2355/4339]: training_loss: tensor(0.3359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2356/4339]: training_loss: tensor(0.1721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2357/4339]: training_loss: tensor(0.6115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2358/4339]: training_loss: tensor(0.5316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2359/4339]: training_loss: tensor(0.6824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2360/4339]: training_loss: tensor(0.1834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2361/4339]: training_loss: tensor(0.1256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2362/4339]: training_loss: tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2363/4339]: training_loss: tensor(0.2769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2364/4339]: training_loss: tensor(0.0616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2365/4339]: training_loss: tensor(0.2395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2366/4339]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2367/4339]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2368/4339]: training_loss: tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2369/4339]: training_loss: tensor(0.2005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2370/4339]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2371/4339]: training_loss: tensor(0.3058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2372/4339]: training_loss: tensor(0.7055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2373/4339]: training_loss: tensor(0.5851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2374/4339]: training_loss: tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2375/4339]: training_loss: tensor(0.2292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2376/4339]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2377/4339]: training_loss: tensor(0.3459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2378/4339]: training_loss: tensor(0.6263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2379/4339]: training_loss: tensor(0.1809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2380/4339]: training_loss: tensor(0.1176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2381/4339]: training_loss: tensor(0.1421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2382/4339]: training_loss: tensor(0.3094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2383/4339]: training_loss: tensor(0.3343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2384/4339]: training_loss: tensor(0.5414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2385/4339]: training_loss: tensor(0.1570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2386/4339]: training_loss: tensor(0.3716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2387/4339]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2388/4339]: training_loss: tensor(0.1894, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2389/4339]: training_loss: tensor(0.1370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2390/4339]: training_loss: tensor(0.1875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2391/4339]: training_loss: tensor(0.3271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2392/4339]: training_loss: tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2393/4339]: training_loss: tensor(0.1326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2394/4339]: training_loss: tensor(0.5258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2395/4339]: training_loss: tensor(0.1313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2396/4339]: training_loss: tensor(0.1962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2397/4339]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2398/4339]: training_loss: tensor(0.3473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2399/4339]: training_loss: tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2400/4339]: training_loss: tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2401/4339]: training_loss: tensor(0.2658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2402/4339]: training_loss: tensor(0.1366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2403/4339]: training_loss: tensor(0.5163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2404/4339]: training_loss: tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2405/4339]: training_loss: tensor(0.2512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2406/4339]: training_loss: tensor(0.2050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2407/4339]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2408/4339]: training_loss: tensor(0.4981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2409/4339]: training_loss: tensor(0.6199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2410/4339]: training_loss: tensor(0.1632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2411/4339]: training_loss: tensor(0.5432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2412/4339]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2413/4339]: training_loss: tensor(0.3654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2414/4339]: training_loss: tensor(0.1818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2415/4339]: training_loss: tensor(0.2878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2416/4339]: training_loss: tensor(0.1369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2417/4339]: training_loss: tensor(0.2578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2418/4339]: training_loss: tensor(0.2218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2419/4339]: training_loss: tensor(0.3152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2420/4339]: training_loss: tensor(0.1522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2421/4339]: training_loss: tensor(0.1680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2422/4339]: training_loss: tensor(0.2103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2423/4339]: training_loss: tensor(0.1155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2424/4339]: training_loss: tensor(0.3491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2425/4339]: training_loss: tensor(0.5120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2426/4339]: training_loss: tensor(0.2569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2427/4339]: training_loss: tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2428/4339]: training_loss: tensor(0.1460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2429/4339]: training_loss: tensor(0.2437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2430/4339]: training_loss: tensor(0.3451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2431/4339]: training_loss: tensor(0.1488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2432/4339]: training_loss: tensor(0.1591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2433/4339]: training_loss: tensor(0.3168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2434/4339]: training_loss: tensor(0.3632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2435/4339]: training_loss: tensor(0.3446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2436/4339]: training_loss: tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2437/4339]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2438/4339]: training_loss: tensor(0.2822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2439/4339]: training_loss: tensor(0.6797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2440/4339]: training_loss: tensor(0.7972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2441/4339]: training_loss: tensor(0.2313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2442/4339]: training_loss: tensor(0.2927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2443/4339]: training_loss: tensor(0.4103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2444/4339]: training_loss: tensor(0.3989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2445/4339]: training_loss: tensor(0.4751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2446/4339]: training_loss: tensor(0.1893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2447/4339]: training_loss: tensor(0.0635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2448/4339]: training_loss: tensor(0.2208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2449/4339]: training_loss: tensor(0.4419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2450/4339]: training_loss: tensor(0.4928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2451/4339]: training_loss: tensor(0.2309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2452/4339]: training_loss: tensor(0.2650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2453/4339]: training_loss: tensor(0.2574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2454/4339]: training_loss: tensor(0.1744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2455/4339]: training_loss: tensor(0.3974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2456/4339]: training_loss: tensor(0.2398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2457/4339]: training_loss: tensor(0.3396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2458/4339]: training_loss: tensor(0.4092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2459/4339]: training_loss: tensor(0.2669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2460/4339]: training_loss: tensor(0.1485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2461/4339]: training_loss: tensor(0.2032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2462/4339]: training_loss: tensor(0.1217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2463/4339]: training_loss: tensor(0.2322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2464/4339]: training_loss: tensor(0.2732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2465/4339]: training_loss: tensor(0.1344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2466/4339]: training_loss: tensor(0.2595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2467/4339]: training_loss: tensor(0.5985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2468/4339]: training_loss: tensor(0.2668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2469/4339]: training_loss: tensor(0.2810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2470/4339]: training_loss: tensor(0.6839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2471/4339]: training_loss: tensor(0.3408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2472/4339]: training_loss: tensor(0.1263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2473/4339]: training_loss: tensor(0.2160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2474/4339]: training_loss: tensor(0.3516, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2475/4339]: training_loss: tensor(0.4266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2476/4339]: training_loss: tensor(0.2630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2477/4339]: training_loss: tensor(0.2332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2478/4339]: training_loss: tensor(0.2716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2479/4339]: training_loss: tensor(0.1362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2480/4339]: training_loss: tensor(0.4741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2481/4339]: training_loss: tensor(0.2537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2482/4339]: training_loss: tensor(0.2624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2483/4339]: training_loss: tensor(0.2532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2484/4339]: training_loss: tensor(0.3434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2485/4339]: training_loss: tensor(0.6190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2486/4339]: training_loss: tensor(0.2327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2487/4339]: training_loss: tensor(0.3047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2488/4339]: training_loss: tensor(0.2584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2489/4339]: training_loss: tensor(0.3902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2490/4339]: training_loss: tensor(0.1929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2491/4339]: training_loss: tensor(0.1496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2492/4339]: training_loss: tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2493/4339]: training_loss: tensor(0.3586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2494/4339]: training_loss: tensor(0.3804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2495/4339]: training_loss: tensor(0.2271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2496/4339]: training_loss: tensor(0.1641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2497/4339]: training_loss: tensor(0.5462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2498/4339]: training_loss: tensor(0.4197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2499/4339]: training_loss: tensor(0.2070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2500/4339]: training_loss: tensor(0.2165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2501/4339]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2502/4339]: training_loss: tensor(0.2066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2503/4339]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2504/4339]: training_loss: tensor(0.1831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2505/4339]: training_loss: tensor(0.2082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2506/4339]: training_loss: tensor(0.1511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2507/4339]: training_loss: tensor(0.2800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2508/4339]: training_loss: tensor(0.2838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2509/4339]: training_loss: tensor(0.1444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2510/4339]: training_loss: tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2511/4339]: training_loss: tensor(0.1238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2512/4339]: training_loss: tensor(0.2413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2513/4339]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2514/4339]: training_loss: tensor(0.1550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2515/4339]: training_loss: tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2516/4339]: training_loss: tensor(0.6121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2517/4339]: training_loss: tensor(0.2101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2518/4339]: training_loss: tensor(0.1539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2519/4339]: training_loss: tensor(0.1763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2520/4339]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2521/4339]: training_loss: tensor(0.2066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2522/4339]: training_loss: tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2523/4339]: training_loss: tensor(0.3638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2524/4339]: training_loss: tensor(0.3007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2525/4339]: training_loss: tensor(0.2571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2526/4339]: training_loss: tensor(0.2070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2527/4339]: training_loss: tensor(0.1691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2528/4339]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2529/4339]: training_loss: tensor(0.1497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2530/4339]: training_loss: tensor(0.5179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2531/4339]: training_loss: tensor(0.3795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2532/4339]: training_loss: tensor(0.6973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2533/4339]: training_loss: tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2534/4339]: training_loss: tensor(0.1545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2535/4339]: training_loss: tensor(0.5535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2536/4339]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2537/4339]: training_loss: tensor(0.2435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2538/4339]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2539/4339]: training_loss: tensor(0.1443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2540/4339]: training_loss: tensor(0.1756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2541/4339]: training_loss: tensor(0.1200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2542/4339]: training_loss: tensor(0.3032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2543/4339]: training_loss: tensor(0.5587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2544/4339]: training_loss: tensor(0.1232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2545/4339]: training_loss: tensor(0.1561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2546/4339]: training_loss: tensor(0.1623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2547/4339]: training_loss: tensor(0.3391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2548/4339]: training_loss: tensor(0.2293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2549/4339]: training_loss: tensor(0.2138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2550/4339]: training_loss: tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2551/4339]: training_loss: tensor(0.4233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2552/4339]: training_loss: tensor(0.1711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2553/4339]: training_loss: tensor(0.3309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2554/4339]: training_loss: tensor(0.5980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2555/4339]: training_loss: tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2556/4339]: training_loss: tensor(0.1442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2557/4339]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2558/4339]: training_loss: tensor(0.4306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2559/4339]: training_loss: tensor(0.3293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2560/4339]: training_loss: tensor(0.2096, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2561/4339]: training_loss: tensor(0.3383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2562/4339]: training_loss: tensor(0.5464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2563/4339]: training_loss: tensor(0.1962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2564/4339]: training_loss: tensor(0.2312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2565/4339]: training_loss: tensor(0.1713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2566/4339]: training_loss: tensor(0.5875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2567/4339]: training_loss: tensor(0.2297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2568/4339]: training_loss: tensor(0.3364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2569/4339]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2570/4339]: training_loss: tensor(0.4805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2571/4339]: training_loss: tensor(0.3099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2572/4339]: training_loss: tensor(0.2477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2573/4339]: training_loss: tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2574/4339]: training_loss: tensor(0.4616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2575/4339]: training_loss: tensor(0.2679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2576/4339]: training_loss: tensor(0.2277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2577/4339]: training_loss: tensor(0.2322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2578/4339]: training_loss: tensor(0.5151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2579/4339]: training_loss: tensor(0.1817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2580/4339]: training_loss: tensor(0.3957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2581/4339]: training_loss: tensor(0.1431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2582/4339]: training_loss: tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2583/4339]: training_loss: tensor(0.2636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2584/4339]: training_loss: tensor(0.1149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2585/4339]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2586/4339]: training_loss: tensor(0.6897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2587/4339]: training_loss: tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2588/4339]: training_loss: tensor(0.1757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2589/4339]: training_loss: tensor(0.5699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2590/4339]: training_loss: tensor(0.2770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2591/4339]: training_loss: tensor(0.1101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2592/4339]: training_loss: tensor(0.3557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2593/4339]: training_loss: tensor(0.1225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2594/4339]: training_loss: tensor(0.2217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2595/4339]: training_loss: tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2596/4339]: training_loss: tensor(0.5160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2597/4339]: training_loss: tensor(0.1288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2598/4339]: training_loss: tensor(0.3212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2599/4339]: training_loss: tensor(0.1106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2600/4339]: training_loss: tensor(0.6972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2601/4339]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2602/4339]: training_loss: tensor(0.5499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2603/4339]: training_loss: tensor(0.4563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2604/4339]: training_loss: tensor(0.1503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2605/4339]: training_loss: tensor(0.3385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2606/4339]: training_loss: tensor(0.2038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2607/4339]: training_loss: tensor(0.4093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2608/4339]: training_loss: tensor(0.4049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2609/4339]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2610/4339]: training_loss: tensor(0.2672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2611/4339]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2612/4339]: training_loss: tensor(0.1742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2613/4339]: training_loss: tensor(0.2450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2614/4339]: training_loss: tensor(0.6079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2615/4339]: training_loss: tensor(0.1285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2616/4339]: training_loss: tensor(0.1167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2617/4339]: training_loss: tensor(0.1807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2618/4339]: training_loss: tensor(0.2588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2619/4339]: training_loss: tensor(0.2125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2620/4339]: training_loss: tensor(0.1339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2621/4339]: training_loss: tensor(0.4119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2622/4339]: training_loss: tensor(0.1347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2623/4339]: training_loss: tensor(0.1192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2624/4339]: training_loss: tensor(0.4414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2625/4339]: training_loss: tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2626/4339]: training_loss: tensor(0.2141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2627/4339]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2628/4339]: training_loss: tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2629/4339]: training_loss: tensor(0.3787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2630/4339]: training_loss: tensor(0.2923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2631/4339]: training_loss: tensor(0.5403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2632/4339]: training_loss: tensor(0.1422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2633/4339]: training_loss: tensor(0.1639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2634/4339]: training_loss: tensor(0.1384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2635/4339]: training_loss: tensor(0.2368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2636/4339]: training_loss: tensor(0.2947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2637/4339]: training_loss: tensor(0.1993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2638/4339]: training_loss: tensor(0.1928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2639/4339]: training_loss: tensor(0.1880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2640/4339]: training_loss: tensor(0.1462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2641/4339]: training_loss: tensor(0.1365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2642/4339]: training_loss: tensor(0.2813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2643/4339]: training_loss: tensor(0.3115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2644/4339]: training_loss: tensor(0.3189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2645/4339]: training_loss: tensor(0.0507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2646/4339]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2647/4339]: training_loss: tensor(0.1743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2648/4339]: training_loss: tensor(0.4263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2649/4339]: training_loss: tensor(0.2621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2650/4339]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2651/4339]: training_loss: tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2652/4339]: training_loss: tensor(0.2215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2653/4339]: training_loss: tensor(0.2035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2654/4339]: training_loss: tensor(0.0507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2655/4339]: training_loss: tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2656/4339]: training_loss: tensor(0.1276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2657/4339]: training_loss: tensor(0.1746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2658/4339]: training_loss: tensor(0.1466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2659/4339]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2660/4339]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2661/4339]: training_loss: tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2662/4339]: training_loss: tensor(0.1358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2663/4339]: training_loss: tensor(0.2437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2664/4339]: training_loss: tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2665/4339]: training_loss: tensor(0.4152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2666/4339]: training_loss: tensor(0.0429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2667/4339]: training_loss: tensor(0.3987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2668/4339]: training_loss: tensor(0.3292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2669/4339]: training_loss: tensor(0.4880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2670/4339]: training_loss: tensor(0.2203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2671/4339]: training_loss: tensor(0.4276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2672/4339]: training_loss: tensor(0.5150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2673/4339]: training_loss: tensor(0.4341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2674/4339]: training_loss: tensor(0.4248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2675/4339]: training_loss: tensor(0.2789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2676/4339]: training_loss: tensor(0.3123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2677/4339]: training_loss: tensor(0.3595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2678/4339]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2679/4339]: training_loss: tensor(0.1592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2680/4339]: training_loss: tensor(0.1178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2681/4339]: training_loss: tensor(0.2452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2682/4339]: training_loss: tensor(0.2749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2683/4339]: training_loss: tensor(0.4937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2684/4339]: training_loss: tensor(0.1638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2685/4339]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2686/4339]: training_loss: tensor(0.1399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2687/4339]: training_loss: tensor(0.2049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2688/4339]: training_loss: tensor(0.2173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2689/4339]: training_loss: tensor(0.1669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2690/4339]: training_loss: tensor(0.3145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2691/4339]: training_loss: tensor(0.1654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2692/4339]: training_loss: tensor(0.1431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2693/4339]: training_loss: tensor(0.4294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2694/4339]: training_loss: tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2695/4339]: training_loss: tensor(0.1349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2696/4339]: training_loss: tensor(0.1306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2697/4339]: training_loss: tensor(0.2305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2698/4339]: training_loss: tensor(0.3680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2699/4339]: training_loss: tensor(0.1823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2700/4339]: training_loss: tensor(0.2563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2701/4339]: training_loss: tensor(0.3979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2702/4339]: training_loss: tensor(0.3769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2703/4339]: training_loss: tensor(0.3726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2704/4339]: training_loss: tensor(0.2128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2705/4339]: training_loss: tensor(0.3522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2706/4339]: training_loss: tensor(0.1866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2707/4339]: training_loss: tensor(0.9863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2708/4339]: training_loss: tensor(0.1220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2709/4339]: training_loss: tensor(0.1771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2710/4339]: training_loss: tensor(0.2895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2711/4339]: training_loss: tensor(0.2941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2712/4339]: training_loss: tensor(0.3306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2713/4339]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2714/4339]: training_loss: tensor(0.2215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2715/4339]: training_loss: tensor(0.1284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2716/4339]: training_loss: tensor(0.2409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2717/4339]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2718/4339]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2719/4339]: training_loss: tensor(0.2716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2720/4339]: training_loss: tensor(0.6736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2721/4339]: training_loss: tensor(0.3016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2722/4339]: training_loss: tensor(0.2143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2723/4339]: training_loss: tensor(0.2016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2724/4339]: training_loss: tensor(0.2021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2725/4339]: training_loss: tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2726/4339]: training_loss: tensor(0.4340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2727/4339]: training_loss: tensor(0.4664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2728/4339]: training_loss: tensor(0.2039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2729/4339]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2730/4339]: training_loss: tensor(0.1872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2731/4339]: training_loss: tensor(0.5072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2732/4339]: training_loss: tensor(0.1561, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2733/4339]: training_loss: tensor(0.5201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2734/4339]: training_loss: tensor(0.5350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2735/4339]: training_loss: tensor(0.4429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2736/4339]: training_loss: tensor(0.5272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2737/4339]: training_loss: tensor(0.3961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2738/4339]: training_loss: tensor(0.2041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2739/4339]: training_loss: tensor(0.2421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2740/4339]: training_loss: tensor(0.3999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2741/4339]: training_loss: tensor(0.3829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2742/4339]: training_loss: tensor(0.3169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2743/4339]: training_loss: tensor(0.6111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2744/4339]: training_loss: tensor(0.1355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2745/4339]: training_loss: tensor(0.2053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2746/4339]: training_loss: tensor(0.2348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2747/4339]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2748/4339]: training_loss: tensor(0.1942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2749/4339]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2750/4339]: training_loss: tensor(0.4892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2751/4339]: training_loss: tensor(0.2552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2752/4339]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2753/4339]: training_loss: tensor(0.1375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2754/4339]: training_loss: tensor(0.4584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2755/4339]: training_loss: tensor(0.1756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2756/4339]: training_loss: tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2757/4339]: training_loss: tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2758/4339]: training_loss: tensor(0.2295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2759/4339]: training_loss: tensor(0.4277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2760/4339]: training_loss: tensor(0.2396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2761/4339]: training_loss: tensor(0.2321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2762/4339]: training_loss: tensor(0.1578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2763/4339]: training_loss: tensor(0.2637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2764/4339]: training_loss: tensor(0.2393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2765/4339]: training_loss: tensor(0.1738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2766/4339]: training_loss: tensor(0.1808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2767/4339]: training_loss: tensor(0.2402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2768/4339]: training_loss: tensor(0.4965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2769/4339]: training_loss: tensor(0.2539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2770/4339]: training_loss: tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2771/4339]: training_loss: tensor(0.3282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2772/4339]: training_loss: tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2773/4339]: training_loss: tensor(0.2421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2774/4339]: training_loss: tensor(0.3664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2775/4339]: training_loss: tensor(0.3285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2776/4339]: training_loss: tensor(0.4538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2777/4339]: training_loss: tensor(0.4305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2778/4339]: training_loss: tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2779/4339]: training_loss: tensor(0.5630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2780/4339]: training_loss: tensor(0.2648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2781/4339]: training_loss: tensor(0.3131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2782/4339]: training_loss: tensor(0.3927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2783/4339]: training_loss: tensor(0.2849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2784/4339]: training_loss: tensor(0.0505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2785/4339]: training_loss: tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2786/4339]: training_loss: tensor(0.3349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2787/4339]: training_loss: tensor(0.2343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2788/4339]: training_loss: tensor(0.2567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2789/4339]: training_loss: tensor(0.1876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2790/4339]: training_loss: tensor(0.1302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2791/4339]: training_loss: tensor(0.2960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2792/4339]: training_loss: tensor(0.1962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2793/4339]: training_loss: tensor(0.4314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2794/4339]: training_loss: tensor(0.3138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2795/4339]: training_loss: tensor(0.2541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2796/4339]: training_loss: tensor(0.1781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2797/4339]: training_loss: tensor(0.4871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2798/4339]: training_loss: tensor(0.4766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2799/4339]: training_loss: tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2800/4339]: training_loss: tensor(0.1351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2801/4339]: training_loss: tensor(0.6812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2802/4339]: training_loss: tensor(0.3464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2803/4339]: training_loss: tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2804/4339]: training_loss: tensor(0.4064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2805/4339]: training_loss: tensor(0.2147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2806/4339]: training_loss: tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2807/4339]: training_loss: tensor(0.3082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2808/4339]: training_loss: tensor(0.3938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2809/4339]: training_loss: tensor(0.1341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2810/4339]: training_loss: tensor(0.9086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2811/4339]: training_loss: tensor(0.1783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2812/4339]: training_loss: tensor(0.5056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2813/4339]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2814/4339]: training_loss: tensor(0.1692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2815/4339]: training_loss: tensor(0.2936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2816/4339]: training_loss: tensor(0.2028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2817/4339]: training_loss: tensor(0.1630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2818/4339]: training_loss: tensor(0.3595, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2819/4339]: training_loss: tensor(0.3071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2820/4339]: training_loss: tensor(0.2164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2821/4339]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2822/4339]: training_loss: tensor(0.1574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2823/4339]: training_loss: tensor(0.2652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2824/4339]: training_loss: tensor(0.1574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2825/4339]: training_loss: tensor(0.4206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2826/4339]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2827/4339]: training_loss: tensor(0.4421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2828/4339]: training_loss: tensor(0.3582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2829/4339]: training_loss: tensor(0.6186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2830/4339]: training_loss: tensor(0.2498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2831/4339]: training_loss: tensor(0.1883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2832/4339]: training_loss: tensor(0.3082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2833/4339]: training_loss: tensor(0.1334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2834/4339]: training_loss: tensor(0.1321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2835/4339]: training_loss: tensor(0.5026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2836/4339]: training_loss: tensor(0.4290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2837/4339]: training_loss: tensor(0.2502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2838/4339]: training_loss: tensor(0.2244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2839/4339]: training_loss: tensor(0.1842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2840/4339]: training_loss: tensor(0.3318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2841/4339]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2842/4339]: training_loss: tensor(0.2904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2843/4339]: training_loss: tensor(0.2914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2844/4339]: training_loss: tensor(0.5228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2845/4339]: training_loss: tensor(0.3326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2846/4339]: training_loss: tensor(0.2437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2847/4339]: training_loss: tensor(0.2454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2848/4339]: training_loss: tensor(0.2885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2849/4339]: training_loss: tensor(0.2855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2850/4339]: training_loss: tensor(0.2396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2851/4339]: training_loss: tensor(0.1506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2852/4339]: training_loss: tensor(0.3456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2853/4339]: training_loss: tensor(0.1417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2854/4339]: training_loss: tensor(0.6346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2855/4339]: training_loss: tensor(0.1569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2856/4339]: training_loss: tensor(0.2981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2857/4339]: training_loss: tensor(0.5546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2858/4339]: training_loss: tensor(0.2127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2859/4339]: training_loss: tensor(0.3863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2860/4339]: training_loss: tensor(0.1899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2861/4339]: training_loss: tensor(0.1343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2862/4339]: training_loss: tensor(0.5661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2863/4339]: training_loss: tensor(0.2543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2864/4339]: training_loss: tensor(0.5431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2865/4339]: training_loss: tensor(0.1698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2866/4339]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2867/4339]: training_loss: tensor(0.3298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2868/4339]: training_loss: tensor(0.3084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2869/4339]: training_loss: tensor(0.4490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2870/4339]: training_loss: tensor(0.2283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2871/4339]: training_loss: tensor(0.4676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2872/4339]: training_loss: tensor(0.6582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2873/4339]: training_loss: tensor(0.2183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2874/4339]: training_loss: tensor(0.1876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2875/4339]: training_loss: tensor(0.2157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2876/4339]: training_loss: tensor(0.2228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2877/4339]: training_loss: tensor(0.2527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2878/4339]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2879/4339]: training_loss: tensor(0.2378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2880/4339]: training_loss: tensor(0.1570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2881/4339]: training_loss: tensor(0.1653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2882/4339]: training_loss: tensor(0.3216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2883/4339]: training_loss: tensor(0.1455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2884/4339]: training_loss: tensor(0.4119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2885/4339]: training_loss: tensor(0.3051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2886/4339]: training_loss: tensor(0.2621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2887/4339]: training_loss: tensor(0.2575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2888/4339]: training_loss: tensor(0.5710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2889/4339]: training_loss: tensor(0.1629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2890/4339]: training_loss: tensor(0.3984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2891/4339]: training_loss: tensor(0.2923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2892/4339]: training_loss: tensor(0.6835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2893/4339]: training_loss: tensor(0.5224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2894/4339]: training_loss: tensor(0.2940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2895/4339]: training_loss: tensor(0.3302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2896/4339]: training_loss: tensor(0.5567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2897/4339]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2898/4339]: training_loss: tensor(0.2438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2899/4339]: training_loss: tensor(0.4133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2900/4339]: training_loss: tensor(0.3699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2901/4339]: training_loss: tensor(0.2681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2902/4339]: training_loss: tensor(0.2711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2903/4339]: training_loss: tensor(0.3221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2904/4339]: training_loss: tensor(0.3351, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2905/4339]: training_loss: tensor(0.4081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2906/4339]: training_loss: tensor(0.2323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2907/4339]: training_loss: tensor(0.2431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2908/4339]: training_loss: tensor(0.2772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2909/4339]: training_loss: tensor(0.3086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2910/4339]: training_loss: tensor(0.2466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2911/4339]: training_loss: tensor(0.1621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2912/4339]: training_loss: tensor(0.3791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2913/4339]: training_loss: tensor(0.4442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2914/4339]: training_loss: tensor(0.1302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2915/4339]: training_loss: tensor(0.4168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2916/4339]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2917/4339]: training_loss: tensor(0.3049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2918/4339]: training_loss: tensor(0.1382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2919/4339]: training_loss: tensor(0.1729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2920/4339]: training_loss: tensor(0.1134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2921/4339]: training_loss: tensor(0.2011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2922/4339]: training_loss: tensor(0.4002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2923/4339]: training_loss: tensor(0.1619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2924/4339]: training_loss: tensor(0.4316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2925/4339]: training_loss: tensor(0.6217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2926/4339]: training_loss: tensor(0.2393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2927/4339]: training_loss: tensor(0.2242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2928/4339]: training_loss: tensor(0.1410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2929/4339]: training_loss: tensor(0.2152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2930/4339]: training_loss: tensor(0.2071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2931/4339]: training_loss: tensor(0.4228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2932/4339]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2933/4339]: training_loss: tensor(0.1661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2934/4339]: training_loss: tensor(0.1421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2935/4339]: training_loss: tensor(0.5813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2936/4339]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2937/4339]: training_loss: tensor(0.2222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2938/4339]: training_loss: tensor(0.5334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2939/4339]: training_loss: tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2940/4339]: training_loss: tensor(0.3615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2941/4339]: training_loss: tensor(0.4732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2942/4339]: training_loss: tensor(0.2327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2943/4339]: training_loss: tensor(0.1605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2944/4339]: training_loss: tensor(0.2980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2945/4339]: training_loss: tensor(0.1724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2946/4339]: training_loss: tensor(0.3781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2947/4339]: training_loss: tensor(0.1872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2948/4339]: training_loss: tensor(0.2933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2949/4339]: training_loss: tensor(0.2908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2950/4339]: training_loss: tensor(0.1335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2951/4339]: training_loss: tensor(0.1847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2952/4339]: training_loss: tensor(0.5763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2953/4339]: training_loss: tensor(0.3275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2954/4339]: training_loss: tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2955/4339]: training_loss: tensor(0.3916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2956/4339]: training_loss: tensor(0.2535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2957/4339]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2958/4339]: training_loss: tensor(0.3226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2959/4339]: training_loss: tensor(0.4679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2960/4339]: training_loss: tensor(0.3109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2961/4339]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2962/4339]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2963/4339]: training_loss: tensor(0.1774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2964/4339]: training_loss: tensor(0.1435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2965/4339]: training_loss: tensor(0.1785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2966/4339]: training_loss: tensor(0.3140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2967/4339]: training_loss: tensor(0.0666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2968/4339]: training_loss: tensor(0.2096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2969/4339]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2970/4339]: training_loss: tensor(0.2800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2971/4339]: training_loss: tensor(0.2571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2972/4339]: training_loss: tensor(0.2274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2973/4339]: training_loss: tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2974/4339]: training_loss: tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2975/4339]: training_loss: tensor(0.4510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2976/4339]: training_loss: tensor(0.1627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2977/4339]: training_loss: tensor(0.1447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2978/4339]: training_loss: tensor(0.1946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2979/4339]: training_loss: tensor(0.3053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2980/4339]: training_loss: tensor(0.3435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2981/4339]: training_loss: tensor(0.3084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2982/4339]: training_loss: tensor(0.2036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2983/4339]: training_loss: tensor(0.6197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2984/4339]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2985/4339]: training_loss: tensor(0.1219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2986/4339]: training_loss: tensor(0.1790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2987/4339]: training_loss: tensor(0.4316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2988/4339]: training_loss: tensor(0.0573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2989/4339]: training_loss: tensor(0.1593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2990/4339]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2991/4339]: training_loss: tensor(0.1767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2992/4339]: training_loss: tensor(0.3747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2993/4339]: training_loss: tensor(0.1669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2994/4339]: training_loss: tensor(0.2279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2995/4339]: training_loss: tensor(0.3495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2996/4339]: training_loss: tensor(0.3757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2997/4339]: training_loss: tensor(0.1955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2998/4339]: training_loss: tensor(0.1618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2999/4339]: training_loss: tensor(0.1217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3000/4339]: training_loss: tensor(0.2698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3001/4339]: training_loss: tensor(0.1821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3002/4339]: training_loss: tensor(0.5212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3003/4339]: training_loss: tensor(0.2704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3004/4339]: training_loss: tensor(0.1839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3005/4339]: training_loss: tensor(0.1914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3006/4339]: training_loss: tensor(0.2078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3007/4339]: training_loss: tensor(0.1834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3008/4339]: training_loss: tensor(0.1233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3009/4339]: training_loss: tensor(0.3609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3010/4339]: training_loss: tensor(0.4474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3011/4339]: training_loss: tensor(0.5279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3012/4339]: training_loss: tensor(0.2075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3013/4339]: training_loss: tensor(0.3966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3014/4339]: training_loss: tensor(0.3990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3015/4339]: training_loss: tensor(0.2317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3016/4339]: training_loss: tensor(0.3243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3017/4339]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3018/4339]: training_loss: tensor(0.6741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3019/4339]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3020/4339]: training_loss: tensor(0.5122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3021/4339]: training_loss: tensor(0.6521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3022/4339]: training_loss: tensor(0.4867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3023/4339]: training_loss: tensor(0.5010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3024/4339]: training_loss: tensor(0.1573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3025/4339]: training_loss: tensor(0.3313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3026/4339]: training_loss: tensor(0.1295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3027/4339]: training_loss: tensor(0.1446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3028/4339]: training_loss: tensor(0.5267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3029/4339]: training_loss: tensor(0.2509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3030/4339]: training_loss: tensor(0.1309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3031/4339]: training_loss: tensor(0.1615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3032/4339]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3033/4339]: training_loss: tensor(0.1520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3034/4339]: training_loss: tensor(0.3446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3035/4339]: training_loss: tensor(0.2833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3036/4339]: training_loss: tensor(0.3322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3037/4339]: training_loss: tensor(0.1605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3038/4339]: training_loss: tensor(0.2442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3039/4339]: training_loss: tensor(0.3450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3040/4339]: training_loss: tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3041/4339]: training_loss: tensor(0.3000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3042/4339]: training_loss: tensor(0.1294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3043/4339]: training_loss: tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3044/4339]: training_loss: tensor(0.1661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3045/4339]: training_loss: tensor(0.3591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3046/4339]: training_loss: tensor(0.4515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3047/4339]: training_loss: tensor(0.1811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3048/4339]: training_loss: tensor(0.1176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3049/4339]: training_loss: tensor(0.5931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3050/4339]: training_loss: tensor(0.7389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3051/4339]: training_loss: tensor(0.4912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3052/4339]: training_loss: tensor(0.2392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3053/4339]: training_loss: tensor(0.4032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3054/4339]: training_loss: tensor(0.5878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3055/4339]: training_loss: tensor(0.3848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3056/4339]: training_loss: tensor(0.1449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3057/4339]: training_loss: tensor(0.2629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3058/4339]: training_loss: tensor(0.1520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3059/4339]: training_loss: tensor(0.2410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3060/4339]: training_loss: tensor(0.1261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3061/4339]: training_loss: tensor(0.5933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3062/4339]: training_loss: tensor(0.1576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3063/4339]: training_loss: tensor(0.3002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3064/4339]: training_loss: tensor(0.2399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3065/4339]: training_loss: tensor(0.1673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3066/4339]: training_loss: tensor(0.2490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3067/4339]: training_loss: tensor(0.2993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3068/4339]: training_loss: tensor(0.1668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3069/4339]: training_loss: tensor(0.1278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3070/4339]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3071/4339]: training_loss: tensor(0.2413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3072/4339]: training_loss: tensor(0.2973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3073/4339]: training_loss: tensor(0.2098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3074/4339]: training_loss: tensor(0.4760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3075/4339]: training_loss: tensor(0.2303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3076/4339]: training_loss: tensor(0.2443, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3077/4339]: training_loss: tensor(0.1424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3078/4339]: training_loss: tensor(0.2035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3079/4339]: training_loss: tensor(0.2311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3080/4339]: training_loss: tensor(0.2405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3081/4339]: training_loss: tensor(0.1618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3082/4339]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3083/4339]: training_loss: tensor(0.1559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3084/4339]: training_loss: tensor(0.1467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3085/4339]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3086/4339]: training_loss: tensor(0.3526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3087/4339]: training_loss: tensor(0.5789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3088/4339]: training_loss: tensor(0.3039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3089/4339]: training_loss: tensor(0.7603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3090/4339]: training_loss: tensor(0.2148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3091/4339]: training_loss: tensor(0.4477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3092/4339]: training_loss: tensor(0.2490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3093/4339]: training_loss: tensor(0.2393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3094/4339]: training_loss: tensor(0.3482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3095/4339]: training_loss: tensor(0.2325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3096/4339]: training_loss: tensor(0.2697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3097/4339]: training_loss: tensor(0.3467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3098/4339]: training_loss: tensor(0.1407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3099/4339]: training_loss: tensor(0.4360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3100/4339]: training_loss: tensor(0.2481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3101/4339]: training_loss: tensor(0.2362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3102/4339]: training_loss: tensor(0.1741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3103/4339]: training_loss: tensor(0.4773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3104/4339]: training_loss: tensor(0.2027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3105/4339]: training_loss: tensor(0.1760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3106/4339]: training_loss: tensor(0.2032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3107/4339]: training_loss: tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3108/4339]: training_loss: tensor(0.2782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3109/4339]: training_loss: tensor(0.1214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3110/4339]: training_loss: tensor(0.4943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3111/4339]: training_loss: tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3112/4339]: training_loss: tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3113/4339]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3114/4339]: training_loss: tensor(0.2121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3115/4339]: training_loss: tensor(0.4862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3116/4339]: training_loss: tensor(0.3214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3117/4339]: training_loss: tensor(0.2459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3118/4339]: training_loss: tensor(0.2999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3119/4339]: training_loss: tensor(0.3470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3120/4339]: training_loss: tensor(0.3004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3121/4339]: training_loss: tensor(0.2546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3122/4339]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3123/4339]: training_loss: tensor(0.5418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3124/4339]: training_loss: tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3125/4339]: training_loss: tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3126/4339]: training_loss: tensor(0.2610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3127/4339]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3128/4339]: training_loss: tensor(0.2258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3129/4339]: training_loss: tensor(0.4635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3130/4339]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3131/4339]: training_loss: tensor(0.6861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3132/4339]: training_loss: tensor(0.2773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3133/4339]: training_loss: tensor(0.3546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3134/4339]: training_loss: tensor(0.3275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3135/4339]: training_loss: tensor(0.5846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3136/4339]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3137/4339]: training_loss: tensor(0.3781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3138/4339]: training_loss: tensor(0.1493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3139/4339]: training_loss: tensor(0.2444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3140/4339]: training_loss: tensor(0.1902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3141/4339]: training_loss: tensor(0.2409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3142/4339]: training_loss: tensor(0.1796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3143/4339]: training_loss: tensor(0.5174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3144/4339]: training_loss: tensor(0.2467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3145/4339]: training_loss: tensor(0.3556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3146/4339]: training_loss: tensor(0.2304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3147/4339]: training_loss: tensor(0.4847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3148/4339]: training_loss: tensor(0.1640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3149/4339]: training_loss: tensor(0.3951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3150/4339]: training_loss: tensor(0.3284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3151/4339]: training_loss: tensor(0.3906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3152/4339]: training_loss: tensor(0.1542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3153/4339]: training_loss: tensor(0.3255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3154/4339]: training_loss: tensor(0.4365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3155/4339]: training_loss: tensor(0.1712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3156/4339]: training_loss: tensor(0.3370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3157/4339]: training_loss: tensor(0.2584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3158/4339]: training_loss: tensor(0.3372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3159/4339]: training_loss: tensor(0.1340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3160/4339]: training_loss: tensor(0.2858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3161/4339]: training_loss: tensor(0.2269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3162/4339]: training_loss: tensor(0.2502, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3163/4339]: training_loss: tensor(0.4025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3164/4339]: training_loss: tensor(0.3798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3165/4339]: training_loss: tensor(0.2880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3166/4339]: training_loss: tensor(0.2405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3167/4339]: training_loss: tensor(0.4682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3168/4339]: training_loss: tensor(0.1506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3169/4339]: training_loss: tensor(0.5937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3170/4339]: training_loss: tensor(0.2704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3171/4339]: training_loss: tensor(0.1164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3172/4339]: training_loss: tensor(0.2859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3173/4339]: training_loss: tensor(0.1757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3174/4339]: training_loss: tensor(0.1367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3175/4339]: training_loss: tensor(0.1479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3176/4339]: training_loss: tensor(0.2148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3177/4339]: training_loss: tensor(0.2999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3178/4339]: training_loss: tensor(0.4427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3179/4339]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3180/4339]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3181/4339]: training_loss: tensor(0.1097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3182/4339]: training_loss: tensor(0.2201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3183/4339]: training_loss: tensor(0.2284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3184/4339]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3185/4339]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3186/4339]: training_loss: tensor(0.6074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3187/4339]: training_loss: tensor(0.1189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3188/4339]: training_loss: tensor(0.2336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3189/4339]: training_loss: tensor(0.1901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3190/4339]: training_loss: tensor(0.1439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3191/4339]: training_loss: tensor(0.3508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3192/4339]: training_loss: tensor(0.2061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3193/4339]: training_loss: tensor(0.2251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3194/4339]: training_loss: tensor(0.1239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3195/4339]: training_loss: tensor(0.5495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3196/4339]: training_loss: tensor(0.1223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3197/4339]: training_loss: tensor(0.2135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3198/4339]: training_loss: tensor(0.1216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3199/4339]: training_loss: tensor(0.2263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3200/4339]: training_loss: tensor(0.3060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3201/4339]: training_loss: tensor(0.1389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3202/4339]: training_loss: tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3203/4339]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3204/4339]: training_loss: tensor(0.5355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3205/4339]: training_loss: tensor(0.2320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3206/4339]: training_loss: tensor(0.1900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3207/4339]: training_loss: tensor(0.7220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3208/4339]: training_loss: tensor(0.4662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3209/4339]: training_loss: tensor(0.2023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3210/4339]: training_loss: tensor(0.1147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3211/4339]: training_loss: tensor(0.3447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3212/4339]: training_loss: tensor(0.4326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3213/4339]: training_loss: tensor(0.1688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3214/4339]: training_loss: tensor(0.3534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3215/4339]: training_loss: tensor(0.3601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3216/4339]: training_loss: tensor(0.2463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3217/4339]: training_loss: tensor(0.3721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3218/4339]: training_loss: tensor(0.2702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3219/4339]: training_loss: tensor(0.2976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3220/4339]: training_loss: tensor(0.1856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3221/4339]: training_loss: tensor(0.2545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3222/4339]: training_loss: tensor(0.3358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3223/4339]: training_loss: tensor(0.3380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3224/4339]: training_loss: tensor(0.1890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3225/4339]: training_loss: tensor(0.2779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3226/4339]: training_loss: tensor(0.1840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3227/4339]: training_loss: tensor(0.1788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3228/4339]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3229/4339]: training_loss: tensor(0.3284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3230/4339]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3231/4339]: training_loss: tensor(0.1064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3232/4339]: training_loss: tensor(0.4094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3233/4339]: training_loss: tensor(0.6052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3234/4339]: training_loss: tensor(0.1966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3235/4339]: training_loss: tensor(0.1978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3236/4339]: training_loss: tensor(0.4471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3237/4339]: training_loss: tensor(0.1165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3238/4339]: training_loss: tensor(0.3279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3239/4339]: training_loss: tensor(0.1387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3240/4339]: training_loss: tensor(0.3397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3241/4339]: training_loss: tensor(0.4571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3242/4339]: training_loss: tensor(0.3416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3243/4339]: training_loss: tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3244/4339]: training_loss: tensor(0.8796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3245/4339]: training_loss: tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3246/4339]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3247/4339]: training_loss: tensor(0.3748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3248/4339]: training_loss: tensor(0.1121, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3249/4339]: training_loss: tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3250/4339]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3251/4339]: training_loss: tensor(0.1775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3252/4339]: training_loss: tensor(0.2876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3253/4339]: training_loss: tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3254/4339]: training_loss: tensor(0.1618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3255/4339]: training_loss: tensor(0.3326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3256/4339]: training_loss: tensor(0.1888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3257/4339]: training_loss: tensor(0.1312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3258/4339]: training_loss: tensor(0.5895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3259/4339]: training_loss: tensor(0.1935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3260/4339]: training_loss: tensor(0.5560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3261/4339]: training_loss: tensor(0.1666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3262/4339]: training_loss: tensor(0.2515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3263/4339]: training_loss: tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3264/4339]: training_loss: tensor(0.3442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3265/4339]: training_loss: tensor(0.1817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3266/4339]: training_loss: tensor(0.1745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3267/4339]: training_loss: tensor(0.1852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3268/4339]: training_loss: tensor(0.0676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3269/4339]: training_loss: tensor(0.1437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3270/4339]: training_loss: tensor(0.2109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3271/4339]: training_loss: tensor(0.1563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3272/4339]: training_loss: tensor(0.2493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3273/4339]: training_loss: tensor(0.3086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3274/4339]: training_loss: tensor(0.2816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3275/4339]: training_loss: tensor(0.6159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3276/4339]: training_loss: tensor(0.5055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3277/4339]: training_loss: tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3278/4339]: training_loss: tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3279/4339]: training_loss: tensor(0.4916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3280/4339]: training_loss: tensor(0.2585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3281/4339]: training_loss: tensor(0.1726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3282/4339]: training_loss: tensor(0.1814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3283/4339]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3284/4339]: training_loss: tensor(0.3674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3285/4339]: training_loss: tensor(0.2829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3286/4339]: training_loss: tensor(0.2503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3287/4339]: training_loss: tensor(0.1160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3288/4339]: training_loss: tensor(0.3659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3289/4339]: training_loss: tensor(0.1671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3290/4339]: training_loss: tensor(0.2325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3291/4339]: training_loss: tensor(0.1362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3292/4339]: training_loss: tensor(0.2397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3293/4339]: training_loss: tensor(0.3169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3294/4339]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3295/4339]: training_loss: tensor(0.2014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3296/4339]: training_loss: tensor(0.1724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3297/4339]: training_loss: tensor(0.1008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3298/4339]: training_loss: tensor(0.1902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3299/4339]: training_loss: tensor(0.3685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3300/4339]: training_loss: tensor(0.3382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3301/4339]: training_loss: tensor(0.1206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3302/4339]: training_loss: tensor(0.1838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3303/4339]: training_loss: tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3304/4339]: training_loss: tensor(0.3047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3305/4339]: training_loss: tensor(0.3595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3306/4339]: training_loss: tensor(0.2478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3307/4339]: training_loss: tensor(0.4668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3308/4339]: training_loss: tensor(0.1632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3309/4339]: training_loss: tensor(0.2773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3310/4339]: training_loss: tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3311/4339]: training_loss: tensor(0.2340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3312/4339]: training_loss: tensor(0.3375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3313/4339]: training_loss: tensor(0.1701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3314/4339]: training_loss: tensor(0.5436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3315/4339]: training_loss: tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3316/4339]: training_loss: tensor(0.5468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3317/4339]: training_loss: tensor(0.6268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3318/4339]: training_loss: tensor(0.3931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3319/4339]: training_loss: tensor(0.1518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3320/4339]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3321/4339]: training_loss: tensor(0.6380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3322/4339]: training_loss: tensor(0.1933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3323/4339]: training_loss: tensor(0.4159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3324/4339]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3325/4339]: training_loss: tensor(0.2627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3326/4339]: training_loss: tensor(0.5238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3327/4339]: training_loss: tensor(0.2158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3328/4339]: training_loss: tensor(0.3314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3329/4339]: training_loss: tensor(0.5813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3330/4339]: training_loss: tensor(0.1502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3331/4339]: training_loss: tensor(0.1476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3332/4339]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3333/4339]: training_loss: tensor(0.1433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3334/4339]: training_loss: tensor(0.3905, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3335/4339]: training_loss: tensor(0.3753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3336/4339]: training_loss: tensor(0.2051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3337/4339]: training_loss: tensor(0.2016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3338/4339]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3339/4339]: training_loss: tensor(0.3122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3340/4339]: training_loss: tensor(0.2606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3341/4339]: training_loss: tensor(0.1899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3342/4339]: training_loss: tensor(0.1474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3343/4339]: training_loss: tensor(0.1701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3344/4339]: training_loss: tensor(0.1242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3345/4339]: training_loss: tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3346/4339]: training_loss: tensor(0.4051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3347/4339]: training_loss: tensor(0.2383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3348/4339]: training_loss: tensor(0.2125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3349/4339]: training_loss: tensor(0.2321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3350/4339]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3351/4339]: training_loss: tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3352/4339]: training_loss: tensor(0.7371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3353/4339]: training_loss: tensor(0.3768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3354/4339]: training_loss: tensor(0.1693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3355/4339]: training_loss: tensor(0.2971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3356/4339]: training_loss: tensor(0.3233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3357/4339]: training_loss: tensor(0.2655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3358/4339]: training_loss: tensor(0.3616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3359/4339]: training_loss: tensor(0.2325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3360/4339]: training_loss: tensor(0.2742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3361/4339]: training_loss: tensor(0.2408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3362/4339]: training_loss: tensor(0.1601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3363/4339]: training_loss: tensor(0.2474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3364/4339]: training_loss: tensor(0.2557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3365/4339]: training_loss: tensor(0.3007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3366/4339]: training_loss: tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3367/4339]: training_loss: tensor(0.1638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3368/4339]: training_loss: tensor(0.2468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3369/4339]: training_loss: tensor(0.1533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3370/4339]: training_loss: tensor(0.2207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3371/4339]: training_loss: tensor(0.2596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3372/4339]: training_loss: tensor(0.4815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3373/4339]: training_loss: tensor(0.2687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3374/4339]: training_loss: tensor(0.1346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3375/4339]: training_loss: tensor(0.1640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3376/4339]: training_loss: tensor(0.1725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3377/4339]: training_loss: tensor(0.1574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3378/4339]: training_loss: tensor(0.3199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3379/4339]: training_loss: tensor(0.3252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3380/4339]: training_loss: tensor(0.1924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3381/4339]: training_loss: tensor(0.3258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3382/4339]: training_loss: tensor(0.3480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3383/4339]: training_loss: tensor(0.1446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3384/4339]: training_loss: tensor(0.3683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3385/4339]: training_loss: tensor(0.2555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3386/4339]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3387/4339]: training_loss: tensor(0.2898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3388/4339]: training_loss: tensor(0.2227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3389/4339]: training_loss: tensor(0.1884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3390/4339]: training_loss: tensor(0.2145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3391/4339]: training_loss: tensor(0.1321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3392/4339]: training_loss: tensor(0.5989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3393/4339]: training_loss: tensor(0.1822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3394/4339]: training_loss: tensor(0.2110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3395/4339]: training_loss: tensor(0.3090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3396/4339]: training_loss: tensor(0.1967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3397/4339]: training_loss: tensor(0.3772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3398/4339]: training_loss: tensor(0.2271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3399/4339]: training_loss: tensor(0.1422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3400/4339]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3401/4339]: training_loss: tensor(0.1683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3402/4339]: training_loss: tensor(0.3837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3403/4339]: training_loss: tensor(0.3536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3404/4339]: training_loss: tensor(0.1870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3405/4339]: training_loss: tensor(0.1867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3406/4339]: training_loss: tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3407/4339]: training_loss: tensor(0.2599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3408/4339]: training_loss: tensor(0.3180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3409/4339]: training_loss: tensor(0.1712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3410/4339]: training_loss: tensor(0.4743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3411/4339]: training_loss: tensor(0.1965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3412/4339]: training_loss: tensor(0.4363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3413/4339]: training_loss: tensor(0.4536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3414/4339]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3415/4339]: training_loss: tensor(0.2875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3416/4339]: training_loss: tensor(0.1759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3417/4339]: training_loss: tensor(0.1981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3418/4339]: training_loss: tensor(0.2597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3419/4339]: training_loss: tensor(0.1559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3420/4339]: training_loss: tensor(0.2084, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3421/4339]: training_loss: tensor(0.2079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3422/4339]: training_loss: tensor(1.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3423/4339]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3424/4339]: training_loss: tensor(0.1144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3425/4339]: training_loss: tensor(0.3324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3426/4339]: training_loss: tensor(0.1816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3427/4339]: training_loss: tensor(0.2947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3428/4339]: training_loss: tensor(0.4074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3429/4339]: training_loss: tensor(0.3471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3430/4339]: training_loss: tensor(0.2719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3431/4339]: training_loss: tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3432/4339]: training_loss: tensor(0.2172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3433/4339]: training_loss: tensor(0.2164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3434/4339]: training_loss: tensor(0.1615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3435/4339]: training_loss: tensor(0.3695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3436/4339]: training_loss: tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3437/4339]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3438/4339]: training_loss: tensor(0.4834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3439/4339]: training_loss: tensor(0.2347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3440/4339]: training_loss: tensor(0.2724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3441/4339]: training_loss: tensor(0.2611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3442/4339]: training_loss: tensor(0.2219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3443/4339]: training_loss: tensor(0.4073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3444/4339]: training_loss: tensor(0.2086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3445/4339]: training_loss: tensor(0.2767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3446/4339]: training_loss: tensor(0.2510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3447/4339]: training_loss: tensor(0.2604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3448/4339]: training_loss: tensor(0.3149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3449/4339]: training_loss: tensor(0.2107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3450/4339]: training_loss: tensor(0.1454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3451/4339]: training_loss: tensor(0.1596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3452/4339]: training_loss: tensor(0.2341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3453/4339]: training_loss: tensor(0.2701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3454/4339]: training_loss: tensor(0.4296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3455/4339]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3456/4339]: training_loss: tensor(0.1680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3457/4339]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3458/4339]: training_loss: tensor(0.3897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3459/4339]: training_loss: tensor(0.1644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3460/4339]: training_loss: tensor(0.1599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3461/4339]: training_loss: tensor(0.2701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3462/4339]: training_loss: tensor(0.2122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3463/4339]: training_loss: tensor(0.1803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3464/4339]: training_loss: tensor(0.3785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3465/4339]: training_loss: tensor(0.1903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3466/4339]: training_loss: tensor(0.1618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3467/4339]: training_loss: tensor(0.3935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3468/4339]: training_loss: tensor(0.1961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3469/4339]: training_loss: tensor(0.5426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3470/4339]: training_loss: tensor(0.5133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3471/4339]: training_loss: tensor(0.1825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3472/4339]: training_loss: tensor(0.2994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3473/4339]: training_loss: tensor(0.2002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3474/4339]: training_loss: tensor(0.2722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3475/4339]: training_loss: tensor(0.3089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3476/4339]: training_loss: tensor(0.2040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3477/4339]: training_loss: tensor(0.3079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3478/4339]: training_loss: tensor(0.2745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3479/4339]: training_loss: tensor(0.1411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3480/4339]: training_loss: tensor(0.2255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3481/4339]: training_loss: tensor(0.1358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3482/4339]: training_loss: tensor(0.1817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3483/4339]: training_loss: tensor(0.1293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3484/4339]: training_loss: tensor(0.1350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3485/4339]: training_loss: tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3486/4339]: training_loss: tensor(0.3283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3487/4339]: training_loss: tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3488/4339]: training_loss: tensor(0.1348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3489/4339]: training_loss: tensor(0.1201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3490/4339]: training_loss: tensor(0.4360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3491/4339]: training_loss: tensor(0.6663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3492/4339]: training_loss: tensor(0.3393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3493/4339]: training_loss: tensor(0.2849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3494/4339]: training_loss: tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3495/4339]: training_loss: tensor(0.1271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3496/4339]: training_loss: tensor(0.2668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3497/4339]: training_loss: tensor(0.4077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3498/4339]: training_loss: tensor(0.1866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3499/4339]: training_loss: tensor(0.2890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3500/4339]: training_loss: tensor(0.1512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3501/4339]: training_loss: tensor(0.2736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3502/4339]: training_loss: tensor(0.3796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3503/4339]: training_loss: tensor(0.2186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3504/4339]: training_loss: tensor(0.3781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3505/4339]: training_loss: tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3506/4339]: training_loss: tensor(0.3096, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3507/4339]: training_loss: tensor(0.3968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3508/4339]: training_loss: tensor(0.2415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3509/4339]: training_loss: tensor(0.3890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3510/4339]: training_loss: tensor(0.4265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3511/4339]: training_loss: tensor(0.1202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3512/4339]: training_loss: tensor(0.5705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3513/4339]: training_loss: tensor(0.1576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3514/4339]: training_loss: tensor(0.1765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3515/4339]: training_loss: tensor(0.3248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3516/4339]: training_loss: tensor(0.4982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3517/4339]: training_loss: tensor(0.2466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3518/4339]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3519/4339]: training_loss: tensor(0.2441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3520/4339]: training_loss: tensor(0.4680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3521/4339]: training_loss: tensor(0.2770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3522/4339]: training_loss: tensor(0.2462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3523/4339]: training_loss: tensor(0.1175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3524/4339]: training_loss: tensor(0.4200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3525/4339]: training_loss: tensor(0.3382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3526/4339]: training_loss: tensor(0.2590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3527/4339]: training_loss: tensor(0.2418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3528/4339]: training_loss: tensor(0.3515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3529/4339]: training_loss: tensor(0.2425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3530/4339]: training_loss: tensor(0.1558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3531/4339]: training_loss: tensor(0.2416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3532/4339]: training_loss: tensor(0.2507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3533/4339]: training_loss: tensor(0.3362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3534/4339]: training_loss: tensor(0.4023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3535/4339]: training_loss: tensor(0.1213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3536/4339]: training_loss: tensor(0.1258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3537/4339]: training_loss: tensor(0.2362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3538/4339]: training_loss: tensor(0.1498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3539/4339]: training_loss: tensor(0.3865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3540/4339]: training_loss: tensor(0.3136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3541/4339]: training_loss: tensor(0.2149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3542/4339]: training_loss: tensor(0.5538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3543/4339]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3544/4339]: training_loss: tensor(0.2594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3545/4339]: training_loss: tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3546/4339]: training_loss: tensor(0.1044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3547/4339]: training_loss: tensor(0.3904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3548/4339]: training_loss: tensor(0.4455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3549/4339]: training_loss: tensor(0.3643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3550/4339]: training_loss: tensor(0.5376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3551/4339]: training_loss: tensor(0.1917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3552/4339]: training_loss: tensor(0.1390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3553/4339]: training_loss: tensor(0.1499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3554/4339]: training_loss: tensor(0.2880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3555/4339]: training_loss: tensor(0.1647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3556/4339]: training_loss: tensor(0.2152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3557/4339]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3558/4339]: training_loss: tensor(0.1120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3559/4339]: training_loss: tensor(0.1019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3560/4339]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3561/4339]: training_loss: tensor(0.1362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3562/4339]: training_loss: tensor(0.4917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3563/4339]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3564/4339]: training_loss: tensor(0.4912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3565/4339]: training_loss: tensor(0.2093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3566/4339]: training_loss: tensor(0.7501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3567/4339]: training_loss: tensor(0.5622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3568/4339]: training_loss: tensor(0.1310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3569/4339]: training_loss: tensor(0.3826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3570/4339]: training_loss: tensor(0.3646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3571/4339]: training_loss: tensor(0.4091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3572/4339]: training_loss: tensor(0.3121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3573/4339]: training_loss: tensor(0.1408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3574/4339]: training_loss: tensor(0.4054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3575/4339]: training_loss: tensor(0.3082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3576/4339]: training_loss: tensor(0.3030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3577/4339]: training_loss: tensor(0.5028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3578/4339]: training_loss: tensor(0.2654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3579/4339]: training_loss: tensor(0.4017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3580/4339]: training_loss: tensor(0.2324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3581/4339]: training_loss: tensor(0.3545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3582/4339]: training_loss: tensor(0.2310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3583/4339]: training_loss: tensor(0.4435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3584/4339]: training_loss: tensor(0.2997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3585/4339]: training_loss: tensor(0.4117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3586/4339]: training_loss: tensor(0.3157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3587/4339]: training_loss: tensor(0.4299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3588/4339]: training_loss: tensor(0.4200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3589/4339]: training_loss: tensor(0.2960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3590/4339]: training_loss: tensor(0.1890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3591/4339]: training_loss: tensor(0.3212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3592/4339]: training_loss: tensor(0.2246, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3593/4339]: training_loss: tensor(0.5219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3594/4339]: training_loss: tensor(0.2568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3595/4339]: training_loss: tensor(0.1295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3596/4339]: training_loss: tensor(0.5886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3597/4339]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3598/4339]: training_loss: tensor(0.3927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3599/4339]: training_loss: tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3600/4339]: training_loss: tensor(0.3201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3601/4339]: training_loss: tensor(0.2095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3602/4339]: training_loss: tensor(0.2547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3603/4339]: training_loss: tensor(0.1149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3604/4339]: training_loss: tensor(0.1730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3605/4339]: training_loss: tensor(0.3505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3606/4339]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3607/4339]: training_loss: tensor(0.3837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3608/4339]: training_loss: tensor(0.1769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3609/4339]: training_loss: tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3610/4339]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3611/4339]: training_loss: tensor(0.4112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3612/4339]: training_loss: tensor(0.2738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3613/4339]: training_loss: tensor(0.2850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3614/4339]: training_loss: tensor(0.1222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3615/4339]: training_loss: tensor(0.2352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3616/4339]: training_loss: tensor(0.2740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3617/4339]: training_loss: tensor(0.3548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3618/4339]: training_loss: tensor(0.4126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3619/4339]: training_loss: tensor(0.2509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3620/4339]: training_loss: tensor(0.7596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3621/4339]: training_loss: tensor(0.3356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3622/4339]: training_loss: tensor(0.4521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3623/4339]: training_loss: tensor(0.2876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3624/4339]: training_loss: tensor(0.3553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3625/4339]: training_loss: tensor(0.4041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3626/4339]: training_loss: tensor(0.3630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3627/4339]: training_loss: tensor(0.2436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3628/4339]: training_loss: tensor(0.4809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3629/4339]: training_loss: tensor(0.3036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3630/4339]: training_loss: tensor(0.1625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3631/4339]: training_loss: tensor(0.3655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3632/4339]: training_loss: tensor(0.3566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3633/4339]: training_loss: tensor(0.2161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3634/4339]: training_loss: tensor(0.2001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3635/4339]: training_loss: tensor(0.2716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3636/4339]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3637/4339]: training_loss: tensor(0.3161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3638/4339]: training_loss: tensor(0.4102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3639/4339]: training_loss: tensor(0.2779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3640/4339]: training_loss: tensor(0.3147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3641/4339]: training_loss: tensor(0.2347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3642/4339]: training_loss: tensor(0.2305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3643/4339]: training_loss: tensor(0.1672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3644/4339]: training_loss: tensor(0.3464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3645/4339]: training_loss: tensor(0.6935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3646/4339]: training_loss: tensor(0.2081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3647/4339]: training_loss: tensor(0.4330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3648/4339]: training_loss: tensor(0.2098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3649/4339]: training_loss: tensor(0.4763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3650/4339]: training_loss: tensor(0.1318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3651/4339]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3652/4339]: training_loss: tensor(0.4230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3653/4339]: training_loss: tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3654/4339]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3655/4339]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3656/4339]: training_loss: tensor(0.2681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3657/4339]: training_loss: tensor(0.1813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3658/4339]: training_loss: tensor(0.1291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3659/4339]: training_loss: tensor(0.2366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3660/4339]: training_loss: tensor(0.1840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3661/4339]: training_loss: tensor(0.4279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3662/4339]: training_loss: tensor(0.2213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3663/4339]: training_loss: tensor(0.3445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3664/4339]: training_loss: tensor(0.2737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3665/4339]: training_loss: tensor(0.4355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3666/4339]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3667/4339]: training_loss: tensor(0.5181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3668/4339]: training_loss: tensor(0.3459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3669/4339]: training_loss: tensor(0.4088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3670/4339]: training_loss: tensor(0.2085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3671/4339]: training_loss: tensor(0.2140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3672/4339]: training_loss: tensor(0.3905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3673/4339]: training_loss: tensor(0.3044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3674/4339]: training_loss: tensor(0.5153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3675/4339]: training_loss: tensor(0.4606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3676/4339]: training_loss: tensor(0.4490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3677/4339]: training_loss: tensor(0.2965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3678/4339]: training_loss: tensor(0.2398, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3679/4339]: training_loss: tensor(0.5870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3680/4339]: training_loss: tensor(0.2858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3681/4339]: training_loss: tensor(0.6150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3682/4339]: training_loss: tensor(0.1854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3683/4339]: training_loss: tensor(0.3437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3684/4339]: training_loss: tensor(0.4902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3685/4339]: training_loss: tensor(0.3493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3686/4339]: training_loss: tensor(0.3030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3687/4339]: training_loss: tensor(0.2763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3688/4339]: training_loss: tensor(0.3466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3689/4339]: training_loss: tensor(0.1146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3690/4339]: training_loss: tensor(0.3923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3691/4339]: training_loss: tensor(0.3354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3692/4339]: training_loss: tensor(0.1831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3693/4339]: training_loss: tensor(0.3105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3694/4339]: training_loss: tensor(0.1664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3695/4339]: training_loss: tensor(0.2966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3696/4339]: training_loss: tensor(0.2308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3697/4339]: training_loss: tensor(0.2383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3698/4339]: training_loss: tensor(0.2640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3699/4339]: training_loss: tensor(0.3022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3700/4339]: training_loss: tensor(0.2645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3701/4339]: training_loss: tensor(0.3884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3702/4339]: training_loss: tensor(0.0587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3703/4339]: training_loss: tensor(0.3268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3704/4339]: training_loss: tensor(0.1571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3705/4339]: training_loss: tensor(0.2123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3706/4339]: training_loss: tensor(0.1600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3707/4339]: training_loss: tensor(0.3239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3708/4339]: training_loss: tensor(0.2642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3709/4339]: training_loss: tensor(0.8001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3710/4339]: training_loss: tensor(0.3331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3711/4339]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3712/4339]: training_loss: tensor(0.1172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3713/4339]: training_loss: tensor(0.2846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3714/4339]: training_loss: tensor(0.3778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3715/4339]: training_loss: tensor(0.3356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3716/4339]: training_loss: tensor(0.2526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3717/4339]: training_loss: tensor(0.2882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3718/4339]: training_loss: tensor(0.2418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3719/4339]: training_loss: tensor(0.1966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3720/4339]: training_loss: tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3721/4339]: training_loss: tensor(0.1375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3722/4339]: training_loss: tensor(0.2323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3723/4339]: training_loss: tensor(0.1627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3724/4339]: training_loss: tensor(0.3571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3725/4339]: training_loss: tensor(0.3285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3726/4339]: training_loss: tensor(0.2393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3727/4339]: training_loss: tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3728/4339]: training_loss: tensor(0.3546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3729/4339]: training_loss: tensor(0.1613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3730/4339]: training_loss: tensor(0.4094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3731/4339]: training_loss: tensor(0.7488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3732/4339]: training_loss: tensor(0.1547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3733/4339]: training_loss: tensor(0.0640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3734/4339]: training_loss: tensor(0.1283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3735/4339]: training_loss: tensor(0.2905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3736/4339]: training_loss: tensor(0.3103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3737/4339]: training_loss: tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3738/4339]: training_loss: tensor(0.4060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3739/4339]: training_loss: tensor(0.1266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3740/4339]: training_loss: tensor(0.6345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3741/4339]: training_loss: tensor(0.1219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3742/4339]: training_loss: tensor(0.3076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3743/4339]: training_loss: tensor(0.5059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3744/4339]: training_loss: tensor(0.4667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3745/4339]: training_loss: tensor(0.1265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3746/4339]: training_loss: tensor(0.4029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3747/4339]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3748/4339]: training_loss: tensor(0.3298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3749/4339]: training_loss: tensor(0.4363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3750/4339]: training_loss: tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3751/4339]: training_loss: tensor(0.3283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3752/4339]: training_loss: tensor(0.1819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3753/4339]: training_loss: tensor(0.4014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3754/4339]: training_loss: tensor(0.4076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3755/4339]: training_loss: tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3756/4339]: training_loss: tensor(0.6184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3757/4339]: training_loss: tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3758/4339]: training_loss: tensor(0.1813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3759/4339]: training_loss: tensor(0.4216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3760/4339]: training_loss: tensor(0.4838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3761/4339]: training_loss: tensor(0.5223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3762/4339]: training_loss: tensor(0.3272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3763/4339]: training_loss: tensor(0.4452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3764/4339]: training_loss: tensor(0.3037, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3765/4339]: training_loss: tensor(0.2704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3766/4339]: training_loss: tensor(0.3090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3767/4339]: training_loss: tensor(0.3604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3768/4339]: training_loss: tensor(0.1657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3769/4339]: training_loss: tensor(0.1496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3770/4339]: training_loss: tensor(0.1169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3771/4339]: training_loss: tensor(0.2196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3772/4339]: training_loss: tensor(0.1098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3773/4339]: training_loss: tensor(0.2530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3774/4339]: training_loss: tensor(0.1145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3775/4339]: training_loss: tensor(0.2588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3776/4339]: training_loss: tensor(0.1120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3777/4339]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3778/4339]: training_loss: tensor(0.1648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3779/4339]: training_loss: tensor(0.8509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3780/4339]: training_loss: tensor(0.1767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3781/4339]: training_loss: tensor(0.3484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3782/4339]: training_loss: tensor(0.1691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3783/4339]: training_loss: tensor(0.2575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3784/4339]: training_loss: tensor(0.3153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3785/4339]: training_loss: tensor(0.3993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3786/4339]: training_loss: tensor(0.1546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3787/4339]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3788/4339]: training_loss: tensor(0.1217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3789/4339]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3790/4339]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3791/4339]: training_loss: tensor(0.5112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3792/4339]: training_loss: tensor(0.3798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3793/4339]: training_loss: tensor(0.1223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3794/4339]: training_loss: tensor(0.5475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3795/4339]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3796/4339]: training_loss: tensor(0.2121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3797/4339]: training_loss: tensor(0.3096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3798/4339]: training_loss: tensor(0.3183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3799/4339]: training_loss: tensor(0.2995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3800/4339]: training_loss: tensor(0.3210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3801/4339]: training_loss: tensor(0.3632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3802/4339]: training_loss: tensor(0.2340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3803/4339]: training_loss: tensor(0.5604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3804/4339]: training_loss: tensor(0.1653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3805/4339]: training_loss: tensor(0.7149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3806/4339]: training_loss: tensor(0.1629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3807/4339]: training_loss: tensor(0.2935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3808/4339]: training_loss: tensor(0.3473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3809/4339]: training_loss: tensor(0.1990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3810/4339]: training_loss: tensor(0.1743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3811/4339]: training_loss: tensor(0.2458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3812/4339]: training_loss: tensor(0.4562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3813/4339]: training_loss: tensor(0.5771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3814/4339]: training_loss: tensor(0.5073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3815/4339]: training_loss: tensor(0.2771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3816/4339]: training_loss: tensor(0.1640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3817/4339]: training_loss: tensor(0.2250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3818/4339]: training_loss: tensor(0.1763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3819/4339]: training_loss: tensor(0.3678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3820/4339]: training_loss: tensor(0.3209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3821/4339]: training_loss: tensor(0.2759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3822/4339]: training_loss: tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3823/4339]: training_loss: tensor(0.1316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3824/4339]: training_loss: tensor(0.1317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3825/4339]: training_loss: tensor(0.6414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3826/4339]: training_loss: tensor(0.1282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3827/4339]: training_loss: tensor(0.2669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3828/4339]: training_loss: tensor(0.2261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3829/4339]: training_loss: tensor(0.3544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3830/4339]: training_loss: tensor(0.3543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3831/4339]: training_loss: tensor(0.5791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3832/4339]: training_loss: tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3833/4339]: training_loss: tensor(0.2314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3834/4339]: training_loss: tensor(0.4989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3835/4339]: training_loss: tensor(0.2054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3836/4339]: training_loss: tensor(0.1856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3837/4339]: training_loss: tensor(0.3847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3838/4339]: training_loss: tensor(0.4214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3839/4339]: training_loss: tensor(0.4940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3840/4339]: training_loss: tensor(0.2268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3841/4339]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3842/4339]: training_loss: tensor(0.3423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3843/4339]: training_loss: tensor(0.3278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3844/4339]: training_loss: tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3845/4339]: training_loss: tensor(0.2418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3846/4339]: training_loss: tensor(0.1596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3847/4339]: training_loss: tensor(0.2989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3848/4339]: training_loss: tensor(0.1301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3849/4339]: training_loss: tensor(0.4286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3850/4339]: training_loss: tensor(0.1504, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3851/4339]: training_loss: tensor(0.2762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3852/4339]: training_loss: tensor(0.3822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3853/4339]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3854/4339]: training_loss: tensor(0.2443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3855/4339]: training_loss: tensor(0.5486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3856/4339]: training_loss: tensor(0.1952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3857/4339]: training_loss: tensor(0.3984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3858/4339]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3859/4339]: training_loss: tensor(0.3468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3860/4339]: training_loss: tensor(0.1949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3861/4339]: training_loss: tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3862/4339]: training_loss: tensor(0.2109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3863/4339]: training_loss: tensor(0.0606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3864/4339]: training_loss: tensor(0.3221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3865/4339]: training_loss: tensor(0.1601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3866/4339]: training_loss: tensor(0.4235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3867/4339]: training_loss: tensor(0.2282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3868/4339]: training_loss: tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3869/4339]: training_loss: tensor(0.2480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3870/4339]: training_loss: tensor(0.2974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3871/4339]: training_loss: tensor(0.3561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3872/4339]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3873/4339]: training_loss: tensor(0.0640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3874/4339]: training_loss: tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3875/4339]: training_loss: tensor(0.2050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3876/4339]: training_loss: tensor(0.2118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3877/4339]: training_loss: tensor(0.3320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3878/4339]: training_loss: tensor(0.7019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3879/4339]: training_loss: tensor(0.2041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3880/4339]: training_loss: tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3881/4339]: training_loss: tensor(0.1671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3882/4339]: training_loss: tensor(0.1183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3883/4339]: training_loss: tensor(0.5085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3884/4339]: training_loss: tensor(0.4882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3885/4339]: training_loss: tensor(0.2078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3886/4339]: training_loss: tensor(0.1762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3887/4339]: training_loss: tensor(0.2755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3888/4339]: training_loss: tensor(0.5605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3889/4339]: training_loss: tensor(0.4660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3890/4339]: training_loss: tensor(0.4623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3891/4339]: training_loss: tensor(0.1195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3892/4339]: training_loss: tensor(0.3261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3893/4339]: training_loss: tensor(0.3693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3894/4339]: training_loss: tensor(0.3030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3895/4339]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3896/4339]: training_loss: tensor(0.4545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3897/4339]: training_loss: tensor(0.1955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3898/4339]: training_loss: tensor(0.4411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3899/4339]: training_loss: tensor(0.3077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3900/4339]: training_loss: tensor(0.2879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3901/4339]: training_loss: tensor(0.3652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3902/4339]: training_loss: tensor(0.5820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3903/4339]: training_loss: tensor(0.3480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3904/4339]: training_loss: tensor(0.1912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3905/4339]: training_loss: tensor(0.2823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3906/4339]: training_loss: tensor(0.3432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3907/4339]: training_loss: tensor(0.3306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3908/4339]: training_loss: tensor(0.1066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3909/4339]: training_loss: tensor(0.2354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3910/4339]: training_loss: tensor(0.6039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3911/4339]: training_loss: tensor(0.3804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3912/4339]: training_loss: tensor(0.3664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3913/4339]: training_loss: tensor(0.3511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3914/4339]: training_loss: tensor(0.2597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3915/4339]: training_loss: tensor(0.3274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3916/4339]: training_loss: tensor(0.2608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3917/4339]: training_loss: tensor(0.2331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3918/4339]: training_loss: tensor(0.3152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3919/4339]: training_loss: tensor(0.3996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3920/4339]: training_loss: tensor(0.1997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3921/4339]: training_loss: tensor(0.4527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3922/4339]: training_loss: tensor(0.3238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3923/4339]: training_loss: tensor(0.2136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3924/4339]: training_loss: tensor(0.4956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3925/4339]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3926/4339]: training_loss: tensor(0.4707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3927/4339]: training_loss: tensor(0.3590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3928/4339]: training_loss: tensor(0.3911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3929/4339]: training_loss: tensor(0.5861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3930/4339]: training_loss: tensor(0.2779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3931/4339]: training_loss: tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3932/4339]: training_loss: tensor(0.2307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3933/4339]: training_loss: tensor(0.5847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3934/4339]: training_loss: tensor(0.2611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3935/4339]: training_loss: tensor(0.2806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3936/4339]: training_loss: tensor(0.2971, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3937/4339]: training_loss: tensor(0.4920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3938/4339]: training_loss: tensor(0.3173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3939/4339]: training_loss: tensor(0.2039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3940/4339]: training_loss: tensor(0.3451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3941/4339]: training_loss: tensor(0.1872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3942/4339]: training_loss: tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3943/4339]: training_loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3944/4339]: training_loss: tensor(0.1680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3945/4339]: training_loss: tensor(0.2270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3946/4339]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3947/4339]: training_loss: tensor(0.2828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3948/4339]: training_loss: tensor(0.2311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3949/4339]: training_loss: tensor(0.2055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3950/4339]: training_loss: tensor(0.4064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3951/4339]: training_loss: tensor(0.3138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3952/4339]: training_loss: tensor(0.0603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3953/4339]: training_loss: tensor(0.2607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3954/4339]: training_loss: tensor(0.2644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3955/4339]: training_loss: tensor(0.2281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3956/4339]: training_loss: tensor(0.1683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3957/4339]: training_loss: tensor(0.2613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3958/4339]: training_loss: tensor(0.1226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3959/4339]: training_loss: tensor(0.2763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3960/4339]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3961/4339]: training_loss: tensor(0.4025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3962/4339]: training_loss: tensor(0.3789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3963/4339]: training_loss: tensor(0.1212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3964/4339]: training_loss: tensor(0.2521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3965/4339]: training_loss: tensor(0.1997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3966/4339]: training_loss: tensor(0.1790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3967/4339]: training_loss: tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3968/4339]: training_loss: tensor(0.1581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3969/4339]: training_loss: tensor(0.2504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3970/4339]: training_loss: tensor(0.3013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3971/4339]: training_loss: tensor(0.3287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3972/4339]: training_loss: tensor(0.1276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3973/4339]: training_loss: tensor(0.1923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3974/4339]: training_loss: tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3975/4339]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3976/4339]: training_loss: tensor(0.3987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3977/4339]: training_loss: tensor(0.2596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3978/4339]: training_loss: tensor(0.4335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3979/4339]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3980/4339]: training_loss: tensor(0.1652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3981/4339]: training_loss: tensor(0.1741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3982/4339]: training_loss: tensor(0.1499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3983/4339]: training_loss: tensor(0.1658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3984/4339]: training_loss: tensor(0.5922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3985/4339]: training_loss: tensor(0.2125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3986/4339]: training_loss: tensor(0.3499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3987/4339]: training_loss: tensor(0.4176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3988/4339]: training_loss: tensor(0.3456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3989/4339]: training_loss: tensor(0.2883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3990/4339]: training_loss: tensor(0.1922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3991/4339]: training_loss: tensor(0.3950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3992/4339]: training_loss: tensor(0.2020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3993/4339]: training_loss: tensor(0.2127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3994/4339]: training_loss: tensor(0.1946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3995/4339]: training_loss: tensor(0.4011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3996/4339]: training_loss: tensor(0.2493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3997/4339]: training_loss: tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3998/4339]: training_loss: tensor(0.1292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3999/4339]: training_loss: tensor(0.1600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4000/4339]: training_loss: tensor(0.2790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4001/4339]: training_loss: tensor(0.1394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4002/4339]: training_loss: tensor(0.1372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4003/4339]: training_loss: tensor(0.3393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4004/4339]: training_loss: tensor(0.2340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4005/4339]: training_loss: tensor(0.2033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4006/4339]: training_loss: tensor(0.6529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4007/4339]: training_loss: tensor(0.2673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4008/4339]: training_loss: tensor(0.5186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4009/4339]: training_loss: tensor(0.2952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4010/4339]: training_loss: tensor(0.2800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4011/4339]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4012/4339]: training_loss: tensor(0.5040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4013/4339]: training_loss: tensor(0.3438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4014/4339]: training_loss: tensor(0.1467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4015/4339]: training_loss: tensor(0.2323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4016/4339]: training_loss: tensor(0.3650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4017/4339]: training_loss: tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4018/4339]: training_loss: tensor(0.3699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4019/4339]: training_loss: tensor(0.2241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4020/4339]: training_loss: tensor(0.1129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4021/4339]: training_loss: tensor(0.1764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4022/4339]: training_loss: tensor(0.4276, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4023/4339]: training_loss: tensor(0.1527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4024/4339]: training_loss: tensor(0.5910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4025/4339]: training_loss: tensor(0.3875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4026/4339]: training_loss: tensor(0.4566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4027/4339]: training_loss: tensor(0.2521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4028/4339]: training_loss: tensor(0.1812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4029/4339]: training_loss: tensor(0.5688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4030/4339]: training_loss: tensor(0.3651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4031/4339]: training_loss: tensor(0.4452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4032/4339]: training_loss: tensor(0.6081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4033/4339]: training_loss: tensor(0.4024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4034/4339]: training_loss: tensor(0.1300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4035/4339]: training_loss: tensor(0.4011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4036/4339]: training_loss: tensor(0.1302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4037/4339]: training_loss: tensor(0.7894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4038/4339]: training_loss: tensor(0.2703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4039/4339]: training_loss: tensor(0.4037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4040/4339]: training_loss: tensor(0.4571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4041/4339]: training_loss: tensor(0.3245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4042/4339]: training_loss: tensor(0.1901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4043/4339]: training_loss: tensor(0.2909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4044/4339]: training_loss: tensor(0.2549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4045/4339]: training_loss: tensor(0.3198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4046/4339]: training_loss: tensor(0.2756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4047/4339]: training_loss: tensor(0.2048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4048/4339]: training_loss: tensor(0.3930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4049/4339]: training_loss: tensor(0.3113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4050/4339]: training_loss: tensor(0.5265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4051/4339]: training_loss: tensor(0.2743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4052/4339]: training_loss: tensor(0.3864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4053/4339]: training_loss: tensor(0.9424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4054/4339]: training_loss: tensor(0.2676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4055/4339]: training_loss: tensor(0.2903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4056/4339]: training_loss: tensor(0.2443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4057/4339]: training_loss: tensor(0.3266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4058/4339]: training_loss: tensor(0.3173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4059/4339]: training_loss: tensor(0.2033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4060/4339]: training_loss: tensor(0.1665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4061/4339]: training_loss: tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4062/4339]: training_loss: tensor(0.2448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4063/4339]: training_loss: tensor(0.4310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4064/4339]: training_loss: tensor(0.1782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4065/4339]: training_loss: tensor(0.1601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4066/4339]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4067/4339]: training_loss: tensor(0.1177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4068/4339]: training_loss: tensor(0.1524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4069/4339]: training_loss: tensor(0.1656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4070/4339]: training_loss: tensor(0.8708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4071/4339]: training_loss: tensor(0.2489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4072/4339]: training_loss: tensor(0.2985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4073/4339]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4074/4339]: training_loss: tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4075/4339]: training_loss: tensor(0.5542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4076/4339]: training_loss: tensor(0.4525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4077/4339]: training_loss: tensor(0.1700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4078/4339]: training_loss: tensor(0.2073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4079/4339]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4080/4339]: training_loss: tensor(0.2488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4081/4339]: training_loss: tensor(0.3883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4082/4339]: training_loss: tensor(0.2930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4083/4339]: training_loss: tensor(0.4705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4084/4339]: training_loss: tensor(0.2213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4085/4339]: training_loss: tensor(0.4062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4086/4339]: training_loss: tensor(0.2760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4087/4339]: training_loss: tensor(0.3480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4088/4339]: training_loss: tensor(0.2752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4089/4339]: training_loss: tensor(0.2232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4090/4339]: training_loss: tensor(0.4026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4091/4339]: training_loss: tensor(0.4844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4092/4339]: training_loss: tensor(0.3608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4093/4339]: training_loss: tensor(0.4679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4094/4339]: training_loss: tensor(0.3202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4095/4339]: training_loss: tensor(0.2417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4096/4339]: training_loss: tensor(0.1909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4097/4339]: training_loss: tensor(0.3265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4098/4339]: training_loss: tensor(0.1392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4099/4339]: training_loss: tensor(0.2235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4100/4339]: training_loss: tensor(0.5252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4101/4339]: training_loss: tensor(0.4955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4102/4339]: training_loss: tensor(0.5590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4103/4339]: training_loss: tensor(0.1331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4104/4339]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4105/4339]: training_loss: tensor(0.5540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4106/4339]: training_loss: tensor(0.4309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4107/4339]: training_loss: tensor(0.3887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4108/4339]: training_loss: tensor(0.2953, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4109/4339]: training_loss: tensor(0.4188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4110/4339]: training_loss: tensor(0.2278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4111/4339]: training_loss: tensor(0.1798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4112/4339]: training_loss: tensor(0.3519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4113/4339]: training_loss: tensor(0.4304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4114/4339]: training_loss: tensor(0.2051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4115/4339]: training_loss: tensor(0.3015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4116/4339]: training_loss: tensor(0.1478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4117/4339]: training_loss: tensor(0.3090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4118/4339]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4119/4339]: training_loss: tensor(0.2582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4120/4339]: training_loss: tensor(0.4856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4121/4339]: training_loss: tensor(0.2553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4122/4339]: training_loss: tensor(0.3845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4123/4339]: training_loss: tensor(0.2173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4124/4339]: training_loss: tensor(0.3664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4125/4339]: training_loss: tensor(0.2555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4126/4339]: training_loss: tensor(0.3613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4127/4339]: training_loss: tensor(0.3732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4128/4339]: training_loss: tensor(0.2489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4129/4339]: training_loss: tensor(0.3448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4130/4339]: training_loss: tensor(0.3332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4131/4339]: training_loss: tensor(0.1548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4132/4339]: training_loss: tensor(0.7863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4133/4339]: training_loss: tensor(0.5586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4134/4339]: training_loss: tensor(0.1898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4135/4339]: training_loss: tensor(0.4606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4136/4339]: training_loss: tensor(0.2161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4137/4339]: training_loss: tensor(0.3423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4138/4339]: training_loss: tensor(0.2600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4139/4339]: training_loss: tensor(0.5356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4140/4339]: training_loss: tensor(0.1430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4141/4339]: training_loss: tensor(0.1381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4142/4339]: training_loss: tensor(0.2713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4143/4339]: training_loss: tensor(0.3681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4144/4339]: training_loss: tensor(0.3850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4145/4339]: training_loss: tensor(0.2046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4146/4339]: training_loss: tensor(0.1796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4147/4339]: training_loss: tensor(0.2976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4148/4339]: training_loss: tensor(0.3493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4149/4339]: training_loss: tensor(0.2994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4150/4339]: training_loss: tensor(0.3687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4151/4339]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4152/4339]: training_loss: tensor(0.2814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4153/4339]: training_loss: tensor(0.2150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4154/4339]: training_loss: tensor(0.1712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4155/4339]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4156/4339]: training_loss: tensor(0.6996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4157/4339]: training_loss: tensor(0.1548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4158/4339]: training_loss: tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4159/4339]: training_loss: tensor(0.6963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4160/4339]: training_loss: tensor(0.3613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4161/4339]: training_loss: tensor(0.4716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4162/4339]: training_loss: tensor(0.2840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4163/4339]: training_loss: tensor(0.1665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4164/4339]: training_loss: tensor(0.5870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4165/4339]: training_loss: tensor(0.3585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4166/4339]: training_loss: tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4167/4339]: training_loss: tensor(0.1811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4168/4339]: training_loss: tensor(0.4422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4169/4339]: training_loss: tensor(0.1987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4170/4339]: training_loss: tensor(0.2435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4171/4339]: training_loss: tensor(0.1526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4172/4339]: training_loss: tensor(0.1935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4173/4339]: training_loss: tensor(0.4701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4174/4339]: training_loss: tensor(0.2184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4175/4339]: training_loss: tensor(0.2580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4176/4339]: training_loss: tensor(0.1736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4177/4339]: training_loss: tensor(0.5779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4178/4339]: training_loss: tensor(0.4415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4179/4339]: training_loss: tensor(0.5376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4180/4339]: training_loss: tensor(0.4683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4181/4339]: training_loss: tensor(0.1869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4182/4339]: training_loss: tensor(0.2982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4183/4339]: training_loss: tensor(0.1706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4184/4339]: training_loss: tensor(0.3357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4185/4339]: training_loss: tensor(0.2468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4186/4339]: training_loss: tensor(0.2444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4187/4339]: training_loss: tensor(0.2718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4188/4339]: training_loss: tensor(0.3431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4189/4339]: training_loss: tensor(0.1994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4190/4339]: training_loss: tensor(0.2155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4191/4339]: training_loss: tensor(0.2808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4192/4339]: training_loss: tensor(0.2878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4193/4339]: training_loss: tensor(0.3050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4194/4339]: training_loss: tensor(0.1130, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4195/4339]: training_loss: tensor(0.2245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4196/4339]: training_loss: tensor(0.1949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4197/4339]: training_loss: tensor(0.3750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4198/4339]: training_loss: tensor(0.4341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4199/4339]: training_loss: tensor(0.1592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4200/4339]: training_loss: tensor(0.1825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4201/4339]: training_loss: tensor(0.2929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4202/4339]: training_loss: tensor(0.2659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4203/4339]: training_loss: tensor(0.6454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4204/4339]: training_loss: tensor(0.2740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4205/4339]: training_loss: tensor(0.1653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4206/4339]: training_loss: tensor(0.1438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4207/4339]: training_loss: tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4208/4339]: training_loss: tensor(0.3650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4209/4339]: training_loss: tensor(0.2472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4210/4339]: training_loss: tensor(0.7741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4211/4339]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4212/4339]: training_loss: tensor(0.1987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4213/4339]: training_loss: tensor(0.6239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4214/4339]: training_loss: tensor(0.2779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4215/4339]: training_loss: tensor(0.4479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4216/4339]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4217/4339]: training_loss: tensor(0.4208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4218/4339]: training_loss: tensor(0.3197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4219/4339]: training_loss: tensor(0.2549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4220/4339]: training_loss: tensor(0.4605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4221/4339]: training_loss: tensor(0.3608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4222/4339]: training_loss: tensor(0.3407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4223/4339]: training_loss: tensor(0.6487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4224/4339]: training_loss: tensor(0.6044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4225/4339]: training_loss: tensor(0.2289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4226/4339]: training_loss: tensor(0.3353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4227/4339]: training_loss: tensor(0.3577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4228/4339]: training_loss: tensor(0.2999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4229/4339]: training_loss: tensor(0.3257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4230/4339]: training_loss: tensor(0.2126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4231/4339]: training_loss: tensor(0.4330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4232/4339]: training_loss: tensor(0.1444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4233/4339]: training_loss: tensor(0.2968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4234/4339]: training_loss: tensor(0.3753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4235/4339]: training_loss: tensor(0.2005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4236/4339]: training_loss: tensor(0.4445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4237/4339]: training_loss: tensor(0.2431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4238/4339]: training_loss: tensor(0.1375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4239/4339]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4240/4339]: training_loss: tensor(0.4300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4241/4339]: training_loss: tensor(0.2231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4242/4339]: training_loss: tensor(0.7530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4243/4339]: training_loss: tensor(0.3804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4244/4339]: training_loss: tensor(0.4584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4245/4339]: training_loss: tensor(0.3475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4246/4339]: training_loss: tensor(0.5340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4247/4339]: training_loss: tensor(0.5862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4248/4339]: training_loss: tensor(0.6429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4249/4339]: training_loss: tensor(0.1128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4250/4339]: training_loss: tensor(0.3282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4251/4339]: training_loss: tensor(0.4506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4252/4339]: training_loss: tensor(0.3488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4253/4339]: training_loss: tensor(0.1424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4254/4339]: training_loss: tensor(0.3523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4255/4339]: training_loss: tensor(0.4368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4256/4339]: training_loss: tensor(0.3776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4257/4339]: training_loss: tensor(0.3001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4258/4339]: training_loss: tensor(0.2203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4259/4339]: training_loss: tensor(0.3310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4260/4339]: training_loss: tensor(0.5037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4261/4339]: training_loss: tensor(0.7569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4262/4339]: training_loss: tensor(0.5395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4263/4339]: training_loss: tensor(0.2693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4264/4339]: training_loss: tensor(0.3942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4265/4339]: training_loss: tensor(0.2389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4266/4339]: training_loss: tensor(0.2084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4267/4339]: training_loss: tensor(0.4136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4268/4339]: training_loss: tensor(0.3382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4269/4339]: training_loss: tensor(0.3575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4270/4339]: training_loss: tensor(0.4094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4271/4339]: training_loss: tensor(0.3555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4272/4339]: training_loss: tensor(0.2973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4273/4339]: training_loss: tensor(0.4093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4274/4339]: training_loss: tensor(0.2876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4275/4339]: training_loss: tensor(0.3137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4276/4339]: training_loss: tensor(0.3332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4277/4339]: training_loss: tensor(0.2991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4278/4339]: training_loss: tensor(0.1494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4279/4339]: training_loss: tensor(0.5251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4280/4339]: training_loss: tensor(0.2091, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4281/4339]: training_loss: tensor(0.2764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4282/4339]: training_loss: tensor(0.6077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4283/4339]: training_loss: tensor(0.2191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4284/4339]: training_loss: tensor(0.3967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4285/4339]: training_loss: tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4286/4339]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4287/4339]: training_loss: tensor(0.1114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4288/4339]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4289/4339]: training_loss: tensor(0.5648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4290/4339]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4291/4339]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4292/4339]: training_loss: tensor(0.2767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4293/4339]: training_loss: tensor(0.3479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4294/4339]: training_loss: tensor(0.2831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4295/4339]: training_loss: tensor(0.1758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4296/4339]: training_loss: tensor(0.2572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4297/4339]: training_loss: tensor(0.2976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4298/4339]: training_loss: tensor(0.1743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4299/4339]: training_loss: tensor(0.3661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4300/4339]: training_loss: tensor(0.1453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4301/4339]: training_loss: tensor(0.2554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4302/4339]: training_loss: tensor(0.3279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4303/4339]: training_loss: tensor(0.1326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4304/4339]: training_loss: tensor(0.2629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4305/4339]: training_loss: tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4306/4339]: training_loss: tensor(0.4166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4307/4339]: training_loss: tensor(0.2366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4308/4339]: training_loss: tensor(0.2260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4309/4339]: training_loss: tensor(0.1697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4310/4339]: training_loss: tensor(0.4125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4311/4339]: training_loss: tensor(0.4495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4312/4339]: training_loss: tensor(0.2271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4313/4339]: training_loss: tensor(0.2102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4314/4339]: training_loss: tensor(0.2565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4315/4339]: training_loss: tensor(0.2669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4316/4339]: training_loss: tensor(0.3118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4317/4339]: training_loss: tensor(0.6346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4318/4339]: training_loss: tensor(0.3164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4319/4339]: training_loss: tensor(0.2603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4320/4339]: training_loss: tensor(0.3667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4321/4339]: training_loss: tensor(0.3626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4322/4339]: training_loss: tensor(0.4240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4323/4339]: training_loss: tensor(0.2057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4324/4339]: training_loss: tensor(0.2143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4325/4339]: training_loss: tensor(0.4945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4326/4339]: training_loss: tensor(0.3659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4327/4339]: training_loss: tensor(0.4559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4328/4339]: training_loss: tensor(0.3355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4329/4339]: training_loss: tensor(0.3323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4330/4339]: training_loss: tensor(0.4125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4331/4339]: training_loss: tensor(0.3621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4332/4339]: training_loss: tensor(0.2218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4333/4339]: training_loss: tensor(0.2858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4334/4339]: training_loss: tensor(0.2148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4335/4339]: training_loss: tensor(0.3472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4336/4339]: training_loss: tensor(0.2281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4337/4339]: training_loss: tensor(0.5137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4338/4339]: training_loss: tensor(0.3760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4339/4339]: training_loss: tensor(0.3454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4340/4339]: training_loss: tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [2/5], Step [8680/21700], Train Loss: 0.2787, Valid Loss: 0.3134\n",
      "batch_no [1/4339]: training_loss: tensor(0.2644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2/4339]: training_loss: tensor(0.2176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3/4339]: training_loss: tensor(0.3413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4/4339]: training_loss: tensor(0.1563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [5/4339]: training_loss: tensor(0.4969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [6/4339]: training_loss: tensor(0.5923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [7/4339]: training_loss: tensor(0.3330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [8/4339]: training_loss: tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [9/4339]: training_loss: tensor(0.1901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [10/4339]: training_loss: tensor(0.1886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [11/4339]: training_loss: tensor(0.1848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [12/4339]: training_loss: tensor(0.2348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [13/4339]: training_loss: tensor(0.0429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [14/4339]: training_loss: tensor(0.3179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [15/4339]: training_loss: tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [16/4339]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [17/4339]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [18/4339]: training_loss: tensor(0.1475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [19/4339]: training_loss: tensor(0.3394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [20/4339]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [21/4339]: training_loss: tensor(0.1288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [22/4339]: training_loss: tensor(0.4027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [23/4339]: training_loss: tensor(0.6761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [24/4339]: training_loss: tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [25/4339]: training_loss: tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [26/4339]: training_loss: tensor(0.1669, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [27/4339]: training_loss: tensor(0.1349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [28/4339]: training_loss: tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [29/4339]: training_loss: tensor(0.0410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [30/4339]: training_loss: tensor(0.4370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [31/4339]: training_loss: tensor(0.2635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [32/4339]: training_loss: tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [33/4339]: training_loss: tensor(0.2035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [34/4339]: training_loss: tensor(0.7985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [35/4339]: training_loss: tensor(0.4027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [36/4339]: training_loss: tensor(0.2778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [37/4339]: training_loss: tensor(0.1263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [38/4339]: training_loss: tensor(0.2725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [39/4339]: training_loss: tensor(0.2539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [40/4339]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [41/4339]: training_loss: tensor(0.3264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [42/4339]: training_loss: tensor(0.1898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [43/4339]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [44/4339]: training_loss: tensor(0.2316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [45/4339]: training_loss: tensor(0.3097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [46/4339]: training_loss: tensor(0.2419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [47/4339]: training_loss: tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [48/4339]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [49/4339]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [50/4339]: training_loss: tensor(0.3386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [51/4339]: training_loss: tensor(0.1345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [52/4339]: training_loss: tensor(0.4498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [53/4339]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [54/4339]: training_loss: tensor(0.4095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [55/4339]: training_loss: tensor(0.1755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [56/4339]: training_loss: tensor(0.3390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [57/4339]: training_loss: tensor(0.1090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [58/4339]: training_loss: tensor(0.4109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [59/4339]: training_loss: tensor(0.2472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [60/4339]: training_loss: tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [61/4339]: training_loss: tensor(0.2687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [62/4339]: training_loss: tensor(0.1766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [63/4339]: training_loss: tensor(0.2839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [64/4339]: training_loss: tensor(0.5437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [65/4339]: training_loss: tensor(0.1398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [66/4339]: training_loss: tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [67/4339]: training_loss: tensor(0.1709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [68/4339]: training_loss: tensor(0.2540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [69/4339]: training_loss: tensor(0.2287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [70/4339]: training_loss: tensor(0.2103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [71/4339]: training_loss: tensor(0.1380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [72/4339]: training_loss: tensor(0.5337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [73/4339]: training_loss: tensor(0.2468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [74/4339]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [75/4339]: training_loss: tensor(0.1322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [76/4339]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [77/4339]: training_loss: tensor(0.4547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [78/4339]: training_loss: tensor(0.0505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [79/4339]: training_loss: tensor(0.1733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [80/4339]: training_loss: tensor(0.4269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [81/4339]: training_loss: tensor(0.2028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [82/4339]: training_loss: tensor(0.2647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [83/4339]: training_loss: tensor(0.6018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [84/4339]: training_loss: tensor(0.2223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [85/4339]: training_loss: tensor(0.6004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [86/4339]: training_loss: tensor(0.1603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [87/4339]: training_loss: tensor(0.1479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [88/4339]: training_loss: tensor(0.2778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [89/4339]: training_loss: tensor(0.1540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [90/4339]: training_loss: tensor(0.2474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [91/4339]: training_loss: tensor(0.3046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [92/4339]: training_loss: tensor(0.2481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [93/4339]: training_loss: tensor(0.2147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [94/4339]: training_loss: tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [95/4339]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [96/4339]: training_loss: tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [97/4339]: training_loss: tensor(0.1730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [98/4339]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [99/4339]: training_loss: tensor(0.1553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [100/4339]: training_loss: tensor(0.2011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [101/4339]: training_loss: tensor(0.1929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [102/4339]: training_loss: tensor(0.1371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [103/4339]: training_loss: tensor(0.4075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [104/4339]: training_loss: tensor(0.1682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [105/4339]: training_loss: tensor(0.2435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [106/4339]: training_loss: tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [107/4339]: training_loss: tensor(0.2385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [108/4339]: training_loss: tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [109/4339]: training_loss: tensor(0.5958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [110/4339]: training_loss: tensor(0.2391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [111/4339]: training_loss: tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [112/4339]: training_loss: tensor(0.1599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [113/4339]: training_loss: tensor(0.2428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [114/4339]: training_loss: tensor(0.4080, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [115/4339]: training_loss: tensor(0.1190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [116/4339]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [117/4339]: training_loss: tensor(0.3192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [118/4339]: training_loss: tensor(0.3240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [119/4339]: training_loss: tensor(0.2236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [120/4339]: training_loss: tensor(0.3978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [121/4339]: training_loss: tensor(0.1852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [122/4339]: training_loss: tensor(0.1272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [123/4339]: training_loss: tensor(0.7617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [124/4339]: training_loss: tensor(0.2248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [125/4339]: training_loss: tensor(0.5200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [126/4339]: training_loss: tensor(0.4586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [127/4339]: training_loss: tensor(0.1572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [128/4339]: training_loss: tensor(0.3542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [129/4339]: training_loss: tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [130/4339]: training_loss: tensor(0.1259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [131/4339]: training_loss: tensor(0.1564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [132/4339]: training_loss: tensor(0.1775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [133/4339]: training_loss: tensor(0.1066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [134/4339]: training_loss: tensor(0.3003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [135/4339]: training_loss: tensor(0.1133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [136/4339]: training_loss: tensor(0.1201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [137/4339]: training_loss: tensor(0.1272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [138/4339]: training_loss: tensor(0.1356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [139/4339]: training_loss: tensor(0.3856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [140/4339]: training_loss: tensor(0.1677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [141/4339]: training_loss: tensor(0.4882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [142/4339]: training_loss: tensor(0.4212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [143/4339]: training_loss: tensor(0.2873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [144/4339]: training_loss: tensor(0.1444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [145/4339]: training_loss: tensor(0.1212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [146/4339]: training_loss: tensor(0.3435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [147/4339]: training_loss: tensor(0.1367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [148/4339]: training_loss: tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [149/4339]: training_loss: tensor(0.1976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [150/4339]: training_loss: tensor(0.3173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [151/4339]: training_loss: tensor(0.1826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [152/4339]: training_loss: tensor(0.1727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [153/4339]: training_loss: tensor(0.2246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [154/4339]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [155/4339]: training_loss: tensor(0.2332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [156/4339]: training_loss: tensor(0.0525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [157/4339]: training_loss: tensor(0.1699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [158/4339]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [159/4339]: training_loss: tensor(0.1889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [160/4339]: training_loss: tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [161/4339]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [162/4339]: training_loss: tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [163/4339]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [164/4339]: training_loss: tensor(0.3358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [165/4339]: training_loss: tensor(0.1952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [166/4339]: training_loss: tensor(0.0650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [167/4339]: training_loss: tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [168/4339]: training_loss: tensor(0.2172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [169/4339]: training_loss: tensor(0.5668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [170/4339]: training_loss: tensor(0.4999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [171/4339]: training_loss: tensor(0.2407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [172/4339]: training_loss: tensor(0.3989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [173/4339]: training_loss: tensor(0.3231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [174/4339]: training_loss: tensor(0.3280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [175/4339]: training_loss: tensor(0.1792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [176/4339]: training_loss: tensor(0.1885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [177/4339]: training_loss: tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [178/4339]: training_loss: tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [179/4339]: training_loss: tensor(0.2828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [180/4339]: training_loss: tensor(0.1405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [181/4339]: training_loss: tensor(0.3040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [182/4339]: training_loss: tensor(0.1130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [183/4339]: training_loss: tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [184/4339]: training_loss: tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [185/4339]: training_loss: tensor(0.1132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [186/4339]: training_loss: tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [187/4339]: training_loss: tensor(0.2693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [188/4339]: training_loss: tensor(0.2863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [189/4339]: training_loss: tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [190/4339]: training_loss: tensor(0.3441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [191/4339]: training_loss: tensor(0.1747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [192/4339]: training_loss: tensor(0.1657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [193/4339]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [194/4339]: training_loss: tensor(0.2351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [195/4339]: training_loss: tensor(0.1753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [196/4339]: training_loss: tensor(0.1848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [197/4339]: training_loss: tensor(0.2841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [198/4339]: training_loss: tensor(0.2425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [199/4339]: training_loss: tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [200/4339]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [201/4339]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [202/4339]: training_loss: tensor(0.2824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [203/4339]: training_loss: tensor(0.5157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [204/4339]: training_loss: tensor(0.3787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [205/4339]: training_loss: tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [206/4339]: training_loss: tensor(0.1392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [207/4339]: training_loss: tensor(0.0354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [208/4339]: training_loss: tensor(0.1397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [209/4339]: training_loss: tensor(0.2039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [210/4339]: training_loss: tensor(0.4869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [211/4339]: training_loss: tensor(0.2881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [212/4339]: training_loss: tensor(0.0683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [213/4339]: training_loss: tensor(0.2780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [214/4339]: training_loss: tensor(0.0547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [215/4339]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [216/4339]: training_loss: tensor(0.1222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [217/4339]: training_loss: tensor(0.1449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [218/4339]: training_loss: tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [219/4339]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [220/4339]: training_loss: tensor(0.2652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [221/4339]: training_loss: tensor(0.3355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [222/4339]: training_loss: tensor(0.5488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [223/4339]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [224/4339]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [225/4339]: training_loss: tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [226/4339]: training_loss: tensor(0.1366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [227/4339]: training_loss: tensor(0.1663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [228/4339]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [229/4339]: training_loss: tensor(0.2342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [230/4339]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [231/4339]: training_loss: tensor(0.3126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [232/4339]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [233/4339]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [234/4339]: training_loss: tensor(0.1185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [235/4339]: training_loss: tensor(0.5924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [236/4339]: training_loss: tensor(0.1369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [237/4339]: training_loss: tensor(0.5739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [238/4339]: training_loss: tensor(0.1326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [239/4339]: training_loss: tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [240/4339]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [241/4339]: training_loss: tensor(0.2140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [242/4339]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [243/4339]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [244/4339]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [245/4339]: training_loss: tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [246/4339]: training_loss: tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [247/4339]: training_loss: tensor(0.5337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [248/4339]: training_loss: tensor(0.5336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [249/4339]: training_loss: tensor(0.1238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [250/4339]: training_loss: tensor(0.4168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [251/4339]: training_loss: tensor(0.1496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [252/4339]: training_loss: tensor(0.1351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [253/4339]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [254/4339]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [255/4339]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [256/4339]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [257/4339]: training_loss: tensor(0.4550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [258/4339]: training_loss: tensor(0.3852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [259/4339]: training_loss: tensor(0.2224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [260/4339]: training_loss: tensor(0.0462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [261/4339]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [262/4339]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [263/4339]: training_loss: tensor(0.1851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [264/4339]: training_loss: tensor(0.2609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [265/4339]: training_loss: tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [266/4339]: training_loss: tensor(0.1979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [267/4339]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [268/4339]: training_loss: tensor(0.3546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [269/4339]: training_loss: tensor(0.4633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [270/4339]: training_loss: tensor(0.1697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [271/4339]: training_loss: tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [272/4339]: training_loss: tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [273/4339]: training_loss: tensor(0.2700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [274/4339]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [275/4339]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [276/4339]: training_loss: tensor(0.5265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [277/4339]: training_loss: tensor(0.2711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [278/4339]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [279/4339]: training_loss: tensor(0.2723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [280/4339]: training_loss: tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [281/4339]: training_loss: tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [282/4339]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [283/4339]: training_loss: tensor(0.3785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [284/4339]: training_loss: tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [285/4339]: training_loss: tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [286/4339]: training_loss: tensor(0.1450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [287/4339]: training_loss: tensor(0.1575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [288/4339]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [289/4339]: training_loss: tensor(0.3019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [290/4339]: training_loss: tensor(0.2988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [291/4339]: training_loss: tensor(0.2074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [292/4339]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [293/4339]: training_loss: tensor(0.1785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [294/4339]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [295/4339]: training_loss: tensor(0.1356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [296/4339]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [297/4339]: training_loss: tensor(0.3578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [298/4339]: training_loss: tensor(0.1526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [299/4339]: training_loss: tensor(0.2438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [300/4339]: training_loss: tensor(0.2647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [301/4339]: training_loss: tensor(0.3901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [302/4339]: training_loss: tensor(0.1805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [303/4339]: training_loss: tensor(0.5241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [304/4339]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [305/4339]: training_loss: tensor(0.4800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [306/4339]: training_loss: tensor(0.2437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [307/4339]: training_loss: tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [308/4339]: training_loss: tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [309/4339]: training_loss: tensor(0.2233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [310/4339]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [311/4339]: training_loss: tensor(0.2028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [312/4339]: training_loss: tensor(0.3817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [313/4339]: training_loss: tensor(0.4138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [314/4339]: training_loss: tensor(0.5261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [315/4339]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [316/4339]: training_loss: tensor(0.1462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [317/4339]: training_loss: tensor(0.1337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [318/4339]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [319/4339]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [320/4339]: training_loss: tensor(0.0620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [321/4339]: training_loss: tensor(0.4790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [322/4339]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [323/4339]: training_loss: tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [324/4339]: training_loss: tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [325/4339]: training_loss: tensor(0.4502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [326/4339]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [327/4339]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [328/4339]: training_loss: tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [329/4339]: training_loss: tensor(0.0354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [330/4339]: training_loss: tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [331/4339]: training_loss: tensor(0.3845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [332/4339]: training_loss: tensor(0.2246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [333/4339]: training_loss: tensor(0.1418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [334/4339]: training_loss: tensor(0.3669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [335/4339]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [336/4339]: training_loss: tensor(0.4323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [337/4339]: training_loss: tensor(0.4080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [338/4339]: training_loss: tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [339/4339]: training_loss: tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [340/4339]: training_loss: tensor(0.1570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [341/4339]: training_loss: tensor(0.1456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [342/4339]: training_loss: tensor(0.1264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [343/4339]: training_loss: tensor(0.2301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [344/4339]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [345/4339]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [346/4339]: training_loss: tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [347/4339]: training_loss: tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [348/4339]: training_loss: tensor(0.0354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [349/4339]: training_loss: tensor(0.4497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [350/4339]: training_loss: tensor(0.1962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [351/4339]: training_loss: tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [352/4339]: training_loss: tensor(0.1487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [353/4339]: training_loss: tensor(0.2489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [354/4339]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [355/4339]: training_loss: tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [356/4339]: training_loss: tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [357/4339]: training_loss: tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [358/4339]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [359/4339]: training_loss: tensor(0.1500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [360/4339]: training_loss: tensor(0.2212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [361/4339]: training_loss: tensor(0.3082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [362/4339]: training_loss: tensor(0.2263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [363/4339]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [364/4339]: training_loss: tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [365/4339]: training_loss: tensor(0.0404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [366/4339]: training_loss: tensor(0.1456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [367/4339]: training_loss: tensor(0.3844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [368/4339]: training_loss: tensor(0.7040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [369/4339]: training_loss: tensor(0.1216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [370/4339]: training_loss: tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [371/4339]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [372/4339]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [373/4339]: training_loss: tensor(0.4832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [374/4339]: training_loss: tensor(0.2324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [375/4339]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [376/4339]: training_loss: tensor(0.4344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [377/4339]: training_loss: tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [378/4339]: training_loss: tensor(0.1118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [379/4339]: training_loss: tensor(0.2219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [380/4339]: training_loss: tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [381/4339]: training_loss: tensor(0.3438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [382/4339]: training_loss: tensor(0.6555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [383/4339]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [384/4339]: training_loss: tensor(0.5352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [385/4339]: training_loss: tensor(0.3392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [386/4339]: training_loss: tensor(0.1306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [387/4339]: training_loss: tensor(0.1295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [388/4339]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [389/4339]: training_loss: tensor(0.1237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [390/4339]: training_loss: tensor(0.2067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [391/4339]: training_loss: tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [392/4339]: training_loss: tensor(0.2286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [393/4339]: training_loss: tensor(0.1214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [394/4339]: training_loss: tensor(0.4579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [395/4339]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [396/4339]: training_loss: tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [397/4339]: training_loss: tensor(0.1437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [398/4339]: training_loss: tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [399/4339]: training_loss: tensor(0.3193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [400/4339]: training_loss: tensor(0.2580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [401/4339]: training_loss: tensor(0.1803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [402/4339]: training_loss: tensor(0.3766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [403/4339]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [404/4339]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [405/4339]: training_loss: tensor(0.1961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [406/4339]: training_loss: tensor(0.2023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [407/4339]: training_loss: tensor(0.1889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [408/4339]: training_loss: tensor(0.1229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [409/4339]: training_loss: tensor(0.5252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [410/4339]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [411/4339]: training_loss: tensor(0.6092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [412/4339]: training_loss: tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [413/4339]: training_loss: tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [414/4339]: training_loss: tensor(0.3060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [415/4339]: training_loss: tensor(0.2824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [416/4339]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [417/4339]: training_loss: tensor(0.3447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [418/4339]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [419/4339]: training_loss: tensor(0.3992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [420/4339]: training_loss: tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [421/4339]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [422/4339]: training_loss: tensor(0.2799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [423/4339]: training_loss: tensor(0.1571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [424/4339]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [425/4339]: training_loss: tensor(0.1419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [426/4339]: training_loss: tensor(0.4709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [427/4339]: training_loss: tensor(0.1608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [428/4339]: training_loss: tensor(0.1200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [429/4339]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [430/4339]: training_loss: tensor(0.0579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [431/4339]: training_loss: tensor(0.2985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [432/4339]: training_loss: tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [433/4339]: training_loss: tensor(0.2730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [434/4339]: training_loss: tensor(0.1651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [435/4339]: training_loss: tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [436/4339]: training_loss: tensor(0.4777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [437/4339]: training_loss: tensor(0.4271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [438/4339]: training_loss: tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [439/4339]: training_loss: tensor(0.2452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [440/4339]: training_loss: tensor(0.3963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [441/4339]: training_loss: tensor(0.2618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [442/4339]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [443/4339]: training_loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [444/4339]: training_loss: tensor(0.3161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [445/4339]: training_loss: tensor(0.1895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [446/4339]: training_loss: tensor(0.2333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [447/4339]: training_loss: tensor(0.3190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [448/4339]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [449/4339]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [450/4339]: training_loss: tensor(0.2386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [451/4339]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [452/4339]: training_loss: tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [453/4339]: training_loss: tensor(0.1429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [454/4339]: training_loss: tensor(0.1194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [455/4339]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [456/4339]: training_loss: tensor(0.3431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [457/4339]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [458/4339]: training_loss: tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [459/4339]: training_loss: tensor(0.1137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [460/4339]: training_loss: tensor(0.3627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [461/4339]: training_loss: tensor(0.3087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [462/4339]: training_loss: tensor(0.1684, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [463/4339]: training_loss: tensor(0.1428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [464/4339]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [465/4339]: training_loss: tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [466/4339]: training_loss: tensor(0.3424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [467/4339]: training_loss: tensor(0.3701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [468/4339]: training_loss: tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [469/4339]: training_loss: tensor(0.1106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [470/4339]: training_loss: tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [471/4339]: training_loss: tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [472/4339]: training_loss: tensor(0.1423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [473/4339]: training_loss: tensor(0.2476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [474/4339]: training_loss: tensor(0.3035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [475/4339]: training_loss: tensor(0.2441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [476/4339]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [477/4339]: training_loss: tensor(0.2586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [478/4339]: training_loss: tensor(0.1352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [479/4339]: training_loss: tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [480/4339]: training_loss: tensor(0.4143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [481/4339]: training_loss: tensor(0.3652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [482/4339]: training_loss: tensor(0.5309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [483/4339]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [484/4339]: training_loss: tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [485/4339]: training_loss: tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [486/4339]: training_loss: tensor(0.2883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [487/4339]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [488/4339]: training_loss: tensor(0.1721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [489/4339]: training_loss: tensor(0.1546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [490/4339]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [491/4339]: training_loss: tensor(0.1319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [492/4339]: training_loss: tensor(0.1405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [493/4339]: training_loss: tensor(0.3384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [494/4339]: training_loss: tensor(0.1077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [495/4339]: training_loss: tensor(0.3499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [496/4339]: training_loss: tensor(0.4146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [497/4339]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [498/4339]: training_loss: tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [499/4339]: training_loss: tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [500/4339]: training_loss: tensor(0.5518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [501/4339]: training_loss: tensor(0.1824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [502/4339]: training_loss: tensor(0.1653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [503/4339]: training_loss: tensor(0.2240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [504/4339]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [505/4339]: training_loss: tensor(0.1799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [506/4339]: training_loss: tensor(0.4497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [507/4339]: training_loss: tensor(0.2889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [508/4339]: training_loss: tensor(0.1304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [509/4339]: training_loss: tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [510/4339]: training_loss: tensor(0.2208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [511/4339]: training_loss: tensor(0.0423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [512/4339]: training_loss: tensor(0.2258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [513/4339]: training_loss: tensor(0.2059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [514/4339]: training_loss: tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [515/4339]: training_loss: tensor(0.1391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [516/4339]: training_loss: tensor(0.1720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [517/4339]: training_loss: tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [518/4339]: training_loss: tensor(0.2744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [519/4339]: training_loss: tensor(0.3089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [520/4339]: training_loss: tensor(0.1308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [521/4339]: training_loss: tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [522/4339]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [523/4339]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [524/4339]: training_loss: tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [525/4339]: training_loss: tensor(0.6345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [526/4339]: training_loss: tensor(0.1772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [527/4339]: training_loss: tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [528/4339]: training_loss: tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [529/4339]: training_loss: tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [530/4339]: training_loss: tensor(0.2811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [531/4339]: training_loss: tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [532/4339]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [533/4339]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [534/4339]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [535/4339]: training_loss: tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [536/4339]: training_loss: tensor(0.5695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [537/4339]: training_loss: tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [538/4339]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [539/4339]: training_loss: tensor(0.0487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [540/4339]: training_loss: tensor(0.1366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [541/4339]: training_loss: tensor(0.0592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [542/4339]: training_loss: tensor(0.2482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [543/4339]: training_loss: tensor(0.2706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [544/4339]: training_loss: tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [545/4339]: training_loss: tensor(0.1735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [546/4339]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [547/4339]: training_loss: tensor(0.2743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [548/4339]: training_loss: tensor(0.4428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [549/4339]: training_loss: tensor(0.1152, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [550/4339]: training_loss: tensor(0.1372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [551/4339]: training_loss: tensor(0.4052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [552/4339]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [553/4339]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [554/4339]: training_loss: tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [555/4339]: training_loss: tensor(0.1655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [556/4339]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [557/4339]: training_loss: tensor(0.2788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [558/4339]: training_loss: tensor(0.1520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [559/4339]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [560/4339]: training_loss: tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [561/4339]: training_loss: tensor(0.3368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [562/4339]: training_loss: tensor(0.1992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [563/4339]: training_loss: tensor(0.3335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [564/4339]: training_loss: tensor(0.1554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [565/4339]: training_loss: tensor(0.2070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [566/4339]: training_loss: tensor(0.0592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [567/4339]: training_loss: tensor(0.2710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [568/4339]: training_loss: tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [569/4339]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [570/4339]: training_loss: tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [571/4339]: training_loss: tensor(0.0447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [572/4339]: training_loss: tensor(0.2452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [573/4339]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [574/4339]: training_loss: tensor(0.3056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [575/4339]: training_loss: tensor(0.2197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [576/4339]: training_loss: tensor(0.1273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [577/4339]: training_loss: tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [578/4339]: training_loss: tensor(0.1378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [579/4339]: training_loss: tensor(0.1727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [580/4339]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [581/4339]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [582/4339]: training_loss: tensor(0.2198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [583/4339]: training_loss: tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [584/4339]: training_loss: tensor(0.5630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [585/4339]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [586/4339]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [587/4339]: training_loss: tensor(0.2545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [588/4339]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [589/4339]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [590/4339]: training_loss: tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [591/4339]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [592/4339]: training_loss: tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [593/4339]: training_loss: tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [594/4339]: training_loss: tensor(0.2084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [595/4339]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [596/4339]: training_loss: tensor(0.3629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [597/4339]: training_loss: tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [598/4339]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [599/4339]: training_loss: tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [600/4339]: training_loss: tensor(0.2208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [601/4339]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [602/4339]: training_loss: tensor(0.3403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [603/4339]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [604/4339]: training_loss: tensor(0.3179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [605/4339]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [606/4339]: training_loss: tensor(0.6081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [607/4339]: training_loss: tensor(0.1387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [608/4339]: training_loss: tensor(0.2973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [609/4339]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [610/4339]: training_loss: tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [611/4339]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [612/4339]: training_loss: tensor(0.1963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [613/4339]: training_loss: tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [614/4339]: training_loss: tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [615/4339]: training_loss: tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [616/4339]: training_loss: tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [617/4339]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [618/4339]: training_loss: tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [619/4339]: training_loss: tensor(0.4920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [620/4339]: training_loss: tensor(0.3205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [621/4339]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [622/4339]: training_loss: tensor(0.1998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [623/4339]: training_loss: tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [624/4339]: training_loss: tensor(0.3171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [625/4339]: training_loss: tensor(0.6062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [626/4339]: training_loss: tensor(0.1725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [627/4339]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [628/4339]: training_loss: tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [629/4339]: training_loss: tensor(0.1343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [630/4339]: training_loss: tensor(0.3008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [631/4339]: training_loss: tensor(0.4811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [632/4339]: training_loss: tensor(0.1490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [633/4339]: training_loss: tensor(0.3299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [634/4339]: training_loss: tensor(0.1466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [635/4339]: training_loss: tensor(0.1152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [636/4339]: training_loss: tensor(0.2121, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [637/4339]: training_loss: tensor(0.0640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [638/4339]: training_loss: tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [639/4339]: training_loss: tensor(0.3500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [640/4339]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [641/4339]: training_loss: tensor(0.8617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [642/4339]: training_loss: tensor(0.2990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [643/4339]: training_loss: tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [644/4339]: training_loss: tensor(0.3802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [645/4339]: training_loss: tensor(0.1557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [646/4339]: training_loss: tensor(0.1549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [647/4339]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [648/4339]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [649/4339]: training_loss: tensor(0.1328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [650/4339]: training_loss: tensor(0.1612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [651/4339]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [652/4339]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [653/4339]: training_loss: tensor(0.4238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [654/4339]: training_loss: tensor(0.3237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [655/4339]: training_loss: tensor(0.2917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [656/4339]: training_loss: tensor(0.1363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [657/4339]: training_loss: tensor(0.1741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [658/4339]: training_loss: tensor(0.1335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [659/4339]: training_loss: tensor(0.2970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [660/4339]: training_loss: tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [661/4339]: training_loss: tensor(0.3372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [662/4339]: training_loss: tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [663/4339]: training_loss: tensor(0.1211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [664/4339]: training_loss: tensor(0.3099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [665/4339]: training_loss: tensor(0.1256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [666/4339]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [667/4339]: training_loss: tensor(0.2961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [668/4339]: training_loss: tensor(0.1425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [669/4339]: training_loss: tensor(0.2523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [670/4339]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [671/4339]: training_loss: tensor(0.1756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [672/4339]: training_loss: tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [673/4339]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [674/4339]: training_loss: tensor(0.1155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [675/4339]: training_loss: tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [676/4339]: training_loss: tensor(0.1492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [677/4339]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [678/4339]: training_loss: tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [679/4339]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [680/4339]: training_loss: tensor(0.3153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [681/4339]: training_loss: tensor(0.3864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [682/4339]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [683/4339]: training_loss: tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [684/4339]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [685/4339]: training_loss: tensor(0.2382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [686/4339]: training_loss: tensor(0.2081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [687/4339]: training_loss: tensor(0.3975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [688/4339]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [689/4339]: training_loss: tensor(0.2650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [690/4339]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [691/4339]: training_loss: tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [692/4339]: training_loss: tensor(0.0269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [693/4339]: training_loss: tensor(0.5079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [694/4339]: training_loss: tensor(0.3244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [695/4339]: training_loss: tensor(0.5231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [696/4339]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [697/4339]: training_loss: tensor(0.2460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [698/4339]: training_loss: tensor(0.2063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [699/4339]: training_loss: tensor(0.3680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [700/4339]: training_loss: tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [701/4339]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [702/4339]: training_loss: tensor(0.1791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [703/4339]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [704/4339]: training_loss: tensor(0.1831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [705/4339]: training_loss: tensor(0.2488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [706/4339]: training_loss: tensor(0.0648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [707/4339]: training_loss: tensor(0.1893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [708/4339]: training_loss: tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [709/4339]: training_loss: tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [710/4339]: training_loss: tensor(0.3190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [711/4339]: training_loss: tensor(0.3094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [712/4339]: training_loss: tensor(0.2721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [713/4339]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [714/4339]: training_loss: tensor(0.4031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [715/4339]: training_loss: tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [716/4339]: training_loss: tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [717/4339]: training_loss: tensor(0.1662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [718/4339]: training_loss: tensor(0.3088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [719/4339]: training_loss: tensor(0.1202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [720/4339]: training_loss: tensor(0.4619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [721/4339]: training_loss: tensor(0.2100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [722/4339]: training_loss: tensor(0.5002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [723/4339]: training_loss: tensor(0.1819, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [724/4339]: training_loss: tensor(0.1920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [725/4339]: training_loss: tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [726/4339]: training_loss: tensor(0.1972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [727/4339]: training_loss: tensor(0.3326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [728/4339]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [729/4339]: training_loss: tensor(0.2051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [730/4339]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [731/4339]: training_loss: tensor(0.3095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [732/4339]: training_loss: tensor(0.1226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [733/4339]: training_loss: tensor(0.1152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [734/4339]: training_loss: tensor(0.1209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [735/4339]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [736/4339]: training_loss: tensor(0.1114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [737/4339]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [738/4339]: training_loss: tensor(0.1816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [739/4339]: training_loss: tensor(0.1744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [740/4339]: training_loss: tensor(0.1269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [741/4339]: training_loss: tensor(0.0596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [742/4339]: training_loss: tensor(0.3042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [743/4339]: training_loss: tensor(0.2798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [744/4339]: training_loss: tensor(0.1385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [745/4339]: training_loss: tensor(0.2368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [746/4339]: training_loss: tensor(0.1691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [747/4339]: training_loss: tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [748/4339]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [749/4339]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [750/4339]: training_loss: tensor(0.1957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [751/4339]: training_loss: tensor(0.1962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [752/4339]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [753/4339]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [754/4339]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [755/4339]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [756/4339]: training_loss: tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [757/4339]: training_loss: tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [758/4339]: training_loss: tensor(0.2038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [759/4339]: training_loss: tensor(0.4852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [760/4339]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [761/4339]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [762/4339]: training_loss: tensor(0.2818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [763/4339]: training_loss: tensor(0.2428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [764/4339]: training_loss: tensor(0.2927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [765/4339]: training_loss: tensor(0.4525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [766/4339]: training_loss: tensor(0.1286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [767/4339]: training_loss: tensor(0.2366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [768/4339]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [769/4339]: training_loss: tensor(0.0436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [770/4339]: training_loss: tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [771/4339]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [772/4339]: training_loss: tensor(0.6452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [773/4339]: training_loss: tensor(0.1432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [774/4339]: training_loss: tensor(0.4864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [775/4339]: training_loss: tensor(0.2769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [776/4339]: training_loss: tensor(0.1744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [777/4339]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [778/4339]: training_loss: tensor(0.4018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [779/4339]: training_loss: tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [780/4339]: training_loss: tensor(0.3326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [781/4339]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [782/4339]: training_loss: tensor(0.2647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [783/4339]: training_loss: tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [784/4339]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [785/4339]: training_loss: tensor(0.2756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [786/4339]: training_loss: tensor(0.1575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [787/4339]: training_loss: tensor(0.2662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [788/4339]: training_loss: tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [789/4339]: training_loss: tensor(0.4881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [790/4339]: training_loss: tensor(0.5254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [791/4339]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [792/4339]: training_loss: tensor(0.1649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [793/4339]: training_loss: tensor(0.4188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [794/4339]: training_loss: tensor(0.2857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [795/4339]: training_loss: tensor(0.7370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [796/4339]: training_loss: tensor(0.2456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [797/4339]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [798/4339]: training_loss: tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [799/4339]: training_loss: tensor(0.1723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [800/4339]: training_loss: tensor(0.1099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [801/4339]: training_loss: tensor(0.3155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [802/4339]: training_loss: tensor(0.2236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [803/4339]: training_loss: tensor(0.2347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [804/4339]: training_loss: tensor(0.2134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [805/4339]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [806/4339]: training_loss: tensor(0.1999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [807/4339]: training_loss: tensor(0.3582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [808/4339]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [809/4339]: training_loss: tensor(0.3979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [810/4339]: training_loss: tensor(0.3887, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [811/4339]: training_loss: tensor(0.1865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [812/4339]: training_loss: tensor(0.2043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [813/4339]: training_loss: tensor(0.1526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [814/4339]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [815/4339]: training_loss: tensor(0.3630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [816/4339]: training_loss: tensor(0.1380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [817/4339]: training_loss: tensor(0.1368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [818/4339]: training_loss: tensor(0.1150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [819/4339]: training_loss: tensor(0.0429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [820/4339]: training_loss: tensor(0.5545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [821/4339]: training_loss: tensor(0.1095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [822/4339]: training_loss: tensor(0.1914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [823/4339]: training_loss: tensor(0.1176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [824/4339]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [825/4339]: training_loss: tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [826/4339]: training_loss: tensor(0.1809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [827/4339]: training_loss: tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [828/4339]: training_loss: tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [829/4339]: training_loss: tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [830/4339]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [831/4339]: training_loss: tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [832/4339]: training_loss: tensor(0.1786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [833/4339]: training_loss: tensor(0.1222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [834/4339]: training_loss: tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [835/4339]: training_loss: tensor(0.3301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [836/4339]: training_loss: tensor(0.1446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [837/4339]: training_loss: tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [838/4339]: training_loss: tensor(0.1827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [839/4339]: training_loss: tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [840/4339]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [841/4339]: training_loss: tensor(0.3595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [842/4339]: training_loss: tensor(0.3373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [843/4339]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [844/4339]: training_loss: tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [845/4339]: training_loss: tensor(0.6028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [846/4339]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [847/4339]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [848/4339]: training_loss: tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [849/4339]: training_loss: tensor(0.1252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [850/4339]: training_loss: tensor(0.3057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [851/4339]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [852/4339]: training_loss: tensor(0.4862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [853/4339]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [854/4339]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [855/4339]: training_loss: tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [856/4339]: training_loss: tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [857/4339]: training_loss: tensor(0.6283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [858/4339]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [859/4339]: training_loss: tensor(0.2504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [860/4339]: training_loss: tensor(0.1763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [861/4339]: training_loss: tensor(0.1188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [862/4339]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [863/4339]: training_loss: tensor(0.1753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [864/4339]: training_loss: tensor(0.2887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [865/4339]: training_loss: tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [866/4339]: training_loss: tensor(0.3036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [867/4339]: training_loss: tensor(0.4200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [868/4339]: training_loss: tensor(0.2285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [869/4339]: training_loss: tensor(0.1900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [870/4339]: training_loss: tensor(0.4370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [871/4339]: training_loss: tensor(0.2866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [872/4339]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [873/4339]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [874/4339]: training_loss: tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [875/4339]: training_loss: tensor(0.2388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [876/4339]: training_loss: tensor(0.3994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [877/4339]: training_loss: tensor(0.1848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [878/4339]: training_loss: tensor(0.2812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [879/4339]: training_loss: tensor(0.3649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [880/4339]: training_loss: tensor(0.2924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [881/4339]: training_loss: tensor(0.3391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [882/4339]: training_loss: tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [883/4339]: training_loss: tensor(0.1700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [884/4339]: training_loss: tensor(0.1172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [885/4339]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [886/4339]: training_loss: tensor(0.1993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [887/4339]: training_loss: tensor(0.2840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [888/4339]: training_loss: tensor(0.4524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [889/4339]: training_loss: tensor(0.1544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [890/4339]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [891/4339]: training_loss: tensor(0.5550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [892/4339]: training_loss: tensor(0.1558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [893/4339]: training_loss: tensor(0.3079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [894/4339]: training_loss: tensor(0.1860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [895/4339]: training_loss: tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [896/4339]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [897/4339]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [898/4339]: training_loss: tensor(0.1294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [899/4339]: training_loss: tensor(0.2713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [900/4339]: training_loss: tensor(0.4361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [901/4339]: training_loss: tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [902/4339]: training_loss: tensor(0.2861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [903/4339]: training_loss: tensor(0.3542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [904/4339]: training_loss: tensor(0.2016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [905/4339]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [906/4339]: training_loss: tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [907/4339]: training_loss: tensor(0.2358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [908/4339]: training_loss: tensor(0.1470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [909/4339]: training_loss: tensor(0.5340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [910/4339]: training_loss: tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [911/4339]: training_loss: tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [912/4339]: training_loss: tensor(0.0384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [913/4339]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [914/4339]: training_loss: tensor(0.1767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [915/4339]: training_loss: tensor(0.2566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [916/4339]: training_loss: tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [917/4339]: training_loss: tensor(0.4722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [918/4339]: training_loss: tensor(0.1718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [919/4339]: training_loss: tensor(0.3567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [920/4339]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [921/4339]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [922/4339]: training_loss: tensor(0.2371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [923/4339]: training_loss: tensor(0.2172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [924/4339]: training_loss: tensor(0.3815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [925/4339]: training_loss: tensor(0.1665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [926/4339]: training_loss: tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [927/4339]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [928/4339]: training_loss: tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [929/4339]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [930/4339]: training_loss: tensor(0.1791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [931/4339]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [932/4339]: training_loss: tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [933/4339]: training_loss: tensor(0.1838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [934/4339]: training_loss: tensor(0.1704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [935/4339]: training_loss: tensor(0.1767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [936/4339]: training_loss: tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [937/4339]: training_loss: tensor(0.5483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [938/4339]: training_loss: tensor(0.1237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [939/4339]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [940/4339]: training_loss: tensor(0.4176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [941/4339]: training_loss: tensor(0.0635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [942/4339]: training_loss: tensor(0.4436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [943/4339]: training_loss: tensor(0.1234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [944/4339]: training_loss: tensor(0.3351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [945/4339]: training_loss: tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [946/4339]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [947/4339]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [948/4339]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [949/4339]: training_loss: tensor(0.1390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [950/4339]: training_loss: tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [951/4339]: training_loss: tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [952/4339]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [953/4339]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [954/4339]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [955/4339]: training_loss: tensor(0.1278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [956/4339]: training_loss: tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [957/4339]: training_loss: tensor(0.2308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [958/4339]: training_loss: tensor(0.3046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [959/4339]: training_loss: tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [960/4339]: training_loss: tensor(0.7192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [961/4339]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [962/4339]: training_loss: tensor(0.1813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [963/4339]: training_loss: tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [964/4339]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [965/4339]: training_loss: tensor(0.1683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [966/4339]: training_loss: tensor(0.1077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [967/4339]: training_loss: tensor(0.3203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [968/4339]: training_loss: tensor(0.2193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [969/4339]: training_loss: tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [970/4339]: training_loss: tensor(0.3690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [971/4339]: training_loss: tensor(0.2709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [972/4339]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [973/4339]: training_loss: tensor(0.3017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [974/4339]: training_loss: tensor(0.6775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [975/4339]: training_loss: tensor(0.2534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [976/4339]: training_loss: tensor(0.4085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [977/4339]: training_loss: tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [978/4339]: training_loss: tensor(0.2192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [979/4339]: training_loss: tensor(0.5464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [980/4339]: training_loss: tensor(0.3425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [981/4339]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [982/4339]: training_loss: tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [983/4339]: training_loss: tensor(0.2845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [984/4339]: training_loss: tensor(0.3255, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [985/4339]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [986/4339]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [987/4339]: training_loss: tensor(0.3139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [988/4339]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [989/4339]: training_loss: tensor(0.2815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [990/4339]: training_loss: tensor(0.1314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [991/4339]: training_loss: tensor(0.2692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [992/4339]: training_loss: tensor(0.1833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [993/4339]: training_loss: tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [994/4339]: training_loss: tensor(0.2533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [995/4339]: training_loss: tensor(0.1719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [996/4339]: training_loss: tensor(0.1966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [997/4339]: training_loss: tensor(0.3745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [998/4339]: training_loss: tensor(0.1520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [999/4339]: training_loss: tensor(0.2712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1000/4339]: training_loss: tensor(0.2379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1001/4339]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1002/4339]: training_loss: tensor(0.2123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1003/4339]: training_loss: tensor(0.2310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1004/4339]: training_loss: tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1005/4339]: training_loss: tensor(0.1175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/4339]: training_loss: tensor(0.4176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/4339]: training_loss: tensor(0.2283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/4339]: training_loss: tensor(0.4944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/4339]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/4339]: training_loss: tensor(0.2819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/4339]: training_loss: tensor(0.1417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/4339]: training_loss: tensor(0.2543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/4339]: training_loss: tensor(0.2002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/4339]: training_loss: tensor(0.2525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/4339]: training_loss: tensor(0.1870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/4339]: training_loss: tensor(0.2282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/4339]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/4339]: training_loss: tensor(0.2746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1019/4339]: training_loss: tensor(0.3347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/4339]: training_loss: tensor(0.4678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/4339]: training_loss: tensor(0.1552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/4339]: training_loss: tensor(0.3809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1023/4339]: training_loss: tensor(0.4530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/4339]: training_loss: tensor(0.2494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/4339]: training_loss: tensor(0.1720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/4339]: training_loss: tensor(0.4903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/4339]: training_loss: tensor(0.3395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/4339]: training_loss: tensor(0.1375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/4339]: training_loss: tensor(0.1372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/4339]: training_loss: tensor(0.1383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/4339]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/4339]: training_loss: tensor(0.3194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/4339]: training_loss: tensor(0.1360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/4339]: training_loss: tensor(0.2711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/4339]: training_loss: tensor(0.2875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/4339]: training_loss: tensor(0.4068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/4339]: training_loss: tensor(0.1181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/4339]: training_loss: tensor(0.1173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/4339]: training_loss: tensor(0.3781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1040/4339]: training_loss: tensor(0.2044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/4339]: training_loss: tensor(0.3476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/4339]: training_loss: tensor(0.3447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1043/4339]: training_loss: tensor(0.1809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/4339]: training_loss: tensor(0.1456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/4339]: training_loss: tensor(0.2356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/4339]: training_loss: tensor(0.1540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/4339]: training_loss: tensor(0.2027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/4339]: training_loss: tensor(0.2985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/4339]: training_loss: tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/4339]: training_loss: tensor(0.2541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/4339]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/4339]: training_loss: tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/4339]: training_loss: tensor(0.2264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/4339]: training_loss: tensor(0.4756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/4339]: training_loss: tensor(0.2932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/4339]: training_loss: tensor(0.3197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/4339]: training_loss: tensor(0.1269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/4339]: training_loss: tensor(0.1817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/4339]: training_loss: tensor(0.1374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/4339]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/4339]: training_loss: tensor(0.1693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/4339]: training_loss: tensor(0.1522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/4339]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/4339]: training_loss: tensor(0.4592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/4339]: training_loss: tensor(0.2519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/4339]: training_loss: tensor(0.3464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/4339]: training_loss: tensor(0.5066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/4339]: training_loss: tensor(0.2479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/4339]: training_loss: tensor(0.2271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/4339]: training_loss: tensor(0.3668, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1071/4339]: training_loss: tensor(0.2762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/4339]: training_loss: tensor(0.4421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/4339]: training_loss: tensor(0.1095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/4339]: training_loss: tensor(0.2097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/4339]: training_loss: tensor(0.1688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/4339]: training_loss: tensor(0.2439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/4339]: training_loss: tensor(0.2520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/4339]: training_loss: tensor(0.1869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/4339]: training_loss: tensor(0.5948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/4339]: training_loss: tensor(0.1397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/4339]: training_loss: tensor(0.2732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/4339]: training_loss: tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/4339]: training_loss: tensor(0.1699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/4339]: training_loss: tensor(0.2422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/4339]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/4339]: training_loss: tensor(0.3507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/4339]: training_loss: tensor(0.1173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/4339]: training_loss: tensor(0.3067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/4339]: training_loss: tensor(0.3453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/4339]: training_loss: tensor(0.2452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/4339]: training_loss: tensor(0.3638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/4339]: training_loss: tensor(0.2155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/4339]: training_loss: tensor(0.2248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/4339]: training_loss: tensor(0.4191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/4339]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/4339]: training_loss: tensor(0.1941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/4339]: training_loss: tensor(0.3217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/4339]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/4339]: training_loss: tensor(0.2003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1100/4339]: training_loss: tensor(0.1188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/4339]: training_loss: tensor(0.2953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/4339]: training_loss: tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/4339]: training_loss: tensor(0.4293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/4339]: training_loss: tensor(0.2205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/4339]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/4339]: training_loss: tensor(0.2726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/4339]: training_loss: tensor(0.2202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/4339]: training_loss: tensor(0.4171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/4339]: training_loss: tensor(0.2826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/4339]: training_loss: tensor(0.3247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/4339]: training_loss: tensor(0.2046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/4339]: training_loss: tensor(0.1306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/4339]: training_loss: tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/4339]: training_loss: tensor(0.2825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/4339]: training_loss: tensor(0.3451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/4339]: training_loss: tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/4339]: training_loss: tensor(0.1970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/4339]: training_loss: tensor(0.4511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/4339]: training_loss: tensor(0.2030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/4339]: training_loss: tensor(0.1256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/4339]: training_loss: tensor(0.1820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/4339]: training_loss: tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/4339]: training_loss: tensor(0.2596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/4339]: training_loss: tensor(0.3026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/4339]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/4339]: training_loss: tensor(0.2932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/4339]: training_loss: tensor(0.5317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/4339]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1129/4339]: training_loss: tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/4339]: training_loss: tensor(0.1935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/4339]: training_loss: tensor(0.5564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/4339]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/4339]: training_loss: tensor(0.2911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/4339]: training_loss: tensor(0.1825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/4339]: training_loss: tensor(0.2135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/4339]: training_loss: tensor(0.2157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/4339]: training_loss: tensor(0.3366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/4339]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/4339]: training_loss: tensor(0.2856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/4339]: training_loss: tensor(0.3152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/4339]: training_loss: tensor(0.1502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/4339]: training_loss: tensor(0.1577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/4339]: training_loss: tensor(0.1241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/4339]: training_loss: tensor(0.1925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/4339]: training_loss: tensor(0.2142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/4339]: training_loss: tensor(0.4517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/4339]: training_loss: tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/4339]: training_loss: tensor(0.1608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/4339]: training_loss: tensor(0.3087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/4339]: training_loss: tensor(0.1083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/4339]: training_loss: tensor(0.3007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/4339]: training_loss: tensor(0.1769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/4339]: training_loss: tensor(0.2718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/4339]: training_loss: tensor(0.3174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/4339]: training_loss: tensor(0.1933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/4339]: training_loss: tensor(0.1892, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1157/4339]: training_loss: tensor(0.2543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/4339]: training_loss: tensor(0.3159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/4339]: training_loss: tensor(0.2805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/4339]: training_loss: tensor(0.2500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/4339]: training_loss: tensor(0.4667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/4339]: training_loss: tensor(0.3080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/4339]: training_loss: tensor(0.2925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/4339]: training_loss: tensor(0.2998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/4339]: training_loss: tensor(0.5711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/4339]: training_loss: tensor(0.3452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/4339]: training_loss: tensor(0.2019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/4339]: training_loss: tensor(0.1555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/4339]: training_loss: tensor(0.2047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/4339]: training_loss: tensor(0.1364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/4339]: training_loss: tensor(0.3402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/4339]: training_loss: tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/4339]: training_loss: tensor(0.3206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/4339]: training_loss: tensor(0.3588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/4339]: training_loss: tensor(0.1251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/4339]: training_loss: tensor(0.3859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/4339]: training_loss: tensor(0.1785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/4339]: training_loss: tensor(0.1502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/4339]: training_loss: tensor(0.2517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/4339]: training_loss: tensor(0.2607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/4339]: training_loss: tensor(0.1369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/4339]: training_loss: tensor(0.1412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/4339]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/4339]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/4339]: training_loss: tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/4339]: training_loss: tensor(0.2319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/4339]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/4339]: training_loss: tensor(0.3867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/4339]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/4339]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/4339]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/4339]: training_loss: tensor(0.3094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/4339]: training_loss: tensor(0.0365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/4339]: training_loss: tensor(0.2324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/4339]: training_loss: tensor(0.1734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/4339]: training_loss: tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/4339]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/4339]: training_loss: tensor(0.1297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/4339]: training_loss: tensor(0.1146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/4339]: training_loss: tensor(0.1713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/4339]: training_loss: tensor(0.1484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/4339]: training_loss: tensor(0.1498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/4339]: training_loss: tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/4339]: training_loss: tensor(0.2579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/4339]: training_loss: tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/4339]: training_loss: tensor(0.2039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/4339]: training_loss: tensor(0.6392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/4339]: training_loss: tensor(0.3215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/4339]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/4339]: training_loss: tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/4339]: training_loss: tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/4339]: training_loss: tensor(0.1475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1213/4339]: training_loss: tensor(0.2188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/4339]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1215/4339]: training_loss: tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/4339]: training_loss: tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/4339]: training_loss: tensor(0.2295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/4339]: training_loss: tensor(0.1157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/4339]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/4339]: training_loss: tensor(0.2023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/4339]: training_loss: tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/4339]: training_loss: tensor(0.5187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/4339]: training_loss: tensor(0.3182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/4339]: training_loss: tensor(0.4568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/4339]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/4339]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/4339]: training_loss: tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/4339]: training_loss: tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/4339]: training_loss: tensor(0.2087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/4339]: training_loss: tensor(0.2032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/4339]: training_loss: tensor(0.2019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/4339]: training_loss: tensor(0.1226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/4339]: training_loss: tensor(0.3651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/4339]: training_loss: tensor(0.2229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/4339]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/4339]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/4339]: training_loss: tensor(0.1362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/4339]: training_loss: tensor(0.4252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/4339]: training_loss: tensor(0.2856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/4339]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/4339]: training_loss: tensor(0.3800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/4339]: training_loss: tensor(0.2328, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1243/4339]: training_loss: tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/4339]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/4339]: training_loss: tensor(0.1869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/4339]: training_loss: tensor(0.1367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/4339]: training_loss: tensor(0.1390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/4339]: training_loss: tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/4339]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/4339]: training_loss: tensor(0.2382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/4339]: training_loss: tensor(0.2532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/4339]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/4339]: training_loss: tensor(0.5929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/4339]: training_loss: tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/4339]: training_loss: tensor(0.1064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/4339]: training_loss: tensor(0.3609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/4339]: training_loss: tensor(0.2115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/4339]: training_loss: tensor(0.1484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/4339]: training_loss: tensor(0.3400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/4339]: training_loss: tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/4339]: training_loss: tensor(0.5606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/4339]: training_loss: tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/4339]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/4339]: training_loss: tensor(0.1499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/4339]: training_loss: tensor(0.2771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/4339]: training_loss: tensor(0.3484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/4339]: training_loss: tensor(0.2053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/4339]: training_loss: tensor(0.2623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/4339]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/4339]: training_loss: tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/4339]: training_loss: tensor(0.1426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/4339]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/4339]: training_loss: tensor(0.1281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/4339]: training_loss: tensor(0.1485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/4339]: training_loss: tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/4339]: training_loss: tensor(0.1172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/4339]: training_loss: tensor(0.1263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/4339]: training_loss: tensor(0.1629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/4339]: training_loss: tensor(0.2424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/4339]: training_loss: tensor(0.1313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/4339]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/4339]: training_loss: tensor(0.1961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/4339]: training_loss: tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/4339]: training_loss: tensor(0.1686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/4339]: training_loss: tensor(0.3487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/4339]: training_loss: tensor(0.0427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/4339]: training_loss: tensor(0.3112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/4339]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/4339]: training_loss: tensor(0.1634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/4339]: training_loss: tensor(0.2957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/4339]: training_loss: tensor(0.1217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/4339]: training_loss: tensor(0.1259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/4339]: training_loss: tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/4339]: training_loss: tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/4339]: training_loss: tensor(0.1844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/4339]: training_loss: tensor(0.2049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/4339]: training_loss: tensor(0.2910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/4339]: training_loss: tensor(0.2062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1299/4339]: training_loss: tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/4339]: training_loss: tensor(0.3968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/4339]: training_loss: tensor(0.2838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/4339]: training_loss: tensor(0.5636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/4339]: training_loss: tensor(0.1571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/4339]: training_loss: tensor(0.2778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/4339]: training_loss: tensor(0.3851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/4339]: training_loss: tensor(0.3607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/4339]: training_loss: tensor(0.2104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/4339]: training_loss: tensor(0.6764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/4339]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/4339]: training_loss: tensor(0.1837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/4339]: training_loss: tensor(0.2118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/4339]: training_loss: tensor(0.3724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/4339]: training_loss: tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/4339]: training_loss: tensor(0.3633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/4339]: training_loss: tensor(0.1935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/4339]: training_loss: tensor(0.4176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/4339]: training_loss: tensor(0.1298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/4339]: training_loss: tensor(0.2378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/4339]: training_loss: tensor(0.1816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/4339]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/4339]: training_loss: tensor(0.3095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/4339]: training_loss: tensor(0.1117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/4339]: training_loss: tensor(0.2110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/4339]: training_loss: tensor(0.1236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/4339]: training_loss: tensor(0.2341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/4339]: training_loss: tensor(0.1558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/4339]: training_loss: tensor(0.2526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/4339]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1329/4339]: training_loss: tensor(0.3607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/4339]: training_loss: tensor(0.1110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/4339]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/4339]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/4339]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/4339]: training_loss: tensor(0.0804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/4339]: training_loss: tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/4339]: training_loss: tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/4339]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/4339]: training_loss: tensor(0.3201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/4339]: training_loss: tensor(0.1738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/4339]: training_loss: tensor(0.2065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/4339]: training_loss: tensor(0.2781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/4339]: training_loss: tensor(0.2375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/4339]: training_loss: tensor(0.4442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/4339]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/4339]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/4339]: training_loss: tensor(0.1694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/4339]: training_loss: tensor(0.1498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/4339]: training_loss: tensor(0.1738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/4339]: training_loss: tensor(0.4649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/4339]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/4339]: training_loss: tensor(0.2431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/4339]: training_loss: tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/4339]: training_loss: tensor(0.4143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/4339]: training_loss: tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/4339]: training_loss: tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/4339]: training_loss: tensor(0.2944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/4339]: training_loss: tensor(0.3834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/4339]: training_loss: tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/4339]: training_loss: tensor(0.2829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/4339]: training_loss: tensor(0.1842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1361/4339]: training_loss: tensor(0.2026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/4339]: training_loss: tensor(0.3188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1363/4339]: training_loss: tensor(0.2158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/4339]: training_loss: tensor(0.1189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/4339]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/4339]: training_loss: tensor(0.5365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/4339]: training_loss: tensor(0.1101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/4339]: training_loss: tensor(0.5230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/4339]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/4339]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/4339]: training_loss: tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/4339]: training_loss: tensor(0.3805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/4339]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/4339]: training_loss: tensor(0.2170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/4339]: training_loss: tensor(0.1257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/4339]: training_loss: tensor(0.1765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/4339]: training_loss: tensor(0.2768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/4339]: training_loss: tensor(0.3069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/4339]: training_loss: tensor(0.2585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/4339]: training_loss: tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/4339]: training_loss: tensor(0.3976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/4339]: training_loss: tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/4339]: training_loss: tensor(0.5321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/4339]: training_loss: tensor(0.1620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1385/4339]: training_loss: tensor(0.2403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1386/4339]: training_loss: tensor(0.1286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1387/4339]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1388/4339]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1389/4339]: training_loss: tensor(0.2296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1390/4339]: training_loss: tensor(0.2829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1391/4339]: training_loss: tensor(0.1713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1392/4339]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1393/4339]: training_loss: tensor(0.1571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1394/4339]: training_loss: tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1395/4339]: training_loss: tensor(0.0686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1396/4339]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1397/4339]: training_loss: tensor(0.1891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1398/4339]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1399/4339]: training_loss: tensor(0.4115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1400/4339]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1401/4339]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1402/4339]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1403/4339]: training_loss: tensor(0.3171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1404/4339]: training_loss: tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1405/4339]: training_loss: tensor(0.0637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1406/4339]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1407/4339]: training_loss: tensor(0.4352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1408/4339]: training_loss: tensor(0.1095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1409/4339]: training_loss: tensor(0.4426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1410/4339]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1411/4339]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1412/4339]: training_loss: tensor(0.2132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1413/4339]: training_loss: tensor(0.5913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1414/4339]: training_loss: tensor(0.6950, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1415/4339]: training_loss: tensor(0.1724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1416/4339]: training_loss: tensor(0.3168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1417/4339]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1418/4339]: training_loss: tensor(0.3883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1419/4339]: training_loss: tensor(0.1428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1420/4339]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1421/4339]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1422/4339]: training_loss: tensor(0.1780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1423/4339]: training_loss: tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1424/4339]: training_loss: tensor(0.3614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1425/4339]: training_loss: tensor(0.1497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1426/4339]: training_loss: tensor(0.2940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1427/4339]: training_loss: tensor(0.3906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1428/4339]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1429/4339]: training_loss: tensor(0.1854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1430/4339]: training_loss: tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1431/4339]: training_loss: tensor(0.3391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1432/4339]: training_loss: tensor(0.2891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1433/4339]: training_loss: tensor(0.1261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1434/4339]: training_loss: tensor(0.1236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1435/4339]: training_loss: tensor(0.2486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1436/4339]: training_loss: tensor(0.1777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1437/4339]: training_loss: tensor(0.2145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1438/4339]: training_loss: tensor(0.1364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1439/4339]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1440/4339]: training_loss: tensor(0.4367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1441/4339]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1442/4339]: training_loss: tensor(0.2821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1443/4339]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1444/4339]: training_loss: tensor(0.2567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1445/4339]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1446/4339]: training_loss: tensor(0.1833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1447/4339]: training_loss: tensor(0.2357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1448/4339]: training_loss: tensor(0.2522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1449/4339]: training_loss: tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1450/4339]: training_loss: tensor(0.1604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1451/4339]: training_loss: tensor(0.1560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1452/4339]: training_loss: tensor(0.4254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1453/4339]: training_loss: tensor(0.1331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1454/4339]: training_loss: tensor(0.3059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1455/4339]: training_loss: tensor(0.2416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1456/4339]: training_loss: tensor(0.2554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1457/4339]: training_loss: tensor(0.1364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1458/4339]: training_loss: tensor(0.2769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1459/4339]: training_loss: tensor(0.3728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1460/4339]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1461/4339]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1462/4339]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1463/4339]: training_loss: tensor(0.1390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1464/4339]: training_loss: tensor(0.3914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1465/4339]: training_loss: tensor(0.1106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1466/4339]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1467/4339]: training_loss: tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1468/4339]: training_loss: tensor(0.3152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1469/4339]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1470/4339]: training_loss: tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1471/4339]: training_loss: tensor(0.1805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1472/4339]: training_loss: tensor(0.3727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1473/4339]: training_loss: tensor(0.2642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1474/4339]: training_loss: tensor(0.1545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1475/4339]: training_loss: tensor(0.4119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1476/4339]: training_loss: tensor(0.1975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1477/4339]: training_loss: tensor(0.3166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1478/4339]: training_loss: tensor(0.2692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1479/4339]: training_loss: tensor(0.3437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1480/4339]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1481/4339]: training_loss: tensor(0.1108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1482/4339]: training_loss: tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1483/4339]: training_loss: tensor(0.1287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1484/4339]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1485/4339]: training_loss: tensor(0.3362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1486/4339]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1487/4339]: training_loss: tensor(0.2546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1488/4339]: training_loss: tensor(0.3150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1489/4339]: training_loss: tensor(0.2459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1490/4339]: training_loss: tensor(0.1143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1491/4339]: training_loss: tensor(0.1698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1492/4339]: training_loss: tensor(0.2648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1493/4339]: training_loss: tensor(0.2422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1494/4339]: training_loss: tensor(0.1728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1495/4339]: training_loss: tensor(0.1746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1496/4339]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1497/4339]: training_loss: tensor(0.1489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1498/4339]: training_loss: tensor(0.1404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1499/4339]: training_loss: tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1500/4339]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1501/4339]: training_loss: tensor(0.1990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1502/4339]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1503/4339]: training_loss: tensor(0.3010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1504/4339]: training_loss: tensor(0.2057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1505/4339]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1506/4339]: training_loss: tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1507/4339]: training_loss: tensor(0.1385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1508/4339]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1509/4339]: training_loss: tensor(0.5164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1510/4339]: training_loss: tensor(0.3221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1511/4339]: training_loss: tensor(0.1353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1512/4339]: training_loss: tensor(0.3770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1513/4339]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1514/4339]: training_loss: tensor(0.2011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1515/4339]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1516/4339]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1517/4339]: training_loss: tensor(0.2217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1518/4339]: training_loss: tensor(0.3094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1519/4339]: training_loss: tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1520/4339]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1521/4339]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1522/4339]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1523/4339]: training_loss: tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1524/4339]: training_loss: tensor(0.3160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1525/4339]: training_loss: tensor(0.6470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1526/4339]: training_loss: tensor(0.1657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1527/4339]: training_loss: tensor(0.2468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1528/4339]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1529/4339]: training_loss: tensor(0.1627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1530/4339]: training_loss: tensor(0.4311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1531/4339]: training_loss: tensor(0.2367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1532/4339]: training_loss: tensor(0.0468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1533/4339]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1534/4339]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1535/4339]: training_loss: tensor(0.3684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1536/4339]: training_loss: tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1537/4339]: training_loss: tensor(0.3674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1538/4339]: training_loss: tensor(0.2030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1539/4339]: training_loss: tensor(0.6533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1540/4339]: training_loss: tensor(0.1414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1541/4339]: training_loss: tensor(0.1657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1542/4339]: training_loss: tensor(0.1893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1543/4339]: training_loss: tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1544/4339]: training_loss: tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1545/4339]: training_loss: tensor(0.2562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1546/4339]: training_loss: tensor(0.2656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1547/4339]: training_loss: tensor(0.2751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1548/4339]: training_loss: tensor(0.2423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1549/4339]: training_loss: tensor(0.2589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1550/4339]: training_loss: tensor(0.3130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1551/4339]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1552/4339]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1553/4339]: training_loss: tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1554/4339]: training_loss: tensor(0.4121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1555/4339]: training_loss: tensor(0.2159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1556/4339]: training_loss: tensor(0.3985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1557/4339]: training_loss: tensor(0.3832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1558/4339]: training_loss: tensor(0.2060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1559/4339]: training_loss: tensor(0.2835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1560/4339]: training_loss: tensor(0.4936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1561/4339]: training_loss: tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1562/4339]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1563/4339]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1564/4339]: training_loss: tensor(0.1680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1565/4339]: training_loss: tensor(0.1508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1566/4339]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1567/4339]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1568/4339]: training_loss: tensor(0.1316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1569/4339]: training_loss: tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1570/4339]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1571/4339]: training_loss: tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1572/4339]: training_loss: tensor(0.2210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1573/4339]: training_loss: tensor(0.5099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1574/4339]: training_loss: tensor(0.1264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1575/4339]: training_loss: tensor(0.3129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1576/4339]: training_loss: tensor(0.1196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1577/4339]: training_loss: tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1578/4339]: training_loss: tensor(0.5123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1579/4339]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1580/4339]: training_loss: tensor(0.3106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1581/4339]: training_loss: tensor(0.2323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1582/4339]: training_loss: tensor(0.3178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1583/4339]: training_loss: tensor(0.1180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1584/4339]: training_loss: tensor(0.1168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1585/4339]: training_loss: tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1586/4339]: training_loss: tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1587/4339]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1588/4339]: training_loss: tensor(0.1820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1589/4339]: training_loss: tensor(0.2048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1590/4339]: training_loss: tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1591/4339]: training_loss: tensor(0.1787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1592/4339]: training_loss: tensor(0.1125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1593/4339]: training_loss: tensor(0.2813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1594/4339]: training_loss: tensor(0.2595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1595/4339]: training_loss: tensor(0.4966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1596/4339]: training_loss: tensor(0.1947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1597/4339]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1598/4339]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1599/4339]: training_loss: tensor(0.1325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1600/4339]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1601/4339]: training_loss: tensor(0.1684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1602/4339]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1603/4339]: training_loss: tensor(0.1098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1604/4339]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1605/4339]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1606/4339]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1607/4339]: training_loss: tensor(0.0566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1608/4339]: training_loss: tensor(0.2705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1609/4339]: training_loss: tensor(0.5148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1610/4339]: training_loss: tensor(0.1637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1611/4339]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1612/4339]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1613/4339]: training_loss: tensor(0.2362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1614/4339]: training_loss: tensor(0.4109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1615/4339]: training_loss: tensor(0.1780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1616/4339]: training_loss: tensor(0.5129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1617/4339]: training_loss: tensor(0.1874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1618/4339]: training_loss: tensor(0.1323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1619/4339]: training_loss: tensor(0.0389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1620/4339]: training_loss: tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1621/4339]: training_loss: tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1622/4339]: training_loss: tensor(0.1281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1623/4339]: training_loss: tensor(0.1106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1624/4339]: training_loss: tensor(0.2255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1625/4339]: training_loss: tensor(0.4841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1626/4339]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1627/4339]: training_loss: tensor(0.1189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1628/4339]: training_loss: tensor(0.3594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1629/4339]: training_loss: tensor(0.2580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1630/4339]: training_loss: tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1631/4339]: training_loss: tensor(0.1383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1632/4339]: training_loss: tensor(0.1803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1633/4339]: training_loss: tensor(0.4684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1634/4339]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1635/4339]: training_loss: tensor(0.1324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1636/4339]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1637/4339]: training_loss: tensor(0.3059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1638/4339]: training_loss: tensor(0.2006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1639/4339]: training_loss: tensor(0.1308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1640/4339]: training_loss: tensor(0.1400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1641/4339]: training_loss: tensor(0.1265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1642/4339]: training_loss: tensor(0.3443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1643/4339]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1644/4339]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1645/4339]: training_loss: tensor(0.0616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1646/4339]: training_loss: tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1647/4339]: training_loss: tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1648/4339]: training_loss: tensor(0.2047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1649/4339]: training_loss: tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1650/4339]: training_loss: tensor(0.1254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1651/4339]: training_loss: tensor(0.4145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1652/4339]: training_loss: tensor(0.2758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1653/4339]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1654/4339]: training_loss: tensor(0.2434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1655/4339]: training_loss: tensor(0.2613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1656/4339]: training_loss: tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1657/4339]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1658/4339]: training_loss: tensor(0.1992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1659/4339]: training_loss: tensor(0.2988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1660/4339]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1661/4339]: training_loss: tensor(0.1181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1662/4339]: training_loss: tensor(0.3827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1663/4339]: training_loss: tensor(0.3163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1664/4339]: training_loss: tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1665/4339]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1666/4339]: training_loss: tensor(0.1772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1667/4339]: training_loss: tensor(0.1967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1668/4339]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1669/4339]: training_loss: tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1670/4339]: training_loss: tensor(0.1340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1671/4339]: training_loss: tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1672/4339]: training_loss: tensor(0.2198, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1673/4339]: training_loss: tensor(0.3979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1674/4339]: training_loss: tensor(0.2136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1675/4339]: training_loss: tensor(0.2829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1676/4339]: training_loss: tensor(0.3454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1677/4339]: training_loss: tensor(0.2146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1678/4339]: training_loss: tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1679/4339]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1680/4339]: training_loss: tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1681/4339]: training_loss: tensor(0.2437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1682/4339]: training_loss: tensor(0.1814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1683/4339]: training_loss: tensor(0.1159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1684/4339]: training_loss: tensor(0.2658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1685/4339]: training_loss: tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1686/4339]: training_loss: tensor(0.1991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1687/4339]: training_loss: tensor(0.1484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1688/4339]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1689/4339]: training_loss: tensor(0.1739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1690/4339]: training_loss: tensor(0.1571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1691/4339]: training_loss: tensor(0.1520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1692/4339]: training_loss: tensor(0.7849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1693/4339]: training_loss: tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1694/4339]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1695/4339]: training_loss: tensor(0.2244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1696/4339]: training_loss: tensor(0.1202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1697/4339]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1698/4339]: training_loss: tensor(0.2300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1699/4339]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1700/4339]: training_loss: tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1701/4339]: training_loss: tensor(0.2400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1702/4339]: training_loss: tensor(0.3815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1703/4339]: training_loss: tensor(0.1809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1704/4339]: training_loss: tensor(0.3069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1705/4339]: training_loss: tensor(0.3214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1706/4339]: training_loss: tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1707/4339]: training_loss: tensor(0.1455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1708/4339]: training_loss: tensor(0.1985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1709/4339]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1710/4339]: training_loss: tensor(0.2517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1711/4339]: training_loss: tensor(0.2626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1712/4339]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1713/4339]: training_loss: tensor(0.1529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1714/4339]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1715/4339]: training_loss: tensor(0.1446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1716/4339]: training_loss: tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1717/4339]: training_loss: tensor(0.2748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1718/4339]: training_loss: tensor(0.3877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1719/4339]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1720/4339]: training_loss: tensor(0.1435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1721/4339]: training_loss: tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1722/4339]: training_loss: tensor(0.0505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1723/4339]: training_loss: tensor(0.1211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1724/4339]: training_loss: tensor(0.4452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1725/4339]: training_loss: tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1726/4339]: training_loss: tensor(0.1441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1727/4339]: training_loss: tensor(0.0502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1728/4339]: training_loss: tensor(0.4240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1729/4339]: training_loss: tensor(0.6099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1730/4339]: training_loss: tensor(0.1864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1731/4339]: training_loss: tensor(0.3926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1732/4339]: training_loss: tensor(0.1437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1733/4339]: training_loss: tensor(0.1902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1734/4339]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1735/4339]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1736/4339]: training_loss: tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1737/4339]: training_loss: tensor(0.4175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1738/4339]: training_loss: tensor(0.1652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1739/4339]: training_loss: tensor(0.1064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1740/4339]: training_loss: tensor(0.1892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1741/4339]: training_loss: tensor(0.2295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1742/4339]: training_loss: tensor(0.0587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1743/4339]: training_loss: tensor(0.2445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1744/4339]: training_loss: tensor(0.2540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1745/4339]: training_loss: tensor(0.1580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1746/4339]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1747/4339]: training_loss: tensor(0.2327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1748/4339]: training_loss: tensor(0.1610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1749/4339]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1750/4339]: training_loss: tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1751/4339]: training_loss: tensor(0.1398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1752/4339]: training_loss: tensor(0.1319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1753/4339]: training_loss: tensor(0.1521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1754/4339]: training_loss: tensor(0.3778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1755/4339]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1756/4339]: training_loss: tensor(0.2742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1757/4339]: training_loss: tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1758/4339]: training_loss: tensor(0.2118, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1759/4339]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1760/4339]: training_loss: tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1761/4339]: training_loss: tensor(0.1101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1762/4339]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1763/4339]: training_loss: tensor(0.2903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1764/4339]: training_loss: tensor(0.0383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1765/4339]: training_loss: tensor(0.3691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1766/4339]: training_loss: tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1767/4339]: training_loss: tensor(0.1845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1768/4339]: training_loss: tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1769/4339]: training_loss: tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1770/4339]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1771/4339]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1772/4339]: training_loss: tensor(0.4626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1773/4339]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1774/4339]: training_loss: tensor(0.1578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1775/4339]: training_loss: tensor(0.2586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1776/4339]: training_loss: tensor(0.1111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1777/4339]: training_loss: tensor(0.3739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1778/4339]: training_loss: tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1779/4339]: training_loss: tensor(0.4667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1780/4339]: training_loss: tensor(0.1438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1781/4339]: training_loss: tensor(0.2287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1782/4339]: training_loss: tensor(0.1147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1783/4339]: training_loss: tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1784/4339]: training_loss: tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1785/4339]: training_loss: tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1786/4339]: training_loss: tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1787/4339]: training_loss: tensor(0.1640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1788/4339]: training_loss: tensor(0.2553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1789/4339]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1790/4339]: training_loss: tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1791/4339]: training_loss: tensor(0.5141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1792/4339]: training_loss: tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1793/4339]: training_loss: tensor(0.1612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1794/4339]: training_loss: tensor(0.1589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1795/4339]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1796/4339]: training_loss: tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1797/4339]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1798/4339]: training_loss: tensor(0.3013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1799/4339]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1800/4339]: training_loss: tensor(0.2187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1801/4339]: training_loss: tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1802/4339]: training_loss: tensor(0.0480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1803/4339]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1804/4339]: training_loss: tensor(0.1736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1805/4339]: training_loss: tensor(0.5612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1806/4339]: training_loss: tensor(0.2686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1807/4339]: training_loss: tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1808/4339]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1809/4339]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1810/4339]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1811/4339]: training_loss: tensor(0.3874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1812/4339]: training_loss: tensor(0.2321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1813/4339]: training_loss: tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1814/4339]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1815/4339]: training_loss: tensor(0.6490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1816/4339]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1817/4339]: training_loss: tensor(0.3469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1818/4339]: training_loss: tensor(0.2605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1819/4339]: training_loss: tensor(0.6678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1820/4339]: training_loss: tensor(0.1347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1821/4339]: training_loss: tensor(0.1695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1822/4339]: training_loss: tensor(0.2121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1823/4339]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1824/4339]: training_loss: tensor(0.2074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1825/4339]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1826/4339]: training_loss: tensor(0.5364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1827/4339]: training_loss: tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1828/4339]: training_loss: tensor(0.3560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1829/4339]: training_loss: tensor(0.1332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1830/4339]: training_loss: tensor(0.1257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1831/4339]: training_loss: tensor(0.1155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1832/4339]: training_loss: tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1833/4339]: training_loss: tensor(0.2042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1834/4339]: training_loss: tensor(0.2245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1835/4339]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1836/4339]: training_loss: tensor(0.1920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1837/4339]: training_loss: tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1838/4339]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1839/4339]: training_loss: tensor(0.1825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1840/4339]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1841/4339]: training_loss: tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1842/4339]: training_loss: tensor(0.5563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1843/4339]: training_loss: tensor(0.3236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1844/4339]: training_loss: tensor(0.1288, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1845/4339]: training_loss: tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1846/4339]: training_loss: tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1847/4339]: training_loss: tensor(0.1564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1848/4339]: training_loss: tensor(0.2371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1849/4339]: training_loss: tensor(0.1803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1850/4339]: training_loss: tensor(0.1974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1851/4339]: training_loss: tensor(0.1318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1852/4339]: training_loss: tensor(0.4613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1853/4339]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1854/4339]: training_loss: tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1855/4339]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1856/4339]: training_loss: tensor(0.3434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1857/4339]: training_loss: tensor(0.1264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1858/4339]: training_loss: tensor(0.2493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1859/4339]: training_loss: tensor(0.1598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1860/4339]: training_loss: tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1861/4339]: training_loss: tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1862/4339]: training_loss: tensor(0.1476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1863/4339]: training_loss: tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1864/4339]: training_loss: tensor(0.1237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1865/4339]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1866/4339]: training_loss: tensor(0.0732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1867/4339]: training_loss: tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1868/4339]: training_loss: tensor(0.3407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1869/4339]: training_loss: tensor(0.3148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1870/4339]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1871/4339]: training_loss: tensor(0.1560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1872/4339]: training_loss: tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1873/4339]: training_loss: tensor(0.3644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1874/4339]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1875/4339]: training_loss: tensor(0.5187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1876/4339]: training_loss: tensor(0.2074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1877/4339]: training_loss: tensor(0.1107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1878/4339]: training_loss: tensor(0.3739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1879/4339]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1880/4339]: training_loss: tensor(0.1107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1881/4339]: training_loss: tensor(0.2645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1882/4339]: training_loss: tensor(0.2060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1883/4339]: training_loss: tensor(0.4532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1884/4339]: training_loss: tensor(0.2214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1885/4339]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1886/4339]: training_loss: tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1887/4339]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1888/4339]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1889/4339]: training_loss: tensor(0.1758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1890/4339]: training_loss: tensor(0.1353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1891/4339]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1892/4339]: training_loss: tensor(0.0575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1893/4339]: training_loss: tensor(0.1526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1894/4339]: training_loss: tensor(0.1930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1895/4339]: training_loss: tensor(0.1625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1896/4339]: training_loss: tensor(0.2584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1897/4339]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1898/4339]: training_loss: tensor(0.2068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1899/4339]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1900/4339]: training_loss: tensor(0.3241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1901/4339]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1902/4339]: training_loss: tensor(0.1482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1903/4339]: training_loss: tensor(0.1461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1904/4339]: training_loss: tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1905/4339]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1906/4339]: training_loss: tensor(0.1098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1907/4339]: training_loss: tensor(0.2637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1908/4339]: training_loss: tensor(0.3406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1909/4339]: training_loss: tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1910/4339]: training_loss: tensor(0.1242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1911/4339]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1912/4339]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1913/4339]: training_loss: tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1914/4339]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1915/4339]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1916/4339]: training_loss: tensor(0.2172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1917/4339]: training_loss: tensor(0.2620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1918/4339]: training_loss: tensor(0.2871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1919/4339]: training_loss: tensor(0.2045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1920/4339]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1921/4339]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1922/4339]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1923/4339]: training_loss: tensor(0.2971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1924/4339]: training_loss: tensor(0.1836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1925/4339]: training_loss: tensor(0.3801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1926/4339]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1927/4339]: training_loss: tensor(0.1078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1928/4339]: training_loss: tensor(0.3635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1929/4339]: training_loss: tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1930/4339]: training_loss: tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1931/4339]: training_loss: tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1932/4339]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1933/4339]: training_loss: tensor(0.1707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1934/4339]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1935/4339]: training_loss: tensor(0.2267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1936/4339]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1937/4339]: training_loss: tensor(0.5771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1938/4339]: training_loss: tensor(0.4784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1939/4339]: training_loss: tensor(0.2231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1940/4339]: training_loss: tensor(0.3355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1941/4339]: training_loss: tensor(0.1294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1942/4339]: training_loss: tensor(0.2125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1943/4339]: training_loss: tensor(0.3232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1944/4339]: training_loss: tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1945/4339]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1946/4339]: training_loss: tensor(0.0427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1947/4339]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1948/4339]: training_loss: tensor(0.3078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1949/4339]: training_loss: tensor(0.3665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1950/4339]: training_loss: tensor(0.2158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1951/4339]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1952/4339]: training_loss: tensor(0.1256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1953/4339]: training_loss: tensor(0.2115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1954/4339]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1955/4339]: training_loss: tensor(0.2638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1956/4339]: training_loss: tensor(0.1256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1957/4339]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1958/4339]: training_loss: tensor(0.2530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1959/4339]: training_loss: tensor(0.1101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1960/4339]: training_loss: tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1961/4339]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1962/4339]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1963/4339]: training_loss: tensor(0.1879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1964/4339]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1965/4339]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1966/4339]: training_loss: tensor(0.4747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1967/4339]: training_loss: tensor(0.3491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1968/4339]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1969/4339]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1970/4339]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1971/4339]: training_loss: tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1972/4339]: training_loss: tensor(0.4806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1973/4339]: training_loss: tensor(0.1769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1974/4339]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1975/4339]: training_loss: tensor(0.1521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1976/4339]: training_loss: tensor(0.1885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1977/4339]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1978/4339]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1979/4339]: training_loss: tensor(0.1802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1980/4339]: training_loss: tensor(0.1146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1981/4339]: training_loss: tensor(0.3450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1982/4339]: training_loss: tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1983/4339]: training_loss: tensor(0.1147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1984/4339]: training_loss: tensor(0.1791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1985/4339]: training_loss: tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1986/4339]: training_loss: tensor(0.1421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1987/4339]: training_loss: tensor(0.1964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1988/4339]: training_loss: tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1989/4339]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1990/4339]: training_loss: tensor(0.1105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1991/4339]: training_loss: tensor(0.1360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1992/4339]: training_loss: tensor(0.1823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1993/4339]: training_loss: tensor(0.2071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1994/4339]: training_loss: tensor(0.1064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1995/4339]: training_loss: tensor(0.5387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1996/4339]: training_loss: tensor(0.1242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1997/4339]: training_loss: tensor(0.2278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1998/4339]: training_loss: tensor(0.1560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1999/4339]: training_loss: tensor(0.2864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2000/4339]: training_loss: tensor(0.1273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2001/4339]: training_loss: tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2002/4339]: training_loss: tensor(0.2360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2003/4339]: training_loss: tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2004/4339]: training_loss: tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2005/4339]: training_loss: tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2006/4339]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2007/4339]: training_loss: tensor(0.1801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2008/4339]: training_loss: tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2009/4339]: training_loss: tensor(0.1279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2010/4339]: training_loss: tensor(0.2228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2011/4339]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2012/4339]: training_loss: tensor(0.3892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2013/4339]: training_loss: tensor(0.0683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2014/4339]: training_loss: tensor(0.1218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2015/4339]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2016/4339]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2017/4339]: training_loss: tensor(0.2614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2018/4339]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2019/4339]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2020/4339]: training_loss: tensor(0.2686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2021/4339]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2022/4339]: training_loss: tensor(0.2653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2023/4339]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2024/4339]: training_loss: tensor(0.3763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2025/4339]: training_loss: tensor(0.4577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2026/4339]: training_loss: tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2027/4339]: training_loss: tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2028/4339]: training_loss: tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2029/4339]: training_loss: tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2030/4339]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2031/4339]: training_loss: tensor(0.3108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2032/4339]: training_loss: tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2033/4339]: training_loss: tensor(0.0456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2034/4339]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2035/4339]: training_loss: tensor(0.1633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2036/4339]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2037/4339]: training_loss: tensor(0.2277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2038/4339]: training_loss: tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2039/4339]: training_loss: tensor(0.1434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2040/4339]: training_loss: tensor(0.2749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2041/4339]: training_loss: tensor(0.3117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2042/4339]: training_loss: tensor(0.3077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2043/4339]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2044/4339]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2045/4339]: training_loss: tensor(0.1273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2046/4339]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2047/4339]: training_loss: tensor(0.2102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2048/4339]: training_loss: tensor(0.1189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2049/4339]: training_loss: tensor(0.1118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2050/4339]: training_loss: tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2051/4339]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2052/4339]: training_loss: tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2053/4339]: training_loss: tensor(0.2190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2054/4339]: training_loss: tensor(0.1695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2055/4339]: training_loss: tensor(0.1388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2056/4339]: training_loss: tensor(0.4333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2057/4339]: training_loss: tensor(0.1677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2058/4339]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2059/4339]: training_loss: tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2060/4339]: training_loss: tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2061/4339]: training_loss: tensor(0.1817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2062/4339]: training_loss: tensor(0.1519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2063/4339]: training_loss: tensor(0.2973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2064/4339]: training_loss: tensor(0.2402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2065/4339]: training_loss: tensor(0.2812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2066/4339]: training_loss: tensor(0.2185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2067/4339]: training_loss: tensor(0.0650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2068/4339]: training_loss: tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2069/4339]: training_loss: tensor(0.0476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2070/4339]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2071/4339]: training_loss: tensor(0.1633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2072/4339]: training_loss: tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2073/4339]: training_loss: tensor(0.1674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2074/4339]: training_loss: tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2075/4339]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2076/4339]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2077/4339]: training_loss: tensor(0.3555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2078/4339]: training_loss: tensor(0.0384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2079/4339]: training_loss: tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2080/4339]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2081/4339]: training_loss: tensor(0.4046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2082/4339]: training_loss: tensor(0.7004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2083/4339]: training_loss: tensor(0.4009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2084/4339]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2085/4339]: training_loss: tensor(0.1186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2086/4339]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2087/4339]: training_loss: tensor(0.1671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2088/4339]: training_loss: tensor(0.1997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2089/4339]: training_loss: tensor(0.3697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2090/4339]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2091/4339]: training_loss: tensor(0.0603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2092/4339]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2093/4339]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2094/4339]: training_loss: tensor(0.2486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2095/4339]: training_loss: tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2096/4339]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2097/4339]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2098/4339]: training_loss: tensor(0.1068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2099/4339]: training_loss: tensor(0.1300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2100/4339]: training_loss: tensor(0.4023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2101/4339]: training_loss: tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2102/4339]: training_loss: tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2103/4339]: training_loss: tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2104/4339]: training_loss: tensor(0.2125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2105/4339]: training_loss: tensor(0.1801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2106/4339]: training_loss: tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2107/4339]: training_loss: tensor(0.2501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2108/4339]: training_loss: tensor(0.3242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2109/4339]: training_loss: tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2110/4339]: training_loss: tensor(0.1689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2111/4339]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2112/4339]: training_loss: tensor(0.2109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2113/4339]: training_loss: tensor(0.1881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2114/4339]: training_loss: tensor(0.2015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2115/4339]: training_loss: tensor(0.2143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2116/4339]: training_loss: tensor(0.4368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2117/4339]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2118/4339]: training_loss: tensor(0.2160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2119/4339]: training_loss: tensor(0.2263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2120/4339]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2121/4339]: training_loss: tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2122/4339]: training_loss: tensor(0.1474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2123/4339]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2124/4339]: training_loss: tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2125/4339]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2126/4339]: training_loss: tensor(0.1204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2127/4339]: training_loss: tensor(0.3863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2128/4339]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2129/4339]: training_loss: tensor(0.1226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2130/4339]: training_loss: tensor(0.2333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2131/4339]: training_loss: tensor(0.0629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2132/4339]: training_loss: tensor(0.2645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2133/4339]: training_loss: tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2134/4339]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2135/4339]: training_loss: tensor(0.1620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2136/4339]: training_loss: tensor(0.4730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2137/4339]: training_loss: tensor(0.2403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2138/4339]: training_loss: tensor(0.3041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2139/4339]: training_loss: tensor(0.0423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2140/4339]: training_loss: tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2141/4339]: training_loss: tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2142/4339]: training_loss: tensor(0.2929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2143/4339]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2144/4339]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2145/4339]: training_loss: tensor(0.2146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2146/4339]: training_loss: tensor(0.2424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2147/4339]: training_loss: tensor(0.2362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2148/4339]: training_loss: tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2149/4339]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2150/4339]: training_loss: tensor(0.1716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2151/4339]: training_loss: tensor(0.2150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2152/4339]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2153/4339]: training_loss: tensor(0.8752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2154/4339]: training_loss: tensor(0.3917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2155/4339]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2156/4339]: training_loss: tensor(0.0475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2157/4339]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2158/4339]: training_loss: tensor(0.2208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2159/4339]: training_loss: tensor(0.1743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2160/4339]: training_loss: tensor(0.2742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2161/4339]: training_loss: tensor(0.3665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2162/4339]: training_loss: tensor(0.1195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2163/4339]: training_loss: tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2164/4339]: training_loss: tensor(0.6416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2165/4339]: training_loss: tensor(0.2325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2166/4339]: training_loss: tensor(0.2601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2167/4339]: training_loss: tensor(0.2143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2168/4339]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2169/4339]: training_loss: tensor(0.1142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2170/4339]: training_loss: tensor(0.0987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [3/5], Step [10850/21700], Train Loss: 0.1902, Valid Loss: 0.4168\n",
      "batch_no [2171/4339]: training_loss: tensor(0.1411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2172/4339]: training_loss: tensor(0.2499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2173/4339]: training_loss: tensor(0.3094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2174/4339]: training_loss: tensor(0.1382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2175/4339]: training_loss: tensor(0.2374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2176/4339]: training_loss: tensor(0.2298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2177/4339]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2178/4339]: training_loss: tensor(0.3530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2179/4339]: training_loss: tensor(0.1152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2180/4339]: training_loss: tensor(0.2576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2181/4339]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2182/4339]: training_loss: tensor(0.0459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2183/4339]: training_loss: tensor(0.4724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2184/4339]: training_loss: tensor(0.3424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2185/4339]: training_loss: tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2186/4339]: training_loss: tensor(0.1925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2187/4339]: training_loss: tensor(0.2066, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2188/4339]: training_loss: tensor(0.3019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2189/4339]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2190/4339]: training_loss: tensor(0.6104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2191/4339]: training_loss: tensor(0.2625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2192/4339]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2193/4339]: training_loss: tensor(0.3058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2194/4339]: training_loss: tensor(0.2744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2195/4339]: training_loss: tensor(0.1324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2196/4339]: training_loss: tensor(0.4086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2197/4339]: training_loss: tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2198/4339]: training_loss: tensor(0.2206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2199/4339]: training_loss: tensor(0.1914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2200/4339]: training_loss: tensor(0.2132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2201/4339]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2202/4339]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2203/4339]: training_loss: tensor(0.6144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2204/4339]: training_loss: tensor(0.2602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2205/4339]: training_loss: tensor(0.3716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2206/4339]: training_loss: tensor(0.1498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2207/4339]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2208/4339]: training_loss: tensor(0.1926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2209/4339]: training_loss: tensor(0.2989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2210/4339]: training_loss: tensor(0.1243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2211/4339]: training_loss: tensor(0.1408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2212/4339]: training_loss: tensor(0.2866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2213/4339]: training_loss: tensor(0.1626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2214/4339]: training_loss: tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2215/4339]: training_loss: tensor(0.3396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2216/4339]: training_loss: tensor(0.1810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2217/4339]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2218/4339]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2219/4339]: training_loss: tensor(0.4257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2220/4339]: training_loss: tensor(0.3027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2221/4339]: training_loss: tensor(0.3035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2222/4339]: training_loss: tensor(0.1090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2223/4339]: training_loss: tensor(0.3905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2224/4339]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2225/4339]: training_loss: tensor(0.1237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2226/4339]: training_loss: tensor(0.5715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2227/4339]: training_loss: tensor(0.3090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2228/4339]: training_loss: tensor(0.1398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2229/4339]: training_loss: tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2230/4339]: training_loss: tensor(0.1403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2231/4339]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2232/4339]: training_loss: tensor(0.1814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2233/4339]: training_loss: tensor(0.1806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2234/4339]: training_loss: tensor(0.2309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2235/4339]: training_loss: tensor(0.2779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2236/4339]: training_loss: tensor(0.5032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2237/4339]: training_loss: tensor(0.0383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2238/4339]: training_loss: tensor(0.2576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2239/4339]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2240/4339]: training_loss: tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2241/4339]: training_loss: tensor(0.3083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2242/4339]: training_loss: tensor(0.2811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2243/4339]: training_loss: tensor(0.1652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2244/4339]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2245/4339]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2246/4339]: training_loss: tensor(0.4098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2247/4339]: training_loss: tensor(0.2134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2248/4339]: training_loss: tensor(0.4972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2249/4339]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2250/4339]: training_loss: tensor(0.5712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2251/4339]: training_loss: tensor(0.2433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2252/4339]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2253/4339]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2254/4339]: training_loss: tensor(0.2879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2255/4339]: training_loss: tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2256/4339]: training_loss: tensor(0.2515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2257/4339]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2258/4339]: training_loss: tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2259/4339]: training_loss: tensor(0.3344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2260/4339]: training_loss: tensor(0.1887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2261/4339]: training_loss: tensor(0.3521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2262/4339]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2263/4339]: training_loss: tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2264/4339]: training_loss: tensor(0.1284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2265/4339]: training_loss: tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2266/4339]: training_loss: tensor(0.3454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2267/4339]: training_loss: tensor(0.1407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2268/4339]: training_loss: tensor(0.1620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2269/4339]: training_loss: tensor(0.1169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2270/4339]: training_loss: tensor(0.2767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2271/4339]: training_loss: tensor(0.3130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2272/4339]: training_loss: tensor(0.1421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2273/4339]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2274/4339]: training_loss: tensor(0.2433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2275/4339]: training_loss: tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2276/4339]: training_loss: tensor(0.1595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2277/4339]: training_loss: tensor(0.2000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2278/4339]: training_loss: tensor(0.1556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2279/4339]: training_loss: tensor(0.2560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2280/4339]: training_loss: tensor(0.1919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2281/4339]: training_loss: tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2282/4339]: training_loss: tensor(0.1658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2283/4339]: training_loss: tensor(0.3184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2284/4339]: training_loss: tensor(0.2182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2285/4339]: training_loss: tensor(0.1776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2286/4339]: training_loss: tensor(0.2019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2287/4339]: training_loss: tensor(0.1973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2288/4339]: training_loss: tensor(0.2309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2289/4339]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2290/4339]: training_loss: tensor(0.2316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2291/4339]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2292/4339]: training_loss: tensor(0.3056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2293/4339]: training_loss: tensor(0.1731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2294/4339]: training_loss: tensor(0.2721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2295/4339]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2296/4339]: training_loss: tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2297/4339]: training_loss: tensor(0.1610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2298/4339]: training_loss: tensor(0.1694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2299/4339]: training_loss: tensor(0.1180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2300/4339]: training_loss: tensor(0.2164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2301/4339]: training_loss: tensor(0.2584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2302/4339]: training_loss: tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2303/4339]: training_loss: tensor(0.2321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2304/4339]: training_loss: tensor(0.4468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2305/4339]: training_loss: tensor(0.2368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2306/4339]: training_loss: tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2307/4339]: training_loss: tensor(0.1334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2308/4339]: training_loss: tensor(0.3987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2309/4339]: training_loss: tensor(0.2373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2310/4339]: training_loss: tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2311/4339]: training_loss: tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2312/4339]: training_loss: tensor(0.2281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2313/4339]: training_loss: tensor(0.1720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2314/4339]: training_loss: tensor(0.2258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2315/4339]: training_loss: tensor(0.1494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2316/4339]: training_loss: tensor(0.3569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2317/4339]: training_loss: tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2318/4339]: training_loss: tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2319/4339]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2320/4339]: training_loss: tensor(0.3787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2321/4339]: training_loss: tensor(0.1845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2322/4339]: training_loss: tensor(0.2290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2323/4339]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2324/4339]: training_loss: tensor(0.4216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2325/4339]: training_loss: tensor(0.2345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2326/4339]: training_loss: tensor(0.2680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2327/4339]: training_loss: tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2328/4339]: training_loss: tensor(0.6070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2329/4339]: training_loss: tensor(0.3183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2330/4339]: training_loss: tensor(0.1268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2331/4339]: training_loss: tensor(0.2761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2332/4339]: training_loss: tensor(0.2690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2333/4339]: training_loss: tensor(0.2701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2334/4339]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2335/4339]: training_loss: tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2336/4339]: training_loss: tensor(0.4058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2337/4339]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2338/4339]: training_loss: tensor(0.4531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2339/4339]: training_loss: tensor(0.4391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2340/4339]: training_loss: tensor(0.3030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2341/4339]: training_loss: tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2342/4339]: training_loss: tensor(0.3855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2343/4339]: training_loss: tensor(0.3055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2344/4339]: training_loss: tensor(0.1530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2345/4339]: training_loss: tensor(0.2039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2346/4339]: training_loss: tensor(0.2481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2347/4339]: training_loss: tensor(0.4873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2348/4339]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2349/4339]: training_loss: tensor(0.1377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2350/4339]: training_loss: tensor(0.1300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2351/4339]: training_loss: tensor(0.1696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2352/4339]: training_loss: tensor(0.3166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2353/4339]: training_loss: tensor(0.2718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2354/4339]: training_loss: tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2355/4339]: training_loss: tensor(0.2696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2356/4339]: training_loss: tensor(0.1450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2357/4339]: training_loss: tensor(0.5108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2358/4339]: training_loss: tensor(0.3833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2359/4339]: training_loss: tensor(0.4272, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2360/4339]: training_loss: tensor(0.1177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2361/4339]: training_loss: tensor(0.1698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2362/4339]: training_loss: tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2363/4339]: training_loss: tensor(0.1757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2364/4339]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2365/4339]: training_loss: tensor(0.2227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2366/4339]: training_loss: tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2367/4339]: training_loss: tensor(0.0462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2368/4339]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2369/4339]: training_loss: tensor(0.1452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2370/4339]: training_loss: tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2371/4339]: training_loss: tensor(0.2711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2372/4339]: training_loss: tensor(0.4501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2373/4339]: training_loss: tensor(0.5128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2374/4339]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2375/4339]: training_loss: tensor(0.1619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2376/4339]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2377/4339]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2378/4339]: training_loss: tensor(0.5722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2379/4339]: training_loss: tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2380/4339]: training_loss: tensor(0.1197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2381/4339]: training_loss: tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2382/4339]: training_loss: tensor(0.2547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2383/4339]: training_loss: tensor(0.2633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2384/4339]: training_loss: tensor(0.5531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2385/4339]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2386/4339]: training_loss: tensor(0.1695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2387/4339]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2388/4339]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2389/4339]: training_loss: tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2390/4339]: training_loss: tensor(0.1107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2391/4339]: training_loss: tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2392/4339]: training_loss: tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2393/4339]: training_loss: tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2394/4339]: training_loss: tensor(0.3220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2395/4339]: training_loss: tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2396/4339]: training_loss: tensor(0.1108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2397/4339]: training_loss: tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2398/4339]: training_loss: tensor(0.2600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2399/4339]: training_loss: tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2400/4339]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2401/4339]: training_loss: tensor(0.2492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2402/4339]: training_loss: tensor(0.1297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2403/4339]: training_loss: tensor(0.2151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2404/4339]: training_loss: tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2405/4339]: training_loss: tensor(0.1328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2406/4339]: training_loss: tensor(0.1830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2407/4339]: training_loss: tensor(0.2318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2408/4339]: training_loss: tensor(0.1992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2409/4339]: training_loss: tensor(0.3012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2410/4339]: training_loss: tensor(0.1184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2411/4339]: training_loss: tensor(0.5132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2412/4339]: training_loss: tensor(0.0572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2413/4339]: training_loss: tensor(0.2632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2414/4339]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2415/4339]: training_loss: tensor(0.2168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2416/4339]: training_loss: tensor(0.1313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2417/4339]: training_loss: tensor(0.1475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2418/4339]: training_loss: tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2419/4339]: training_loss: tensor(0.1595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2420/4339]: training_loss: tensor(0.1920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2421/4339]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2422/4339]: training_loss: tensor(0.1895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2423/4339]: training_loss: tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2424/4339]: training_loss: tensor(0.3886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2425/4339]: training_loss: tensor(0.1869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2426/4339]: training_loss: tensor(0.1494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2427/4339]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2428/4339]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2429/4339]: training_loss: tensor(0.2146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2430/4339]: training_loss: tensor(0.2516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2431/4339]: training_loss: tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2432/4339]: training_loss: tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2433/4339]: training_loss: tensor(0.3115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2434/4339]: training_loss: tensor(0.2073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2435/4339]: training_loss: tensor(0.1966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2436/4339]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2437/4339]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2438/4339]: training_loss: tensor(0.2071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2439/4339]: training_loss: tensor(0.5485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2440/4339]: training_loss: tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2441/4339]: training_loss: tensor(0.2064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2442/4339]: training_loss: tensor(0.2181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2443/4339]: training_loss: tensor(0.3391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2444/4339]: training_loss: tensor(0.3361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2445/4339]: training_loss: tensor(0.4386, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2446/4339]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2447/4339]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2448/4339]: training_loss: tensor(0.1906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2449/4339]: training_loss: tensor(0.3891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2450/4339]: training_loss: tensor(0.3606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2451/4339]: training_loss: tensor(0.1753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2452/4339]: training_loss: tensor(0.1711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2453/4339]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2454/4339]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2455/4339]: training_loss: tensor(0.2324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2456/4339]: training_loss: tensor(0.2469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2457/4339]: training_loss: tensor(0.2901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2458/4339]: training_loss: tensor(0.4078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2459/4339]: training_loss: tensor(0.2847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2460/4339]: training_loss: tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2461/4339]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2462/4339]: training_loss: tensor(0.0487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2463/4339]: training_loss: tensor(0.1961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2464/4339]: training_loss: tensor(0.2179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2465/4339]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2466/4339]: training_loss: tensor(0.3070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2467/4339]: training_loss: tensor(0.5609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2468/4339]: training_loss: tensor(0.2557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2469/4339]: training_loss: tensor(0.2181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2470/4339]: training_loss: tensor(0.3362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2471/4339]: training_loss: tensor(0.2604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2472/4339]: training_loss: tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2473/4339]: training_loss: tensor(0.0436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2474/4339]: training_loss: tensor(0.3374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2475/4339]: training_loss: tensor(0.3442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2476/4339]: training_loss: tensor(0.2804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2477/4339]: training_loss: tensor(0.1391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2478/4339]: training_loss: tensor(0.2505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2479/4339]: training_loss: tensor(0.1219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2480/4339]: training_loss: tensor(0.3751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2481/4339]: training_loss: tensor(0.1552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2482/4339]: training_loss: tensor(0.2137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2483/4339]: training_loss: tensor(0.2570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2484/4339]: training_loss: tensor(0.2177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2485/4339]: training_loss: tensor(0.4824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2486/4339]: training_loss: tensor(0.2504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2487/4339]: training_loss: tensor(0.1857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2488/4339]: training_loss: tensor(0.1768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2489/4339]: training_loss: tensor(0.4070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2490/4339]: training_loss: tensor(0.2399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2491/4339]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2492/4339]: training_loss: tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2493/4339]: training_loss: tensor(0.2976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2494/4339]: training_loss: tensor(0.4084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2495/4339]: training_loss: tensor(0.2102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2496/4339]: training_loss: tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2497/4339]: training_loss: tensor(0.4530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2498/4339]: training_loss: tensor(0.4140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2499/4339]: training_loss: tensor(0.1258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2500/4339]: training_loss: tensor(0.1483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2501/4339]: training_loss: tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2502/4339]: training_loss: tensor(0.1360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2503/4339]: training_loss: tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2504/4339]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2505/4339]: training_loss: tensor(0.1240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2506/4339]: training_loss: tensor(0.1408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2507/4339]: training_loss: tensor(0.2094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2508/4339]: training_loss: tensor(0.2128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2509/4339]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2510/4339]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2511/4339]: training_loss: tensor(0.1222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2512/4339]: training_loss: tensor(0.1168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2513/4339]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2514/4339]: training_loss: tensor(0.1238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2515/4339]: training_loss: tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2516/4339]: training_loss: tensor(0.4509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2517/4339]: training_loss: tensor(0.1432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2518/4339]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2519/4339]: training_loss: tensor(0.1802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2520/4339]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2521/4339]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2522/4339]: training_loss: tensor(0.1574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2523/4339]: training_loss: tensor(0.3710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2524/4339]: training_loss: tensor(0.2071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2525/4339]: training_loss: tensor(0.2369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2526/4339]: training_loss: tensor(0.1802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2527/4339]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2528/4339]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2529/4339]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2530/4339]: training_loss: tensor(0.4527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2531/4339]: training_loss: tensor(0.2064, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2532/4339]: training_loss: tensor(0.5789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2533/4339]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2534/4339]: training_loss: tensor(0.1873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2535/4339]: training_loss: tensor(0.4090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2536/4339]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2537/4339]: training_loss: tensor(0.2022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2538/4339]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2539/4339]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2540/4339]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2541/4339]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2542/4339]: training_loss: tensor(0.3064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2543/4339]: training_loss: tensor(0.3367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2544/4339]: training_loss: tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2545/4339]: training_loss: tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2546/4339]: training_loss: tensor(0.1891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2547/4339]: training_loss: tensor(0.3552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2548/4339]: training_loss: tensor(0.1550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2549/4339]: training_loss: tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2550/4339]: training_loss: tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2551/4339]: training_loss: tensor(0.2913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2552/4339]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2553/4339]: training_loss: tensor(0.3701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2554/4339]: training_loss: tensor(0.5150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2555/4339]: training_loss: tensor(0.0592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2556/4339]: training_loss: tensor(0.1090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2557/4339]: training_loss: tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2558/4339]: training_loss: tensor(0.1588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2559/4339]: training_loss: tensor(0.2183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2560/4339]: training_loss: tensor(0.1997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2561/4339]: training_loss: tensor(0.3074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2562/4339]: training_loss: tensor(0.4446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2563/4339]: training_loss: tensor(0.0620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2564/4339]: training_loss: tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2565/4339]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2566/4339]: training_loss: tensor(0.5398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2567/4339]: training_loss: tensor(0.1734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2568/4339]: training_loss: tensor(0.2273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2569/4339]: training_loss: tensor(0.1039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2570/4339]: training_loss: tensor(0.5650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2571/4339]: training_loss: tensor(0.2221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2572/4339]: training_loss: tensor(0.1214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2573/4339]: training_loss: tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2574/4339]: training_loss: tensor(0.3402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2575/4339]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2576/4339]: training_loss: tensor(0.1932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2577/4339]: training_loss: tensor(0.1322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2578/4339]: training_loss: tensor(0.2276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2579/4339]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2580/4339]: training_loss: tensor(0.3959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2581/4339]: training_loss: tensor(0.1503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2582/4339]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2583/4339]: training_loss: tensor(0.2555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2584/4339]: training_loss: tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2585/4339]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2586/4339]: training_loss: tensor(0.5076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2587/4339]: training_loss: tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2588/4339]: training_loss: tensor(0.0683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2589/4339]: training_loss: tensor(0.5082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2590/4339]: training_loss: tensor(0.2459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2591/4339]: training_loss: tensor(0.1145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2592/4339]: training_loss: tensor(0.2690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2593/4339]: training_loss: tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2594/4339]: training_loss: tensor(0.1817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2595/4339]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2596/4339]: training_loss: tensor(0.5030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2597/4339]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2598/4339]: training_loss: tensor(0.3412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2599/4339]: training_loss: tensor(0.1701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2600/4339]: training_loss: tensor(0.4219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2601/4339]: training_loss: tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2602/4339]: training_loss: tensor(0.5151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2603/4339]: training_loss: tensor(0.2672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2604/4339]: training_loss: tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2605/4339]: training_loss: tensor(0.3637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2606/4339]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2607/4339]: training_loss: tensor(0.5015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2608/4339]: training_loss: tensor(0.2565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2609/4339]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2610/4339]: training_loss: tensor(0.3821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2611/4339]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2612/4339]: training_loss: tensor(0.1270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2613/4339]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2614/4339]: training_loss: tensor(0.5618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2615/4339]: training_loss: tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2616/4339]: training_loss: tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2617/4339]: training_loss: tensor(0.1106, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2618/4339]: training_loss: tensor(0.1574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2619/4339]: training_loss: tensor(0.2113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2620/4339]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2621/4339]: training_loss: tensor(0.1735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2622/4339]: training_loss: tensor(0.1214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2623/4339]: training_loss: tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2624/4339]: training_loss: tensor(0.4330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2625/4339]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2626/4339]: training_loss: tensor(0.2013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2627/4339]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2628/4339]: training_loss: tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2629/4339]: training_loss: tensor(0.1818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2630/4339]: training_loss: tensor(0.1857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2631/4339]: training_loss: tensor(0.2931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2632/4339]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2633/4339]: training_loss: tensor(0.1232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2634/4339]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2635/4339]: training_loss: tensor(0.1799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2636/4339]: training_loss: tensor(0.2923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2637/4339]: training_loss: tensor(0.1239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2638/4339]: training_loss: tensor(0.1496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2639/4339]: training_loss: tensor(0.2333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2640/4339]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2641/4339]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2642/4339]: training_loss: tensor(0.2433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2643/4339]: training_loss: tensor(0.1997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2644/4339]: training_loss: tensor(0.1985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2645/4339]: training_loss: tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2646/4339]: training_loss: tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2647/4339]: training_loss: tensor(0.1719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2648/4339]: training_loss: tensor(0.4175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2649/4339]: training_loss: tensor(0.2367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2650/4339]: training_loss: tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2651/4339]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2652/4339]: training_loss: tensor(0.2379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2653/4339]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2654/4339]: training_loss: tensor(0.0517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2655/4339]: training_loss: tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2656/4339]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2657/4339]: training_loss: tensor(0.2214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2658/4339]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2659/4339]: training_loss: tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2660/4339]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2661/4339]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2662/4339]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2663/4339]: training_loss: tensor(0.1427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2664/4339]: training_loss: tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2665/4339]: training_loss: tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2666/4339]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2667/4339]: training_loss: tensor(0.2290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2668/4339]: training_loss: tensor(0.1734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2669/4339]: training_loss: tensor(0.2009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2670/4339]: training_loss: tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2671/4339]: training_loss: tensor(0.4408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2672/4339]: training_loss: tensor(0.1684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2673/4339]: training_loss: tensor(0.3818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2674/4339]: training_loss: tensor(0.1746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2675/4339]: training_loss: tensor(0.2695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2676/4339]: training_loss: tensor(0.2288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2677/4339]: training_loss: tensor(0.1465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2678/4339]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2679/4339]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2680/4339]: training_loss: tensor(0.0545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2681/4339]: training_loss: tensor(0.1524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2682/4339]: training_loss: tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2683/4339]: training_loss: tensor(0.3336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2684/4339]: training_loss: tensor(0.1223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2685/4339]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2686/4339]: training_loss: tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2687/4339]: training_loss: tensor(0.1613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2688/4339]: training_loss: tensor(0.2492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2689/4339]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2690/4339]: training_loss: tensor(0.3004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2691/4339]: training_loss: tensor(0.2206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2692/4339]: training_loss: tensor(0.1555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2693/4339]: training_loss: tensor(0.3482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2694/4339]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2695/4339]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2696/4339]: training_loss: tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2697/4339]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2698/4339]: training_loss: tensor(0.5123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2699/4339]: training_loss: tensor(0.1670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2700/4339]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2701/4339]: training_loss: tensor(0.2705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2702/4339]: training_loss: tensor(0.3691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2703/4339]: training_loss: tensor(0.2797, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2704/4339]: training_loss: tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2705/4339]: training_loss: tensor(0.3809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2706/4339]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2707/4339]: training_loss: tensor(0.7626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2708/4339]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2709/4339]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2710/4339]: training_loss: tensor(0.2479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2711/4339]: training_loss: tensor(0.2212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2712/4339]: training_loss: tensor(0.3376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2713/4339]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2714/4339]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2715/4339]: training_loss: tensor(0.1526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2716/4339]: training_loss: tensor(0.1542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2717/4339]: training_loss: tensor(0.0624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2718/4339]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2719/4339]: training_loss: tensor(0.2274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2720/4339]: training_loss: tensor(0.4834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2721/4339]: training_loss: tensor(0.2701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2722/4339]: training_loss: tensor(0.2661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2723/4339]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2724/4339]: training_loss: tensor(0.1681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2725/4339]: training_loss: tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2726/4339]: training_loss: tensor(0.3228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2727/4339]: training_loss: tensor(0.2225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2728/4339]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2729/4339]: training_loss: tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2730/4339]: training_loss: tensor(0.1336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2731/4339]: training_loss: tensor(0.4314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2732/4339]: training_loss: tensor(0.1403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2733/4339]: training_loss: tensor(0.4419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2734/4339]: training_loss: tensor(0.2424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2735/4339]: training_loss: tensor(0.3628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2736/4339]: training_loss: tensor(0.4778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2737/4339]: training_loss: tensor(0.1953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2738/4339]: training_loss: tensor(0.1752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2739/4339]: training_loss: tensor(0.1476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2740/4339]: training_loss: tensor(0.1946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2741/4339]: training_loss: tensor(0.2337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2742/4339]: training_loss: tensor(0.2482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2743/4339]: training_loss: tensor(0.4158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2744/4339]: training_loss: tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2745/4339]: training_loss: tensor(0.1540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2746/4339]: training_loss: tensor(0.1269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2747/4339]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2748/4339]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2749/4339]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2750/4339]: training_loss: tensor(0.3061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2751/4339]: training_loss: tensor(0.1395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2752/4339]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2753/4339]: training_loss: tensor(0.1238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2754/4339]: training_loss: tensor(0.3650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2755/4339]: training_loss: tensor(0.1639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2756/4339]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2757/4339]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2758/4339]: training_loss: tensor(0.2063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2759/4339]: training_loss: tensor(0.3754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2760/4339]: training_loss: tensor(0.3220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2761/4339]: training_loss: tensor(0.1576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2762/4339]: training_loss: tensor(0.1451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2763/4339]: training_loss: tensor(0.2204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2764/4339]: training_loss: tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2765/4339]: training_loss: tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2766/4339]: training_loss: tensor(0.1186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2767/4339]: training_loss: tensor(0.1805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2768/4339]: training_loss: tensor(0.3586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2769/4339]: training_loss: tensor(0.1476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2770/4339]: training_loss: tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2771/4339]: training_loss: tensor(0.3148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2772/4339]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2773/4339]: training_loss: tensor(0.2277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2774/4339]: training_loss: tensor(0.4205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2775/4339]: training_loss: tensor(0.2201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2776/4339]: training_loss: tensor(0.4182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2777/4339]: training_loss: tensor(0.3236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2778/4339]: training_loss: tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2779/4339]: training_loss: tensor(0.4086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2780/4339]: training_loss: tensor(0.2478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2781/4339]: training_loss: tensor(0.2397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2782/4339]: training_loss: tensor(0.2806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2783/4339]: training_loss: tensor(0.1370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2784/4339]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2785/4339]: training_loss: tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2786/4339]: training_loss: tensor(0.2493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2787/4339]: training_loss: tensor(0.2155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2788/4339]: training_loss: tensor(0.1982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2789/4339]: training_loss: tensor(0.2447, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2790/4339]: training_loss: tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2791/4339]: training_loss: tensor(0.2384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2792/4339]: training_loss: tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2793/4339]: training_loss: tensor(0.4209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2794/4339]: training_loss: tensor(0.3107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2795/4339]: training_loss: tensor(0.2679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2796/4339]: training_loss: tensor(0.1165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2797/4339]: training_loss: tensor(0.3215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2798/4339]: training_loss: tensor(0.4575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2799/4339]: training_loss: tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2800/4339]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2801/4339]: training_loss: tensor(0.5539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2802/4339]: training_loss: tensor(0.2715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2803/4339]: training_loss: tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2804/4339]: training_loss: tensor(0.1555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2805/4339]: training_loss: tensor(0.1935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2806/4339]: training_loss: tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2807/4339]: training_loss: tensor(0.2828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2808/4339]: training_loss: tensor(0.4498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2809/4339]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2810/4339]: training_loss: tensor(0.7827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2811/4339]: training_loss: tensor(0.1952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2812/4339]: training_loss: tensor(0.3622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2813/4339]: training_loss: tensor(0.0505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2814/4339]: training_loss: tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2815/4339]: training_loss: tensor(0.2243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2816/4339]: training_loss: tensor(0.1185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2817/4339]: training_loss: tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2818/4339]: training_loss: tensor(0.2387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2819/4339]: training_loss: tensor(0.2923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2820/4339]: training_loss: tensor(0.1915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2821/4339]: training_loss: tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2822/4339]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2823/4339]: training_loss: tensor(0.2171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2824/4339]: training_loss: tensor(0.1064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2825/4339]: training_loss: tensor(0.2985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2826/4339]: training_loss: tensor(0.0429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2827/4339]: training_loss: tensor(0.3038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2828/4339]: training_loss: tensor(0.2257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2829/4339]: training_loss: tensor(0.5985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2830/4339]: training_loss: tensor(0.1144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2831/4339]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2832/4339]: training_loss: tensor(0.1455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2833/4339]: training_loss: tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2834/4339]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2835/4339]: training_loss: tensor(0.2755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2836/4339]: training_loss: tensor(0.3698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2837/4339]: training_loss: tensor(0.1483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2838/4339]: training_loss: tensor(0.1968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2839/4339]: training_loss: tensor(0.1846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2840/4339]: training_loss: tensor(0.2285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2841/4339]: training_loss: tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2842/4339]: training_loss: tensor(0.1605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2843/4339]: training_loss: tensor(0.2491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2844/4339]: training_loss: tensor(0.3743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2845/4339]: training_loss: tensor(0.1948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2846/4339]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2847/4339]: training_loss: tensor(0.2326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2848/4339]: training_loss: tensor(0.2027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2849/4339]: training_loss: tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2850/4339]: training_loss: tensor(0.1371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2851/4339]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2852/4339]: training_loss: tensor(0.3137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2853/4339]: training_loss: tensor(0.1198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2854/4339]: training_loss: tensor(0.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2855/4339]: training_loss: tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2856/4339]: training_loss: tensor(0.1520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2857/4339]: training_loss: tensor(0.4524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2858/4339]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2859/4339]: training_loss: tensor(0.3730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2860/4339]: training_loss: tensor(0.1470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2861/4339]: training_loss: tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2862/4339]: training_loss: tensor(0.4883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2863/4339]: training_loss: tensor(0.2469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2864/4339]: training_loss: tensor(0.5365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2865/4339]: training_loss: tensor(0.1963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2866/4339]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2867/4339]: training_loss: tensor(0.2926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2868/4339]: training_loss: tensor(0.1620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2869/4339]: training_loss: tensor(0.4630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2870/4339]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2871/4339]: training_loss: tensor(0.2988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2872/4339]: training_loss: tensor(0.6166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2873/4339]: training_loss: tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2874/4339]: training_loss: tensor(0.2035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2875/4339]: training_loss: tensor(0.1317, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2876/4339]: training_loss: tensor(0.1467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2877/4339]: training_loss: tensor(0.2290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2878/4339]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2879/4339]: training_loss: tensor(0.3419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2880/4339]: training_loss: tensor(0.1480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2881/4339]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2882/4339]: training_loss: tensor(0.2351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2883/4339]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2884/4339]: training_loss: tensor(0.2665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2885/4339]: training_loss: tensor(0.2060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2886/4339]: training_loss: tensor(0.1469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2887/4339]: training_loss: tensor(0.2242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2888/4339]: training_loss: tensor(0.3730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2889/4339]: training_loss: tensor(0.1564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2890/4339]: training_loss: tensor(0.3847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2891/4339]: training_loss: tensor(0.2387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2892/4339]: training_loss: tensor(0.4946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2893/4339]: training_loss: tensor(0.2307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2894/4339]: training_loss: tensor(0.2392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2895/4339]: training_loss: tensor(0.2664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2896/4339]: training_loss: tensor(0.4974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2897/4339]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2898/4339]: training_loss: tensor(0.2416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2899/4339]: training_loss: tensor(0.2079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2900/4339]: training_loss: tensor(0.3231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2901/4339]: training_loss: tensor(0.2752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2902/4339]: training_loss: tensor(0.2526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2903/4339]: training_loss: tensor(0.4183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2904/4339]: training_loss: tensor(0.2386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2905/4339]: training_loss: tensor(0.4136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2906/4339]: training_loss: tensor(0.1635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2907/4339]: training_loss: tensor(0.1593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2908/4339]: training_loss: tensor(0.2298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2909/4339]: training_loss: tensor(0.3435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2910/4339]: training_loss: tensor(0.2212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2911/4339]: training_loss: tensor(0.1400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2912/4339]: training_loss: tensor(0.2456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2913/4339]: training_loss: tensor(0.3652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2914/4339]: training_loss: tensor(0.1935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2915/4339]: training_loss: tensor(0.2674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2916/4339]: training_loss: tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2917/4339]: training_loss: tensor(0.2887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2918/4339]: training_loss: tensor(0.1747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2919/4339]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2920/4339]: training_loss: tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2921/4339]: training_loss: tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2922/4339]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2923/4339]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2924/4339]: training_loss: tensor(0.3461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2925/4339]: training_loss: tensor(0.5380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2926/4339]: training_loss: tensor(0.2411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2927/4339]: training_loss: tensor(0.2858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2928/4339]: training_loss: tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2929/4339]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2930/4339]: training_loss: tensor(0.2576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2931/4339]: training_loss: tensor(0.4063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2932/4339]: training_loss: tensor(0.2340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2933/4339]: training_loss: tensor(0.2367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2934/4339]: training_loss: tensor(0.1145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2935/4339]: training_loss: tensor(0.3775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2936/4339]: training_loss: tensor(0.1197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2937/4339]: training_loss: tensor(0.1175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2938/4339]: training_loss: tensor(0.3780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2939/4339]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2940/4339]: training_loss: tensor(0.2788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2941/4339]: training_loss: tensor(0.4238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2942/4339]: training_loss: tensor(0.1969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2943/4339]: training_loss: tensor(0.1786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2944/4339]: training_loss: tensor(0.2724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2945/4339]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2946/4339]: training_loss: tensor(0.3939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2947/4339]: training_loss: tensor(0.1656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2948/4339]: training_loss: tensor(0.2430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2949/4339]: training_loss: tensor(0.2175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2950/4339]: training_loss: tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2951/4339]: training_loss: tensor(0.1370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2952/4339]: training_loss: tensor(0.5238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2953/4339]: training_loss: tensor(0.1812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2954/4339]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2955/4339]: training_loss: tensor(0.2788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2956/4339]: training_loss: tensor(0.2318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2957/4339]: training_loss: tensor(0.1286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2958/4339]: training_loss: tensor(0.1233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2959/4339]: training_loss: tensor(0.3691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2960/4339]: training_loss: tensor(0.1556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2961/4339]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2962/4339]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2963/4339]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2964/4339]: training_loss: tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2965/4339]: training_loss: tensor(0.2207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2966/4339]: training_loss: tensor(0.2946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2967/4339]: training_loss: tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2968/4339]: training_loss: tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2969/4339]: training_loss: tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2970/4339]: training_loss: tensor(0.2809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2971/4339]: training_loss: tensor(0.3068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2972/4339]: training_loss: tensor(0.1608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2973/4339]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2974/4339]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2975/4339]: training_loss: tensor(0.2860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2976/4339]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2977/4339]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2978/4339]: training_loss: tensor(0.1213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2979/4339]: training_loss: tensor(0.1603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2980/4339]: training_loss: tensor(0.3305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2981/4339]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2982/4339]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2983/4339]: training_loss: tensor(0.4810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2984/4339]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2985/4339]: training_loss: tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2986/4339]: training_loss: tensor(0.1526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2987/4339]: training_loss: tensor(0.2745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2988/4339]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2989/4339]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2990/4339]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2991/4339]: training_loss: tensor(0.1309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2992/4339]: training_loss: tensor(0.3312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2993/4339]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2994/4339]: training_loss: tensor(0.1399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2995/4339]: training_loss: tensor(0.2256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2996/4339]: training_loss: tensor(0.2059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2997/4339]: training_loss: tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2998/4339]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2999/4339]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3000/4339]: training_loss: tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3001/4339]: training_loss: tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3002/4339]: training_loss: tensor(0.6400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3003/4339]: training_loss: tensor(0.1399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3004/4339]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3005/4339]: training_loss: tensor(0.1260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3006/4339]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3007/4339]: training_loss: tensor(0.1267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3008/4339]: training_loss: tensor(0.2358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3009/4339]: training_loss: tensor(0.2139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3010/4339]: training_loss: tensor(0.1543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3011/4339]: training_loss: tensor(0.5626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3012/4339]: training_loss: tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3013/4339]: training_loss: tensor(0.2562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3014/4339]: training_loss: tensor(0.4451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3015/4339]: training_loss: tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3016/4339]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3017/4339]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3018/4339]: training_loss: tensor(0.6339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3019/4339]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3020/4339]: training_loss: tensor(0.5327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3021/4339]: training_loss: tensor(0.5662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3022/4339]: training_loss: tensor(0.5279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3023/4339]: training_loss: tensor(0.4345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3024/4339]: training_loss: tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3025/4339]: training_loss: tensor(0.2760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3026/4339]: training_loss: tensor(0.1601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3027/4339]: training_loss: tensor(0.1785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3028/4339]: training_loss: tensor(0.4891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3029/4339]: training_loss: tensor(0.2227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3030/4339]: training_loss: tensor(0.1410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3031/4339]: training_loss: tensor(0.2018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3032/4339]: training_loss: tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3033/4339]: training_loss: tensor(0.1019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3034/4339]: training_loss: tensor(0.2626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3035/4339]: training_loss: tensor(0.2848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3036/4339]: training_loss: tensor(0.3146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3037/4339]: training_loss: tensor(0.1270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3038/4339]: training_loss: tensor(0.1696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3039/4339]: training_loss: tensor(0.3420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3040/4339]: training_loss: tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3041/4339]: training_loss: tensor(0.2411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3042/4339]: training_loss: tensor(0.1402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3043/4339]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3044/4339]: training_loss: tensor(0.1372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3045/4339]: training_loss: tensor(0.3405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3046/4339]: training_loss: tensor(0.3944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3047/4339]: training_loss: tensor(0.1863, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3048/4339]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3049/4339]: training_loss: tensor(0.5083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3050/4339]: training_loss: tensor(0.6416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3051/4339]: training_loss: tensor(0.4161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3052/4339]: training_loss: tensor(0.2601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3053/4339]: training_loss: tensor(0.3950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3054/4339]: training_loss: tensor(0.4952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3055/4339]: training_loss: tensor(0.3891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3056/4339]: training_loss: tensor(0.1270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3057/4339]: training_loss: tensor(0.1637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3058/4339]: training_loss: tensor(0.1340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3059/4339]: training_loss: tensor(0.2080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3060/4339]: training_loss: tensor(0.1450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3061/4339]: training_loss: tensor(0.4488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3062/4339]: training_loss: tensor(0.1674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3063/4339]: training_loss: tensor(0.1406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3064/4339]: training_loss: tensor(0.2194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3065/4339]: training_loss: tensor(0.1709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3066/4339]: training_loss: tensor(0.1988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3067/4339]: training_loss: tensor(0.2420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3068/4339]: training_loss: tensor(0.1768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3069/4339]: training_loss: tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3070/4339]: training_loss: tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3071/4339]: training_loss: tensor(0.2033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3072/4339]: training_loss: tensor(0.2276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3073/4339]: training_loss: tensor(0.1397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3074/4339]: training_loss: tensor(0.3024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3075/4339]: training_loss: tensor(0.1652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3076/4339]: training_loss: tensor(0.1851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3077/4339]: training_loss: tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3078/4339]: training_loss: tensor(0.1754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3079/4339]: training_loss: tensor(0.2381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3080/4339]: training_loss: tensor(0.1160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3081/4339]: training_loss: tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3082/4339]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3083/4339]: training_loss: tensor(0.1289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3084/4339]: training_loss: tensor(0.1543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3085/4339]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3086/4339]: training_loss: tensor(0.3679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3087/4339]: training_loss: tensor(0.4996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3088/4339]: training_loss: tensor(0.1815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3089/4339]: training_loss: tensor(0.7226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3090/4339]: training_loss: tensor(0.3165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3091/4339]: training_loss: tensor(0.3762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3092/4339]: training_loss: tensor(0.1509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3093/4339]: training_loss: tensor(0.2000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3094/4339]: training_loss: tensor(0.3295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3095/4339]: training_loss: tensor(0.1401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3096/4339]: training_loss: tensor(0.2504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3097/4339]: training_loss: tensor(0.2333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3098/4339]: training_loss: tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3099/4339]: training_loss: tensor(0.2044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3100/4339]: training_loss: tensor(0.1644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3101/4339]: training_loss: tensor(0.1380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3102/4339]: training_loss: tensor(0.1571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3103/4339]: training_loss: tensor(0.3220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3104/4339]: training_loss: tensor(0.1370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3105/4339]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3106/4339]: training_loss: tensor(0.2415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3107/4339]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3108/4339]: training_loss: tensor(0.2263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3109/4339]: training_loss: tensor(0.1448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3110/4339]: training_loss: tensor(0.5201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3111/4339]: training_loss: tensor(0.1451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3112/4339]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3113/4339]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3114/4339]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3115/4339]: training_loss: tensor(0.3587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3116/4339]: training_loss: tensor(0.1392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3117/4339]: training_loss: tensor(0.2683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3118/4339]: training_loss: tensor(0.2024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3119/4339]: training_loss: tensor(0.2100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3120/4339]: training_loss: tensor(0.2949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3121/4339]: training_loss: tensor(0.1413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3122/4339]: training_loss: tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3123/4339]: training_loss: tensor(0.2645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3124/4339]: training_loss: tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3125/4339]: training_loss: tensor(0.0666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3126/4339]: training_loss: tensor(0.2512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3127/4339]: training_loss: tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3128/4339]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3129/4339]: training_loss: tensor(0.4079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3130/4339]: training_loss: tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3131/4339]: training_loss: tensor(0.6089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3132/4339]: training_loss: tensor(0.2608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3133/4339]: training_loss: tensor(0.2285, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3134/4339]: training_loss: tensor(0.3187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3135/4339]: training_loss: tensor(0.5258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3136/4339]: training_loss: tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3137/4339]: training_loss: tensor(0.4100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3138/4339]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3139/4339]: training_loss: tensor(0.2733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3140/4339]: training_loss: tensor(0.2611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3141/4339]: training_loss: tensor(0.1734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3142/4339]: training_loss: tensor(0.1991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3143/4339]: training_loss: tensor(0.3085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3144/4339]: training_loss: tensor(0.1700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3145/4339]: training_loss: tensor(0.3022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3146/4339]: training_loss: tensor(0.1753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3147/4339]: training_loss: tensor(0.3157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3148/4339]: training_loss: tensor(0.1186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3149/4339]: training_loss: tensor(0.3823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3150/4339]: training_loss: tensor(0.2579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3151/4339]: training_loss: tensor(0.3136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3152/4339]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3153/4339]: training_loss: tensor(0.2509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3154/4339]: training_loss: tensor(0.3086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3155/4339]: training_loss: tensor(0.1344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3156/4339]: training_loss: tensor(0.3543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3157/4339]: training_loss: tensor(0.1879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3158/4339]: training_loss: tensor(0.1696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3159/4339]: training_loss: tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3160/4339]: training_loss: tensor(0.2020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3161/4339]: training_loss: tensor(0.2488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3162/4339]: training_loss: tensor(0.1916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3163/4339]: training_loss: tensor(0.2529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3164/4339]: training_loss: tensor(0.3215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3165/4339]: training_loss: tensor(0.1867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3166/4339]: training_loss: tensor(0.1544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3167/4339]: training_loss: tensor(0.3178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3168/4339]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3169/4339]: training_loss: tensor(0.4458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3170/4339]: training_loss: tensor(0.1559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3171/4339]: training_loss: tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3172/4339]: training_loss: tensor(0.2118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3173/4339]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3174/4339]: training_loss: tensor(0.1600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3175/4339]: training_loss: tensor(0.0706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3176/4339]: training_loss: tensor(0.1606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3177/4339]: training_loss: tensor(0.2387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3178/4339]: training_loss: tensor(0.3752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3179/4339]: training_loss: tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3180/4339]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3181/4339]: training_loss: tensor(0.0572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3182/4339]: training_loss: tensor(0.0888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3183/4339]: training_loss: tensor(0.0462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3184/4339]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3185/4339]: training_loss: tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3186/4339]: training_loss: tensor(0.6771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3187/4339]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3188/4339]: training_loss: tensor(0.1819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3189/4339]: training_loss: tensor(0.1720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3190/4339]: training_loss: tensor(0.1497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3191/4339]: training_loss: tensor(0.1915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3192/4339]: training_loss: tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3193/4339]: training_loss: tensor(0.1828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3194/4339]: training_loss: tensor(0.1437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3195/4339]: training_loss: tensor(0.1926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3196/4339]: training_loss: tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3197/4339]: training_loss: tensor(0.1324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3198/4339]: training_loss: tensor(0.1737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3199/4339]: training_loss: tensor(0.2784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3200/4339]: training_loss: tensor(0.2442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3201/4339]: training_loss: tensor(0.2049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3202/4339]: training_loss: tensor(0.0475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3203/4339]: training_loss: tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3204/4339]: training_loss: tensor(0.5721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3205/4339]: training_loss: tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3206/4339]: training_loss: tensor(0.2070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3207/4339]: training_loss: tensor(0.4029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3208/4339]: training_loss: tensor(0.2001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3209/4339]: training_loss: tensor(0.1929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3210/4339]: training_loss: tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3211/4339]: training_loss: tensor(0.1675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3212/4339]: training_loss: tensor(0.2150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3213/4339]: training_loss: tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3214/4339]: training_loss: tensor(0.2745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3215/4339]: training_loss: tensor(0.1869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3216/4339]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3217/4339]: training_loss: tensor(0.2908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3218/4339]: training_loss: tensor(0.2281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3219/4339]: training_loss: tensor(0.3733, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3220/4339]: training_loss: tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3221/4339]: training_loss: tensor(0.1713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3222/4339]: training_loss: tensor(0.2987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3223/4339]: training_loss: tensor(0.2797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3224/4339]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3225/4339]: training_loss: tensor(0.2345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3226/4339]: training_loss: tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3227/4339]: training_loss: tensor(0.1954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3228/4339]: training_loss: tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3229/4339]: training_loss: tensor(0.3007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3230/4339]: training_loss: tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3231/4339]: training_loss: tensor(0.1407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3232/4339]: training_loss: tensor(0.3588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3233/4339]: training_loss: tensor(0.6480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3234/4339]: training_loss: tensor(0.1177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3235/4339]: training_loss: tensor(0.1776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3236/4339]: training_loss: tensor(0.4217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3237/4339]: training_loss: tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3238/4339]: training_loss: tensor(0.3747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3239/4339]: training_loss: tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3240/4339]: training_loss: tensor(0.2954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3241/4339]: training_loss: tensor(0.3055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3242/4339]: training_loss: tensor(0.2727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3243/4339]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3244/4339]: training_loss: tensor(0.5837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3245/4339]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3246/4339]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3247/4339]: training_loss: tensor(0.2525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3248/4339]: training_loss: tensor(0.1236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3249/4339]: training_loss: tensor(0.1299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3250/4339]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3251/4339]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3252/4339]: training_loss: tensor(0.2566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3253/4339]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3254/4339]: training_loss: tensor(0.1194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3255/4339]: training_loss: tensor(0.3493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3256/4339]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3257/4339]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3258/4339]: training_loss: tensor(0.6968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3259/4339]: training_loss: tensor(0.1046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3260/4339]: training_loss: tensor(0.5386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3261/4339]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3262/4339]: training_loss: tensor(0.2773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3263/4339]: training_loss: tensor(0.0545, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3264/4339]: training_loss: tensor(0.2467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3265/4339]: training_loss: tensor(0.1753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3266/4339]: training_loss: tensor(0.1544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3267/4339]: training_loss: tensor(0.1326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3268/4339]: training_loss: tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3269/4339]: training_loss: tensor(0.1233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3270/4339]: training_loss: tensor(0.2093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3271/4339]: training_loss: tensor(0.1421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3272/4339]: training_loss: tensor(0.2325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3273/4339]: training_loss: tensor(0.2229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3274/4339]: training_loss: tensor(0.1880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3275/4339]: training_loss: tensor(0.4201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3276/4339]: training_loss: tensor(0.3659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3277/4339]: training_loss: tensor(0.1264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3278/4339]: training_loss: tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3279/4339]: training_loss: tensor(0.3177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3280/4339]: training_loss: tensor(0.2447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3281/4339]: training_loss: tensor(0.1603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3282/4339]: training_loss: tensor(0.1385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3283/4339]: training_loss: tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3284/4339]: training_loss: tensor(0.1688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3285/4339]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3286/4339]: training_loss: tensor(0.1893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3287/4339]: training_loss: tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3288/4339]: training_loss: tensor(0.3199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3289/4339]: training_loss: tensor(0.1068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3290/4339]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3291/4339]: training_loss: tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3292/4339]: training_loss: tensor(0.2470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3293/4339]: training_loss: tensor(0.2696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3294/4339]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3295/4339]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3296/4339]: training_loss: tensor(0.1875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3297/4339]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3298/4339]: training_loss: tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3299/4339]: training_loss: tensor(0.1431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3300/4339]: training_loss: tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3301/4339]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3302/4339]: training_loss: tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3303/4339]: training_loss: tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3304/4339]: training_loss: tensor(0.1675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3305/4339]: training_loss: tensor(0.3090, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3306/4339]: training_loss: tensor(0.1491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3307/4339]: training_loss: tensor(0.2943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3308/4339]: training_loss: tensor(0.1752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3309/4339]: training_loss: tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3310/4339]: training_loss: tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3311/4339]: training_loss: tensor(0.1988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3312/4339]: training_loss: tensor(0.1931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3313/4339]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3314/4339]: training_loss: tensor(0.5265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3315/4339]: training_loss: tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3316/4339]: training_loss: tensor(0.5004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3317/4339]: training_loss: tensor(0.2821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3318/4339]: training_loss: tensor(0.2215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3319/4339]: training_loss: tensor(0.1936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3320/4339]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3321/4339]: training_loss: tensor(0.5303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3322/4339]: training_loss: tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3323/4339]: training_loss: tensor(0.4602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3324/4339]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3325/4339]: training_loss: tensor(0.3734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3326/4339]: training_loss: tensor(0.6553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3327/4339]: training_loss: tensor(0.2671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3328/4339]: training_loss: tensor(0.1664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3329/4339]: training_loss: tensor(0.4082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3330/4339]: training_loss: tensor(0.1103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3331/4339]: training_loss: tensor(0.1408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3332/4339]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3333/4339]: training_loss: tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3334/4339]: training_loss: tensor(0.4274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3335/4339]: training_loss: tensor(0.2352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3336/4339]: training_loss: tensor(0.2297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3337/4339]: training_loss: tensor(0.1068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3338/4339]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3339/4339]: training_loss: tensor(0.2148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3340/4339]: training_loss: tensor(0.1896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3341/4339]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3342/4339]: training_loss: tensor(0.1384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3343/4339]: training_loss: tensor(0.1428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3344/4339]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3345/4339]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3346/4339]: training_loss: tensor(0.3891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3347/4339]: training_loss: tensor(0.1629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3348/4339]: training_loss: tensor(0.2867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3349/4339]: training_loss: tensor(0.2312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3350/4339]: training_loss: tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3351/4339]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3352/4339]: training_loss: tensor(0.7112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3353/4339]: training_loss: tensor(0.3943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3354/4339]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3355/4339]: training_loss: tensor(0.2143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3356/4339]: training_loss: tensor(0.3506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3357/4339]: training_loss: tensor(0.3444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3358/4339]: training_loss: tensor(0.2968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3359/4339]: training_loss: tensor(0.2018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3360/4339]: training_loss: tensor(0.1915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3361/4339]: training_loss: tensor(0.2131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3362/4339]: training_loss: tensor(0.1437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3363/4339]: training_loss: tensor(0.1412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3364/4339]: training_loss: tensor(0.1878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3365/4339]: training_loss: tensor(0.2532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3366/4339]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3367/4339]: training_loss: tensor(0.1269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3368/4339]: training_loss: tensor(0.2560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3369/4339]: training_loss: tensor(0.1412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3370/4339]: training_loss: tensor(0.1932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3371/4339]: training_loss: tensor(0.1912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3372/4339]: training_loss: tensor(0.3776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3373/4339]: training_loss: tensor(0.1172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3374/4339]: training_loss: tensor(0.1356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3375/4339]: training_loss: tensor(0.1494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3376/4339]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3377/4339]: training_loss: tensor(0.1334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3378/4339]: training_loss: tensor(0.2662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3379/4339]: training_loss: tensor(0.2620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3380/4339]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3381/4339]: training_loss: tensor(0.2265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3382/4339]: training_loss: tensor(0.2882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3383/4339]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3384/4339]: training_loss: tensor(0.2743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3385/4339]: training_loss: tensor(0.1722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3386/4339]: training_loss: tensor(0.0389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3387/4339]: training_loss: tensor(0.1365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3388/4339]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3389/4339]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3390/4339]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3391/4339]: training_loss: tensor(0.1341, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3392/4339]: training_loss: tensor(0.5498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3393/4339]: training_loss: tensor(0.1392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3394/4339]: training_loss: tensor(0.1324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3395/4339]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3396/4339]: training_loss: tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3397/4339]: training_loss: tensor(0.2214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3398/4339]: training_loss: tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3399/4339]: training_loss: tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3400/4339]: training_loss: tensor(0.0469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3401/4339]: training_loss: tensor(0.1309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3402/4339]: training_loss: tensor(0.1370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3403/4339]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3404/4339]: training_loss: tensor(0.1153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3405/4339]: training_loss: tensor(0.1495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3406/4339]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3407/4339]: training_loss: tensor(0.1465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3408/4339]: training_loss: tensor(0.1528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3409/4339]: training_loss: tensor(0.1621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3410/4339]: training_loss: tensor(0.1491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3411/4339]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3412/4339]: training_loss: tensor(0.4196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3413/4339]: training_loss: tensor(0.1948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3414/4339]: training_loss: tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3415/4339]: training_loss: tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3416/4339]: training_loss: tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3417/4339]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3418/4339]: training_loss: tensor(0.1550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3419/4339]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3420/4339]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3421/4339]: training_loss: tensor(0.1453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3422/4339]: training_loss: tensor(0.5384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3423/4339]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3424/4339]: training_loss: tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3425/4339]: training_loss: tensor(0.2506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3426/4339]: training_loss: tensor(0.2438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3427/4339]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3428/4339]: training_loss: tensor(0.3065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3429/4339]: training_loss: tensor(0.5078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3430/4339]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3431/4339]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3432/4339]: training_loss: tensor(0.1069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3433/4339]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3434/4339]: training_loss: tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3435/4339]: training_loss: tensor(0.3512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3436/4339]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3437/4339]: training_loss: tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3438/4339]: training_loss: tensor(0.4621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3439/4339]: training_loss: tensor(0.2015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3440/4339]: training_loss: tensor(0.1503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3441/4339]: training_loss: tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3442/4339]: training_loss: tensor(0.2954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3443/4339]: training_loss: tensor(0.6309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3444/4339]: training_loss: tensor(0.1547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3445/4339]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3446/4339]: training_loss: tensor(0.2377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3447/4339]: training_loss: tensor(0.2571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3448/4339]: training_loss: tensor(0.1945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3449/4339]: training_loss: tensor(0.1186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3450/4339]: training_loss: tensor(0.1185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3451/4339]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3452/4339]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3453/4339]: training_loss: tensor(0.1816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3454/4339]: training_loss: tensor(0.1937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3455/4339]: training_loss: tensor(0.1582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3456/4339]: training_loss: tensor(0.1737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3457/4339]: training_loss: tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3458/4339]: training_loss: tensor(0.1105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3459/4339]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3460/4339]: training_loss: tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3461/4339]: training_loss: tensor(0.2491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3462/4339]: training_loss: tensor(0.1424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3463/4339]: training_loss: tensor(0.2023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3464/4339]: training_loss: tensor(0.3991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3465/4339]: training_loss: tensor(0.1394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3466/4339]: training_loss: tensor(0.1441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3467/4339]: training_loss: tensor(0.4612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3468/4339]: training_loss: tensor(0.1467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3469/4339]: training_loss: tensor(0.2039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3470/4339]: training_loss: tensor(0.3733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3471/4339]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3472/4339]: training_loss: tensor(0.3576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3473/4339]: training_loss: tensor(0.1353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3474/4339]: training_loss: tensor(0.2436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3475/4339]: training_loss: tensor(0.1813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3476/4339]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3477/4339]: training_loss: tensor(0.2060, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3478/4339]: training_loss: tensor(0.1449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3479/4339]: training_loss: tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3480/4339]: training_loss: tensor(0.2185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3481/4339]: training_loss: tensor(0.1111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3482/4339]: training_loss: tensor(0.1817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3483/4339]: training_loss: tensor(0.1272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3484/4339]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3485/4339]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3486/4339]: training_loss: tensor(0.2448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3487/4339]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3488/4339]: training_loss: tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3489/4339]: training_loss: tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3490/4339]: training_loss: tensor(0.3476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3491/4339]: training_loss: tensor(0.4868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3492/4339]: training_loss: tensor(0.1957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3493/4339]: training_loss: tensor(0.2647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3494/4339]: training_loss: tensor(0.5624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3495/4339]: training_loss: tensor(0.1448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3496/4339]: training_loss: tensor(0.1854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3497/4339]: training_loss: tensor(0.4101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3498/4339]: training_loss: tensor(0.1160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3499/4339]: training_loss: tensor(0.2590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3500/4339]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3501/4339]: training_loss: tensor(0.1146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3502/4339]: training_loss: tensor(0.2387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3503/4339]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3504/4339]: training_loss: tensor(0.2706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3505/4339]: training_loss: tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3506/4339]: training_loss: tensor(0.1775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3507/4339]: training_loss: tensor(0.1577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3508/4339]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3509/4339]: training_loss: tensor(0.1512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3510/4339]: training_loss: tensor(0.2653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3511/4339]: training_loss: tensor(0.1925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3512/4339]: training_loss: tensor(0.2464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3513/4339]: training_loss: tensor(0.1529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3514/4339]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3515/4339]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3516/4339]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3517/4339]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3518/4339]: training_loss: tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3519/4339]: training_loss: tensor(0.2729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3520/4339]: training_loss: tensor(0.6196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3521/4339]: training_loss: tensor(0.6110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3522/4339]: training_loss: tensor(0.1480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3523/4339]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3524/4339]: training_loss: tensor(0.3841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3525/4339]: training_loss: tensor(0.2881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3526/4339]: training_loss: tensor(0.1618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3527/4339]: training_loss: tensor(0.1827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3528/4339]: training_loss: tensor(0.2857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3529/4339]: training_loss: tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3530/4339]: training_loss: tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3531/4339]: training_loss: tensor(0.1408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3532/4339]: training_loss: tensor(0.2040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3533/4339]: training_loss: tensor(0.1833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3534/4339]: training_loss: tensor(0.2397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3535/4339]: training_loss: tensor(0.1198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3536/4339]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3537/4339]: training_loss: tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3538/4339]: training_loss: tensor(0.1525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3539/4339]: training_loss: tensor(0.2210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3540/4339]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3541/4339]: training_loss: tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3542/4339]: training_loss: tensor(0.5918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3543/4339]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3544/4339]: training_loss: tensor(0.2045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3545/4339]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3546/4339]: training_loss: tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3547/4339]: training_loss: tensor(0.2243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3548/4339]: training_loss: tensor(0.3128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3549/4339]: training_loss: tensor(0.1575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3550/4339]: training_loss: tensor(0.3125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3551/4339]: training_loss: tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3552/4339]: training_loss: tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3553/4339]: training_loss: tensor(0.1272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3554/4339]: training_loss: tensor(0.3051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3555/4339]: training_loss: tensor(0.2180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3556/4339]: training_loss: tensor(0.1231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3557/4339]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3558/4339]: training_loss: tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3559/4339]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3560/4339]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3561/4339]: training_loss: tensor(0.1333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3562/4339]: training_loss: tensor(0.4352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3563/4339]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3564/4339]: training_loss: tensor(0.3504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3565/4339]: training_loss: tensor(0.3333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3566/4339]: training_loss: tensor(0.5389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3567/4339]: training_loss: tensor(0.5547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3568/4339]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3569/4339]: training_loss: tensor(0.6806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3570/4339]: training_loss: tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3571/4339]: training_loss: tensor(0.4136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3572/4339]: training_loss: tensor(0.3699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3573/4339]: training_loss: tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3574/4339]: training_loss: tensor(0.3465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3575/4339]: training_loss: tensor(0.1919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3576/4339]: training_loss: tensor(0.1879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3577/4339]: training_loss: tensor(0.3259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3578/4339]: training_loss: tensor(0.2390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3579/4339]: training_loss: tensor(0.2047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3580/4339]: training_loss: tensor(0.2573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3581/4339]: training_loss: tensor(0.2970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3582/4339]: training_loss: tensor(0.2217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3583/4339]: training_loss: tensor(0.3897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3584/4339]: training_loss: tensor(0.2400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3585/4339]: training_loss: tensor(0.3803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3586/4339]: training_loss: tensor(0.2326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3587/4339]: training_loss: tensor(0.4386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3588/4339]: training_loss: tensor(0.4197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3589/4339]: training_loss: tensor(0.2968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3590/4339]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3591/4339]: training_loss: tensor(0.2195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3592/4339]: training_loss: tensor(0.2313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3593/4339]: training_loss: tensor(0.3840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3594/4339]: training_loss: tensor(0.2378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3595/4339]: training_loss: tensor(0.1426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3596/4339]: training_loss: tensor(0.5178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3597/4339]: training_loss: tensor(0.0587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3598/4339]: training_loss: tensor(0.4911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3599/4339]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3600/4339]: training_loss: tensor(0.1704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3601/4339]: training_loss: tensor(0.2164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3602/4339]: training_loss: tensor(0.1694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3603/4339]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3604/4339]: training_loss: tensor(0.1371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3605/4339]: training_loss: tensor(0.3188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3606/4339]: training_loss: tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3607/4339]: training_loss: tensor(0.4206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3608/4339]: training_loss: tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3609/4339]: training_loss: tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3610/4339]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3611/4339]: training_loss: tensor(0.2965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3612/4339]: training_loss: tensor(0.2124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3613/4339]: training_loss: tensor(0.1856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3614/4339]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3615/4339]: training_loss: tensor(0.2556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3616/4339]: training_loss: tensor(0.2469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3617/4339]: training_loss: tensor(0.3387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3618/4339]: training_loss: tensor(0.2596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3619/4339]: training_loss: tensor(0.2227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3620/4339]: training_loss: tensor(0.6077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3621/4339]: training_loss: tensor(0.2285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3622/4339]: training_loss: tensor(0.4707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3623/4339]: training_loss: tensor(0.1841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3624/4339]: training_loss: tensor(0.2655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3625/4339]: training_loss: tensor(0.4954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3626/4339]: training_loss: tensor(0.2234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3627/4339]: training_loss: tensor(0.2397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3628/4339]: training_loss: tensor(0.2849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3629/4339]: training_loss: tensor(0.3163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3630/4339]: training_loss: tensor(0.1225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3631/4339]: training_loss: tensor(0.4002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3632/4339]: training_loss: tensor(0.1852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3633/4339]: training_loss: tensor(0.1543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3634/4339]: training_loss: tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3635/4339]: training_loss: tensor(0.2760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3636/4339]: training_loss: tensor(0.2629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3637/4339]: training_loss: tensor(0.2238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3638/4339]: training_loss: tensor(0.3092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3639/4339]: training_loss: tensor(0.2910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3640/4339]: training_loss: tensor(0.1892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3641/4339]: training_loss: tensor(0.2161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3642/4339]: training_loss: tensor(0.1857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3643/4339]: training_loss: tensor(0.1415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3644/4339]: training_loss: tensor(0.4056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3645/4339]: training_loss: tensor(0.5270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3646/4339]: training_loss: tensor(0.1757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3647/4339]: training_loss: tensor(0.4009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3648/4339]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3649/4339]: training_loss: tensor(0.3763, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3650/4339]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3651/4339]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3652/4339]: training_loss: tensor(0.4861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3653/4339]: training_loss: tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3654/4339]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3655/4339]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3656/4339]: training_loss: tensor(0.2024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3657/4339]: training_loss: tensor(0.3728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3658/4339]: training_loss: tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3659/4339]: training_loss: tensor(0.2196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3660/4339]: training_loss: tensor(0.1200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3661/4339]: training_loss: tensor(0.2960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3662/4339]: training_loss: tensor(0.2448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3663/4339]: training_loss: tensor(0.3394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3664/4339]: training_loss: tensor(0.3112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3665/4339]: training_loss: tensor(0.2551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3666/4339]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3667/4339]: training_loss: tensor(0.5684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3668/4339]: training_loss: tensor(0.2976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3669/4339]: training_loss: tensor(0.3502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3670/4339]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3671/4339]: training_loss: tensor(0.2170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3672/4339]: training_loss: tensor(0.4340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3673/4339]: training_loss: tensor(0.1909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3674/4339]: training_loss: tensor(0.4119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3675/4339]: training_loss: tensor(0.2737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3676/4339]: training_loss: tensor(0.3782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3677/4339]: training_loss: tensor(0.2566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3678/4339]: training_loss: tensor(0.2297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3679/4339]: training_loss: tensor(0.3209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3680/4339]: training_loss: tensor(0.2331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3681/4339]: training_loss: tensor(0.5418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3682/4339]: training_loss: tensor(0.1974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3683/4339]: training_loss: tensor(0.2444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3684/4339]: training_loss: tensor(0.4121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3685/4339]: training_loss: tensor(0.2394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3686/4339]: training_loss: tensor(0.2832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3687/4339]: training_loss: tensor(0.2411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3688/4339]: training_loss: tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3689/4339]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3690/4339]: training_loss: tensor(0.3876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3691/4339]: training_loss: tensor(0.1946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3692/4339]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3693/4339]: training_loss: tensor(0.1968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3694/4339]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3695/4339]: training_loss: tensor(0.1732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3696/4339]: training_loss: tensor(0.1380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3697/4339]: training_loss: tensor(0.1650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3698/4339]: training_loss: tensor(0.2189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3699/4339]: training_loss: tensor(0.2115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3700/4339]: training_loss: tensor(0.1283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3701/4339]: training_loss: tensor(0.4075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3702/4339]: training_loss: tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3703/4339]: training_loss: tensor(0.2383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3704/4339]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3705/4339]: training_loss: tensor(0.1778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3706/4339]: training_loss: tensor(0.1526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3707/4339]: training_loss: tensor(0.3087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3708/4339]: training_loss: tensor(0.1972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3709/4339]: training_loss: tensor(0.6438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3710/4339]: training_loss: tensor(0.1500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3711/4339]: training_loss: tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3712/4339]: training_loss: tensor(0.0592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3713/4339]: training_loss: tensor(0.1445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3714/4339]: training_loss: tensor(0.3252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3715/4339]: training_loss: tensor(0.2639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3716/4339]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3717/4339]: training_loss: tensor(0.2464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3718/4339]: training_loss: tensor(0.1582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3719/4339]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3720/4339]: training_loss: tensor(0.0841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3721/4339]: training_loss: tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3722/4339]: training_loss: tensor(0.1596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3723/4339]: training_loss: tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3724/4339]: training_loss: tensor(0.3543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3725/4339]: training_loss: tensor(0.1988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3726/4339]: training_loss: tensor(0.1077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3727/4339]: training_loss: tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3728/4339]: training_loss: tensor(0.1519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3729/4339]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3730/4339]: training_loss: tensor(0.1568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3731/4339]: training_loss: tensor(0.6341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3732/4339]: training_loss: tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3733/4339]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3734/4339]: training_loss: tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3735/4339]: training_loss: tensor(0.1950, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3736/4339]: training_loss: tensor(0.2078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3737/4339]: training_loss: tensor(0.1552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3738/4339]: training_loss: tensor(0.1986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3739/4339]: training_loss: tensor(0.1853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3740/4339]: training_loss: tensor(0.5447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3741/4339]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3742/4339]: training_loss: tensor(0.3045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3743/4339]: training_loss: tensor(0.2703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3744/4339]: training_loss: tensor(0.2763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3745/4339]: training_loss: tensor(0.1357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3746/4339]: training_loss: tensor(0.2515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3747/4339]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3748/4339]: training_loss: tensor(0.2614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3749/4339]: training_loss: tensor(0.3584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3750/4339]: training_loss: tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3751/4339]: training_loss: tensor(0.3537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3752/4339]: training_loss: tensor(0.1814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3753/4339]: training_loss: tensor(0.3186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3754/4339]: training_loss: tensor(0.3955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3755/4339]: training_loss: tensor(0.0555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3756/4339]: training_loss: tensor(0.6081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3757/4339]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3758/4339]: training_loss: tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3759/4339]: training_loss: tensor(0.3901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3760/4339]: training_loss: tensor(0.6244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3761/4339]: training_loss: tensor(0.5231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3762/4339]: training_loss: tensor(0.3224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3763/4339]: training_loss: tensor(0.3101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3764/4339]: training_loss: tensor(0.2359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3765/4339]: training_loss: tensor(0.2522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3766/4339]: training_loss: tensor(0.3119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3767/4339]: training_loss: tensor(0.1724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3768/4339]: training_loss: tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3769/4339]: training_loss: tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3770/4339]: training_loss: tensor(0.1197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3771/4339]: training_loss: tensor(0.2033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3772/4339]: training_loss: tensor(0.2465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3773/4339]: training_loss: tensor(0.1953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3774/4339]: training_loss: tensor(0.1172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3775/4339]: training_loss: tensor(0.3108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3776/4339]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3777/4339]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3778/4339]: training_loss: tensor(0.1548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3779/4339]: training_loss: tensor(0.7531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3780/4339]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3781/4339]: training_loss: tensor(0.3067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3782/4339]: training_loss: tensor(0.1596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3783/4339]: training_loss: tensor(0.2365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3784/4339]: training_loss: tensor(0.1738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3785/4339]: training_loss: tensor(0.1665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3786/4339]: training_loss: tensor(0.1416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3787/4339]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3788/4339]: training_loss: tensor(0.1420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3789/4339]: training_loss: tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3790/4339]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3791/4339]: training_loss: tensor(0.3631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3792/4339]: training_loss: tensor(0.3830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3793/4339]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3794/4339]: training_loss: tensor(0.5655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3795/4339]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3796/4339]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3797/4339]: training_loss: tensor(0.2912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3798/4339]: training_loss: tensor(0.1376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3799/4339]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3800/4339]: training_loss: tensor(0.2999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3801/4339]: training_loss: tensor(0.1677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3802/4339]: training_loss: tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3803/4339]: training_loss: tensor(0.5054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3804/4339]: training_loss: tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3805/4339]: training_loss: tensor(0.3927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3806/4339]: training_loss: tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3807/4339]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3808/4339]: training_loss: tensor(0.2230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3809/4339]: training_loss: tensor(0.3032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3810/4339]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3811/4339]: training_loss: tensor(0.1678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3812/4339]: training_loss: tensor(0.5076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3813/4339]: training_loss: tensor(0.5546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3814/4339]: training_loss: tensor(0.2896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3815/4339]: training_loss: tensor(0.2007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3816/4339]: training_loss: tensor(0.1820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3817/4339]: training_loss: tensor(0.2320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3818/4339]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3819/4339]: training_loss: tensor(0.3365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3820/4339]: training_loss: tensor(0.2862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3821/4339]: training_loss: tensor(0.2775, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3822/4339]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3823/4339]: training_loss: tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3824/4339]: training_loss: tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3825/4339]: training_loss: tensor(0.7877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3826/4339]: training_loss: tensor(0.1393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3827/4339]: training_loss: tensor(0.2298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3828/4339]: training_loss: tensor(0.1710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3829/4339]: training_loss: tensor(0.5024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3830/4339]: training_loss: tensor(0.3201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3831/4339]: training_loss: tensor(0.4467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3832/4339]: training_loss: tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3833/4339]: training_loss: tensor(0.1436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3834/4339]: training_loss: tensor(0.3757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3835/4339]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3836/4339]: training_loss: tensor(0.1718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3837/4339]: training_loss: tensor(0.4280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3838/4339]: training_loss: tensor(0.3110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3839/4339]: training_loss: tensor(0.4864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3840/4339]: training_loss: tensor(0.1127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3841/4339]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3842/4339]: training_loss: tensor(0.2383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3843/4339]: training_loss: tensor(0.2855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3844/4339]: training_loss: tensor(0.1959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3845/4339]: training_loss: tensor(0.1109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3846/4339]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3847/4339]: training_loss: tensor(0.1257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3848/4339]: training_loss: tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3849/4339]: training_loss: tensor(0.3783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3850/4339]: training_loss: tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3851/4339]: training_loss: tensor(0.1550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3852/4339]: training_loss: tensor(0.3079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3853/4339]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3854/4339]: training_loss: tensor(0.2752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3855/4339]: training_loss: tensor(0.5669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3856/4339]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3857/4339]: training_loss: tensor(0.3217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3858/4339]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3859/4339]: training_loss: tensor(0.2590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3860/4339]: training_loss: tensor(0.1902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3861/4339]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3862/4339]: training_loss: tensor(0.2057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3863/4339]: training_loss: tensor(0.0383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3864/4339]: training_loss: tensor(0.2711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3865/4339]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3866/4339]: training_loss: tensor(0.3006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3867/4339]: training_loss: tensor(0.1711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3868/4339]: training_loss: tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3869/4339]: training_loss: tensor(0.2535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3870/4339]: training_loss: tensor(0.3190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3871/4339]: training_loss: tensor(0.3551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3872/4339]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3873/4339]: training_loss: tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3874/4339]: training_loss: tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3875/4339]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3876/4339]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3877/4339]: training_loss: tensor(0.1387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3878/4339]: training_loss: tensor(0.6242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3879/4339]: training_loss: tensor(0.1529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3880/4339]: training_loss: tensor(0.4610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3881/4339]: training_loss: tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3882/4339]: training_loss: tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3883/4339]: training_loss: tensor(0.4562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3884/4339]: training_loss: tensor(0.4213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3885/4339]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3886/4339]: training_loss: tensor(0.2192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3887/4339]: training_loss: tensor(0.3527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3888/4339]: training_loss: tensor(0.4041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3889/4339]: training_loss: tensor(0.2072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3890/4339]: training_loss: tensor(0.2650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3891/4339]: training_loss: tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3892/4339]: training_loss: tensor(0.2275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3893/4339]: training_loss: tensor(0.2756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3894/4339]: training_loss: tensor(0.3001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3895/4339]: training_loss: tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3896/4339]: training_loss: tensor(0.5508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3897/4339]: training_loss: tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3898/4339]: training_loss: tensor(0.3872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3899/4339]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3900/4339]: training_loss: tensor(0.1762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3901/4339]: training_loss: tensor(0.1282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3902/4339]: training_loss: tensor(0.4464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3903/4339]: training_loss: tensor(0.2179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3904/4339]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3905/4339]: training_loss: tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3906/4339]: training_loss: tensor(0.3422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3907/4339]: training_loss: tensor(0.3444, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3908/4339]: training_loss: tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3909/4339]: training_loss: tensor(0.2256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3910/4339]: training_loss: tensor(0.4449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3911/4339]: training_loss: tensor(0.3148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3912/4339]: training_loss: tensor(0.4401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3913/4339]: training_loss: tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3914/4339]: training_loss: tensor(0.1780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3915/4339]: training_loss: tensor(0.3449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3916/4339]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3917/4339]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3918/4339]: training_loss: tensor(0.2622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3919/4339]: training_loss: tensor(0.2653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3920/4339]: training_loss: tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3921/4339]: training_loss: tensor(0.5064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3922/4339]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3923/4339]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3924/4339]: training_loss: tensor(0.2938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3925/4339]: training_loss: tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3926/4339]: training_loss: tensor(0.3826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3927/4339]: training_loss: tensor(0.2118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3928/4339]: training_loss: tensor(0.2497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3929/4339]: training_loss: tensor(0.4291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3930/4339]: training_loss: tensor(0.1142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3931/4339]: training_loss: tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3932/4339]: training_loss: tensor(0.2060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3933/4339]: training_loss: tensor(0.4282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3934/4339]: training_loss: tensor(0.1775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3935/4339]: training_loss: tensor(0.1632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3936/4339]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3937/4339]: training_loss: tensor(0.2478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3938/4339]: training_loss: tensor(0.1159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3939/4339]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3940/4339]: training_loss: tensor(0.1669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3941/4339]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3942/4339]: training_loss: tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3943/4339]: training_loss: tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3944/4339]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3945/4339]: training_loss: tensor(0.2077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3946/4339]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3947/4339]: training_loss: tensor(0.2177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3948/4339]: training_loss: tensor(0.1259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3949/4339]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3950/4339]: training_loss: tensor(0.3267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3951/4339]: training_loss: tensor(0.3823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3952/4339]: training_loss: tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3953/4339]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3954/4339]: training_loss: tensor(0.1989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3955/4339]: training_loss: tensor(0.1213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3956/4339]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3957/4339]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3958/4339]: training_loss: tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3959/4339]: training_loss: tensor(0.1490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3960/4339]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3961/4339]: training_loss: tensor(0.1550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3962/4339]: training_loss: tensor(0.2833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3963/4339]: training_loss: tensor(0.1506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3964/4339]: training_loss: tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3965/4339]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3966/4339]: training_loss: tensor(0.1346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3967/4339]: training_loss: tensor(0.1164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3968/4339]: training_loss: tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3969/4339]: training_loss: tensor(0.4375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3970/4339]: training_loss: tensor(0.1787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3971/4339]: training_loss: tensor(0.2680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3972/4339]: training_loss: tensor(0.1542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3973/4339]: training_loss: tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3974/4339]: training_loss: tensor(0.1316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3975/4339]: training_loss: tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3976/4339]: training_loss: tensor(0.1708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3977/4339]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3978/4339]: training_loss: tensor(0.3114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3979/4339]: training_loss: tensor(0.0572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3980/4339]: training_loss: tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3981/4339]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3982/4339]: training_loss: tensor(0.0525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3983/4339]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3984/4339]: training_loss: tensor(0.4943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3985/4339]: training_loss: tensor(0.2453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3986/4339]: training_loss: tensor(0.2578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3987/4339]: training_loss: tensor(0.4554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3988/4339]: training_loss: tensor(0.2980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3989/4339]: training_loss: tensor(0.1264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3990/4339]: training_loss: tensor(0.1325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3991/4339]: training_loss: tensor(0.2223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3992/4339]: training_loss: tensor(0.2257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3993/4339]: training_loss: tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3994/4339]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3995/4339]: training_loss: tensor(0.2269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3996/4339]: training_loss: tensor(0.1411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3997/4339]: training_loss: tensor(0.1840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3998/4339]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3999/4339]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4000/4339]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4001/4339]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4002/4339]: training_loss: tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4003/4339]: training_loss: tensor(0.1705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4004/4339]: training_loss: tensor(0.1716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4005/4339]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4006/4339]: training_loss: tensor(0.4051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4007/4339]: training_loss: tensor(0.2720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4008/4339]: training_loss: tensor(0.3460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4009/4339]: training_loss: tensor(0.2049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4010/4339]: training_loss: tensor(0.2906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4011/4339]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4012/4339]: training_loss: tensor(0.2615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4013/4339]: training_loss: tensor(0.2910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4014/4339]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4015/4339]: training_loss: tensor(0.1223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4016/4339]: training_loss: tensor(0.2100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4017/4339]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4018/4339]: training_loss: tensor(0.3271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4019/4339]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4020/4339]: training_loss: tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4021/4339]: training_loss: tensor(0.2563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4022/4339]: training_loss: tensor(0.3001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4023/4339]: training_loss: tensor(0.1959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4024/4339]: training_loss: tensor(0.3753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4025/4339]: training_loss: tensor(0.3317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4026/4339]: training_loss: tensor(0.3711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4027/4339]: training_loss: tensor(0.1373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4028/4339]: training_loss: tensor(0.1526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4029/4339]: training_loss: tensor(0.3866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4030/4339]: training_loss: tensor(0.3275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4031/4339]: training_loss: tensor(0.6866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4032/4339]: training_loss: tensor(0.5169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4033/4339]: training_loss: tensor(0.3741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4034/4339]: training_loss: tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4035/4339]: training_loss: tensor(0.2608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4036/4339]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4037/4339]: training_loss: tensor(0.5705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4038/4339]: training_loss: tensor(0.2320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4039/4339]: training_loss: tensor(0.2456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4040/4339]: training_loss: tensor(0.4244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4041/4339]: training_loss: tensor(0.3057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4042/4339]: training_loss: tensor(0.1894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4043/4339]: training_loss: tensor(0.2143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4044/4339]: training_loss: tensor(0.1915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4045/4339]: training_loss: tensor(0.2773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4046/4339]: training_loss: tensor(0.2433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4047/4339]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4048/4339]: training_loss: tensor(0.3489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4049/4339]: training_loss: tensor(0.1458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4050/4339]: training_loss: tensor(0.4194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4051/4339]: training_loss: tensor(0.1460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4052/4339]: training_loss: tensor(0.3327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4053/4339]: training_loss: tensor(0.6217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4054/4339]: training_loss: tensor(0.2798, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4055/4339]: training_loss: tensor(0.2296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4056/4339]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4057/4339]: training_loss: tensor(0.3309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4058/4339]: training_loss: tensor(0.1721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4059/4339]: training_loss: tensor(0.2737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4060/4339]: training_loss: tensor(0.0495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4061/4339]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4062/4339]: training_loss: tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4063/4339]: training_loss: tensor(0.3634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4064/4339]: training_loss: tensor(0.0620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4065/4339]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4066/4339]: training_loss: tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4067/4339]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4068/4339]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4069/4339]: training_loss: tensor(0.1321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4070/4339]: training_loss: tensor(0.8318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4071/4339]: training_loss: tensor(0.2341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4072/4339]: training_loss: tensor(0.2351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4073/4339]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4074/4339]: training_loss: tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4075/4339]: training_loss: tensor(0.4436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4076/4339]: training_loss: tensor(0.4587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4077/4339]: training_loss: tensor(0.1492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4078/4339]: training_loss: tensor(0.1585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4079/4339]: training_loss: tensor(0.1189, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4080/4339]: training_loss: tensor(0.1419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4081/4339]: training_loss: tensor(0.2237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4082/4339]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4083/4339]: training_loss: tensor(0.2609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4084/4339]: training_loss: tensor(0.1760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4085/4339]: training_loss: tensor(0.2064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4086/4339]: training_loss: tensor(0.0575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4087/4339]: training_loss: tensor(0.4115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4088/4339]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4089/4339]: training_loss: tensor(0.1256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4090/4339]: training_loss: tensor(0.3157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4091/4339]: training_loss: tensor(0.1921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4092/4339]: training_loss: tensor(0.3119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4093/4339]: training_loss: tensor(0.2933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4094/4339]: training_loss: tensor(0.3369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4095/4339]: training_loss: tensor(0.2442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4096/4339]: training_loss: tensor(0.1647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4097/4339]: training_loss: tensor(0.2952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4098/4339]: training_loss: tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4099/4339]: training_loss: tensor(0.1333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4100/4339]: training_loss: tensor(0.4511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4101/4339]: training_loss: tensor(0.5897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4102/4339]: training_loss: tensor(0.4522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4103/4339]: training_loss: tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4104/4339]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4105/4339]: training_loss: tensor(0.4640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4106/4339]: training_loss: tensor(0.2750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4107/4339]: training_loss: tensor(0.2947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4108/4339]: training_loss: tensor(0.2500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4109/4339]: training_loss: tensor(0.3065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4110/4339]: training_loss: tensor(0.1835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4111/4339]: training_loss: tensor(0.1804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4112/4339]: training_loss: tensor(0.3442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4113/4339]: training_loss: tensor(0.3980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4114/4339]: training_loss: tensor(0.1596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4115/4339]: training_loss: tensor(0.2956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4116/4339]: training_loss: tensor(0.1233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4117/4339]: training_loss: tensor(0.2744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4118/4339]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4119/4339]: training_loss: tensor(0.2566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4120/4339]: training_loss: tensor(0.4200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4121/4339]: training_loss: tensor(0.2673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4122/4339]: training_loss: tensor(0.3058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4123/4339]: training_loss: tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4124/4339]: training_loss: tensor(0.3265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4125/4339]: training_loss: tensor(0.2614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4126/4339]: training_loss: tensor(0.1661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4127/4339]: training_loss: tensor(0.4419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4128/4339]: training_loss: tensor(0.1202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4129/4339]: training_loss: tensor(0.2697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4130/4339]: training_loss: tensor(0.1739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4131/4339]: training_loss: tensor(0.0606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4132/4339]: training_loss: tensor(0.4786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4133/4339]: training_loss: tensor(0.5200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4134/4339]: training_loss: tensor(0.1197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4135/4339]: training_loss: tensor(0.3809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4136/4339]: training_loss: tensor(0.1600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4137/4339]: training_loss: tensor(0.2791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4138/4339]: training_loss: tensor(0.2140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4139/4339]: training_loss: tensor(0.4815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4140/4339]: training_loss: tensor(0.1442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4141/4339]: training_loss: tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4142/4339]: training_loss: tensor(0.2142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4143/4339]: training_loss: tensor(0.2939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4144/4339]: training_loss: tensor(0.4181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4145/4339]: training_loss: tensor(0.1525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4146/4339]: training_loss: tensor(0.1450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4147/4339]: training_loss: tensor(0.1347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4148/4339]: training_loss: tensor(0.2318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4149/4339]: training_loss: tensor(0.3850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4150/4339]: training_loss: tensor(0.3207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4151/4339]: training_loss: tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4152/4339]: training_loss: tensor(0.3609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4153/4339]: training_loss: tensor(0.2729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4154/4339]: training_loss: tensor(0.1510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4155/4339]: training_loss: tensor(0.1232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4156/4339]: training_loss: tensor(0.6709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4157/4339]: training_loss: tensor(0.2111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4158/4339]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4159/4339]: training_loss: tensor(0.4893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4160/4339]: training_loss: tensor(0.3708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4161/4339]: training_loss: tensor(0.3057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4162/4339]: training_loss: tensor(0.2468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4163/4339]: training_loss: tensor(0.1560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4164/4339]: training_loss: tensor(0.5284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4165/4339]: training_loss: tensor(0.2427, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4166/4339]: training_loss: tensor(0.0507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4167/4339]: training_loss: tensor(0.1242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4168/4339]: training_loss: tensor(0.3315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4169/4339]: training_loss: tensor(0.1944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4170/4339]: training_loss: tensor(0.2254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4171/4339]: training_loss: tensor(0.1197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4172/4339]: training_loss: tensor(0.1517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4173/4339]: training_loss: tensor(0.3767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4174/4339]: training_loss: tensor(0.1756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4175/4339]: training_loss: tensor(0.2428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4176/4339]: training_loss: tensor(0.2190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4177/4339]: training_loss: tensor(0.5442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4178/4339]: training_loss: tensor(0.4112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4179/4339]: training_loss: tensor(0.4636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4180/4339]: training_loss: tensor(0.4333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4181/4339]: training_loss: tensor(0.1676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4182/4339]: training_loss: tensor(0.2217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4183/4339]: training_loss: tensor(0.1276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4184/4339]: training_loss: tensor(0.2872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4185/4339]: training_loss: tensor(0.2001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4186/4339]: training_loss: tensor(0.1617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4187/4339]: training_loss: tensor(0.1425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4188/4339]: training_loss: tensor(0.1664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4189/4339]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4190/4339]: training_loss: tensor(0.1980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4191/4339]: training_loss: tensor(0.1472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4192/4339]: training_loss: tensor(0.1527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4193/4339]: training_loss: tensor(0.1846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4194/4339]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4195/4339]: training_loss: tensor(0.2318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4196/4339]: training_loss: tensor(0.1342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4197/4339]: training_loss: tensor(0.3368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4198/4339]: training_loss: tensor(0.3579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4199/4339]: training_loss: tensor(0.1614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4200/4339]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4201/4339]: training_loss: tensor(0.2205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4202/4339]: training_loss: tensor(0.1928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4203/4339]: training_loss: tensor(0.2299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4204/4339]: training_loss: tensor(0.1525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4205/4339]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4206/4339]: training_loss: tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4207/4339]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4208/4339]: training_loss: tensor(0.1134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4209/4339]: training_loss: tensor(0.1375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4210/4339]: training_loss: tensor(0.5026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4211/4339]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4212/4339]: training_loss: tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4213/4339]: training_loss: tensor(0.5192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4214/4339]: training_loss: tensor(0.1430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4215/4339]: training_loss: tensor(0.3373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4216/4339]: training_loss: tensor(0.1014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4217/4339]: training_loss: tensor(0.3052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4218/4339]: training_loss: tensor(0.2581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4219/4339]: training_loss: tensor(0.1383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4220/4339]: training_loss: tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4221/4339]: training_loss: tensor(0.1634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4222/4339]: training_loss: tensor(0.1701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4223/4339]: training_loss: tensor(0.6744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4224/4339]: training_loss: tensor(0.6006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4225/4339]: training_loss: tensor(0.4324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4226/4339]: training_loss: tensor(0.2863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4227/4339]: training_loss: tensor(0.4122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4228/4339]: training_loss: tensor(0.2079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4229/4339]: training_loss: tensor(0.2148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4230/4339]: training_loss: tensor(0.2110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4231/4339]: training_loss: tensor(0.3218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4232/4339]: training_loss: tensor(0.1624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4233/4339]: training_loss: tensor(0.2204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4234/4339]: training_loss: tensor(0.3419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4235/4339]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4236/4339]: training_loss: tensor(0.2856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4237/4339]: training_loss: tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4238/4339]: training_loss: tensor(0.1159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4239/4339]: training_loss: tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4240/4339]: training_loss: tensor(0.3581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4241/4339]: training_loss: tensor(0.2279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4242/4339]: training_loss: tensor(0.8312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4243/4339]: training_loss: tensor(0.3754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4244/4339]: training_loss: tensor(0.2268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4245/4339]: training_loss: tensor(0.4749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4246/4339]: training_loss: tensor(0.4421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4247/4339]: training_loss: tensor(0.3307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4248/4339]: training_loss: tensor(0.4464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4249/4339]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4250/4339]: training_loss: tensor(0.1528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4251/4339]: training_loss: tensor(0.4200, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4252/4339]: training_loss: tensor(0.3134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4253/4339]: training_loss: tensor(0.1475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4254/4339]: training_loss: tensor(0.2821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4255/4339]: training_loss: tensor(0.4522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4256/4339]: training_loss: tensor(0.2967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4257/4339]: training_loss: tensor(0.2719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4258/4339]: training_loss: tensor(0.1827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4259/4339]: training_loss: tensor(0.2965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4260/4339]: training_loss: tensor(0.3943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4261/4339]: training_loss: tensor(0.7542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4262/4339]: training_loss: tensor(0.4159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4263/4339]: training_loss: tensor(0.2379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4264/4339]: training_loss: tensor(0.4377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4265/4339]: training_loss: tensor(0.2408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4266/4339]: training_loss: tensor(0.3070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4267/4339]: training_loss: tensor(0.3205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4268/4339]: training_loss: tensor(0.3038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4269/4339]: training_loss: tensor(0.2570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4270/4339]: training_loss: tensor(0.3879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4271/4339]: training_loss: tensor(0.3313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4272/4339]: training_loss: tensor(0.2762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4273/4339]: training_loss: tensor(0.2960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4274/4339]: training_loss: tensor(0.1888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4275/4339]: training_loss: tensor(0.2955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4276/4339]: training_loss: tensor(0.3073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4277/4339]: training_loss: tensor(0.2563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4278/4339]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4279/4339]: training_loss: tensor(0.5233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4280/4339]: training_loss: tensor(0.1398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4281/4339]: training_loss: tensor(0.1529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4282/4339]: training_loss: tensor(0.4880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4283/4339]: training_loss: tensor(0.1341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4284/4339]: training_loss: tensor(0.2904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4285/4339]: training_loss: tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4286/4339]: training_loss: tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4287/4339]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4288/4339]: training_loss: tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4289/4339]: training_loss: tensor(0.4129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4290/4339]: training_loss: tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4291/4339]: training_loss: tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4292/4339]: training_loss: tensor(0.2060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4293/4339]: training_loss: tensor(0.3350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4294/4339]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4295/4339]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4296/4339]: training_loss: tensor(0.2025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4297/4339]: training_loss: tensor(0.1695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4298/4339]: training_loss: tensor(0.1343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4299/4339]: training_loss: tensor(0.3572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4300/4339]: training_loss: tensor(0.0505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4301/4339]: training_loss: tensor(0.1995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4302/4339]: training_loss: tensor(0.3532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4303/4339]: training_loss: tensor(0.0389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4304/4339]: training_loss: tensor(0.1600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4305/4339]: training_loss: tensor(0.0884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4306/4339]: training_loss: tensor(0.4069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4307/4339]: training_loss: tensor(0.1883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4308/4339]: training_loss: tensor(0.1272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4309/4339]: training_loss: tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4310/4339]: training_loss: tensor(0.4295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4311/4339]: training_loss: tensor(0.2775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4312/4339]: training_loss: tensor(0.1727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4313/4339]: training_loss: tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4314/4339]: training_loss: tensor(0.1435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4315/4339]: training_loss: tensor(0.2434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4316/4339]: training_loss: tensor(0.1354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4317/4339]: training_loss: tensor(0.4277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4318/4339]: training_loss: tensor(0.1316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4319/4339]: training_loss: tensor(0.2244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4320/4339]: training_loss: tensor(0.2561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4321/4339]: training_loss: tensor(0.4323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4322/4339]: training_loss: tensor(0.3299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4323/4339]: training_loss: tensor(0.1346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4324/4339]: training_loss: tensor(0.2522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4325/4339]: training_loss: tensor(0.3649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4326/4339]: training_loss: tensor(0.3455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4327/4339]: training_loss: tensor(0.2215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4328/4339]: training_loss: tensor(0.1677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4329/4339]: training_loss: tensor(0.1726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4330/4339]: training_loss: tensor(0.2408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4331/4339]: training_loss: tensor(0.3328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4332/4339]: training_loss: tensor(0.2727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4333/4339]: training_loss: tensor(0.2313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4334/4339]: training_loss: tensor(0.1420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4335/4339]: training_loss: tensor(0.2591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4336/4339]: training_loss: tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4337/4339]: training_loss: tensor(0.5636, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4338/4339]: training_loss: tensor(0.5083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4339/4339]: training_loss: tensor(0.3081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4340/4339]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [3/5], Step [13020/21700], Train Loss: 0.2116, Valid Loss: 0.3635\n",
      "batch_no [1/4339]: training_loss: tensor(0.2087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2/4339]: training_loss: tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3/4339]: training_loss: tensor(0.3858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4/4339]: training_loss: tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [5/4339]: training_loss: tensor(0.3996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [6/4339]: training_loss: tensor(0.5713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [7/4339]: training_loss: tensor(0.2791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [8/4339]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [9/4339]: training_loss: tensor(0.1790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [10/4339]: training_loss: tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [11/4339]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [12/4339]: training_loss: tensor(0.1933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [13/4339]: training_loss: tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [14/4339]: training_loss: tensor(0.3603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [15/4339]: training_loss: tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [16/4339]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [17/4339]: training_loss: tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [18/4339]: training_loss: tensor(0.0645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [19/4339]: training_loss: tensor(0.1195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [20/4339]: training_loss: tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [21/4339]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [22/4339]: training_loss: tensor(0.3493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [23/4339]: training_loss: tensor(0.3649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [24/4339]: training_loss: tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [25/4339]: training_loss: tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [26/4339]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [27/4339]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [28/4339]: training_loss: tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [29/4339]: training_loss: tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [30/4339]: training_loss: tensor(0.2974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [31/4339]: training_loss: tensor(0.3509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [32/4339]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [33/4339]: training_loss: tensor(0.1356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [34/4339]: training_loss: tensor(0.9889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [35/4339]: training_loss: tensor(0.3663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [36/4339]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [37/4339]: training_loss: tensor(0.1470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [38/4339]: training_loss: tensor(0.1822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [39/4339]: training_loss: tensor(0.1658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [40/4339]: training_loss: tensor(0.0476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [41/4339]: training_loss: tensor(0.4156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [42/4339]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [43/4339]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [44/4339]: training_loss: tensor(0.1707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [45/4339]: training_loss: tensor(0.2388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [46/4339]: training_loss: tensor(0.1870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [47/4339]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [48/4339]: training_loss: tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [49/4339]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [50/4339]: training_loss: tensor(0.2298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [51/4339]: training_loss: tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [52/4339]: training_loss: tensor(0.4631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [53/4339]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [54/4339]: training_loss: tensor(0.4401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [55/4339]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [56/4339]: training_loss: tensor(0.2226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [57/4339]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [58/4339]: training_loss: tensor(0.3794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [59/4339]: training_loss: tensor(0.2652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [60/4339]: training_loss: tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [61/4339]: training_loss: tensor(0.1548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [62/4339]: training_loss: tensor(0.2722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [63/4339]: training_loss: tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [64/4339]: training_loss: tensor(0.3957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [65/4339]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [66/4339]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [67/4339]: training_loss: tensor(0.1734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [68/4339]: training_loss: tensor(0.1186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [69/4339]: training_loss: tensor(0.2229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [70/4339]: training_loss: tensor(0.1919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [71/4339]: training_loss: tensor(0.1620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [72/4339]: training_loss: tensor(0.3743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [73/4339]: training_loss: tensor(0.2614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [74/4339]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [75/4339]: training_loss: tensor(0.1385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [76/4339]: training_loss: tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [77/4339]: training_loss: tensor(0.2317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [78/4339]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [79/4339]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [80/4339]: training_loss: tensor(0.3949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [81/4339]: training_loss: tensor(0.1156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [82/4339]: training_loss: tensor(0.1617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [83/4339]: training_loss: tensor(0.6292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [84/4339]: training_loss: tensor(0.1364, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [85/4339]: training_loss: tensor(0.4826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [86/4339]: training_loss: tensor(0.1328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [87/4339]: training_loss: tensor(0.1093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [88/4339]: training_loss: tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [89/4339]: training_loss: tensor(0.1666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [90/4339]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [91/4339]: training_loss: tensor(0.1358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [92/4339]: training_loss: tensor(0.1547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [93/4339]: training_loss: tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [94/4339]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [95/4339]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [96/4339]: training_loss: tensor(0.1191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [97/4339]: training_loss: tensor(0.1722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [98/4339]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [99/4339]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [100/4339]: training_loss: tensor(0.1285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [101/4339]: training_loss: tensor(0.1114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [102/4339]: training_loss: tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [103/4339]: training_loss: tensor(0.3765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [104/4339]: training_loss: tensor(0.1528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [105/4339]: training_loss: tensor(0.1625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [106/4339]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [107/4339]: training_loss: tensor(0.2249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [108/4339]: training_loss: tensor(0.0389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [109/4339]: training_loss: tensor(0.5320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [110/4339]: training_loss: tensor(0.1229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [111/4339]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [112/4339]: training_loss: tensor(0.1409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [113/4339]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [114/4339]: training_loss: tensor(0.2856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [115/4339]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [116/4339]: training_loss: tensor(0.0546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [117/4339]: training_loss: tensor(0.2746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [118/4339]: training_loss: tensor(0.3451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [119/4339]: training_loss: tensor(0.2109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [120/4339]: training_loss: tensor(0.3810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [121/4339]: training_loss: tensor(0.1251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [122/4339]: training_loss: tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [123/4339]: training_loss: tensor(0.5097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [124/4339]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [125/4339]: training_loss: tensor(0.4390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [126/4339]: training_loss: tensor(0.3882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [127/4339]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [128/4339]: training_loss: tensor(0.4558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [129/4339]: training_loss: tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [130/4339]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [131/4339]: training_loss: tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [132/4339]: training_loss: tensor(0.1656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [133/4339]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [134/4339]: training_loss: tensor(0.2170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [135/4339]: training_loss: tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [136/4339]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [137/4339]: training_loss: tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [138/4339]: training_loss: tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [139/4339]: training_loss: tensor(0.2645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [140/4339]: training_loss: tensor(0.1264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [141/4339]: training_loss: tensor(0.3899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [142/4339]: training_loss: tensor(0.4585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [143/4339]: training_loss: tensor(0.2772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [144/4339]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [145/4339]: training_loss: tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [146/4339]: training_loss: tensor(0.3199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [147/4339]: training_loss: tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [148/4339]: training_loss: tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [149/4339]: training_loss: tensor(0.3128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [150/4339]: training_loss: tensor(0.2972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [151/4339]: training_loss: tensor(0.0560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [152/4339]: training_loss: tensor(0.1308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [153/4339]: training_loss: tensor(0.1316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [154/4339]: training_loss: tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [155/4339]: training_loss: tensor(0.1379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [156/4339]: training_loss: tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [157/4339]: training_loss: tensor(0.1414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [158/4339]: training_loss: tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [159/4339]: training_loss: tensor(0.1771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [160/4339]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [161/4339]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [162/4339]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [163/4339]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [164/4339]: training_loss: tensor(0.4063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [165/4339]: training_loss: tensor(0.1891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [166/4339]: training_loss: tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [167/4339]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [168/4339]: training_loss: tensor(0.1354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [169/4339]: training_loss: tensor(0.4298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [170/4339]: training_loss: tensor(0.4167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [171/4339]: training_loss: tensor(0.1838, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [172/4339]: training_loss: tensor(0.3165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [173/4339]: training_loss: tensor(0.2730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [174/4339]: training_loss: tensor(0.3753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [175/4339]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [176/4339]: training_loss: tensor(0.1580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [177/4339]: training_loss: tensor(0.2446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [178/4339]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [179/4339]: training_loss: tensor(0.2031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [180/4339]: training_loss: tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [181/4339]: training_loss: tensor(0.1865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [182/4339]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [183/4339]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [184/4339]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [185/4339]: training_loss: tensor(0.1364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [186/4339]: training_loss: tensor(0.0637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [187/4339]: training_loss: tensor(0.1628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [188/4339]: training_loss: tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [189/4339]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [190/4339]: training_loss: tensor(0.1962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [191/4339]: training_loss: tensor(0.1201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [192/4339]: training_loss: tensor(0.1271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [193/4339]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [194/4339]: training_loss: tensor(0.2789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [195/4339]: training_loss: tensor(0.1169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [196/4339]: training_loss: tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [197/4339]: training_loss: tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [198/4339]: training_loss: tensor(0.2079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [199/4339]: training_loss: tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [200/4339]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [201/4339]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [202/4339]: training_loss: tensor(0.2220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [203/4339]: training_loss: tensor(0.1556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [204/4339]: training_loss: tensor(0.2304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [205/4339]: training_loss: tensor(0.0429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [206/4339]: training_loss: tensor(0.0487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [207/4339]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [208/4339]: training_loss: tensor(0.1128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [209/4339]: training_loss: tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [210/4339]: training_loss: tensor(0.5031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [211/4339]: training_loss: tensor(0.2562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [212/4339]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [213/4339]: training_loss: tensor(0.2208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [214/4339]: training_loss: tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [215/4339]: training_loss: tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [216/4339]: training_loss: tensor(0.1464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [217/4339]: training_loss: tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [218/4339]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [219/4339]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [220/4339]: training_loss: tensor(0.1833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [221/4339]: training_loss: tensor(0.3383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [222/4339]: training_loss: tensor(0.3549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [223/4339]: training_loss: tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [224/4339]: training_loss: tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [225/4339]: training_loss: tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [226/4339]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [227/4339]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [228/4339]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [229/4339]: training_loss: tensor(0.1529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [230/4339]: training_loss: tensor(0.3455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [231/4339]: training_loss: tensor(0.2037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [232/4339]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [233/4339]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [234/4339]: training_loss: tensor(0.1360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [235/4339]: training_loss: tensor(0.3172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [236/4339]: training_loss: tensor(0.2580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [237/4339]: training_loss: tensor(0.4442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [238/4339]: training_loss: tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [239/4339]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [240/4339]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [241/4339]: training_loss: tensor(0.1599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [242/4339]: training_loss: tensor(0.0362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [243/4339]: training_loss: tensor(0.1292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [244/4339]: training_loss: tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [245/4339]: training_loss: tensor(0.1231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [246/4339]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [247/4339]: training_loss: tensor(0.1711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [248/4339]: training_loss: tensor(0.4027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [249/4339]: training_loss: tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [250/4339]: training_loss: tensor(0.4179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [251/4339]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [252/4339]: training_loss: tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [253/4339]: training_loss: tensor(0.1607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [254/4339]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [255/4339]: training_loss: tensor(0.1628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [256/4339]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [257/4339]: training_loss: tensor(0.1364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [258/4339]: training_loss: tensor(0.2796, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [259/4339]: training_loss: tensor(0.1107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [260/4339]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [261/4339]: training_loss: tensor(0.1623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [262/4339]: training_loss: tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [263/4339]: training_loss: tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [264/4339]: training_loss: tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [265/4339]: training_loss: tensor(0.0361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [266/4339]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [267/4339]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [268/4339]: training_loss: tensor(0.1957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [269/4339]: training_loss: tensor(0.3720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [270/4339]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [271/4339]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [272/4339]: training_loss: tensor(0.0445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [273/4339]: training_loss: tensor(0.1888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [274/4339]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [275/4339]: training_loss: tensor(0.3075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [276/4339]: training_loss: tensor(0.3910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [277/4339]: training_loss: tensor(0.2158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [278/4339]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [279/4339]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [280/4339]: training_loss: tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [281/4339]: training_loss: tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [282/4339]: training_loss: tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [283/4339]: training_loss: tensor(0.3023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [284/4339]: training_loss: tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [285/4339]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [286/4339]: training_loss: tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [287/4339]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [288/4339]: training_loss: tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [289/4339]: training_loss: tensor(0.3277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [290/4339]: training_loss: tensor(0.3438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [291/4339]: training_loss: tensor(0.2040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [292/4339]: training_loss: tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [293/4339]: training_loss: tensor(0.1244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [294/4339]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [295/4339]: training_loss: tensor(0.0743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [296/4339]: training_loss: tensor(0.0566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [297/4339]: training_loss: tensor(0.2964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [298/4339]: training_loss: tensor(0.1433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [299/4339]: training_loss: tensor(0.2055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [300/4339]: training_loss: tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [301/4339]: training_loss: tensor(0.2554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [302/4339]: training_loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [303/4339]: training_loss: tensor(0.5189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [304/4339]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [305/4339]: training_loss: tensor(0.4802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [306/4339]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [307/4339]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [308/4339]: training_loss: tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [309/4339]: training_loss: tensor(0.1975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [310/4339]: training_loss: tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [311/4339]: training_loss: tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [312/4339]: training_loss: tensor(0.1366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [313/4339]: training_loss: tensor(0.4272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [314/4339]: training_loss: tensor(0.2439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [315/4339]: training_loss: tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [316/4339]: training_loss: tensor(0.0462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [317/4339]: training_loss: tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [318/4339]: training_loss: tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [319/4339]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [320/4339]: training_loss: tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [321/4339]: training_loss: tensor(0.2488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [322/4339]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [323/4339]: training_loss: tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [324/4339]: training_loss: tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [325/4339]: training_loss: tensor(0.4365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [326/4339]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [327/4339]: training_loss: tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [328/4339]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [329/4339]: training_loss: tensor(0.0365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [330/4339]: training_loss: tensor(0.0616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [331/4339]: training_loss: tensor(0.5097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [332/4339]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [333/4339]: training_loss: tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [334/4339]: training_loss: tensor(0.1222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [335/4339]: training_loss: tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [336/4339]: training_loss: tensor(0.3212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [337/4339]: training_loss: tensor(0.4818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [338/4339]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [339/4339]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [340/4339]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [341/4339]: training_loss: tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [342/4339]: training_loss: tensor(0.0939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [343/4339]: training_loss: tensor(0.1531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [344/4339]: training_loss: tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [345/4339]: training_loss: tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [346/4339]: training_loss: tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [347/4339]: training_loss: tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [348/4339]: training_loss: tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [349/4339]: training_loss: tensor(0.4717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [350/4339]: training_loss: tensor(0.2680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [351/4339]: training_loss: tensor(0.0973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [352/4339]: training_loss: tensor(0.1270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [353/4339]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [354/4339]: training_loss: tensor(0.2162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [355/4339]: training_loss: tensor(0.3014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [356/4339]: training_loss: tensor(0.0429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [357/4339]: training_loss: tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [358/4339]: training_loss: tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [359/4339]: training_loss: tensor(0.2160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [360/4339]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [361/4339]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [362/4339]: training_loss: tensor(0.2354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [363/4339]: training_loss: tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [364/4339]: training_loss: tensor(0.1533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [365/4339]: training_loss: tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [366/4339]: training_loss: tensor(0.1091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [367/4339]: training_loss: tensor(0.2012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [368/4339]: training_loss: tensor(0.5250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [369/4339]: training_loss: tensor(0.0975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [370/4339]: training_loss: tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [371/4339]: training_loss: tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [372/4339]: training_loss: tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [373/4339]: training_loss: tensor(0.2312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [374/4339]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [375/4339]: training_loss: tensor(0.1044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [376/4339]: training_loss: tensor(0.3140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [377/4339]: training_loss: tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [378/4339]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [379/4339]: training_loss: tensor(0.3102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [380/4339]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [381/4339]: training_loss: tensor(0.2004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [382/4339]: training_loss: tensor(0.6155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [383/4339]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [384/4339]: training_loss: tensor(0.3384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [385/4339]: training_loss: tensor(0.3148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [386/4339]: training_loss: tensor(0.2240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [387/4339]: training_loss: tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [388/4339]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [389/4339]: training_loss: tensor(0.2612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [390/4339]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [391/4339]: training_loss: tensor(0.1704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [392/4339]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [393/4339]: training_loss: tensor(0.2825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [394/4339]: training_loss: tensor(0.4044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [395/4339]: training_loss: tensor(0.0445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [396/4339]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [397/4339]: training_loss: tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [398/4339]: training_loss: tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [399/4339]: training_loss: tensor(0.3049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [400/4339]: training_loss: tensor(0.1796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [401/4339]: training_loss: tensor(0.2901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [402/4339]: training_loss: tensor(0.3036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [403/4339]: training_loss: tensor(0.0570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [404/4339]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [405/4339]: training_loss: tensor(0.1773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [406/4339]: training_loss: tensor(0.2209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [407/4339]: training_loss: tensor(0.0576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [408/4339]: training_loss: tensor(0.0509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [409/4339]: training_loss: tensor(0.4951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [410/4339]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [411/4339]: training_loss: tensor(0.5802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [412/4339]: training_loss: tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [413/4339]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [414/4339]: training_loss: tensor(0.2447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [415/4339]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [416/4339]: training_loss: tensor(0.0384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [417/4339]: training_loss: tensor(0.3626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [418/4339]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [419/4339]: training_loss: tensor(0.3635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [420/4339]: training_loss: tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [421/4339]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [422/4339]: training_loss: tensor(0.3180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [423/4339]: training_loss: tensor(0.1450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [424/4339]: training_loss: tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [425/4339]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [426/4339]: training_loss: tensor(0.3101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [427/4339]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [428/4339]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [429/4339]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [430/4339]: training_loss: tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [431/4339]: training_loss: tensor(0.1567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [432/4339]: training_loss: tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [433/4339]: training_loss: tensor(0.1258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [434/4339]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [435/4339]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [436/4339]: training_loss: tensor(0.4398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [437/4339]: training_loss: tensor(0.4013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [438/4339]: training_loss: tensor(0.0620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [439/4339]: training_loss: tensor(0.1645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [440/4339]: training_loss: tensor(0.4461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [441/4339]: training_loss: tensor(0.1534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [442/4339]: training_loss: tensor(0.0560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [443/4339]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [444/4339]: training_loss: tensor(0.3166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [445/4339]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [446/4339]: training_loss: tensor(0.1211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [447/4339]: training_loss: tensor(0.2111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [448/4339]: training_loss: tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [449/4339]: training_loss: tensor(0.0447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [450/4339]: training_loss: tensor(0.1385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [451/4339]: training_loss: tensor(0.1498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [452/4339]: training_loss: tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [453/4339]: training_loss: tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [454/4339]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [455/4339]: training_loss: tensor(0.1607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [456/4339]: training_loss: tensor(0.5046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [457/4339]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [458/4339]: training_loss: tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [459/4339]: training_loss: tensor(0.1735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [460/4339]: training_loss: tensor(0.3639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [461/4339]: training_loss: tensor(0.2910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [462/4339]: training_loss: tensor(0.0582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [463/4339]: training_loss: tensor(0.1927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [464/4339]: training_loss: tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [465/4339]: training_loss: tensor(0.1044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [466/4339]: training_loss: tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [467/4339]: training_loss: tensor(0.4559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [468/4339]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [469/4339]: training_loss: tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [470/4339]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [471/4339]: training_loss: tensor(0.0313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [472/4339]: training_loss: tensor(0.0556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [473/4339]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [474/4339]: training_loss: tensor(0.3219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [475/4339]: training_loss: tensor(0.1169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [476/4339]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [477/4339]: training_loss: tensor(0.1593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [478/4339]: training_loss: tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [479/4339]: training_loss: tensor(0.0290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [480/4339]: training_loss: tensor(0.3903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [481/4339]: training_loss: tensor(0.3064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [482/4339]: training_loss: tensor(0.3208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [483/4339]: training_loss: tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [484/4339]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [485/4339]: training_loss: tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [486/4339]: training_loss: tensor(0.1710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [487/4339]: training_loss: tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [488/4339]: training_loss: tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [489/4339]: training_loss: tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [490/4339]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [491/4339]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [492/4339]: training_loss: tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [493/4339]: training_loss: tensor(0.3187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [494/4339]: training_loss: tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [495/4339]: training_loss: tensor(0.2305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [496/4339]: training_loss: tensor(0.4310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [497/4339]: training_loss: tensor(0.1112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [498/4339]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [499/4339]: training_loss: tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [500/4339]: training_loss: tensor(0.7005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [501/4339]: training_loss: tensor(0.2002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [502/4339]: training_loss: tensor(0.1936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [503/4339]: training_loss: tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [504/4339]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [505/4339]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [506/4339]: training_loss: tensor(0.2705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [507/4339]: training_loss: tensor(0.2157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [508/4339]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [509/4339]: training_loss: tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [510/4339]: training_loss: tensor(0.2279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [511/4339]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [512/4339]: training_loss: tensor(0.1144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [513/4339]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [514/4339]: training_loss: tensor(0.0547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [515/4339]: training_loss: tensor(0.1685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [516/4339]: training_loss: tensor(0.1857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [517/4339]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [518/4339]: training_loss: tensor(0.2343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [519/4339]: training_loss: tensor(0.0404, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [520/4339]: training_loss: tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [521/4339]: training_loss: tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [522/4339]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [523/4339]: training_loss: tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [524/4339]: training_loss: tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [525/4339]: training_loss: tensor(0.6014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [526/4339]: training_loss: tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [527/4339]: training_loss: tensor(0.0543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [528/4339]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [529/4339]: training_loss: tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [530/4339]: training_loss: tensor(0.1628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [531/4339]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [532/4339]: training_loss: tensor(0.1324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [533/4339]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [534/4339]: training_loss: tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [535/4339]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [536/4339]: training_loss: tensor(0.3899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [537/4339]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [538/4339]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [539/4339]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [540/4339]: training_loss: tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [541/4339]: training_loss: tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [542/4339]: training_loss: tensor(0.2873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [543/4339]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [544/4339]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [545/4339]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [546/4339]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [547/4339]: training_loss: tensor(0.1201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [548/4339]: training_loss: tensor(0.3648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [549/4339]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [550/4339]: training_loss: tensor(0.0361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [551/4339]: training_loss: tensor(0.1137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [552/4339]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [553/4339]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [554/4339]: training_loss: tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [555/4339]: training_loss: tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [556/4339]: training_loss: tensor(0.3523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [557/4339]: training_loss: tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [558/4339]: training_loss: tensor(0.1387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [559/4339]: training_loss: tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [560/4339]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [561/4339]: training_loss: tensor(0.2377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [562/4339]: training_loss: tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [563/4339]: training_loss: tensor(0.1938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [564/4339]: training_loss: tensor(0.0534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [565/4339]: training_loss: tensor(0.1982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [566/4339]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [567/4339]: training_loss: tensor(0.3177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [568/4339]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [569/4339]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [570/4339]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [571/4339]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [572/4339]: training_loss: tensor(0.1883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [573/4339]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [574/4339]: training_loss: tensor(0.0365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [575/4339]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [576/4339]: training_loss: tensor(0.1552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [577/4339]: training_loss: tensor(0.1606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [578/4339]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [579/4339]: training_loss: tensor(0.1471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [580/4339]: training_loss: tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [581/4339]: training_loss: tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [582/4339]: training_loss: tensor(0.1748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [583/4339]: training_loss: tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [584/4339]: training_loss: tensor(0.3732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [585/4339]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [586/4339]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [587/4339]: training_loss: tensor(0.2463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [588/4339]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [589/4339]: training_loss: tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [590/4339]: training_loss: tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [591/4339]: training_loss: tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [592/4339]: training_loss: tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [593/4339]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [594/4339]: training_loss: tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [595/4339]: training_loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [596/4339]: training_loss: tensor(0.2132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [597/4339]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [598/4339]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [599/4339]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [600/4339]: training_loss: tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [601/4339]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [602/4339]: training_loss: tensor(0.2149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [603/4339]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [604/4339]: training_loss: tensor(0.1794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [605/4339]: training_loss: tensor(0.2923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [606/4339]: training_loss: tensor(0.3758, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [607/4339]: training_loss: tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [608/4339]: training_loss: tensor(0.2320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [609/4339]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [610/4339]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [611/4339]: training_loss: tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [612/4339]: training_loss: tensor(0.1283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [613/4339]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [614/4339]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [615/4339]: training_loss: tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [616/4339]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [617/4339]: training_loss: tensor(0.1306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [618/4339]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [619/4339]: training_loss: tensor(0.1618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [620/4339]: training_loss: tensor(0.2515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [621/4339]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [622/4339]: training_loss: tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [623/4339]: training_loss: tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [624/4339]: training_loss: tensor(0.5340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [625/4339]: training_loss: tensor(0.1590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [626/4339]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [627/4339]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [628/4339]: training_loss: tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [629/4339]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [630/4339]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [631/4339]: training_loss: tensor(0.1709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [632/4339]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [633/4339]: training_loss: tensor(0.3026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [634/4339]: training_loss: tensor(0.2008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [635/4339]: training_loss: tensor(0.2435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [636/4339]: training_loss: tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [637/4339]: training_loss: tensor(0.1587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [638/4339]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [639/4339]: training_loss: tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [640/4339]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [641/4339]: training_loss: tensor(0.3954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [642/4339]: training_loss: tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [643/4339]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [644/4339]: training_loss: tensor(0.2464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [645/4339]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [646/4339]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [647/4339]: training_loss: tensor(0.1212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [648/4339]: training_loss: tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [649/4339]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [650/4339]: training_loss: tensor(0.1950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [651/4339]: training_loss: tensor(0.2038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [652/4339]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [653/4339]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [654/4339]: training_loss: tensor(0.2611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [655/4339]: training_loss: tensor(0.2254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [656/4339]: training_loss: tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [657/4339]: training_loss: tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [658/4339]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [659/4339]: training_loss: tensor(0.2224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [660/4339]: training_loss: tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [661/4339]: training_loss: tensor(0.3030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [662/4339]: training_loss: tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [663/4339]: training_loss: tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [664/4339]: training_loss: tensor(0.2449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [665/4339]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [666/4339]: training_loss: tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [667/4339]: training_loss: tensor(0.2981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [668/4339]: training_loss: tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [669/4339]: training_loss: tensor(0.1369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [670/4339]: training_loss: tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [671/4339]: training_loss: tensor(0.1229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [672/4339]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [673/4339]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [674/4339]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [675/4339]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [676/4339]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [677/4339]: training_loss: tensor(0.1539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [678/4339]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [679/4339]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [680/4339]: training_loss: tensor(0.2598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [681/4339]: training_loss: tensor(0.2053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [682/4339]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [683/4339]: training_loss: tensor(0.1749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [684/4339]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [685/4339]: training_loss: tensor(0.2342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [686/4339]: training_loss: tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [687/4339]: training_loss: tensor(0.2223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [688/4339]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [689/4339]: training_loss: tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [690/4339]: training_loss: tensor(0.2332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [691/4339]: training_loss: tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [692/4339]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [693/4339]: training_loss: tensor(0.6096, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [694/4339]: training_loss: tensor(0.3186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [695/4339]: training_loss: tensor(0.3778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [696/4339]: training_loss: tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [697/4339]: training_loss: tensor(0.2027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [698/4339]: training_loss: tensor(0.2699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [699/4339]: training_loss: tensor(0.2939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [700/4339]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [701/4339]: training_loss: tensor(0.0476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [702/4339]: training_loss: tensor(0.2471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [703/4339]: training_loss: tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [704/4339]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [705/4339]: training_loss: tensor(0.2024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [706/4339]: training_loss: tensor(0.0305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [707/4339]: training_loss: tensor(0.1376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [708/4339]: training_loss: tensor(0.0620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [709/4339]: training_loss: tensor(0.0570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [710/4339]: training_loss: tensor(0.3189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [711/4339]: training_loss: tensor(0.1400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [712/4339]: training_loss: tensor(0.1469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [713/4339]: training_loss: tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [714/4339]: training_loss: tensor(0.1513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [715/4339]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [716/4339]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [717/4339]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [718/4339]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [719/4339]: training_loss: tensor(0.0621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [720/4339]: training_loss: tensor(0.1718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [721/4339]: training_loss: tensor(0.1479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [722/4339]: training_loss: tensor(0.3313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [723/4339]: training_loss: tensor(0.1362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [724/4339]: training_loss: tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [725/4339]: training_loss: tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [726/4339]: training_loss: tensor(0.1255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [727/4339]: training_loss: tensor(0.2126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [728/4339]: training_loss: tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [729/4339]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [730/4339]: training_loss: tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [731/4339]: training_loss: tensor(0.3291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [732/4339]: training_loss: tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [733/4339]: training_loss: tensor(0.3166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [734/4339]: training_loss: tensor(0.1430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [735/4339]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [736/4339]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [737/4339]: training_loss: tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [738/4339]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [739/4339]: training_loss: tensor(0.1394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [740/4339]: training_loss: tensor(0.1404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [741/4339]: training_loss: tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [742/4339]: training_loss: tensor(0.1682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [743/4339]: training_loss: tensor(0.1967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [744/4339]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [745/4339]: training_loss: tensor(0.1914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [746/4339]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [747/4339]: training_loss: tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [748/4339]: training_loss: tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [749/4339]: training_loss: tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [750/4339]: training_loss: tensor(0.0305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [751/4339]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [752/4339]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [753/4339]: training_loss: tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [754/4339]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [755/4339]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [756/4339]: training_loss: tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [757/4339]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [758/4339]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [759/4339]: training_loss: tensor(0.1953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [760/4339]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [761/4339]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [762/4339]: training_loss: tensor(0.2066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [763/4339]: training_loss: tensor(0.2247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [764/4339]: training_loss: tensor(0.1541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [765/4339]: training_loss: tensor(0.3826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [766/4339]: training_loss: tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [767/4339]: training_loss: tensor(0.3040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [768/4339]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [769/4339]: training_loss: tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [770/4339]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [771/4339]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [772/4339]: training_loss: tensor(0.1449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [773/4339]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [774/4339]: training_loss: tensor(0.4675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [775/4339]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [776/4339]: training_loss: tensor(0.0541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [777/4339]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [778/4339]: training_loss: tensor(0.2042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [779/4339]: training_loss: tensor(0.1964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [780/4339]: training_loss: tensor(0.2843, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [781/4339]: training_loss: tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [782/4339]: training_loss: tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [783/4339]: training_loss: tensor(0.0384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [784/4339]: training_loss: tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [785/4339]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [786/4339]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [787/4339]: training_loss: tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [788/4339]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [789/4339]: training_loss: tensor(0.4105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [790/4339]: training_loss: tensor(0.2539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [791/4339]: training_loss: tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [792/4339]: training_loss: tensor(0.0467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [793/4339]: training_loss: tensor(0.4984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [794/4339]: training_loss: tensor(0.2731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [795/4339]: training_loss: tensor(0.4290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [796/4339]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [797/4339]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [798/4339]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [799/4339]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [800/4339]: training_loss: tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [801/4339]: training_loss: tensor(0.1471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [802/4339]: training_loss: tensor(0.2181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [803/4339]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [804/4339]: training_loss: tensor(0.2302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [805/4339]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [806/4339]: training_loss: tensor(0.1499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [807/4339]: training_loss: tensor(0.2325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [808/4339]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [809/4339]: training_loss: tensor(0.3799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [810/4339]: training_loss: tensor(0.3544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [811/4339]: training_loss: tensor(0.2833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [812/4339]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [813/4339]: training_loss: tensor(0.1727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [814/4339]: training_loss: tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [815/4339]: training_loss: tensor(0.2582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [816/4339]: training_loss: tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [817/4339]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [818/4339]: training_loss: tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [819/4339]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [820/4339]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [821/4339]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [822/4339]: training_loss: tensor(0.1395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [823/4339]: training_loss: tensor(0.1366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [824/4339]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [825/4339]: training_loss: tensor(0.1303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [826/4339]: training_loss: tensor(0.1530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [827/4339]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [828/4339]: training_loss: tensor(0.1609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [829/4339]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [830/4339]: training_loss: tensor(0.1336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [831/4339]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [832/4339]: training_loss: tensor(0.1936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [833/4339]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [834/4339]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [835/4339]: training_loss: tensor(0.4077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [836/4339]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [837/4339]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [838/4339]: training_loss: tensor(0.1497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [839/4339]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [840/4339]: training_loss: tensor(0.3387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [841/4339]: training_loss: tensor(0.3864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [842/4339]: training_loss: tensor(0.2217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [843/4339]: training_loss: tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [844/4339]: training_loss: tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [845/4339]: training_loss: tensor(0.3467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [846/4339]: training_loss: tensor(0.2891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [847/4339]: training_loss: tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [848/4339]: training_loss: tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [849/4339]: training_loss: tensor(0.1396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [850/4339]: training_loss: tensor(0.1492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [851/4339]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [852/4339]: training_loss: tensor(0.5240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [853/4339]: training_loss: tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [854/4339]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [855/4339]: training_loss: tensor(0.1409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [856/4339]: training_loss: tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [857/4339]: training_loss: tensor(0.3237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [858/4339]: training_loss: tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [859/4339]: training_loss: tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [860/4339]: training_loss: tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [861/4339]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [862/4339]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [863/4339]: training_loss: tensor(0.1978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [864/4339]: training_loss: tensor(0.1279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [865/4339]: training_loss: tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [866/4339]: training_loss: tensor(0.1068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [867/4339]: training_loss: tensor(0.2572, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [868/4339]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [869/4339]: training_loss: tensor(0.1845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [870/4339]: training_loss: tensor(0.6386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [871/4339]: training_loss: tensor(0.1156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [872/4339]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [873/4339]: training_loss: tensor(0.1637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [874/4339]: training_loss: tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [875/4339]: training_loss: tensor(0.1447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [876/4339]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [877/4339]: training_loss: tensor(0.1403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [878/4339]: training_loss: tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [879/4339]: training_loss: tensor(0.2589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [880/4339]: training_loss: tensor(0.1448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [881/4339]: training_loss: tensor(0.1738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [882/4339]: training_loss: tensor(0.0594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [883/4339]: training_loss: tensor(0.1059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [884/4339]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [885/4339]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [886/4339]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [887/4339]: training_loss: tensor(0.2293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [888/4339]: training_loss: tensor(0.2810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [889/4339]: training_loss: tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [890/4339]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [891/4339]: training_loss: tensor(0.5000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [892/4339]: training_loss: tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [893/4339]: training_loss: tensor(0.2099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [894/4339]: training_loss: tensor(0.1522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [895/4339]: training_loss: tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [896/4339]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [897/4339]: training_loss: tensor(0.1590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [898/4339]: training_loss: tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [899/4339]: training_loss: tensor(0.1192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [900/4339]: training_loss: tensor(0.5091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [901/4339]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [902/4339]: training_loss: tensor(0.2496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [903/4339]: training_loss: tensor(0.1959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [904/4339]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [905/4339]: training_loss: tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [906/4339]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [907/4339]: training_loss: tensor(0.1251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [908/4339]: training_loss: tensor(0.1335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [909/4339]: training_loss: tensor(0.4184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [910/4339]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [911/4339]: training_loss: tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [912/4339]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [913/4339]: training_loss: tensor(0.0572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [914/4339]: training_loss: tensor(0.0389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [915/4339]: training_loss: tensor(0.1474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [916/4339]: training_loss: tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [917/4339]: training_loss: tensor(0.4216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [918/4339]: training_loss: tensor(0.2796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [919/4339]: training_loss: tensor(0.2606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [920/4339]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [921/4339]: training_loss: tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [922/4339]: training_loss: tensor(0.1494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [923/4339]: training_loss: tensor(0.1189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [924/4339]: training_loss: tensor(0.2309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [925/4339]: training_loss: tensor(0.1307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [926/4339]: training_loss: tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [927/4339]: training_loss: tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [928/4339]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [929/4339]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [930/4339]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [931/4339]: training_loss: tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [932/4339]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [933/4339]: training_loss: tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [934/4339]: training_loss: tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [935/4339]: training_loss: tensor(0.1730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [936/4339]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [937/4339]: training_loss: tensor(0.6031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [938/4339]: training_loss: tensor(0.0475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [939/4339]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [940/4339]: training_loss: tensor(0.3535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [941/4339]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [942/4339]: training_loss: tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [943/4339]: training_loss: tensor(0.1240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [944/4339]: training_loss: tensor(0.2440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [945/4339]: training_loss: tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [946/4339]: training_loss: tensor(0.0469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [947/4339]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [948/4339]: training_loss: tensor(0.1544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [949/4339]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [950/4339]: training_loss: tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [951/4339]: training_loss: tensor(0.1097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [952/4339]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [953/4339]: training_loss: tensor(0.2064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [954/4339]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [955/4339]: training_loss: tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [956/4339]: training_loss: tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [957/4339]: training_loss: tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [958/4339]: training_loss: tensor(0.3121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [959/4339]: training_loss: tensor(0.0629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [960/4339]: training_loss: tensor(0.6304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [961/4339]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [962/4339]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [963/4339]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [964/4339]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [965/4339]: training_loss: tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [966/4339]: training_loss: tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [967/4339]: training_loss: tensor(0.3068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [968/4339]: training_loss: tensor(0.1836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [969/4339]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [970/4339]: training_loss: tensor(0.4134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [971/4339]: training_loss: tensor(0.2877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [972/4339]: training_loss: tensor(0.2222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [973/4339]: training_loss: tensor(0.1433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [974/4339]: training_loss: tensor(0.5969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [975/4339]: training_loss: tensor(0.1782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [976/4339]: training_loss: tensor(0.3476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [977/4339]: training_loss: tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [978/4339]: training_loss: tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [979/4339]: training_loss: tensor(0.4890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [980/4339]: training_loss: tensor(0.2169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [981/4339]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [982/4339]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [983/4339]: training_loss: tensor(0.2570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [984/4339]: training_loss: tensor(0.1321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [985/4339]: training_loss: tensor(0.0342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [986/4339]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [987/4339]: training_loss: tensor(0.2543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [988/4339]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [989/4339]: training_loss: tensor(0.1263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [990/4339]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [991/4339]: training_loss: tensor(0.1598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [992/4339]: training_loss: tensor(0.1567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [993/4339]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [994/4339]: training_loss: tensor(0.2106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [995/4339]: training_loss: tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [996/4339]: training_loss: tensor(0.1473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [997/4339]: training_loss: tensor(0.3734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [998/4339]: training_loss: tensor(0.1290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [999/4339]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1000/4339]: training_loss: tensor(0.1454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1001/4339]: training_loss: tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1002/4339]: training_loss: tensor(0.2115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1003/4339]: training_loss: tensor(0.2172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1004/4339]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1005/4339]: training_loss: tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/4339]: training_loss: tensor(0.3640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/4339]: training_loss: tensor(0.1854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/4339]: training_loss: tensor(0.4579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/4339]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/4339]: training_loss: tensor(0.1801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1011/4339]: training_loss: tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/4339]: training_loss: tensor(0.1479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/4339]: training_loss: tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/4339]: training_loss: tensor(0.1613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/4339]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/4339]: training_loss: tensor(0.1671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/4339]: training_loss: tensor(0.0487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/4339]: training_loss: tensor(0.1554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1019/4339]: training_loss: tensor(0.1973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/4339]: training_loss: tensor(0.2103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/4339]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/4339]: training_loss: tensor(0.3678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1023/4339]: training_loss: tensor(0.3716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/4339]: training_loss: tensor(0.2057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/4339]: training_loss: tensor(0.1103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/4339]: training_loss: tensor(0.3298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/4339]: training_loss: tensor(0.3303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/4339]: training_loss: tensor(0.1384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/4339]: training_loss: tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/4339]: training_loss: tensor(0.0469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/4339]: training_loss: tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/4339]: training_loss: tensor(0.2732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/4339]: training_loss: tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/4339]: training_loss: tensor(0.1251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/4339]: training_loss: tensor(0.2148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/4339]: training_loss: tensor(0.1434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/4339]: training_loss: tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/4339]: training_loss: tensor(0.1944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/4339]: training_loss: tensor(0.2244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1040/4339]: training_loss: tensor(0.1177, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1041/4339]: training_loss: tensor(0.1871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/4339]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1043/4339]: training_loss: tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/4339]: training_loss: tensor(0.0606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/4339]: training_loss: tensor(0.2575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/4339]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/4339]: training_loss: tensor(0.3177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/4339]: training_loss: tensor(0.3023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/4339]: training_loss: tensor(0.0364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/4339]: training_loss: tensor(0.1397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/4339]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/4339]: training_loss: tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/4339]: training_loss: tensor(0.1813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/4339]: training_loss: tensor(0.3823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/4339]: training_loss: tensor(0.1097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/4339]: training_loss: tensor(0.1483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/4339]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/4339]: training_loss: tensor(0.1531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/4339]: training_loss: tensor(0.1355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/4339]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/4339]: training_loss: tensor(0.0563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/4339]: training_loss: tensor(0.1674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/4339]: training_loss: tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/4339]: training_loss: tensor(0.3934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/4339]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/4339]: training_loss: tensor(0.2478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/4339]: training_loss: tensor(0.3943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/4339]: training_loss: tensor(0.1093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/4339]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/4339]: training_loss: tensor(0.5126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/4339]: training_loss: tensor(0.3520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/4339]: training_loss: tensor(0.3564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/4339]: training_loss: tensor(0.1622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/4339]: training_loss: tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/4339]: training_loss: tensor(0.2199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/4339]: training_loss: tensor(0.1962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/4339]: training_loss: tensor(0.2211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/4339]: training_loss: tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/4339]: training_loss: tensor(0.2974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/4339]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/4339]: training_loss: tensor(0.3180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/4339]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/4339]: training_loss: tensor(0.1626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/4339]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/4339]: training_loss: tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/4339]: training_loss: tensor(0.2625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/4339]: training_loss: tensor(0.1967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/4339]: training_loss: tensor(0.3331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/4339]: training_loss: tensor(0.1831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/4339]: training_loss: tensor(0.2809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/4339]: training_loss: tensor(0.3408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/4339]: training_loss: tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/4339]: training_loss: tensor(0.2121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/4339]: training_loss: tensor(0.2851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/4339]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/4339]: training_loss: tensor(0.0362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1097/4339]: training_loss: tensor(0.3686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/4339]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/4339]: training_loss: tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1100/4339]: training_loss: tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/4339]: training_loss: tensor(0.1780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/4339]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/4339]: training_loss: tensor(0.2696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/4339]: training_loss: tensor(0.1844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/4339]: training_loss: tensor(0.2092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/4339]: training_loss: tensor(0.2312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/4339]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/4339]: training_loss: tensor(0.3030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/4339]: training_loss: tensor(0.1782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/4339]: training_loss: tensor(0.2532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/4339]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/4339]: training_loss: tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/4339]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/4339]: training_loss: tensor(0.1742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/4339]: training_loss: tensor(0.1353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/4339]: training_loss: tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/4339]: training_loss: tensor(0.2424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/4339]: training_loss: tensor(0.3431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/4339]: training_loss: tensor(0.1317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/4339]: training_loss: tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/4339]: training_loss: tensor(0.1409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/4339]: training_loss: tensor(0.0495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/4339]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/4339]: training_loss: tensor(0.2727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/4339]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/4339]: training_loss: tensor(0.1201, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1127/4339]: training_loss: tensor(0.4480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/4339]: training_loss: tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1129/4339]: training_loss: tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/4339]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/4339]: training_loss: tensor(0.2355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/4339]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/4339]: training_loss: tensor(0.3158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/4339]: training_loss: tensor(0.1304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/4339]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/4339]: training_loss: tensor(0.1974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/4339]: training_loss: tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/4339]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/4339]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/4339]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/4339]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/4339]: training_loss: tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/4339]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/4339]: training_loss: tensor(0.1994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/4339]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/4339]: training_loss: tensor(0.2705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/4339]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/4339]: training_loss: tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/4339]: training_loss: tensor(0.1487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/4339]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/4339]: training_loss: tensor(0.1339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/4339]: training_loss: tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/4339]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/4339]: training_loss: tensor(0.0410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/4339]: training_loss: tensor(0.1856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/4339]: training_loss: tensor(0.1667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/4339]: training_loss: tensor(0.2823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/4339]: training_loss: tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/4339]: training_loss: tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/4339]: training_loss: tensor(0.2889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/4339]: training_loss: tensor(0.2509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/4339]: training_loss: tensor(0.2296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/4339]: training_loss: tensor(0.1389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/4339]: training_loss: tensor(0.1453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/4339]: training_loss: tensor(0.3395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/4339]: training_loss: tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/4339]: training_loss: tensor(0.1304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/4339]: training_loss: tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/4339]: training_loss: tensor(0.2142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/4339]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/4339]: training_loss: tensor(0.2186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/4339]: training_loss: tensor(0.2145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/4339]: training_loss: tensor(0.2990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/4339]: training_loss: tensor(0.1921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/4339]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/4339]: training_loss: tensor(0.1979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/4339]: training_loss: tensor(0.3199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/4339]: training_loss: tensor(0.2043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/4339]: training_loss: tensor(0.1888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/4339]: training_loss: tensor(0.1789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/4339]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/4339]: training_loss: tensor(0.1658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1183/4339]: training_loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/4339]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/4339]: training_loss: tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/4339]: training_loss: tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/4339]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/4339]: training_loss: tensor(0.4349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/4339]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/4339]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/4339]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/4339]: training_loss: tensor(0.2752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/4339]: training_loss: tensor(0.0615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/4339]: training_loss: tensor(0.1269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/4339]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/4339]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/4339]: training_loss: tensor(0.0290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/4339]: training_loss: tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/4339]: training_loss: tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/4339]: training_loss: tensor(0.1347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/4339]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/4339]: training_loss: tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/4339]: training_loss: tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/4339]: training_loss: tensor(0.2300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/4339]: training_loss: tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/4339]: training_loss: tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/4339]: training_loss: tensor(0.3755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/4339]: training_loss: tensor(0.1238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/4339]: training_loss: tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/4339]: training_loss: tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/4339]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/4339]: training_loss: tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1213/4339]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/4339]: training_loss: tensor(0.1319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1215/4339]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/4339]: training_loss: tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/4339]: training_loss: tensor(0.3300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/4339]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/4339]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/4339]: training_loss: tensor(0.2086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/4339]: training_loss: tensor(0.0429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/4339]: training_loss: tensor(0.3722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/4339]: training_loss: tensor(0.1370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/4339]: training_loss: tensor(0.3343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/4339]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/4339]: training_loss: tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/4339]: training_loss: tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/4339]: training_loss: tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/4339]: training_loss: tensor(0.1319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/4339]: training_loss: tensor(0.1743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/4339]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/4339]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/4339]: training_loss: tensor(0.2274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/4339]: training_loss: tensor(0.2342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/4339]: training_loss: tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/4339]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/4339]: training_loss: tensor(0.3615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/4339]: training_loss: tensor(0.2092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/4339]: training_loss: tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/4339]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/4339]: training_loss: tensor(0.3924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/4339]: training_loss: tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/4339]: training_loss: tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/4339]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/4339]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/4339]: training_loss: tensor(0.2351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/4339]: training_loss: tensor(0.1881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/4339]: training_loss: tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/4339]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/4339]: training_loss: tensor(0.1646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/4339]: training_loss: tensor(0.1394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/4339]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/4339]: training_loss: tensor(0.4762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/4339]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/4339]: training_loss: tensor(0.0969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/4339]: training_loss: tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/4339]: training_loss: tensor(0.1576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/4339]: training_loss: tensor(0.1932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/4339]: training_loss: tensor(0.2222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/4339]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/4339]: training_loss: tensor(0.2589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/4339]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/4339]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/4339]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/4339]: training_loss: tensor(0.2048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/4339]: training_loss: tensor(0.4249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/4339]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/4339]: training_loss: tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1269/4339]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/4339]: training_loss: tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/4339]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/4339]: training_loss: tensor(0.1744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/4339]: training_loss: tensor(0.3135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/4339]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/4339]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/4339]: training_loss: tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/4339]: training_loss: tensor(0.1200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/4339]: training_loss: tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/4339]: training_loss: tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/4339]: training_loss: tensor(0.1554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/4339]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/4339]: training_loss: tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/4339]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/4339]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/4339]: training_loss: tensor(0.2813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/4339]: training_loss: tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/4339]: training_loss: tensor(0.1742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/4339]: training_loss: tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/4339]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/4339]: training_loss: tensor(0.1985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/4339]: training_loss: tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/4339]: training_loss: tensor(0.1232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/4339]: training_loss: tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/4339]: training_loss: tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/4339]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/4339]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/4339]: training_loss: tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/4339]: training_loss: tensor(0.2010, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1299/4339]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/4339]: training_loss: tensor(0.3562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/4339]: training_loss: tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/4339]: training_loss: tensor(0.2009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/4339]: training_loss: tensor(0.1742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/4339]: training_loss: tensor(0.2310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/4339]: training_loss: tensor(0.3026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/4339]: training_loss: tensor(0.3855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/4339]: training_loss: tensor(0.1939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/4339]: training_loss: tensor(0.6107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/4339]: training_loss: tensor(0.0484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/4339]: training_loss: tensor(0.1730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/4339]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/4339]: training_loss: tensor(0.3039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/4339]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/4339]: training_loss: tensor(0.2319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/4339]: training_loss: tensor(0.0502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/4339]: training_loss: tensor(0.2284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/4339]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/4339]: training_loss: tensor(0.1400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/4339]: training_loss: tensor(0.2567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/4339]: training_loss: tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/4339]: training_loss: tensor(0.1431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/4339]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/4339]: training_loss: tensor(0.2435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/4339]: training_loss: tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/4339]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/4339]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/4339]: training_loss: tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/4339]: training_loss: tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/4339]: training_loss: tensor(0.3729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/4339]: training_loss: tensor(0.1886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/4339]: training_loss: tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/4339]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/4339]: training_loss: tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/4339]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/4339]: training_loss: tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/4339]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/4339]: training_loss: tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/4339]: training_loss: tensor(0.2134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/4339]: training_loss: tensor(0.1694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/4339]: training_loss: tensor(0.2014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/4339]: training_loss: tensor(0.1216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/4339]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/4339]: training_loss: tensor(0.2528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/4339]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/4339]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/4339]: training_loss: tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/4339]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/4339]: training_loss: tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/4339]: training_loss: tensor(0.2134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/4339]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/4339]: training_loss: tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/4339]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/4339]: training_loss: tensor(0.3340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/4339]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1355/4339]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/4339]: training_loss: tensor(0.1447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/4339]: training_loss: tensor(0.2253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/4339]: training_loss: tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/4339]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/4339]: training_loss: tensor(0.1339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1361/4339]: training_loss: tensor(0.1797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/4339]: training_loss: tensor(0.2023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1363/4339]: training_loss: tensor(0.0983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/4339]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/4339]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/4339]: training_loss: tensor(0.2734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/4339]: training_loss: tensor(0.1605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/4339]: training_loss: tensor(0.2164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/4339]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/4339]: training_loss: tensor(0.1069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/4339]: training_loss: tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/4339]: training_loss: tensor(0.2233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/4339]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/4339]: training_loss: tensor(0.3232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/4339]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/4339]: training_loss: tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/4339]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/4339]: training_loss: tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/4339]: training_loss: tensor(0.3733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/4339]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/4339]: training_loss: tensor(0.3098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/4339]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/4339]: training_loss: tensor(0.6712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/4339]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1385/4339]: training_loss: tensor(0.1359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1386/4339]: training_loss: tensor(0.0501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1387/4339]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1388/4339]: training_loss: tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1389/4339]: training_loss: tensor(0.1315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1390/4339]: training_loss: tensor(0.1230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1391/4339]: training_loss: tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1392/4339]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1393/4339]: training_loss: tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1394/4339]: training_loss: tensor(0.1466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1395/4339]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1396/4339]: training_loss: tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1397/4339]: training_loss: tensor(0.1948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1398/4339]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1399/4339]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1400/4339]: training_loss: tensor(0.0342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1401/4339]: training_loss: tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1402/4339]: training_loss: tensor(0.0456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1403/4339]: training_loss: tensor(0.1672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1404/4339]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1405/4339]: training_loss: tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1406/4339]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1407/4339]: training_loss: tensor(0.3150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1408/4339]: training_loss: tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1409/4339]: training_loss: tensor(0.2236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1410/4339]: training_loss: tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1411/4339]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1412/4339]: training_loss: tensor(0.1709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1413/4339]: training_loss: tensor(0.2171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1414/4339]: training_loss: tensor(0.4473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1415/4339]: training_loss: tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1416/4339]: training_loss: tensor(0.2657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1417/4339]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1418/4339]: training_loss: tensor(0.3674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1419/4339]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1420/4339]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1421/4339]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1422/4339]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1423/4339]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1424/4339]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1425/4339]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1426/4339]: training_loss: tensor(0.2135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1427/4339]: training_loss: tensor(0.5275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1428/4339]: training_loss: tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1429/4339]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1430/4339]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1431/4339]: training_loss: tensor(0.2242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1432/4339]: training_loss: tensor(0.1868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1433/4339]: training_loss: tensor(0.1892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1434/4339]: training_loss: tensor(0.0587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1435/4339]: training_loss: tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1436/4339]: training_loss: tensor(0.2342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1437/4339]: training_loss: tensor(0.0365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1438/4339]: training_loss: tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1439/4339]: training_loss: tensor(0.0844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1440/4339]: training_loss: tensor(0.2273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1441/4339]: training_loss: tensor(0.1351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1442/4339]: training_loss: tensor(0.1442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1443/4339]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1444/4339]: training_loss: tensor(0.1476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1445/4339]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1446/4339]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1447/4339]: training_loss: tensor(0.1955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1448/4339]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1449/4339]: training_loss: tensor(0.1318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1450/4339]: training_loss: tensor(0.1712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1451/4339]: training_loss: tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1452/4339]: training_loss: tensor(0.2239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1453/4339]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1454/4339]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1455/4339]: training_loss: tensor(0.2717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1456/4339]: training_loss: tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1457/4339]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1458/4339]: training_loss: tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1459/4339]: training_loss: tensor(0.2465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1460/4339]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1461/4339]: training_loss: tensor(0.2065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1462/4339]: training_loss: tensor(0.1357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1463/4339]: training_loss: tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1464/4339]: training_loss: tensor(0.2602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1465/4339]: training_loss: tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1466/4339]: training_loss: tensor(0.0592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1467/4339]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1468/4339]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1469/4339]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1470/4339]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1471/4339]: training_loss: tensor(0.0872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1472/4339]: training_loss: tensor(0.3346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1473/4339]: training_loss: tensor(0.1487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1474/4339]: training_loss: tensor(0.1919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1475/4339]: training_loss: tensor(0.2165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1476/4339]: training_loss: tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1477/4339]: training_loss: tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1478/4339]: training_loss: tensor(0.1975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1479/4339]: training_loss: tensor(0.1715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1480/4339]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1481/4339]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1482/4339]: training_loss: tensor(0.3372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1483/4339]: training_loss: tensor(0.1608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1484/4339]: training_loss: tensor(0.1439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1485/4339]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1486/4339]: training_loss: tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1487/4339]: training_loss: tensor(0.0997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1488/4339]: training_loss: tensor(0.2330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1489/4339]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1490/4339]: training_loss: tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1491/4339]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1492/4339]: training_loss: tensor(0.2273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1493/4339]: training_loss: tensor(0.1363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1494/4339]: training_loss: tensor(0.2204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1495/4339]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1496/4339]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1497/4339]: training_loss: tensor(0.1740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1498/4339]: training_loss: tensor(0.0586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1499/4339]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1500/4339]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1501/4339]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1502/4339]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1503/4339]: training_loss: tensor(0.2559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1504/4339]: training_loss: tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1505/4339]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1506/4339]: training_loss: tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1507/4339]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1508/4339]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1509/4339]: training_loss: tensor(0.4357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1510/4339]: training_loss: tensor(0.1579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1511/4339]: training_loss: tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1512/4339]: training_loss: tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1513/4339]: training_loss: tensor(0.0502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1514/4339]: training_loss: tensor(0.1960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1515/4339]: training_loss: tensor(0.1492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1516/4339]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1517/4339]: training_loss: tensor(0.1610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1518/4339]: training_loss: tensor(0.1859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1519/4339]: training_loss: tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1520/4339]: training_loss: tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1521/4339]: training_loss: tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1522/4339]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1523/4339]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1524/4339]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1525/4339]: training_loss: tensor(0.2802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1526/4339]: training_loss: tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1527/4339]: training_loss: tensor(0.1462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1528/4339]: training_loss: tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1529/4339]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1530/4339]: training_loss: tensor(0.3575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1531/4339]: training_loss: tensor(0.1295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1532/4339]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1533/4339]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1534/4339]: training_loss: tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1535/4339]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1536/4339]: training_loss: tensor(0.2578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1537/4339]: training_loss: tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1538/4339]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1539/4339]: training_loss: tensor(0.2925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1540/4339]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1541/4339]: training_loss: tensor(0.1441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1542/4339]: training_loss: tensor(0.2020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1543/4339]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1544/4339]: training_loss: tensor(0.0560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1545/4339]: training_loss: tensor(0.1969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1546/4339]: training_loss: tensor(0.1448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1547/4339]: training_loss: tensor(0.2255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1548/4339]: training_loss: tensor(0.1628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1549/4339]: training_loss: tensor(0.2422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1550/4339]: training_loss: tensor(0.1257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1551/4339]: training_loss: tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1552/4339]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1553/4339]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1554/4339]: training_loss: tensor(0.4046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1555/4339]: training_loss: tensor(0.3096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1556/4339]: training_loss: tensor(0.3766, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1557/4339]: training_loss: tensor(0.3485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1558/4339]: training_loss: tensor(0.2196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1559/4339]: training_loss: tensor(0.0517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1560/4339]: training_loss: tensor(0.2314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1561/4339]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1562/4339]: training_loss: tensor(0.1562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1563/4339]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1564/4339]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1565/4339]: training_loss: tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1566/4339]: training_loss: tensor(0.1263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1567/4339]: training_loss: tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1568/4339]: training_loss: tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1569/4339]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1570/4339]: training_loss: tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1571/4339]: training_loss: tensor(0.1154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1572/4339]: training_loss: tensor(0.1220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1573/4339]: training_loss: tensor(0.5000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1574/4339]: training_loss: tensor(0.1583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1575/4339]: training_loss: tensor(0.2268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1576/4339]: training_loss: tensor(0.1189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1577/4339]: training_loss: tensor(0.2886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1578/4339]: training_loss: tensor(0.3326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1579/4339]: training_loss: tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1580/4339]: training_loss: tensor(0.3667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1581/4339]: training_loss: tensor(0.1188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1582/4339]: training_loss: tensor(0.1113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1583/4339]: training_loss: tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1584/4339]: training_loss: tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1585/4339]: training_loss: tensor(0.1775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1586/4339]: training_loss: tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1587/4339]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1588/4339]: training_loss: tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1589/4339]: training_loss: tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1590/4339]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1591/4339]: training_loss: tensor(0.1787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1592/4339]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1593/4339]: training_loss: tensor(0.2703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1594/4339]: training_loss: tensor(0.3518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1595/4339]: training_loss: tensor(0.1856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1596/4339]: training_loss: tensor(0.0594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1597/4339]: training_loss: tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1598/4339]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1599/4339]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1600/4339]: training_loss: tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1601/4339]: training_loss: tensor(0.1351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1602/4339]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1603/4339]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1604/4339]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1605/4339]: training_loss: tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1606/4339]: training_loss: tensor(0.1288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1607/4339]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1608/4339]: training_loss: tensor(0.3361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1609/4339]: training_loss: tensor(0.3663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1610/4339]: training_loss: tensor(0.3841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1611/4339]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1612/4339]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1613/4339]: training_loss: tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1614/4339]: training_loss: tensor(0.4766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1615/4339]: training_loss: tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1616/4339]: training_loss: tensor(0.4550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1617/4339]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1618/4339]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1619/4339]: training_loss: tensor(0.0575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1620/4339]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1621/4339]: training_loss: tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1622/4339]: training_loss: tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1623/4339]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1624/4339]: training_loss: tensor(0.2143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1625/4339]: training_loss: tensor(0.3609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1626/4339]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1627/4339]: training_loss: tensor(0.0410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1628/4339]: training_loss: tensor(0.3861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1629/4339]: training_loss: tensor(0.2170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1630/4339]: training_loss: tensor(0.2170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1631/4339]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1632/4339]: training_loss: tensor(0.2095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1633/4339]: training_loss: tensor(0.2902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1634/4339]: training_loss: tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1635/4339]: training_loss: tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1636/4339]: training_loss: tensor(0.1460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1637/4339]: training_loss: tensor(0.1823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1638/4339]: training_loss: tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1639/4339]: training_loss: tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1640/4339]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1641/4339]: training_loss: tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1642/4339]: training_loss: tensor(0.3322, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1643/4339]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1644/4339]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1645/4339]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1646/4339]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1647/4339]: training_loss: tensor(0.0298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1648/4339]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1649/4339]: training_loss: tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1650/4339]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1651/4339]: training_loss: tensor(0.1332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1652/4339]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1653/4339]: training_loss: tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1654/4339]: training_loss: tensor(0.2518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1655/4339]: training_loss: tensor(0.1968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1656/4339]: training_loss: tensor(0.0410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1657/4339]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1658/4339]: training_loss: tensor(0.1830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1659/4339]: training_loss: tensor(0.3243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1660/4339]: training_loss: tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1661/4339]: training_loss: tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1662/4339]: training_loss: tensor(0.2341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1663/4339]: training_loss: tensor(0.1482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1664/4339]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1665/4339]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1666/4339]: training_loss: tensor(0.1510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1667/4339]: training_loss: tensor(0.1875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1668/4339]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1669/4339]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1670/4339]: training_loss: tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1671/4339]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1672/4339]: training_loss: tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1673/4339]: training_loss: tensor(0.3721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1674/4339]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1675/4339]: training_loss: tensor(0.2308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1676/4339]: training_loss: tensor(0.2577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1677/4339]: training_loss: tensor(0.1534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1678/4339]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1679/4339]: training_loss: tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1680/4339]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1681/4339]: training_loss: tensor(0.1580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1682/4339]: training_loss: tensor(0.1354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1683/4339]: training_loss: tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1684/4339]: training_loss: tensor(0.2308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1685/4339]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1686/4339]: training_loss: tensor(0.1850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1687/4339]: training_loss: tensor(0.4086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1688/4339]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1689/4339]: training_loss: tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1690/4339]: training_loss: tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1691/4339]: training_loss: tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1692/4339]: training_loss: tensor(0.6004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1693/4339]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1694/4339]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1695/4339]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1696/4339]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1697/4339]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1698/4339]: training_loss: tensor(0.1240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1699/4339]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1700/4339]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1701/4339]: training_loss: tensor(0.4111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1702/4339]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1703/4339]: training_loss: tensor(0.1270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1704/4339]: training_loss: tensor(0.2289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1705/4339]: training_loss: tensor(0.1346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1706/4339]: training_loss: tensor(0.2947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1707/4339]: training_loss: tensor(0.0566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1708/4339]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1709/4339]: training_loss: tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1710/4339]: training_loss: tensor(0.1994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1711/4339]: training_loss: tensor(0.1928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1712/4339]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1713/4339]: training_loss: tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1714/4339]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1715/4339]: training_loss: tensor(0.1172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1716/4339]: training_loss: tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1717/4339]: training_loss: tensor(0.1656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1718/4339]: training_loss: tensor(0.1455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1719/4339]: training_loss: tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1720/4339]: training_loss: tensor(0.0475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1721/4339]: training_loss: tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1722/4339]: training_loss: tensor(0.0413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1723/4339]: training_loss: tensor(0.0619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1724/4339]: training_loss: tensor(0.1732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1725/4339]: training_loss: tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1726/4339]: training_loss: tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1727/4339]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1728/4339]: training_loss: tensor(0.0633, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1729/4339]: training_loss: tensor(0.3936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1730/4339]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1731/4339]: training_loss: tensor(0.2857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1732/4339]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1733/4339]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1734/4339]: training_loss: tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1735/4339]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1736/4339]: training_loss: tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1737/4339]: training_loss: tensor(0.3096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1738/4339]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1739/4339]: training_loss: tensor(0.1142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1740/4339]: training_loss: tensor(0.2686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1741/4339]: training_loss: tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1742/4339]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1743/4339]: training_loss: tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1744/4339]: training_loss: tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1745/4339]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1746/4339]: training_loss: tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1747/4339]: training_loss: tensor(0.1696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1748/4339]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1749/4339]: training_loss: tensor(0.0543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1750/4339]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1751/4339]: training_loss: tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1752/4339]: training_loss: tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1753/4339]: training_loss: tensor(0.0436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1754/4339]: training_loss: tensor(0.1270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1755/4339]: training_loss: tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1756/4339]: training_loss: tensor(0.2514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1757/4339]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1758/4339]: training_loss: tensor(0.3110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1759/4339]: training_loss: tensor(0.1133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1760/4339]: training_loss: tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1761/4339]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1762/4339]: training_loss: tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1763/4339]: training_loss: tensor(0.3418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1764/4339]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1765/4339]: training_loss: tensor(0.2979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1766/4339]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1767/4339]: training_loss: tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1768/4339]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1769/4339]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1770/4339]: training_loss: tensor(0.1127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1771/4339]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1772/4339]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1773/4339]: training_loss: tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1774/4339]: training_loss: tensor(0.2248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1775/4339]: training_loss: tensor(0.1186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1776/4339]: training_loss: tensor(0.3514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1777/4339]: training_loss: tensor(0.3897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1778/4339]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1779/4339]: training_loss: tensor(0.4678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1780/4339]: training_loss: tensor(0.4193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1781/4339]: training_loss: tensor(0.1267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1782/4339]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1783/4339]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1784/4339]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1785/4339]: training_loss: tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1786/4339]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1787/4339]: training_loss: tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1788/4339]: training_loss: tensor(0.1931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1789/4339]: training_loss: tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1790/4339]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1791/4339]: training_loss: tensor(0.2729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1792/4339]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1793/4339]: training_loss: tensor(0.1954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1794/4339]: training_loss: tensor(0.1563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1795/4339]: training_loss: tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1796/4339]: training_loss: tensor(0.1406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1797/4339]: training_loss: tensor(0.1345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1798/4339]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1799/4339]: training_loss: tensor(0.1992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1800/4339]: training_loss: tensor(0.1181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1801/4339]: training_loss: tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1802/4339]: training_loss: tensor(0.1498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1803/4339]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1804/4339]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1805/4339]: training_loss: tensor(0.4515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1806/4339]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1807/4339]: training_loss: tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1808/4339]: training_loss: tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1809/4339]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1810/4339]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1811/4339]: training_loss: tensor(0.3419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1812/4339]: training_loss: tensor(0.1222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1813/4339]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1814/4339]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1815/4339]: training_loss: tensor(0.7046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1816/4339]: training_loss: tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1817/4339]: training_loss: tensor(0.1572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1818/4339]: training_loss: tensor(0.1939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1819/4339]: training_loss: tensor(0.7535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1820/4339]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1821/4339]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1822/4339]: training_loss: tensor(0.1874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1823/4339]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1824/4339]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1825/4339]: training_loss: tensor(0.1929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1826/4339]: training_loss: tensor(0.3215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1827/4339]: training_loss: tensor(0.0505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1828/4339]: training_loss: tensor(0.1730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1829/4339]: training_loss: tensor(0.1380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1830/4339]: training_loss: tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1831/4339]: training_loss: tensor(0.1286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1832/4339]: training_loss: tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1833/4339]: training_loss: tensor(0.2220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1834/4339]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1835/4339]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1836/4339]: training_loss: tensor(0.1365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1837/4339]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1838/4339]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1839/4339]: training_loss: tensor(0.1945, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1840/4339]: training_loss: tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1841/4339]: training_loss: tensor(0.0413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1842/4339]: training_loss: tensor(0.3198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1843/4339]: training_loss: tensor(0.2828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1844/4339]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1845/4339]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1846/4339]: training_loss: tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1847/4339]: training_loss: tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1848/4339]: training_loss: tensor(0.1446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1849/4339]: training_loss: tensor(0.1400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1850/4339]: training_loss: tensor(0.2428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1851/4339]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1852/4339]: training_loss: tensor(0.3294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1853/4339]: training_loss: tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1854/4339]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1855/4339]: training_loss: tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1856/4339]: training_loss: tensor(0.1172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1857/4339]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1858/4339]: training_loss: tensor(0.2203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1859/4339]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1860/4339]: training_loss: tensor(0.0795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1861/4339]: training_loss: tensor(0.1514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1862/4339]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1863/4339]: training_loss: tensor(0.1256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1864/4339]: training_loss: tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1865/4339]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1866/4339]: training_loss: tensor(0.2054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1867/4339]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1868/4339]: training_loss: tensor(0.1129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1869/4339]: training_loss: tensor(0.3066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1870/4339]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1871/4339]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1872/4339]: training_loss: tensor(0.1281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1873/4339]: training_loss: tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1874/4339]: training_loss: tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1875/4339]: training_loss: tensor(0.2647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1876/4339]: training_loss: tensor(0.2106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1877/4339]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1878/4339]: training_loss: tensor(0.2721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1879/4339]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1880/4339]: training_loss: tensor(0.1375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1881/4339]: training_loss: tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1882/4339]: training_loss: tensor(0.1944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1883/4339]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1884/4339]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1885/4339]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1886/4339]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1887/4339]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1888/4339]: training_loss: tensor(0.1732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1889/4339]: training_loss: tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1890/4339]: training_loss: tensor(0.4883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1891/4339]: training_loss: tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1892/4339]: training_loss: tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1893/4339]: training_loss: tensor(0.1672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1894/4339]: training_loss: tensor(0.1861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1895/4339]: training_loss: tensor(0.1153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1896/4339]: training_loss: tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1897/4339]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1898/4339]: training_loss: tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1899/4339]: training_loss: tensor(0.2393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1900/4339]: training_loss: tensor(0.2963, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1901/4339]: training_loss: tensor(0.1488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1902/4339]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1903/4339]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1904/4339]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1905/4339]: training_loss: tensor(0.0447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1906/4339]: training_loss: tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1907/4339]: training_loss: tensor(0.2684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1908/4339]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1909/4339]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1910/4339]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1911/4339]: training_loss: tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1912/4339]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1913/4339]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1914/4339]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1915/4339]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1916/4339]: training_loss: tensor(0.1874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1917/4339]: training_loss: tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1918/4339]: training_loss: tensor(0.2348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1919/4339]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1920/4339]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1921/4339]: training_loss: tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1922/4339]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1923/4339]: training_loss: tensor(0.2785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1924/4339]: training_loss: tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1925/4339]: training_loss: tensor(0.1572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1926/4339]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1927/4339]: training_loss: tensor(0.1318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1928/4339]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1929/4339]: training_loss: tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1930/4339]: training_loss: tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1931/4339]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1932/4339]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1933/4339]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1934/4339]: training_loss: tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1935/4339]: training_loss: tensor(0.1097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1936/4339]: training_loss: tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1937/4339]: training_loss: tensor(0.5689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1938/4339]: training_loss: tensor(0.1405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1939/4339]: training_loss: tensor(0.1459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1940/4339]: training_loss: tensor(0.2024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1941/4339]: training_loss: tensor(0.0493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1942/4339]: training_loss: tensor(0.3415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1943/4339]: training_loss: tensor(0.2838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1944/4339]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1945/4339]: training_loss: tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1946/4339]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1947/4339]: training_loss: tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1948/4339]: training_loss: tensor(0.2206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1949/4339]: training_loss: tensor(0.4367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1950/4339]: training_loss: tensor(0.2765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1951/4339]: training_loss: tensor(0.1361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1952/4339]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1953/4339]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1954/4339]: training_loss: tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1955/4339]: training_loss: tensor(0.0446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1956/4339]: training_loss: tensor(0.1507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1957/4339]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1958/4339]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1959/4339]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1960/4339]: training_loss: tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1961/4339]: training_loss: tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1962/4339]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1963/4339]: training_loss: tensor(0.2201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1964/4339]: training_loss: tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1965/4339]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1966/4339]: training_loss: tensor(0.3313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1967/4339]: training_loss: tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1968/4339]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1969/4339]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1970/4339]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1971/4339]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1972/4339]: training_loss: tensor(0.1424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1973/4339]: training_loss: tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1974/4339]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1975/4339]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1976/4339]: training_loss: tensor(0.1392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1977/4339]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1978/4339]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1979/4339]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1980/4339]: training_loss: tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1981/4339]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1982/4339]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1983/4339]: training_loss: tensor(0.1952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1984/4339]: training_loss: tensor(0.1834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1985/4339]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1986/4339]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1987/4339]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1988/4339]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1989/4339]: training_loss: tensor(0.0688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1990/4339]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1991/4339]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1992/4339]: training_loss: tensor(0.1243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1993/4339]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1994/4339]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1995/4339]: training_loss: tensor(0.1106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1996/4339]: training_loss: tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1997/4339]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1998/4339]: training_loss: tensor(0.1349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1999/4339]: training_loss: tensor(0.2043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2000/4339]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2001/4339]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2002/4339]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2003/4339]: training_loss: tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2004/4339]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2005/4339]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2006/4339]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2007/4339]: training_loss: tensor(0.1276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2008/4339]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2009/4339]: training_loss: tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2010/4339]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2011/4339]: training_loss: tensor(0.1066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2012/4339]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2013/4339]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2014/4339]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2015/4339]: training_loss: tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2016/4339]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2017/4339]: training_loss: tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2018/4339]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2019/4339]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2020/4339]: training_loss: tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2021/4339]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2022/4339]: training_loss: tensor(0.1143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2023/4339]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2024/4339]: training_loss: tensor(0.3375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2025/4339]: training_loss: tensor(0.1196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2026/4339]: training_loss: tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2027/4339]: training_loss: tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2028/4339]: training_loss: tensor(0.0445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2029/4339]: training_loss: tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2030/4339]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2031/4339]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2032/4339]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2033/4339]: training_loss: tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2034/4339]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2035/4339]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2036/4339]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2037/4339]: training_loss: tensor(0.1769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2038/4339]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2039/4339]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2040/4339]: training_loss: tensor(0.1480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2041/4339]: training_loss: tensor(0.2682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2042/4339]: training_loss: tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2043/4339]: training_loss: tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2044/4339]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2045/4339]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2046/4339]: training_loss: tensor(0.2140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2047/4339]: training_loss: tensor(0.1970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2048/4339]: training_loss: tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2049/4339]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2050/4339]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2051/4339]: training_loss: tensor(0.1192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2052/4339]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2053/4339]: training_loss: tensor(0.1837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2054/4339]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2055/4339]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2056/4339]: training_loss: tensor(0.2825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2057/4339]: training_loss: tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2058/4339]: training_loss: tensor(0.1666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2059/4339]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2060/4339]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2061/4339]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2062/4339]: training_loss: tensor(0.1781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2063/4339]: training_loss: tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2064/4339]: training_loss: tensor(0.1620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2065/4339]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2066/4339]: training_loss: tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2067/4339]: training_loss: tensor(0.0594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2068/4339]: training_loss: tensor(0.1157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2069/4339]: training_loss: tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2070/4339]: training_loss: tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2071/4339]: training_loss: tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2072/4339]: training_loss: tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2073/4339]: training_loss: tensor(0.1078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2074/4339]: training_loss: tensor(0.2949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2075/4339]: training_loss: tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2076/4339]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2077/4339]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2078/4339]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2079/4339]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2080/4339]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2081/4339]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2082/4339]: training_loss: tensor(0.5881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2083/4339]: training_loss: tensor(0.4794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2084/4339]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2085/4339]: training_loss: tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2086/4339]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2087/4339]: training_loss: tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2088/4339]: training_loss: tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2089/4339]: training_loss: tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2090/4339]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2091/4339]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2092/4339]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2093/4339]: training_loss: tensor(0.2282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2094/4339]: training_loss: tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2095/4339]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2096/4339]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2097/4339]: training_loss: tensor(0.3559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2098/4339]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2099/4339]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2100/4339]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2101/4339]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2102/4339]: training_loss: tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2103/4339]: training_loss: tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2104/4339]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2105/4339]: training_loss: tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2106/4339]: training_loss: tensor(0.3074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2107/4339]: training_loss: tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2108/4339]: training_loss: tensor(0.0469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2109/4339]: training_loss: tensor(0.0533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2110/4339]: training_loss: tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2111/4339]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2112/4339]: training_loss: tensor(0.1220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2113/4339]: training_loss: tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2114/4339]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2115/4339]: training_loss: tensor(0.3552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2116/4339]: training_loss: tensor(0.4457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2117/4339]: training_loss: tensor(0.0575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2118/4339]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2119/4339]: training_loss: tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2120/4339]: training_loss: tensor(0.2430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2121/4339]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2122/4339]: training_loss: tensor(0.1556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2123/4339]: training_loss: tensor(0.1177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2124/4339]: training_loss: tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2125/4339]: training_loss: tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2126/4339]: training_loss: tensor(0.3491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2127/4339]: training_loss: tensor(0.5204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2128/4339]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2129/4339]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2130/4339]: training_loss: tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2131/4339]: training_loss: tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2132/4339]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2133/4339]: training_loss: tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2134/4339]: training_loss: tensor(0.1479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2135/4339]: training_loss: tensor(0.2604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2136/4339]: training_loss: tensor(0.2109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2137/4339]: training_loss: tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2138/4339]: training_loss: tensor(0.1454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2139/4339]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2140/4339]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2141/4339]: training_loss: tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2142/4339]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2143/4339]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2144/4339]: training_loss: tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2145/4339]: training_loss: tensor(0.1859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2146/4339]: training_loss: tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2147/4339]: training_loss: tensor(0.1946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2148/4339]: training_loss: tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2149/4339]: training_loss: tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2150/4339]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2151/4339]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2152/4339]: training_loss: tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2153/4339]: training_loss: tensor(0.6808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2154/4339]: training_loss: tensor(0.4732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2155/4339]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2156/4339]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2157/4339]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2158/4339]: training_loss: tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2159/4339]: training_loss: tensor(0.1690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2160/4339]: training_loss: tensor(0.1651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2161/4339]: training_loss: tensor(0.3266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2162/4339]: training_loss: tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2163/4339]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2164/4339]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2165/4339]: training_loss: tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2166/4339]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2167/4339]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2168/4339]: training_loss: tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2169/4339]: training_loss: tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2170/4339]: training_loss: tensor(0.0886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [4/5], Step [15190/21700], Train Loss: 0.1306, Valid Loss: 0.4992\n",
      "batch_no [2171/4339]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2172/4339]: training_loss: tensor(0.2942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2173/4339]: training_loss: tensor(0.2089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2174/4339]: training_loss: tensor(0.2763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2175/4339]: training_loss: tensor(0.1812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2176/4339]: training_loss: tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2177/4339]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2178/4339]: training_loss: tensor(0.1286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2179/4339]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2180/4339]: training_loss: tensor(0.1376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2181/4339]: training_loss: tensor(0.1822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2182/4339]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2183/4339]: training_loss: tensor(0.2031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2184/4339]: training_loss: tensor(0.1374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2185/4339]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2186/4339]: training_loss: tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2187/4339]: training_loss: tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2188/4339]: training_loss: tensor(0.2626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2189/4339]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2190/4339]: training_loss: tensor(0.3351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2191/4339]: training_loss: tensor(0.3002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2192/4339]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2193/4339]: training_loss: tensor(0.2767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2194/4339]: training_loss: tensor(0.0820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2195/4339]: training_loss: tensor(0.1083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2196/4339]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2197/4339]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2198/4339]: training_loss: tensor(0.1673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2199/4339]: training_loss: tensor(0.1273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2200/4339]: training_loss: tensor(0.1391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2201/4339]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2202/4339]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2203/4339]: training_loss: tensor(0.5883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2204/4339]: training_loss: tensor(0.1761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2205/4339]: training_loss: tensor(0.2654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2206/4339]: training_loss: tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2207/4339]: training_loss: tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2208/4339]: training_loss: tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2209/4339]: training_loss: tensor(0.3402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2210/4339]: training_loss: tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2211/4339]: training_loss: tensor(0.1303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2212/4339]: training_loss: tensor(0.1657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2213/4339]: training_loss: tensor(0.1861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2214/4339]: training_loss: tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2215/4339]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2216/4339]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2217/4339]: training_loss: tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2218/4339]: training_loss: tensor(0.1217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2219/4339]: training_loss: tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2220/4339]: training_loss: tensor(0.1105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2221/4339]: training_loss: tensor(0.1981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2222/4339]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2223/4339]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2224/4339]: training_loss: tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2225/4339]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2226/4339]: training_loss: tensor(0.4626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2227/4339]: training_loss: tensor(0.0671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2228/4339]: training_loss: tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2229/4339]: training_loss: tensor(0.1414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2230/4339]: training_loss: tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2231/4339]: training_loss: tensor(0.0555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2232/4339]: training_loss: tensor(0.1706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2233/4339]: training_loss: tensor(0.0525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2234/4339]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2235/4339]: training_loss: tensor(0.1277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2236/4339]: training_loss: tensor(0.3182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2237/4339]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2238/4339]: training_loss: tensor(0.2247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2239/4339]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2240/4339]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2241/4339]: training_loss: tensor(0.2272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2242/4339]: training_loss: tensor(0.1698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2243/4339]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2244/4339]: training_loss: tensor(0.3386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2245/4339]: training_loss: tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2246/4339]: training_loss: tensor(0.2928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2247/4339]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2248/4339]: training_loss: tensor(0.4607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2249/4339]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2250/4339]: training_loss: tensor(0.1977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2251/4339]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2252/4339]: training_loss: tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2253/4339]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2254/4339]: training_loss: tensor(0.3078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2255/4339]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2256/4339]: training_loss: tensor(0.2383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2257/4339]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2258/4339]: training_loss: tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2259/4339]: training_loss: tensor(0.2542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2260/4339]: training_loss: tensor(0.1379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2261/4339]: training_loss: tensor(0.2413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2262/4339]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2263/4339]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2264/4339]: training_loss: tensor(0.1302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2265/4339]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2266/4339]: training_loss: tensor(0.3240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2267/4339]: training_loss: tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2268/4339]: training_loss: tensor(0.1713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2269/4339]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2270/4339]: training_loss: tensor(0.1417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2271/4339]: training_loss: tensor(0.1877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2272/4339]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2273/4339]: training_loss: tensor(0.1393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2274/4339]: training_loss: tensor(0.1373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2275/4339]: training_loss: tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2276/4339]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2277/4339]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2278/4339]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2279/4339]: training_loss: tensor(0.2089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2280/4339]: training_loss: tensor(0.1152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2281/4339]: training_loss: tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2282/4339]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2283/4339]: training_loss: tensor(0.1898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2284/4339]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2285/4339]: training_loss: tensor(0.1347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2286/4339]: training_loss: tensor(0.2124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2287/4339]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2288/4339]: training_loss: tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2289/4339]: training_loss: tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2290/4339]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2291/4339]: training_loss: tensor(0.2871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2292/4339]: training_loss: tensor(0.1310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2293/4339]: training_loss: tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2294/4339]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2295/4339]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2296/4339]: training_loss: tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2297/4339]: training_loss: tensor(0.1327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2298/4339]: training_loss: tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2299/4339]: training_loss: tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2300/4339]: training_loss: tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2301/4339]: training_loss: tensor(0.2594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2302/4339]: training_loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2303/4339]: training_loss: tensor(0.1234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2304/4339]: training_loss: tensor(0.3624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2305/4339]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2306/4339]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2307/4339]: training_loss: tensor(0.2151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2308/4339]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2309/4339]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2310/4339]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2311/4339]: training_loss: tensor(0.0494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2312/4339]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2313/4339]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2314/4339]: training_loss: tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2315/4339]: training_loss: tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2316/4339]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2317/4339]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2318/4339]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2319/4339]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2320/4339]: training_loss: tensor(0.4398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2321/4339]: training_loss: tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2322/4339]: training_loss: tensor(0.2321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2323/4339]: training_loss: tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2324/4339]: training_loss: tensor(0.3584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2325/4339]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2326/4339]: training_loss: tensor(0.1553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2327/4339]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2328/4339]: training_loss: tensor(0.4129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2329/4339]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2330/4339]: training_loss: tensor(0.1586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2331/4339]: training_loss: tensor(0.1876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2332/4339]: training_loss: tensor(0.1514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2333/4339]: training_loss: tensor(0.2490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2334/4339]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2335/4339]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2336/4339]: training_loss: tensor(0.3345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2337/4339]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2338/4339]: training_loss: tensor(0.3312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2339/4339]: training_loss: tensor(0.4320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2340/4339]: training_loss: tensor(0.3301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2341/4339]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2342/4339]: training_loss: tensor(0.3118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2343/4339]: training_loss: tensor(0.3817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2344/4339]: training_loss: tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2345/4339]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2346/4339]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2347/4339]: training_loss: tensor(0.1505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2348/4339]: training_loss: tensor(0.0427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2349/4339]: training_loss: tensor(0.0480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2350/4339]: training_loss: tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2351/4339]: training_loss: tensor(0.1897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2352/4339]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2353/4339]: training_loss: tensor(0.1983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2354/4339]: training_loss: tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2355/4339]: training_loss: tensor(0.2820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2356/4339]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2357/4339]: training_loss: tensor(0.2808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2358/4339]: training_loss: tensor(0.2072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2359/4339]: training_loss: tensor(0.5485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2360/4339]: training_loss: tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2361/4339]: training_loss: tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2362/4339]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2363/4339]: training_loss: tensor(0.1351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2364/4339]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2365/4339]: training_loss: tensor(0.1487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2366/4339]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2367/4339]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2368/4339]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2369/4339]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2370/4339]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2371/4339]: training_loss: tensor(0.1523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2372/4339]: training_loss: tensor(0.4885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2373/4339]: training_loss: tensor(0.2091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2374/4339]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2375/4339]: training_loss: tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2376/4339]: training_loss: tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2377/4339]: training_loss: tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2378/4339]: training_loss: tensor(0.2643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2379/4339]: training_loss: tensor(0.1496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2380/4339]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2381/4339]: training_loss: tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2382/4339]: training_loss: tensor(0.2179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2383/4339]: training_loss: tensor(0.2099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2384/4339]: training_loss: tensor(0.5158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2385/4339]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2386/4339]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2387/4339]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2388/4339]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2389/4339]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2390/4339]: training_loss: tensor(0.0541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2391/4339]: training_loss: tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2392/4339]: training_loss: tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2393/4339]: training_loss: tensor(0.1727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2394/4339]: training_loss: tensor(0.1756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2395/4339]: training_loss: tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2396/4339]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2397/4339]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2398/4339]: training_loss: tensor(0.2378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2399/4339]: training_loss: tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2400/4339]: training_loss: tensor(0.1425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2401/4339]: training_loss: tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2402/4339]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2403/4339]: training_loss: tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2404/4339]: training_loss: tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2405/4339]: training_loss: tensor(0.3613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2406/4339]: training_loss: tensor(0.2589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2407/4339]: training_loss: tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2408/4339]: training_loss: tensor(0.1139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2409/4339]: training_loss: tensor(0.3230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2410/4339]: training_loss: tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2411/4339]: training_loss: tensor(0.3151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2412/4339]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2413/4339]: training_loss: tensor(0.1558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2414/4339]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2415/4339]: training_loss: tensor(0.1712, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2416/4339]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2417/4339]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2418/4339]: training_loss: tensor(0.2155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2419/4339]: training_loss: tensor(0.1591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2420/4339]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2421/4339]: training_loss: tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2422/4339]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2423/4339]: training_loss: tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2424/4339]: training_loss: tensor(0.4524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2425/4339]: training_loss: tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2426/4339]: training_loss: tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2427/4339]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2428/4339]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2429/4339]: training_loss: tensor(0.2422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2430/4339]: training_loss: tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2431/4339]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2432/4339]: training_loss: tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2433/4339]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2434/4339]: training_loss: tensor(0.0616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2435/4339]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2436/4339]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2437/4339]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2438/4339]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2439/4339]: training_loss: tensor(0.4421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2440/4339]: training_loss: tensor(0.4812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2441/4339]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2442/4339]: training_loss: tensor(0.1886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2443/4339]: training_loss: tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2444/4339]: training_loss: tensor(0.5652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2445/4339]: training_loss: tensor(0.3608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2446/4339]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2447/4339]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2448/4339]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2449/4339]: training_loss: tensor(0.3511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2450/4339]: training_loss: tensor(0.1790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2451/4339]: training_loss: tensor(0.0645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2452/4339]: training_loss: tensor(0.0858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2453/4339]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2454/4339]: training_loss: tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2455/4339]: training_loss: tensor(0.1891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2456/4339]: training_loss: tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2457/4339]: training_loss: tensor(0.3384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2458/4339]: training_loss: tensor(0.2800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2459/4339]: training_loss: tensor(0.3247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2460/4339]: training_loss: tensor(0.0541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2461/4339]: training_loss: tensor(0.2564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2462/4339]: training_loss: tensor(0.1424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2463/4339]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2464/4339]: training_loss: tensor(0.1709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2465/4339]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2466/4339]: training_loss: tensor(0.2048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2467/4339]: training_loss: tensor(0.3001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2468/4339]: training_loss: tensor(0.1354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2469/4339]: training_loss: tensor(0.1441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2470/4339]: training_loss: tensor(0.2744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2471/4339]: training_loss: tensor(0.2668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2472/4339]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2473/4339]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2474/4339]: training_loss: tensor(0.3593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2475/4339]: training_loss: tensor(0.2822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2476/4339]: training_loss: tensor(0.2248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2477/4339]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2478/4339]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2479/4339]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2480/4339]: training_loss: tensor(0.2680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2481/4339]: training_loss: tensor(0.2358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2482/4339]: training_loss: tensor(0.1835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2483/4339]: training_loss: tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2484/4339]: training_loss: tensor(0.1313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2485/4339]: training_loss: tensor(0.3451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2486/4339]: training_loss: tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2487/4339]: training_loss: tensor(0.1348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2488/4339]: training_loss: tensor(0.1219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2489/4339]: training_loss: tensor(0.2903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2490/4339]: training_loss: tensor(0.2448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2491/4339]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2492/4339]: training_loss: tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2493/4339]: training_loss: tensor(0.4296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2494/4339]: training_loss: tensor(0.3251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2495/4339]: training_loss: tensor(0.0676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2496/4339]: training_loss: tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2497/4339]: training_loss: tensor(0.2576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2498/4339]: training_loss: tensor(0.2456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2499/4339]: training_loss: tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2500/4339]: training_loss: tensor(0.1281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2501/4339]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2502/4339]: training_loss: tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2503/4339]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2504/4339]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2505/4339]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2506/4339]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2507/4339]: training_loss: tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2508/4339]: training_loss: tensor(0.1429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2509/4339]: training_loss: tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2510/4339]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2511/4339]: training_loss: tensor(0.2865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2512/4339]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2513/4339]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2514/4339]: training_loss: tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2515/4339]: training_loss: tensor(0.1380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2516/4339]: training_loss: tensor(0.4732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2517/4339]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2518/4339]: training_loss: tensor(0.0362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2519/4339]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2520/4339]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2521/4339]: training_loss: tensor(0.0502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2522/4339]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2523/4339]: training_loss: tensor(0.3962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2524/4339]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2525/4339]: training_loss: tensor(0.1551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2526/4339]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2527/4339]: training_loss: tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2528/4339]: training_loss: tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2529/4339]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2530/4339]: training_loss: tensor(0.4164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2531/4339]: training_loss: tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2532/4339]: training_loss: tensor(0.5282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2533/4339]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2534/4339]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2535/4339]: training_loss: tensor(0.4143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2536/4339]: training_loss: tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2537/4339]: training_loss: tensor(0.2277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2538/4339]: training_loss: tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2539/4339]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2540/4339]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2541/4339]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2542/4339]: training_loss: tensor(0.2094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2543/4339]: training_loss: tensor(0.2092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2544/4339]: training_loss: tensor(0.1218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2545/4339]: training_loss: tensor(0.1611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2546/4339]: training_loss: tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2547/4339]: training_loss: tensor(0.3409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2548/4339]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2549/4339]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2550/4339]: training_loss: tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2551/4339]: training_loss: tensor(0.1651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2552/4339]: training_loss: tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2553/4339]: training_loss: tensor(0.1345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2554/4339]: training_loss: tensor(0.4796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2555/4339]: training_loss: tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2556/4339]: training_loss: tensor(0.0364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2557/4339]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2558/4339]: training_loss: tensor(0.1278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2559/4339]: training_loss: tensor(0.2663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2560/4339]: training_loss: tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2561/4339]: training_loss: tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2562/4339]: training_loss: tensor(0.1803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2563/4339]: training_loss: tensor(0.2996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2564/4339]: training_loss: tensor(0.2661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2565/4339]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2566/4339]: training_loss: tensor(0.4727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2567/4339]: training_loss: tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2568/4339]: training_loss: tensor(0.2814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2569/4339]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2570/4339]: training_loss: tensor(0.4103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2571/4339]: training_loss: tensor(0.2854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2572/4339]: training_loss: tensor(0.0579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2573/4339]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2574/4339]: training_loss: tensor(0.2172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2575/4339]: training_loss: tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2576/4339]: training_loss: tensor(0.1280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2577/4339]: training_loss: tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2578/4339]: training_loss: tensor(0.1850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2579/4339]: training_loss: tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2580/4339]: training_loss: tensor(0.3120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2581/4339]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2582/4339]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2583/4339]: training_loss: tensor(0.2643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2584/4339]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2585/4339]: training_loss: tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2586/4339]: training_loss: tensor(0.3520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2587/4339]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2588/4339]: training_loss: tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2589/4339]: training_loss: tensor(0.3202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2590/4339]: training_loss: tensor(0.1952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2591/4339]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2592/4339]: training_loss: tensor(0.1881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2593/4339]: training_loss: tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2594/4339]: training_loss: tensor(0.1587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2595/4339]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2596/4339]: training_loss: tensor(0.5271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2597/4339]: training_loss: tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2598/4339]: training_loss: tensor(0.2983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2599/4339]: training_loss: tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2600/4339]: training_loss: tensor(0.4268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2601/4339]: training_loss: tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2602/4339]: training_loss: tensor(0.3673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2603/4339]: training_loss: tensor(0.2959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2604/4339]: training_loss: tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2605/4339]: training_loss: tensor(0.1234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2606/4339]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2607/4339]: training_loss: tensor(0.1859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2608/4339]: training_loss: tensor(0.1520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2609/4339]: training_loss: tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2610/4339]: training_loss: tensor(0.1597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2611/4339]: training_loss: tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2612/4339]: training_loss: tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2613/4339]: training_loss: tensor(0.0928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2614/4339]: training_loss: tensor(0.5222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2615/4339]: training_loss: tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2616/4339]: training_loss: tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2617/4339]: training_loss: tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2618/4339]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2619/4339]: training_loss: tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2620/4339]: training_loss: tensor(0.0475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2621/4339]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2622/4339]: training_loss: tensor(0.0361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2623/4339]: training_loss: tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2624/4339]: training_loss: tensor(0.3564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2625/4339]: training_loss: tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2626/4339]: training_loss: tensor(0.1985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2627/4339]: training_loss: tensor(0.0305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2628/4339]: training_loss: tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2629/4339]: training_loss: tensor(0.0617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2630/4339]: training_loss: tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2631/4339]: training_loss: tensor(0.0423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2632/4339]: training_loss: tensor(0.0587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2633/4339]: training_loss: tensor(0.1607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2634/4339]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2635/4339]: training_loss: tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2636/4339]: training_loss: tensor(0.2682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2637/4339]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2638/4339]: training_loss: tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2639/4339]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2640/4339]: training_loss: tensor(0.1646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2641/4339]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2642/4339]: training_loss: tensor(0.2609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2643/4339]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2644/4339]: training_loss: tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2645/4339]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2646/4339]: training_loss: tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2647/4339]: training_loss: tensor(0.2871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2648/4339]: training_loss: tensor(0.2836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2649/4339]: training_loss: tensor(0.2557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2650/4339]: training_loss: tensor(0.1406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2651/4339]: training_loss: tensor(0.1871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2652/4339]: training_loss: tensor(0.2122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2653/4339]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2654/4339]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2655/4339]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2656/4339]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2657/4339]: training_loss: tensor(0.1044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2658/4339]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2659/4339]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2660/4339]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2661/4339]: training_loss: tensor(0.1352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2662/4339]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2663/4339]: training_loss: tensor(0.1219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2664/4339]: training_loss: tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2665/4339]: training_loss: tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2666/4339]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2667/4339]: training_loss: tensor(0.1943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2668/4339]: training_loss: tensor(0.2002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2669/4339]: training_loss: tensor(0.2180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2670/4339]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2671/4339]: training_loss: tensor(0.2657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2672/4339]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2673/4339]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2674/4339]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2675/4339]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2676/4339]: training_loss: tensor(0.1964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2677/4339]: training_loss: tensor(0.3010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2678/4339]: training_loss: tensor(0.2960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2679/4339]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2680/4339]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2681/4339]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2682/4339]: training_loss: tensor(0.3224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2683/4339]: training_loss: tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2684/4339]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2685/4339]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2686/4339]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2687/4339]: training_loss: tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2688/4339]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2689/4339]: training_loss: tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2690/4339]: training_loss: tensor(0.1734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2691/4339]: training_loss: tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2692/4339]: training_loss: tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2693/4339]: training_loss: tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2694/4339]: training_loss: tensor(0.1585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2695/4339]: training_loss: tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2696/4339]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2697/4339]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2698/4339]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2699/4339]: training_loss: tensor(0.1903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2700/4339]: training_loss: tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2701/4339]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2702/4339]: training_loss: tensor(0.1497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2703/4339]: training_loss: tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2704/4339]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2705/4339]: training_loss: tensor(0.1600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2706/4339]: training_loss: tensor(0.0429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2707/4339]: training_loss: tensor(0.7774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2708/4339]: training_loss: tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2709/4339]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2710/4339]: training_loss: tensor(0.1840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2711/4339]: training_loss: tensor(0.1567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2712/4339]: training_loss: tensor(0.1823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2713/4339]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2714/4339]: training_loss: tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2715/4339]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2716/4339]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2717/4339]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2718/4339]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2719/4339]: training_loss: tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2720/4339]: training_loss: tensor(0.4992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2721/4339]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2722/4339]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2723/4339]: training_loss: tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2724/4339]: training_loss: tensor(0.1145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2725/4339]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2726/4339]: training_loss: tensor(0.1110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2727/4339]: training_loss: tensor(0.5471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2728/4339]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2729/4339]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2730/4339]: training_loss: tensor(0.1598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2731/4339]: training_loss: tensor(0.4407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2732/4339]: training_loss: tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2733/4339]: training_loss: tensor(0.2164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2734/4339]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2735/4339]: training_loss: tensor(0.2706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2736/4339]: training_loss: tensor(0.3077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2737/4339]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2738/4339]: training_loss: tensor(0.1961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2739/4339]: training_loss: tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2740/4339]: training_loss: tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2741/4339]: training_loss: tensor(0.3386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2742/4339]: training_loss: tensor(0.1177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2743/4339]: training_loss: tensor(0.2670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2744/4339]: training_loss: tensor(0.1226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2745/4339]: training_loss: tensor(0.1193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2746/4339]: training_loss: tensor(0.1267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2747/4339]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2748/4339]: training_loss: tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2749/4339]: training_loss: tensor(0.1487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2750/4339]: training_loss: tensor(0.1948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2751/4339]: training_loss: tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2752/4339]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2753/4339]: training_loss: tensor(0.0598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2754/4339]: training_loss: tensor(0.2920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2755/4339]: training_loss: tensor(0.1500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2756/4339]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2757/4339]: training_loss: tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2758/4339]: training_loss: tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2759/4339]: training_loss: tensor(0.4437, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2760/4339]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2761/4339]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2762/4339]: training_loss: tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2763/4339]: training_loss: tensor(0.1760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2764/4339]: training_loss: tensor(0.2577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2765/4339]: training_loss: tensor(0.1893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2766/4339]: training_loss: tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2767/4339]: training_loss: tensor(0.0362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2768/4339]: training_loss: tensor(0.3377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2769/4339]: training_loss: tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2770/4339]: training_loss: tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2771/4339]: training_loss: tensor(0.2713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2772/4339]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2773/4339]: training_loss: tensor(0.1777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2774/4339]: training_loss: tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2775/4339]: training_loss: tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2776/4339]: training_loss: tensor(0.1834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2777/4339]: training_loss: tensor(0.1718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2778/4339]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2779/4339]: training_loss: tensor(0.2573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2780/4339]: training_loss: tensor(0.1283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2781/4339]: training_loss: tensor(0.1682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2782/4339]: training_loss: tensor(0.2741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2783/4339]: training_loss: tensor(0.1740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2784/4339]: training_loss: tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2785/4339]: training_loss: tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2786/4339]: training_loss: tensor(0.2480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2787/4339]: training_loss: tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2788/4339]: training_loss: tensor(0.2122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2789/4339]: training_loss: tensor(0.1640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2790/4339]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2791/4339]: training_loss: tensor(0.1903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2792/4339]: training_loss: tensor(0.2179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2793/4339]: training_loss: tensor(0.1629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2794/4339]: training_loss: tensor(0.2906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2795/4339]: training_loss: tensor(0.1853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2796/4339]: training_loss: tensor(0.1324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2797/4339]: training_loss: tensor(0.1748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2798/4339]: training_loss: tensor(0.2288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2799/4339]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2800/4339]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2801/4339]: training_loss: tensor(0.5267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2802/4339]: training_loss: tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2803/4339]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2804/4339]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2805/4339]: training_loss: tensor(0.2550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2806/4339]: training_loss: tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2807/4339]: training_loss: tensor(0.2909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2808/4339]: training_loss: tensor(0.4707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2809/4339]: training_loss: tensor(0.0771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2810/4339]: training_loss: tensor(0.7220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2811/4339]: training_loss: tensor(0.2662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2812/4339]: training_loss: tensor(0.2441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2813/4339]: training_loss: tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2814/4339]: training_loss: tensor(0.2860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2815/4339]: training_loss: tensor(0.2276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2816/4339]: training_loss: tensor(0.1382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2817/4339]: training_loss: tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2818/4339]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2819/4339]: training_loss: tensor(0.3355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2820/4339]: training_loss: tensor(0.1822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2821/4339]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2822/4339]: training_loss: tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2823/4339]: training_loss: tensor(0.1676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2824/4339]: training_loss: tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2825/4339]: training_loss: tensor(0.3607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2826/4339]: training_loss: tensor(0.0446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2827/4339]: training_loss: tensor(0.1758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2828/4339]: training_loss: tensor(0.0944, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2829/4339]: training_loss: tensor(0.5233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2830/4339]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2831/4339]: training_loss: tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2832/4339]: training_loss: tensor(0.0621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2833/4339]: training_loss: tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2834/4339]: training_loss: tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2835/4339]: training_loss: tensor(0.2130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2836/4339]: training_loss: tensor(0.1379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2837/4339]: training_loss: tensor(0.0515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2838/4339]: training_loss: tensor(0.1417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2839/4339]: training_loss: tensor(0.1700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2840/4339]: training_loss: tensor(0.2291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2841/4339]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2842/4339]: training_loss: tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2843/4339]: training_loss: tensor(0.1111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2844/4339]: training_loss: tensor(0.3814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2845/4339]: training_loss: tensor(0.1663, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2846/4339]: training_loss: tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2847/4339]: training_loss: tensor(0.1905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2848/4339]: training_loss: tensor(0.2388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2849/4339]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2850/4339]: training_loss: tensor(0.1184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2851/4339]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2852/4339]: training_loss: tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2853/4339]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2854/4339]: training_loss: tensor(0.1473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2855/4339]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2856/4339]: training_loss: tensor(0.1556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2857/4339]: training_loss: tensor(0.1546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2858/4339]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2859/4339]: training_loss: tensor(0.1098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2860/4339]: training_loss: tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2861/4339]: training_loss: tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2862/4339]: training_loss: tensor(0.3797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2863/4339]: training_loss: tensor(0.2814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2864/4339]: training_loss: tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2865/4339]: training_loss: tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2866/4339]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2867/4339]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2868/4339]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2869/4339]: training_loss: tensor(0.3688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2870/4339]: training_loss: tensor(0.1449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2871/4339]: training_loss: tensor(0.1707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2872/4339]: training_loss: tensor(0.2880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2873/4339]: training_loss: tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2874/4339]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2875/4339]: training_loss: tensor(0.3812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2876/4339]: training_loss: tensor(0.1396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2877/4339]: training_loss: tensor(0.4768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2878/4339]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2879/4339]: training_loss: tensor(0.1585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2880/4339]: training_loss: tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2881/4339]: training_loss: tensor(0.2594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2882/4339]: training_loss: tensor(0.1275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2883/4339]: training_loss: tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2884/4339]: training_loss: tensor(0.1567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2885/4339]: training_loss: tensor(0.1621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2886/4339]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2887/4339]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2888/4339]: training_loss: tensor(0.2315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2889/4339]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2890/4339]: training_loss: tensor(0.4879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2891/4339]: training_loss: tensor(0.1776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2892/4339]: training_loss: tensor(0.2176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2893/4339]: training_loss: tensor(0.2603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2894/4339]: training_loss: tensor(0.2596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2895/4339]: training_loss: tensor(0.1445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2896/4339]: training_loss: tensor(0.2007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2897/4339]: training_loss: tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2898/4339]: training_loss: tensor(0.3099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2899/4339]: training_loss: tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2900/4339]: training_loss: tensor(0.2977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2901/4339]: training_loss: tensor(0.1647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2902/4339]: training_loss: tensor(0.1208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2903/4339]: training_loss: tensor(0.1186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2904/4339]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2905/4339]: training_loss: tensor(0.3516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2906/4339]: training_loss: tensor(0.1604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2907/4339]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2908/4339]: training_loss: tensor(0.1696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2909/4339]: training_loss: tensor(0.2106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2910/4339]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2911/4339]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2912/4339]: training_loss: tensor(0.1211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2913/4339]: training_loss: tensor(0.0922, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2914/4339]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2915/4339]: training_loss: tensor(0.2051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2916/4339]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2917/4339]: training_loss: tensor(0.1390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2918/4339]: training_loss: tensor(0.2477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2919/4339]: training_loss: tensor(0.0427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2920/4339]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2921/4339]: training_loss: tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2922/4339]: training_loss: tensor(0.1291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2923/4339]: training_loss: tensor(0.1266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2924/4339]: training_loss: tensor(0.1183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2925/4339]: training_loss: tensor(0.3200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2926/4339]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2927/4339]: training_loss: tensor(0.2616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2928/4339]: training_loss: tensor(0.1120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2929/4339]: training_loss: tensor(0.1317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2930/4339]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2931/4339]: training_loss: tensor(0.3879, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2932/4339]: training_loss: tensor(0.1365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2933/4339]: training_loss: tensor(0.2412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2934/4339]: training_loss: tensor(0.0971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2935/4339]: training_loss: tensor(0.3853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2936/4339]: training_loss: tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2937/4339]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2938/4339]: training_loss: tensor(0.1134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2939/4339]: training_loss: tensor(0.0480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2940/4339]: training_loss: tensor(0.2127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2941/4339]: training_loss: tensor(0.3647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2942/4339]: training_loss: tensor(0.1333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2943/4339]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2944/4339]: training_loss: tensor(0.1294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2945/4339]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2946/4339]: training_loss: tensor(0.3715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2947/4339]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2948/4339]: training_loss: tensor(0.2568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2949/4339]: training_loss: tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2950/4339]: training_loss: tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2951/4339]: training_loss: tensor(0.2267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2952/4339]: training_loss: tensor(0.3693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2953/4339]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2954/4339]: training_loss: tensor(0.1516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2955/4339]: training_loss: tensor(0.0458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2956/4339]: training_loss: tensor(0.2563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2957/4339]: training_loss: tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2958/4339]: training_loss: tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2959/4339]: training_loss: tensor(0.2038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2960/4339]: training_loss: tensor(0.0305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2961/4339]: training_loss: tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2962/4339]: training_loss: tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2963/4339]: training_loss: tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2964/4339]: training_loss: tensor(0.1662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2965/4339]: training_loss: tensor(0.1625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2966/4339]: training_loss: tensor(0.3527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2967/4339]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2968/4339]: training_loss: tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2969/4339]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2970/4339]: training_loss: tensor(0.2115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2971/4339]: training_loss: tensor(0.1277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2972/4339]: training_loss: tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2973/4339]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2974/4339]: training_loss: tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2975/4339]: training_loss: tensor(0.1827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2976/4339]: training_loss: tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2977/4339]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2978/4339]: training_loss: tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2979/4339]: training_loss: tensor(0.2074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2980/4339]: training_loss: tensor(0.3358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2981/4339]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2982/4339]: training_loss: tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2983/4339]: training_loss: tensor(0.3809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2984/4339]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2985/4339]: training_loss: tensor(0.0621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2986/4339]: training_loss: tensor(0.1741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2987/4339]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2988/4339]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2989/4339]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2990/4339]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2991/4339]: training_loss: tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2992/4339]: training_loss: tensor(0.1314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2993/4339]: training_loss: tensor(0.1445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2994/4339]: training_loss: tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2995/4339]: training_loss: tensor(0.2539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2996/4339]: training_loss: tensor(0.1561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2997/4339]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2998/4339]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2999/4339]: training_loss: tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3000/4339]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3001/4339]: training_loss: tensor(0.1387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3002/4339]: training_loss: tensor(0.6859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3003/4339]: training_loss: tensor(0.3717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3004/4339]: training_loss: tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3005/4339]: training_loss: tensor(0.1513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3006/4339]: training_loss: tensor(0.1543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3007/4339]: training_loss: tensor(0.2335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3008/4339]: training_loss: tensor(0.2070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3009/4339]: training_loss: tensor(0.1573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3010/4339]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3011/4339]: training_loss: tensor(0.4153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3012/4339]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3013/4339]: training_loss: tensor(0.2715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3014/4339]: training_loss: tensor(0.1564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3015/4339]: training_loss: tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3016/4339]: training_loss: tensor(0.1537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3017/4339]: training_loss: tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3018/4339]: training_loss: tensor(0.4487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3019/4339]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3020/4339]: training_loss: tensor(0.2366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3021/4339]: training_loss: tensor(0.3955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3022/4339]: training_loss: tensor(0.1442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3023/4339]: training_loss: tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3024/4339]: training_loss: tensor(0.1372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3025/4339]: training_loss: tensor(0.0495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3026/4339]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3027/4339]: training_loss: tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3028/4339]: training_loss: tensor(0.2241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3029/4339]: training_loss: tensor(0.3070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3030/4339]: training_loss: tensor(0.1414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3031/4339]: training_loss: tensor(0.1658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3032/4339]: training_loss: tensor(0.1070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3033/4339]: training_loss: tensor(0.1168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3034/4339]: training_loss: tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3035/4339]: training_loss: tensor(0.1697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3036/4339]: training_loss: tensor(0.1334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3037/4339]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3038/4339]: training_loss: tensor(0.1324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3039/4339]: training_loss: tensor(0.2980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3040/4339]: training_loss: tensor(0.0532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3041/4339]: training_loss: tensor(0.2249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3042/4339]: training_loss: tensor(0.1110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3043/4339]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3044/4339]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3045/4339]: training_loss: tensor(0.2211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3046/4339]: training_loss: tensor(0.1819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3047/4339]: training_loss: tensor(0.2491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3048/4339]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3049/4339]: training_loss: tensor(0.2850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3050/4339]: training_loss: tensor(0.4512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3051/4339]: training_loss: tensor(0.2876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3052/4339]: training_loss: tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3053/4339]: training_loss: tensor(0.3827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3054/4339]: training_loss: tensor(0.2710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3055/4339]: training_loss: tensor(0.2406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3056/4339]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3057/4339]: training_loss: tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3058/4339]: training_loss: tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3059/4339]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3060/4339]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3061/4339]: training_loss: tensor(0.4465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3062/4339]: training_loss: tensor(0.1276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3063/4339]: training_loss: tensor(0.2313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3064/4339]: training_loss: tensor(0.1719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3065/4339]: training_loss: tensor(0.1685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3066/4339]: training_loss: tensor(0.1696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3067/4339]: training_loss: tensor(0.2597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3068/4339]: training_loss: tensor(0.1995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3069/4339]: training_loss: tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3070/4339]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3071/4339]: training_loss: tensor(0.1120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3072/4339]: training_loss: tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3073/4339]: training_loss: tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3074/4339]: training_loss: tensor(0.1581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3075/4339]: training_loss: tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3076/4339]: training_loss: tensor(0.1456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3077/4339]: training_loss: tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3078/4339]: training_loss: tensor(0.1350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3079/4339]: training_loss: tensor(0.1430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3080/4339]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3081/4339]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3082/4339]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3083/4339]: training_loss: tensor(0.1591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3084/4339]: training_loss: tensor(0.1213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3085/4339]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3086/4339]: training_loss: tensor(0.3742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3087/4339]: training_loss: tensor(0.2548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3088/4339]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3089/4339]: training_loss: tensor(0.6964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3090/4339]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3091/4339]: training_loss: tensor(0.1970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3092/4339]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3093/4339]: training_loss: tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3094/4339]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3095/4339]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3096/4339]: training_loss: tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3097/4339]: training_loss: tensor(0.2635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3098/4339]: training_loss: tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3099/4339]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3100/4339]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3101/4339]: training_loss: tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3102/4339]: training_loss: tensor(0.1551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3103/4339]: training_loss: tensor(0.1802, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3104/4339]: training_loss: tensor(0.0616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3105/4339]: training_loss: tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3106/4339]: training_loss: tensor(0.1159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3107/4339]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3108/4339]: training_loss: tensor(0.3751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3109/4339]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3110/4339]: training_loss: tensor(0.3779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3111/4339]: training_loss: tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3112/4339]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3113/4339]: training_loss: tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3114/4339]: training_loss: tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3115/4339]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3116/4339]: training_loss: tensor(0.1346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3117/4339]: training_loss: tensor(0.3100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3118/4339]: training_loss: tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3119/4339]: training_loss: tensor(0.1111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3120/4339]: training_loss: tensor(0.1321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3121/4339]: training_loss: tensor(0.1639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3122/4339]: training_loss: tensor(0.1271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3123/4339]: training_loss: tensor(0.1990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3124/4339]: training_loss: tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3125/4339]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3126/4339]: training_loss: tensor(0.2856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3127/4339]: training_loss: tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3128/4339]: training_loss: tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3129/4339]: training_loss: tensor(0.2472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3130/4339]: training_loss: tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3131/4339]: training_loss: tensor(0.4284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3132/4339]: training_loss: tensor(0.1859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3133/4339]: training_loss: tensor(0.1438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3134/4339]: training_loss: tensor(0.2415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3135/4339]: training_loss: tensor(0.2540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3136/4339]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3137/4339]: training_loss: tensor(0.3150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3138/4339]: training_loss: tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3139/4339]: training_loss: tensor(0.1329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3140/4339]: training_loss: tensor(0.1605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3141/4339]: training_loss: tensor(0.1556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3142/4339]: training_loss: tensor(0.3004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3143/4339]: training_loss: tensor(0.2710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3144/4339]: training_loss: tensor(0.2060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3145/4339]: training_loss: tensor(0.2745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3146/4339]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3147/4339]: training_loss: tensor(0.1803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3148/4339]: training_loss: tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3149/4339]: training_loss: tensor(0.1500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3150/4339]: training_loss: tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3151/4339]: training_loss: tensor(0.2026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3152/4339]: training_loss: tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3153/4339]: training_loss: tensor(0.1212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3154/4339]: training_loss: tensor(0.2198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3155/4339]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3156/4339]: training_loss: tensor(0.1771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3157/4339]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3158/4339]: training_loss: tensor(0.1732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3159/4339]: training_loss: tensor(0.1572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3160/4339]: training_loss: tensor(0.1478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3161/4339]: training_loss: tensor(0.1395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3162/4339]: training_loss: tensor(0.0505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3163/4339]: training_loss: tensor(0.5216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3164/4339]: training_loss: tensor(0.3278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3165/4339]: training_loss: tensor(0.2487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3166/4339]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3167/4339]: training_loss: tensor(0.1851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3168/4339]: training_loss: tensor(0.1071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3169/4339]: training_loss: tensor(0.2052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3170/4339]: training_loss: tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3171/4339]: training_loss: tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3172/4339]: training_loss: tensor(0.1450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3173/4339]: training_loss: tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3174/4339]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3175/4339]: training_loss: tensor(0.1455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3176/4339]: training_loss: tensor(0.2111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3177/4339]: training_loss: tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3178/4339]: training_loss: tensor(0.1649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3179/4339]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3180/4339]: training_loss: tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3181/4339]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3182/4339]: training_loss: tensor(0.1157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3183/4339]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3184/4339]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3185/4339]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3186/4339]: training_loss: tensor(0.4497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3187/4339]: training_loss: tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3188/4339]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3189/4339]: training_loss: tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3190/4339]: training_loss: tensor(0.1349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3191/4339]: training_loss: tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3192/4339]: training_loss: tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3193/4339]: training_loss: tensor(0.2686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3194/4339]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3195/4339]: training_loss: tensor(0.1229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3196/4339]: training_loss: tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3197/4339]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3198/4339]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3199/4339]: training_loss: tensor(0.1978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3200/4339]: training_loss: tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3201/4339]: training_loss: tensor(0.1284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3202/4339]: training_loss: tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3203/4339]: training_loss: tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3204/4339]: training_loss: tensor(0.3984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3205/4339]: training_loss: tensor(0.1298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3206/4339]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3207/4339]: training_loss: tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3208/4339]: training_loss: tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3209/4339]: training_loss: tensor(0.1828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3210/4339]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3211/4339]: training_loss: tensor(0.2665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3212/4339]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3213/4339]: training_loss: tensor(0.2212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3214/4339]: training_loss: tensor(0.1546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3215/4339]: training_loss: tensor(0.1660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3216/4339]: training_loss: tensor(0.1237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3217/4339]: training_loss: tensor(0.1468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3218/4339]: training_loss: tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3219/4339]: training_loss: tensor(0.1518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3220/4339]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3221/4339]: training_loss: tensor(0.1792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3222/4339]: training_loss: tensor(0.2424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3223/4339]: training_loss: tensor(0.1936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3224/4339]: training_loss: tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3225/4339]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3226/4339]: training_loss: tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3227/4339]: training_loss: tensor(0.0806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3228/4339]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3229/4339]: training_loss: tensor(0.2105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3230/4339]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3231/4339]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3232/4339]: training_loss: tensor(0.4463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3233/4339]: training_loss: tensor(0.0635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3234/4339]: training_loss: tensor(0.2410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3235/4339]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3236/4339]: training_loss: tensor(0.3728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3237/4339]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3238/4339]: training_loss: tensor(0.2419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3239/4339]: training_loss: tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3240/4339]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3241/4339]: training_loss: tensor(0.2804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3242/4339]: training_loss: tensor(0.1829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3243/4339]: training_loss: tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3244/4339]: training_loss: tensor(0.3818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3245/4339]: training_loss: tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3246/4339]: training_loss: tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3247/4339]: training_loss: tensor(0.2195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3248/4339]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3249/4339]: training_loss: tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3250/4339]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3251/4339]: training_loss: tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3252/4339]: training_loss: tensor(0.1546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3253/4339]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3254/4339]: training_loss: tensor(0.1353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3255/4339]: training_loss: tensor(0.1503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3256/4339]: training_loss: tensor(0.1198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3257/4339]: training_loss: tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3258/4339]: training_loss: tensor(0.3229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3259/4339]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3260/4339]: training_loss: tensor(0.5558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3261/4339]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3262/4339]: training_loss: tensor(0.2230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3263/4339]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3264/4339]: training_loss: tensor(0.0398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3265/4339]: training_loss: tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3266/4339]: training_loss: tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3267/4339]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3268/4339]: training_loss: tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3269/4339]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3270/4339]: training_loss: tensor(0.1217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3271/4339]: training_loss: tensor(0.0462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3272/4339]: training_loss: tensor(0.3009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3273/4339]: training_loss: tensor(0.1751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3274/4339]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3275/4339]: training_loss: tensor(0.4171, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3276/4339]: training_loss: tensor(0.4111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3277/4339]: training_loss: tensor(0.0940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3278/4339]: training_loss: tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3279/4339]: training_loss: tensor(0.3193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3280/4339]: training_loss: tensor(0.1508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3281/4339]: training_loss: tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3282/4339]: training_loss: tensor(0.1160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3283/4339]: training_loss: tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3284/4339]: training_loss: tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3285/4339]: training_loss: tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3286/4339]: training_loss: tensor(0.3752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3287/4339]: training_loss: tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3288/4339]: training_loss: tensor(0.2767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3289/4339]: training_loss: tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3290/4339]: training_loss: tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3291/4339]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3292/4339]: training_loss: tensor(0.3171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3293/4339]: training_loss: tensor(0.1615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3294/4339]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3295/4339]: training_loss: tensor(0.1640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3296/4339]: training_loss: tensor(0.2022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3297/4339]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3298/4339]: training_loss: tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3299/4339]: training_loss: tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3300/4339]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3301/4339]: training_loss: tensor(0.0961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3302/4339]: training_loss: tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3303/4339]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3304/4339]: training_loss: tensor(0.1473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3305/4339]: training_loss: tensor(0.1658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3306/4339]: training_loss: tensor(0.1432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3307/4339]: training_loss: tensor(0.0497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3308/4339]: training_loss: tensor(0.1407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3309/4339]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3310/4339]: training_loss: tensor(0.0544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3311/4339]: training_loss: tensor(0.1759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3312/4339]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3313/4339]: training_loss: tensor(0.0304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3314/4339]: training_loss: tensor(0.5427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3315/4339]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3316/4339]: training_loss: tensor(0.1933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3317/4339]: training_loss: tensor(0.3269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3318/4339]: training_loss: tensor(0.1369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3319/4339]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3320/4339]: training_loss: tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3321/4339]: training_loss: tensor(0.1850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3322/4339]: training_loss: tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3323/4339]: training_loss: tensor(0.3731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3324/4339]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3325/4339]: training_loss: tensor(0.3200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3326/4339]: training_loss: tensor(0.4885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3327/4339]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3328/4339]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3329/4339]: training_loss: tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3330/4339]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3331/4339]: training_loss: tensor(0.1910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3332/4339]: training_loss: tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3333/4339]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3334/4339]: training_loss: tensor(0.1846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3335/4339]: training_loss: tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3336/4339]: training_loss: tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3337/4339]: training_loss: tensor(0.1810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3338/4339]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3339/4339]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3340/4339]: training_loss: tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3341/4339]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3342/4339]: training_loss: tensor(0.1529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3343/4339]: training_loss: tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3344/4339]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3345/4339]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3346/4339]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3347/4339]: training_loss: tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3348/4339]: training_loss: tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3349/4339]: training_loss: tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3350/4339]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3351/4339]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3352/4339]: training_loss: tensor(0.3992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3353/4339]: training_loss: tensor(0.4336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3354/4339]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3355/4339]: training_loss: tensor(0.1157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3356/4339]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3357/4339]: training_loss: tensor(0.3983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3358/4339]: training_loss: tensor(0.3514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3359/4339]: training_loss: tensor(0.1127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3360/4339]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3361/4339]: training_loss: tensor(0.1884, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3362/4339]: training_loss: tensor(0.1413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3363/4339]: training_loss: tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3364/4339]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3365/4339]: training_loss: tensor(0.1785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3366/4339]: training_loss: tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3367/4339]: training_loss: tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3368/4339]: training_loss: tensor(0.1738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3369/4339]: training_loss: tensor(0.1195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3370/4339]: training_loss: tensor(0.1580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3371/4339]: training_loss: tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3372/4339]: training_loss: tensor(0.1628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3373/4339]: training_loss: tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3374/4339]: training_loss: tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3375/4339]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3376/4339]: training_loss: tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3377/4339]: training_loss: tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3378/4339]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3379/4339]: training_loss: tensor(0.2226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3380/4339]: training_loss: tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3381/4339]: training_loss: tensor(0.2912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3382/4339]: training_loss: tensor(0.1149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3383/4339]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3384/4339]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3385/4339]: training_loss: tensor(0.1062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3386/4339]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3387/4339]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3388/4339]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3389/4339]: training_loss: tensor(0.2104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3390/4339]: training_loss: tensor(0.0475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3391/4339]: training_loss: tensor(0.1479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3392/4339]: training_loss: tensor(0.5664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3393/4339]: training_loss: tensor(0.1597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3394/4339]: training_loss: tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3395/4339]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3396/4339]: training_loss: tensor(0.0881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3397/4339]: training_loss: tensor(0.2179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3398/4339]: training_loss: tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3399/4339]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3400/4339]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3401/4339]: training_loss: tensor(0.0950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3402/4339]: training_loss: tensor(0.1202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3403/4339]: training_loss: tensor(0.2076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3404/4339]: training_loss: tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3405/4339]: training_loss: tensor(0.1225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3406/4339]: training_loss: tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3407/4339]: training_loss: tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3408/4339]: training_loss: tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3409/4339]: training_loss: tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3410/4339]: training_loss: tensor(0.1352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3411/4339]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3412/4339]: training_loss: tensor(0.1300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3413/4339]: training_loss: tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3414/4339]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3415/4339]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3416/4339]: training_loss: tensor(0.0290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3417/4339]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3418/4339]: training_loss: tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3419/4339]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3420/4339]: training_loss: tensor(0.0576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3421/4339]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3422/4339]: training_loss: tensor(0.8091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3423/4339]: training_loss: tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3424/4339]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3425/4339]: training_loss: tensor(0.4159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3426/4339]: training_loss: tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3427/4339]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3428/4339]: training_loss: tensor(0.2956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3429/4339]: training_loss: tensor(0.3089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3430/4339]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3431/4339]: training_loss: tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3432/4339]: training_loss: tensor(0.0492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3433/4339]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3434/4339]: training_loss: tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3435/4339]: training_loss: tensor(0.3455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3436/4339]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3437/4339]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3438/4339]: training_loss: tensor(0.4073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3439/4339]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3440/4339]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3441/4339]: training_loss: tensor(0.1930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3442/4339]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3443/4339]: training_loss: tensor(0.1335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3444/4339]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3445/4339]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3446/4339]: training_loss: tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3447/4339]: training_loss: tensor(0.3109, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3448/4339]: training_loss: tensor(0.2168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3449/4339]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3450/4339]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3451/4339]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3452/4339]: training_loss: tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3453/4339]: training_loss: tensor(0.1259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3454/4339]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3455/4339]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3456/4339]: training_loss: tensor(0.0615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3457/4339]: training_loss: tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3458/4339]: training_loss: tensor(0.0469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3459/4339]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3460/4339]: training_loss: tensor(0.1318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3461/4339]: training_loss: tensor(0.1865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3462/4339]: training_loss: tensor(0.2570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3463/4339]: training_loss: tensor(0.1363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3464/4339]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3465/4339]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3466/4339]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3467/4339]: training_loss: tensor(0.1863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3468/4339]: training_loss: tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3469/4339]: training_loss: tensor(0.2133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3470/4339]: training_loss: tensor(0.2421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3471/4339]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3472/4339]: training_loss: tensor(0.2412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3473/4339]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3474/4339]: training_loss: tensor(0.1804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3475/4339]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3476/4339]: training_loss: tensor(0.1836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3477/4339]: training_loss: tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3478/4339]: training_loss: tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3479/4339]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3480/4339]: training_loss: tensor(0.2353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3481/4339]: training_loss: tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3482/4339]: training_loss: tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3483/4339]: training_loss: tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3484/4339]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3485/4339]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3486/4339]: training_loss: tensor(0.2269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3487/4339]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3488/4339]: training_loss: tensor(0.0383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3489/4339]: training_loss: tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3490/4339]: training_loss: tensor(0.0572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3491/4339]: training_loss: tensor(0.1211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3492/4339]: training_loss: tensor(0.2017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3493/4339]: training_loss: tensor(0.2748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3494/4339]: training_loss: tensor(0.0991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3495/4339]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3496/4339]: training_loss: tensor(0.1408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3497/4339]: training_loss: tensor(0.2231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3498/4339]: training_loss: tensor(0.1226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3499/4339]: training_loss: tensor(0.1157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3500/4339]: training_loss: tensor(0.3305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3501/4339]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3502/4339]: training_loss: tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3503/4339]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3504/4339]: training_loss: tensor(0.2377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3505/4339]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3506/4339]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3507/4339]: training_loss: tensor(0.1881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3508/4339]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3509/4339]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3510/4339]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3511/4339]: training_loss: tensor(0.2742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3512/4339]: training_loss: tensor(0.3331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3513/4339]: training_loss: tensor(0.0655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3514/4339]: training_loss: tensor(0.1312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3515/4339]: training_loss: tensor(0.2917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3516/4339]: training_loss: tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3517/4339]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3518/4339]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3519/4339]: training_loss: tensor(0.1387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3520/4339]: training_loss: tensor(0.4562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3521/4339]: training_loss: tensor(0.4365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3522/4339]: training_loss: tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3523/4339]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3524/4339]: training_loss: tensor(0.2053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3525/4339]: training_loss: tensor(0.1511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3526/4339]: training_loss: tensor(0.4300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3527/4339]: training_loss: tensor(0.3714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3528/4339]: training_loss: tensor(0.3778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3529/4339]: training_loss: tensor(0.2089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3530/4339]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3531/4339]: training_loss: tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3532/4339]: training_loss: tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3533/4339]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3534/4339]: training_loss: tensor(0.2404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3535/4339]: training_loss: tensor(0.1636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3536/4339]: training_loss: tensor(0.1283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3537/4339]: training_loss: tensor(0.3080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3538/4339]: training_loss: tensor(0.1446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3539/4339]: training_loss: tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3540/4339]: training_loss: tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3541/4339]: training_loss: tensor(0.1167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3542/4339]: training_loss: tensor(0.4106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3543/4339]: training_loss: tensor(0.0313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3544/4339]: training_loss: tensor(0.2160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3545/4339]: training_loss: tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3546/4339]: training_loss: tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3547/4339]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3548/4339]: training_loss: tensor(0.1670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3549/4339]: training_loss: tensor(0.2776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3550/4339]: training_loss: tensor(0.1728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3551/4339]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3552/4339]: training_loss: tensor(0.0413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3553/4339]: training_loss: tensor(0.4968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3554/4339]: training_loss: tensor(0.1292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3555/4339]: training_loss: tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3556/4339]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3557/4339]: training_loss: tensor(0.1241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3558/4339]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3559/4339]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3560/4339]: training_loss: tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3561/4339]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3562/4339]: training_loss: tensor(0.4113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3563/4339]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3564/4339]: training_loss: tensor(0.1120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3565/4339]: training_loss: tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3566/4339]: training_loss: tensor(0.5335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3567/4339]: training_loss: tensor(0.3321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3568/4339]: training_loss: tensor(0.2391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3569/4339]: training_loss: tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3570/4339]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3571/4339]: training_loss: tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3572/4339]: training_loss: tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3573/4339]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3574/4339]: training_loss: tensor(0.1059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3575/4339]: training_loss: tensor(0.1156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3576/4339]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3577/4339]: training_loss: tensor(0.5090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3578/4339]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3579/4339]: training_loss: tensor(0.2508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3580/4339]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3581/4339]: training_loss: tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3582/4339]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3583/4339]: training_loss: tensor(0.4060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3584/4339]: training_loss: tensor(0.1139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3585/4339]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3586/4339]: training_loss: tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3587/4339]: training_loss: tensor(0.4327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3588/4339]: training_loss: tensor(0.3030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3589/4339]: training_loss: tensor(0.1472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3590/4339]: training_loss: tensor(0.1800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3591/4339]: training_loss: tensor(0.0494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3592/4339]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3593/4339]: training_loss: tensor(0.4644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3594/4339]: training_loss: tensor(0.2592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3595/4339]: training_loss: tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3596/4339]: training_loss: tensor(0.3020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3597/4339]: training_loss: tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3598/4339]: training_loss: tensor(0.5322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3599/4339]: training_loss: tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3600/4339]: training_loss: tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3601/4339]: training_loss: tensor(0.2631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3602/4339]: training_loss: tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3603/4339]: training_loss: tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3604/4339]: training_loss: tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3605/4339]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3606/4339]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3607/4339]: training_loss: tensor(0.1887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3608/4339]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3609/4339]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3610/4339]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3611/4339]: training_loss: tensor(0.2116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3612/4339]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3613/4339]: training_loss: tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3614/4339]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3615/4339]: training_loss: tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3616/4339]: training_loss: tensor(0.4022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3617/4339]: training_loss: tensor(0.2094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3618/4339]: training_loss: tensor(0.3740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3619/4339]: training_loss: tensor(0.1787, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3620/4339]: training_loss: tensor(0.5204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3621/4339]: training_loss: tensor(0.1851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3622/4339]: training_loss: tensor(0.2780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3623/4339]: training_loss: tensor(0.1678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3624/4339]: training_loss: tensor(0.2756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3625/4339]: training_loss: tensor(0.3329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3626/4339]: training_loss: tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3627/4339]: training_loss: tensor(0.2148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3628/4339]: training_loss: tensor(0.1291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3629/4339]: training_loss: tensor(0.2906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3630/4339]: training_loss: tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3631/4339]: training_loss: tensor(0.2444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3632/4339]: training_loss: tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3633/4339]: training_loss: tensor(0.1712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3634/4339]: training_loss: tensor(0.1529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3635/4339]: training_loss: tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3636/4339]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3637/4339]: training_loss: tensor(0.3011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3638/4339]: training_loss: tensor(0.1484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3639/4339]: training_loss: tensor(0.2591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3640/4339]: training_loss: tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3641/4339]: training_loss: tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3642/4339]: training_loss: tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3643/4339]: training_loss: tensor(0.2794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3644/4339]: training_loss: tensor(0.3577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3645/4339]: training_loss: tensor(0.4776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3646/4339]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3647/4339]: training_loss: tensor(0.2233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3648/4339]: training_loss: tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3649/4339]: training_loss: tensor(0.4150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3650/4339]: training_loss: tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3651/4339]: training_loss: tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3652/4339]: training_loss: tensor(0.5361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3653/4339]: training_loss: tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3654/4339]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3655/4339]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3656/4339]: training_loss: tensor(0.1129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3657/4339]: training_loss: tensor(0.2080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3658/4339]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3659/4339]: training_loss: tensor(0.1172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3660/4339]: training_loss: tensor(0.2463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3661/4339]: training_loss: tensor(0.1680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3662/4339]: training_loss: tensor(0.1297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3663/4339]: training_loss: tensor(0.3775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3664/4339]: training_loss: tensor(0.3367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3665/4339]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3666/4339]: training_loss: tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3667/4339]: training_loss: tensor(0.1736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3668/4339]: training_loss: tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3669/4339]: training_loss: tensor(0.2890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3670/4339]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3671/4339]: training_loss: tensor(0.1688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3672/4339]: training_loss: tensor(0.1668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3673/4339]: training_loss: tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3674/4339]: training_loss: tensor(0.1844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3675/4339]: training_loss: tensor(0.1781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3676/4339]: training_loss: tensor(0.1491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3677/4339]: training_loss: tensor(0.1195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3678/4339]: training_loss: tensor(0.2198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3679/4339]: training_loss: tensor(0.2551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3680/4339]: training_loss: tensor(0.2579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3681/4339]: training_loss: tensor(0.3975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3682/4339]: training_loss: tensor(0.1755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3683/4339]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3684/4339]: training_loss: tensor(0.2851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3685/4339]: training_loss: tensor(0.2425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3686/4339]: training_loss: tensor(0.2079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3687/4339]: training_loss: tensor(0.2562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3688/4339]: training_loss: tensor(0.1314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3689/4339]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3690/4339]: training_loss: tensor(0.4483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3691/4339]: training_loss: tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3692/4339]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3693/4339]: training_loss: tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3694/4339]: training_loss: tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3695/4339]: training_loss: tensor(0.2791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3696/4339]: training_loss: tensor(0.0597, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3697/4339]: training_loss: tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3698/4339]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3699/4339]: training_loss: tensor(0.2065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3700/4339]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3701/4339]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3702/4339]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3703/4339]: training_loss: tensor(0.0818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3704/4339]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3705/4339]: training_loss: tensor(0.1561, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3706/4339]: training_loss: tensor(0.1599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3707/4339]: training_loss: tensor(0.1554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3708/4339]: training_loss: tensor(0.2565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3709/4339]: training_loss: tensor(0.7955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3710/4339]: training_loss: tensor(0.0676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3711/4339]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3712/4339]: training_loss: tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3713/4339]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3714/4339]: training_loss: tensor(0.3280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3715/4339]: training_loss: tensor(0.2249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3716/4339]: training_loss: tensor(0.1068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3717/4339]: training_loss: tensor(0.2135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3718/4339]: training_loss: tensor(0.1303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3719/4339]: training_loss: tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3720/4339]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3721/4339]: training_loss: tensor(0.0608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3722/4339]: training_loss: tensor(0.1832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3723/4339]: training_loss: tensor(0.0365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3724/4339]: training_loss: tensor(0.3259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3725/4339]: training_loss: tensor(0.2288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3726/4339]: training_loss: tensor(0.3735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3727/4339]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3728/4339]: training_loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3729/4339]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3730/4339]: training_loss: tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3731/4339]: training_loss: tensor(0.5062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3732/4339]: training_loss: tensor(0.1688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3733/4339]: training_loss: tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3734/4339]: training_loss: tensor(0.0686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3735/4339]: training_loss: tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3736/4339]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3737/4339]: training_loss: tensor(0.1424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3738/4339]: training_loss: tensor(0.1315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3739/4339]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3740/4339]: training_loss: tensor(0.3488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3741/4339]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3742/4339]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3743/4339]: training_loss: tensor(0.1915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3744/4339]: training_loss: tensor(0.1259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3745/4339]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3746/4339]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3747/4339]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3748/4339]: training_loss: tensor(0.1255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3749/4339]: training_loss: tensor(0.2679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3750/4339]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3751/4339]: training_loss: tensor(0.3950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3752/4339]: training_loss: tensor(0.1495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3753/4339]: training_loss: tensor(0.3591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3754/4339]: training_loss: tensor(0.3818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3755/4339]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3756/4339]: training_loss: tensor(0.5199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3757/4339]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3758/4339]: training_loss: tensor(0.1947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3759/4339]: training_loss: tensor(0.3071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3760/4339]: training_loss: tensor(0.1805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3761/4339]: training_loss: tensor(0.1669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3762/4339]: training_loss: tensor(0.5681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3763/4339]: training_loss: tensor(0.1475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3764/4339]: training_loss: tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3765/4339]: training_loss: tensor(0.1229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3766/4339]: training_loss: tensor(0.2934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3767/4339]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3768/4339]: training_loss: tensor(0.1578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3769/4339]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3770/4339]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3771/4339]: training_loss: tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3772/4339]: training_loss: tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3773/4339]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3774/4339]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3775/4339]: training_loss: tensor(0.0555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3776/4339]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3777/4339]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3778/4339]: training_loss: tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3779/4339]: training_loss: tensor(0.8096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3780/4339]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3781/4339]: training_loss: tensor(0.1257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3782/4339]: training_loss: tensor(0.1588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3783/4339]: training_loss: tensor(0.1885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3784/4339]: training_loss: tensor(0.1149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3785/4339]: training_loss: tensor(0.2063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3786/4339]: training_loss: tensor(0.1225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3787/4339]: training_loss: tensor(0.0427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3788/4339]: training_loss: tensor(0.2053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3789/4339]: training_loss: tensor(0.0505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3790/4339]: training_loss: tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3791/4339]: training_loss: tensor(0.2682, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3792/4339]: training_loss: tensor(0.3064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3793/4339]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3794/4339]: training_loss: tensor(0.5477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3795/4339]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3796/4339]: training_loss: tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3797/4339]: training_loss: tensor(0.1857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3798/4339]: training_loss: tensor(0.1984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3799/4339]: training_loss: tensor(0.1516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3800/4339]: training_loss: tensor(0.2469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3801/4339]: training_loss: tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3802/4339]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3803/4339]: training_loss: tensor(0.4692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3804/4339]: training_loss: tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3805/4339]: training_loss: tensor(0.2543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3806/4339]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3807/4339]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3808/4339]: training_loss: tensor(0.2914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3809/4339]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3810/4339]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3811/4339]: training_loss: tensor(0.0547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3812/4339]: training_loss: tensor(0.4087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3813/4339]: training_loss: tensor(0.3903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3814/4339]: training_loss: tensor(0.1987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3815/4339]: training_loss: tensor(0.2364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3816/4339]: training_loss: tensor(0.0892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3817/4339]: training_loss: tensor(0.2561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3818/4339]: training_loss: tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3819/4339]: training_loss: tensor(0.1670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3820/4339]: training_loss: tensor(0.2530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3821/4339]: training_loss: tensor(0.2216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3822/4339]: training_loss: tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3823/4339]: training_loss: tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3824/4339]: training_loss: tensor(0.0540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3825/4339]: training_loss: tensor(0.6777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3826/4339]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3827/4339]: training_loss: tensor(0.2146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3828/4339]: training_loss: tensor(0.1556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3829/4339]: training_loss: tensor(0.5571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3830/4339]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3831/4339]: training_loss: tensor(0.1364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3832/4339]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3833/4339]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3834/4339]: training_loss: tensor(0.5830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3835/4339]: training_loss: tensor(0.2251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3836/4339]: training_loss: tensor(0.1940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3837/4339]: training_loss: tensor(0.2957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3838/4339]: training_loss: tensor(0.3780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3839/4339]: training_loss: tensor(0.4254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3840/4339]: training_loss: tensor(0.0442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3841/4339]: training_loss: tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3842/4339]: training_loss: tensor(0.1953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3843/4339]: training_loss: tensor(0.2429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3844/4339]: training_loss: tensor(0.1782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3845/4339]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3846/4339]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3847/4339]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3848/4339]: training_loss: tensor(0.2512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3849/4339]: training_loss: tensor(0.2480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3850/4339]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3851/4339]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3852/4339]: training_loss: tensor(0.3416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3853/4339]: training_loss: tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3854/4339]: training_loss: tensor(0.2475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3855/4339]: training_loss: tensor(0.3033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3856/4339]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3857/4339]: training_loss: tensor(0.1958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3858/4339]: training_loss: tensor(0.0570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3859/4339]: training_loss: tensor(0.1099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3860/4339]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3861/4339]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3862/4339]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3863/4339]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3864/4339]: training_loss: tensor(0.2669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3865/4339]: training_loss: tensor(0.1941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3866/4339]: training_loss: tensor(0.1319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3867/4339]: training_loss: tensor(0.1561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3868/4339]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3869/4339]: training_loss: tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3870/4339]: training_loss: tensor(0.3181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3871/4339]: training_loss: tensor(0.2375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3872/4339]: training_loss: tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3873/4339]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3874/4339]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3875/4339]: training_loss: tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3876/4339]: training_loss: tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3877/4339]: training_loss: tensor(0.3662, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3878/4339]: training_loss: tensor(0.4264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3879/4339]: training_loss: tensor(0.2076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3880/4339]: training_loss: tensor(0.2872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3881/4339]: training_loss: tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3882/4339]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3883/4339]: training_loss: tensor(0.4029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3884/4339]: training_loss: tensor(0.4515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3885/4339]: training_loss: tensor(0.1703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3886/4339]: training_loss: tensor(0.1273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3887/4339]: training_loss: tensor(0.1931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3888/4339]: training_loss: tensor(0.4219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3889/4339]: training_loss: tensor(0.1679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3890/4339]: training_loss: tensor(0.2857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3891/4339]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3892/4339]: training_loss: tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3893/4339]: training_loss: tensor(0.2368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3894/4339]: training_loss: tensor(0.1729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3895/4339]: training_loss: tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3896/4339]: training_loss: tensor(0.4983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3897/4339]: training_loss: tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3898/4339]: training_loss: tensor(0.3006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3899/4339]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3900/4339]: training_loss: tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3901/4339]: training_loss: tensor(0.1981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3902/4339]: training_loss: tensor(0.2243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3903/4339]: training_loss: tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3904/4339]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3905/4339]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3906/4339]: training_loss: tensor(0.1241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3907/4339]: training_loss: tensor(0.3687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3908/4339]: training_loss: tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3909/4339]: training_loss: tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3910/4339]: training_loss: tensor(0.2872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3911/4339]: training_loss: tensor(0.2152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3912/4339]: training_loss: tensor(0.2758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3913/4339]: training_loss: tensor(0.2172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3914/4339]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3915/4339]: training_loss: tensor(0.1230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3916/4339]: training_loss: tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3917/4339]: training_loss: tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3918/4339]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3919/4339]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3920/4339]: training_loss: tensor(0.0389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3921/4339]: training_loss: tensor(0.4790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3922/4339]: training_loss: tensor(0.3179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3923/4339]: training_loss: tensor(0.0663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3924/4339]: training_loss: tensor(0.0445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3925/4339]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3926/4339]: training_loss: tensor(0.4244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3927/4339]: training_loss: tensor(0.1481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3928/4339]: training_loss: tensor(0.1889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3929/4339]: training_loss: tensor(0.6240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3930/4339]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3931/4339]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3932/4339]: training_loss: tensor(0.1315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3933/4339]: training_loss: tensor(0.3430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3934/4339]: training_loss: tensor(0.1105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3935/4339]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3936/4339]: training_loss: tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3937/4339]: training_loss: tensor(0.1180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3938/4339]: training_loss: tensor(0.2258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3939/4339]: training_loss: tensor(0.2432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3940/4339]: training_loss: tensor(0.2640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3941/4339]: training_loss: tensor(0.2257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3942/4339]: training_loss: tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3943/4339]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3944/4339]: training_loss: tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3945/4339]: training_loss: tensor(0.1629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3946/4339]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3947/4339]: training_loss: tensor(0.2666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3948/4339]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3949/4339]: training_loss: tensor(0.1589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3950/4339]: training_loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3951/4339]: training_loss: tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3952/4339]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3953/4339]: training_loss: tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3954/4339]: training_loss: tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3955/4339]: training_loss: tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3956/4339]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3957/4339]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3958/4339]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3959/4339]: training_loss: tensor(0.1120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3960/4339]: training_loss: tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3961/4339]: training_loss: tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3962/4339]: training_loss: tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3963/4339]: training_loss: tensor(0.1252, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3964/4339]: training_loss: tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3965/4339]: training_loss: tensor(0.2189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3966/4339]: training_loss: tensor(0.1792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3967/4339]: training_loss: tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3968/4339]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3969/4339]: training_loss: tensor(0.2209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3970/4339]: training_loss: tensor(0.0534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3971/4339]: training_loss: tensor(0.2449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3972/4339]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3973/4339]: training_loss: tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3974/4339]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3975/4339]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3976/4339]: training_loss: tensor(0.4039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3977/4339]: training_loss: tensor(0.1354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3978/4339]: training_loss: tensor(0.5280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3979/4339]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3980/4339]: training_loss: tensor(0.1240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3981/4339]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3982/4339]: training_loss: tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3983/4339]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3984/4339]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3985/4339]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3986/4339]: training_loss: tensor(0.3871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3987/4339]: training_loss: tensor(0.3217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3988/4339]: training_loss: tensor(0.0594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3989/4339]: training_loss: tensor(0.0643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3990/4339]: training_loss: tensor(0.1574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3991/4339]: training_loss: tensor(0.1868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3992/4339]: training_loss: tensor(0.1722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3993/4339]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3994/4339]: training_loss: tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3995/4339]: training_loss: tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3996/4339]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3997/4339]: training_loss: tensor(0.1867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3998/4339]: training_loss: tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3999/4339]: training_loss: tensor(0.2185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4000/4339]: training_loss: tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4001/4339]: training_loss: tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4002/4339]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4003/4339]: training_loss: tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4004/4339]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4005/4339]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4006/4339]: training_loss: tensor(0.1426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4007/4339]: training_loss: tensor(0.1473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4008/4339]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4009/4339]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4010/4339]: training_loss: tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4011/4339]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4012/4339]: training_loss: tensor(0.2422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4013/4339]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4014/4339]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4015/4339]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4016/4339]: training_loss: tensor(0.0436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4017/4339]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4018/4339]: training_loss: tensor(0.1555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4019/4339]: training_loss: tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4020/4339]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4021/4339]: training_loss: tensor(0.1862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4022/4339]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4023/4339]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4024/4339]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4025/4339]: training_loss: tensor(0.3826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4026/4339]: training_loss: tensor(0.4198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4027/4339]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4028/4339]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4029/4339]: training_loss: tensor(0.1543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4030/4339]: training_loss: tensor(0.2275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4031/4339]: training_loss: tensor(0.4848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4032/4339]: training_loss: tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4033/4339]: training_loss: tensor(0.2513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4034/4339]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4035/4339]: training_loss: tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4036/4339]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4037/4339]: training_loss: tensor(0.1251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4038/4339]: training_loss: tensor(0.1753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4039/4339]: training_loss: tensor(0.1417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4040/4339]: training_loss: tensor(0.2310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4041/4339]: training_loss: tensor(0.2149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4042/4339]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4043/4339]: training_loss: tensor(0.1322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4044/4339]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4045/4339]: training_loss: tensor(0.3756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4046/4339]: training_loss: tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4047/4339]: training_loss: tensor(0.1778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4048/4339]: training_loss: tensor(0.1738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4049/4339]: training_loss: tensor(0.1058, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4050/4339]: training_loss: tensor(0.2382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4051/4339]: training_loss: tensor(0.1260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4052/4339]: training_loss: tensor(0.4165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4053/4339]: training_loss: tensor(0.5797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4054/4339]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4055/4339]: training_loss: tensor(0.1078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4056/4339]: training_loss: tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4057/4339]: training_loss: tensor(0.2448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4058/4339]: training_loss: tensor(0.1421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4059/4339]: training_loss: tensor(0.2678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4060/4339]: training_loss: tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4061/4339]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4062/4339]: training_loss: tensor(0.1436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4063/4339]: training_loss: tensor(0.3728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4064/4339]: training_loss: tensor(0.1315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4065/4339]: training_loss: tensor(0.1920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4066/4339]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4067/4339]: training_loss: tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4068/4339]: training_loss: tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4069/4339]: training_loss: tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4070/4339]: training_loss: tensor(0.5212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4071/4339]: training_loss: tensor(0.1704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4072/4339]: training_loss: tensor(0.3846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4073/4339]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4074/4339]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4075/4339]: training_loss: tensor(0.3649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4076/4339]: training_loss: tensor(0.4082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4077/4339]: training_loss: tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4078/4339]: training_loss: tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4079/4339]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4080/4339]: training_loss: tensor(0.1843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4081/4339]: training_loss: tensor(0.1527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4082/4339]: training_loss: tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4083/4339]: training_loss: tensor(0.1987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4084/4339]: training_loss: tensor(0.1512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4085/4339]: training_loss: tensor(0.1339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4086/4339]: training_loss: tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4087/4339]: training_loss: tensor(0.3278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4088/4339]: training_loss: tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4089/4339]: training_loss: tensor(0.0665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4090/4339]: training_loss: tensor(0.3392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4091/4339]: training_loss: tensor(0.1383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4092/4339]: training_loss: tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4093/4339]: training_loss: tensor(0.1173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4094/4339]: training_loss: tensor(0.2307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4095/4339]: training_loss: tensor(0.1592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4096/4339]: training_loss: tensor(0.1500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4097/4339]: training_loss: tensor(0.2757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4098/4339]: training_loss: tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4099/4339]: training_loss: tensor(0.1182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4100/4339]: training_loss: tensor(0.3994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4101/4339]: training_loss: tensor(0.3454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4102/4339]: training_loss: tensor(0.2707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4103/4339]: training_loss: tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4104/4339]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4105/4339]: training_loss: tensor(0.4823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4106/4339]: training_loss: tensor(0.4633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4107/4339]: training_loss: tensor(0.2786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4108/4339]: training_loss: tensor(0.3606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4109/4339]: training_loss: tensor(0.2181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4110/4339]: training_loss: tensor(0.0773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4111/4339]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4112/4339]: training_loss: tensor(0.2127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4113/4339]: training_loss: tensor(0.3565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4114/4339]: training_loss: tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4115/4339]: training_loss: tensor(0.2913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4116/4339]: training_loss: tensor(0.1340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4117/4339]: training_loss: tensor(0.2694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4118/4339]: training_loss: tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4119/4339]: training_loss: tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4120/4339]: training_loss: tensor(0.4175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4121/4339]: training_loss: tensor(0.1321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4122/4339]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4123/4339]: training_loss: tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4124/4339]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4125/4339]: training_loss: tensor(0.2533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4126/4339]: training_loss: tensor(0.1282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4127/4339]: training_loss: tensor(0.3409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4128/4339]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4129/4339]: training_loss: tensor(0.2157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4130/4339]: training_loss: tensor(0.0717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4131/4339]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4132/4339]: training_loss: tensor(0.3384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4133/4339]: training_loss: tensor(0.4271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4134/4339]: training_loss: tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4135/4339]: training_loss: tensor(0.1374, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4136/4339]: training_loss: tensor(0.1143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4137/4339]: training_loss: tensor(0.1854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4138/4339]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4139/4339]: training_loss: tensor(0.2938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4140/4339]: training_loss: tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4141/4339]: training_loss: tensor(0.2304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4142/4339]: training_loss: tensor(0.1198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4143/4339]: training_loss: tensor(0.1273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4144/4339]: training_loss: tensor(0.3559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4145/4339]: training_loss: tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4146/4339]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4147/4339]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4148/4339]: training_loss: tensor(0.2120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4149/4339]: training_loss: tensor(0.2240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4150/4339]: training_loss: tensor(0.1650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4151/4339]: training_loss: tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4152/4339]: training_loss: tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4153/4339]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4154/4339]: training_loss: tensor(0.1476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4155/4339]: training_loss: tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4156/4339]: training_loss: tensor(0.6276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4157/4339]: training_loss: tensor(0.1834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4158/4339]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4159/4339]: training_loss: tensor(0.4888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4160/4339]: training_loss: tensor(0.1620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4161/4339]: training_loss: tensor(0.2488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4162/4339]: training_loss: tensor(0.1682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4163/4339]: training_loss: tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4164/4339]: training_loss: tensor(0.4034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4165/4339]: training_loss: tensor(0.2950, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4166/4339]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4167/4339]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4168/4339]: training_loss: tensor(0.2822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4169/4339]: training_loss: tensor(0.2219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4170/4339]: training_loss: tensor(0.2180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4171/4339]: training_loss: tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4172/4339]: training_loss: tensor(0.1756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4173/4339]: training_loss: tensor(0.3558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4174/4339]: training_loss: tensor(0.1931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4175/4339]: training_loss: tensor(0.1910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4176/4339]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4177/4339]: training_loss: tensor(0.4416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4178/4339]: training_loss: tensor(0.3693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4179/4339]: training_loss: tensor(0.5259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4180/4339]: training_loss: tensor(0.3573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4181/4339]: training_loss: tensor(0.1620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4182/4339]: training_loss: tensor(0.2133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4183/4339]: training_loss: tensor(0.1237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4184/4339]: training_loss: tensor(0.2265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4185/4339]: training_loss: tensor(0.1538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4186/4339]: training_loss: tensor(0.1304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4187/4339]: training_loss: tensor(0.1280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4188/4339]: training_loss: tensor(0.2286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4189/4339]: training_loss: tensor(0.1673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4190/4339]: training_loss: tensor(0.2058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4191/4339]: training_loss: tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4192/4339]: training_loss: tensor(0.1967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4193/4339]: training_loss: tensor(0.1487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4194/4339]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4195/4339]: training_loss: tensor(0.2411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4196/4339]: training_loss: tensor(0.0445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4197/4339]: training_loss: tensor(0.3291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4198/4339]: training_loss: tensor(0.1346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4199/4339]: training_loss: tensor(0.2518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4200/4339]: training_loss: tensor(0.1294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4201/4339]: training_loss: tensor(0.1580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4202/4339]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4203/4339]: training_loss: tensor(0.1593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4204/4339]: training_loss: tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4205/4339]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4206/4339]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4207/4339]: training_loss: tensor(0.1071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4208/4339]: training_loss: tensor(0.0776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4209/4339]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4210/4339]: training_loss: tensor(0.4006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4211/4339]: training_loss: tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4212/4339]: training_loss: tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4213/4339]: training_loss: tensor(0.3104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4214/4339]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4215/4339]: training_loss: tensor(0.1396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4216/4339]: training_loss: tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4217/4339]: training_loss: tensor(0.3023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4218/4339]: training_loss: tensor(0.1694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4219/4339]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4220/4339]: training_loss: tensor(0.1674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4221/4339]: training_loss: tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4222/4339]: training_loss: tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4223/4339]: training_loss: tensor(0.3485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4224/4339]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4225/4339]: training_loss: tensor(0.4265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4226/4339]: training_loss: tensor(0.1436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4227/4339]: training_loss: tensor(0.2916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4228/4339]: training_loss: tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4229/4339]: training_loss: tensor(0.2223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4230/4339]: training_loss: tensor(0.1222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4231/4339]: training_loss: tensor(0.3499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4232/4339]: training_loss: tensor(0.1523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4233/4339]: training_loss: tensor(0.3442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4234/4339]: training_loss: tensor(0.3491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4235/4339]: training_loss: tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4236/4339]: training_loss: tensor(0.3899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4237/4339]: training_loss: tensor(0.2808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4238/4339]: training_loss: tensor(0.0883, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4239/4339]: training_loss: tensor(0.1356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4240/4339]: training_loss: tensor(0.2475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4241/4339]: training_loss: tensor(0.1472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4242/4339]: training_loss: tensor(0.6931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4243/4339]: training_loss: tensor(0.2475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4244/4339]: training_loss: tensor(0.1738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4245/4339]: training_loss: tensor(0.3334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4246/4339]: training_loss: tensor(0.4030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4247/4339]: training_loss: tensor(0.3395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4248/4339]: training_loss: tensor(0.3177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4249/4339]: training_loss: tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4250/4339]: training_loss: tensor(0.1378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4251/4339]: training_loss: tensor(0.2170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4252/4339]: training_loss: tensor(0.1794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4253/4339]: training_loss: tensor(0.1763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4254/4339]: training_loss: tensor(0.1875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4255/4339]: training_loss: tensor(0.3552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4256/4339]: training_loss: tensor(0.2046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4257/4339]: training_loss: tensor(0.2020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4258/4339]: training_loss: tensor(0.1581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4259/4339]: training_loss: tensor(0.2010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4260/4339]: training_loss: tensor(0.2735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4261/4339]: training_loss: tensor(0.4836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4262/4339]: training_loss: tensor(0.5408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4263/4339]: training_loss: tensor(0.1696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4264/4339]: training_loss: tensor(0.3357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4265/4339]: training_loss: tensor(0.1142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4266/4339]: training_loss: tensor(0.3852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4267/4339]: training_loss: tensor(0.2480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4268/4339]: training_loss: tensor(0.3163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4269/4339]: training_loss: tensor(0.2484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4270/4339]: training_loss: tensor(0.3077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4271/4339]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4272/4339]: training_loss: tensor(0.2308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4273/4339]: training_loss: tensor(0.2948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4274/4339]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4275/4339]: training_loss: tensor(0.3133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4276/4339]: training_loss: tensor(0.1669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4277/4339]: training_loss: tensor(0.1730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4278/4339]: training_loss: tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4279/4339]: training_loss: tensor(0.2428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4280/4339]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4281/4339]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4282/4339]: training_loss: tensor(0.4125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4283/4339]: training_loss: tensor(0.0559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4284/4339]: training_loss: tensor(0.4831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4285/4339]: training_loss: tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4286/4339]: training_loss: tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4287/4339]: training_loss: tensor(0.1308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4288/4339]: training_loss: tensor(0.0446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4289/4339]: training_loss: tensor(0.2633, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4290/4339]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4291/4339]: training_loss: tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4292/4339]: training_loss: tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4293/4339]: training_loss: tensor(0.2846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4294/4339]: training_loss: tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4295/4339]: training_loss: tensor(0.1528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4296/4339]: training_loss: tensor(0.1222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4297/4339]: training_loss: tensor(0.1484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4298/4339]: training_loss: tensor(0.1234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4299/4339]: training_loss: tensor(0.3075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4300/4339]: training_loss: tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4301/4339]: training_loss: tensor(0.1134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4302/4339]: training_loss: tensor(0.3881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4303/4339]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4304/4339]: training_loss: tensor(0.1343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4305/4339]: training_loss: tensor(0.0931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4306/4339]: training_loss: tensor(0.3014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4307/4339]: training_loss: tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4308/4339]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4309/4339]: training_loss: tensor(0.2080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4310/4339]: training_loss: tensor(0.3087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4311/4339]: training_loss: tensor(0.2366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4312/4339]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4313/4339]: training_loss: tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4314/4339]: training_loss: tensor(0.1276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4315/4339]: training_loss: tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4316/4339]: training_loss: tensor(0.1234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4317/4339]: training_loss: tensor(0.3938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4318/4339]: training_loss: tensor(0.0582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4319/4339]: training_loss: tensor(0.1232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4320/4339]: training_loss: tensor(0.1877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4321/4339]: training_loss: tensor(0.4667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4322/4339]: training_loss: tensor(0.2552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4323/4339]: training_loss: tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4324/4339]: training_loss: tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4325/4339]: training_loss: tensor(0.2424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4326/4339]: training_loss: tensor(0.3078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4327/4339]: training_loss: tensor(0.1205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4328/4339]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4329/4339]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4330/4339]: training_loss: tensor(0.1445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4331/4339]: training_loss: tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4332/4339]: training_loss: tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4333/4339]: training_loss: tensor(0.4487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4334/4339]: training_loss: tensor(0.2229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4335/4339]: training_loss: tensor(0.1183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4336/4339]: training_loss: tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4337/4339]: training_loss: tensor(0.4930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4338/4339]: training_loss: tensor(0.2880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4339/4339]: training_loss: tensor(0.2682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4340/4339]: training_loss: tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [4/5], Step [17360/21700], Train Loss: 0.1482, Valid Loss: 0.4316\n",
      "batch_no [1/4339]: training_loss: tensor(0.2158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2/4339]: training_loss: tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3/4339]: training_loss: tensor(0.3049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4/4339]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [5/4339]: training_loss: tensor(0.3361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [6/4339]: training_loss: tensor(0.5053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [7/4339]: training_loss: tensor(0.1458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [8/4339]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [9/4339]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [10/4339]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [11/4339]: training_loss: tensor(0.1424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [12/4339]: training_loss: tensor(0.1132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [13/4339]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [14/4339]: training_loss: tensor(0.3590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [15/4339]: training_loss: tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [16/4339]: training_loss: tensor(0.0493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [17/4339]: training_loss: tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [18/4339]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [19/4339]: training_loss: tensor(0.1906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [20/4339]: training_loss: tensor(0.1505, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [21/4339]: training_loss: tensor(0.3041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [22/4339]: training_loss: tensor(0.2435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [23/4339]: training_loss: tensor(0.2303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [24/4339]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [25/4339]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [26/4339]: training_loss: tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [27/4339]: training_loss: tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [28/4339]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [29/4339]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [30/4339]: training_loss: tensor(0.1627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [31/4339]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [32/4339]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [33/4339]: training_loss: tensor(0.2440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [34/4339]: training_loss: tensor(0.7639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [35/4339]: training_loss: tensor(0.2717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [36/4339]: training_loss: tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [37/4339]: training_loss: tensor(0.1616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [38/4339]: training_loss: tensor(0.2730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [39/4339]: training_loss: tensor(0.1959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [40/4339]: training_loss: tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [41/4339]: training_loss: tensor(0.2073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [42/4339]: training_loss: tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [43/4339]: training_loss: tensor(0.0621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [44/4339]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [45/4339]: training_loss: tensor(0.0878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [46/4339]: training_loss: tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [47/4339]: training_loss: tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [48/4339]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [49/4339]: training_loss: tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [50/4339]: training_loss: tensor(0.2150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [51/4339]: training_loss: tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [52/4339]: training_loss: tensor(0.4056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [53/4339]: training_loss: tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [54/4339]: training_loss: tensor(0.3594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [55/4339]: training_loss: tensor(0.0410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [56/4339]: training_loss: tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [57/4339]: training_loss: tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [58/4339]: training_loss: tensor(0.3339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [59/4339]: training_loss: tensor(0.1748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [60/4339]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [61/4339]: training_loss: tensor(0.0495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [62/4339]: training_loss: tensor(0.1205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [63/4339]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [64/4339]: training_loss: tensor(0.4712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [65/4339]: training_loss: tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [66/4339]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [67/4339]: training_loss: tensor(0.1390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [68/4339]: training_loss: tensor(0.0380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [69/4339]: training_loss: tensor(0.2015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [70/4339]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [71/4339]: training_loss: tensor(0.1254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [72/4339]: training_loss: tensor(0.2443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [73/4339]: training_loss: tensor(0.1186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [74/4339]: training_loss: tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [75/4339]: training_loss: tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [76/4339]: training_loss: tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [77/4339]: training_loss: tensor(0.1787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [78/4339]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [79/4339]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [80/4339]: training_loss: tensor(0.2952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [81/4339]: training_loss: tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [82/4339]: training_loss: tensor(0.2224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [83/4339]: training_loss: tensor(0.5682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [84/4339]: training_loss: tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [85/4339]: training_loss: tensor(0.4121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [86/4339]: training_loss: tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [87/4339]: training_loss: tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [88/4339]: training_loss: tensor(0.2871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [89/4339]: training_loss: tensor(0.1515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [90/4339]: training_loss: tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [91/4339]: training_loss: tensor(0.1572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [92/4339]: training_loss: tensor(0.0650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [93/4339]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [94/4339]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [95/4339]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [96/4339]: training_loss: tensor(0.1372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [97/4339]: training_loss: tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [98/4339]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [99/4339]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [100/4339]: training_loss: tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [101/4339]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [102/4339]: training_loss: tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [103/4339]: training_loss: tensor(0.2764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [104/4339]: training_loss: tensor(0.1631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [105/4339]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [106/4339]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [107/4339]: training_loss: tensor(0.2436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [108/4339]: training_loss: tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [109/4339]: training_loss: tensor(0.5058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [110/4339]: training_loss: tensor(0.1414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [111/4339]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [112/4339]: training_loss: tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [113/4339]: training_loss: tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [114/4339]: training_loss: tensor(0.3023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [115/4339]: training_loss: tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [116/4339]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [117/4339]: training_loss: tensor(0.3258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [118/4339]: training_loss: tensor(0.2461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [119/4339]: training_loss: tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [120/4339]: training_loss: tensor(0.2779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [121/4339]: training_loss: tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [122/4339]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [123/4339]: training_loss: tensor(0.5134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [124/4339]: training_loss: tensor(0.1066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [125/4339]: training_loss: tensor(0.4542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [126/4339]: training_loss: tensor(0.2227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [127/4339]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [128/4339]: training_loss: tensor(0.3269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [129/4339]: training_loss: tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [130/4339]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [131/4339]: training_loss: tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [132/4339]: training_loss: tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [133/4339]: training_loss: tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [134/4339]: training_loss: tensor(0.2271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [135/4339]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [136/4339]: training_loss: tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [137/4339]: training_loss: tensor(0.0493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [138/4339]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [139/4339]: training_loss: tensor(0.1871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [140/4339]: training_loss: tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [141/4339]: training_loss: tensor(0.4331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [142/4339]: training_loss: tensor(0.2850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [143/4339]: training_loss: tensor(0.1130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [144/4339]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [145/4339]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [146/4339]: training_loss: tensor(0.3311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [147/4339]: training_loss: tensor(0.0873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [148/4339]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [149/4339]: training_loss: tensor(0.1444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [150/4339]: training_loss: tensor(0.1572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [151/4339]: training_loss: tensor(0.2070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [152/4339]: training_loss: tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [153/4339]: training_loss: tensor(0.1205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [154/4339]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [155/4339]: training_loss: tensor(0.0554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [156/4339]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [157/4339]: training_loss: tensor(0.1283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [158/4339]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [159/4339]: training_loss: tensor(0.1479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [160/4339]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [161/4339]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [162/4339]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [163/4339]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [164/4339]: training_loss: tensor(0.4095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [165/4339]: training_loss: tensor(0.1422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [166/4339]: training_loss: tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [167/4339]: training_loss: tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [168/4339]: training_loss: tensor(0.0507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [169/4339]: training_loss: tensor(0.4147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [170/4339]: training_loss: tensor(0.2754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [171/4339]: training_loss: tensor(0.1677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [172/4339]: training_loss: tensor(0.1581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [173/4339]: training_loss: tensor(0.1932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [174/4339]: training_loss: tensor(0.1389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [175/4339]: training_loss: tensor(0.2187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [176/4339]: training_loss: tensor(0.1346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [177/4339]: training_loss: tensor(0.2010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [178/4339]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [179/4339]: training_loss: tensor(0.1508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [180/4339]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [181/4339]: training_loss: tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [182/4339]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [183/4339]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [184/4339]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [185/4339]: training_loss: tensor(0.1182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [186/4339]: training_loss: tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [187/4339]: training_loss: tensor(0.1937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [188/4339]: training_loss: tensor(0.1065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [189/4339]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [190/4339]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [191/4339]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [192/4339]: training_loss: tensor(0.1304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [193/4339]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [194/4339]: training_loss: tensor(0.1953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [195/4339]: training_loss: tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [196/4339]: training_loss: tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [197/4339]: training_loss: tensor(0.0550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [198/4339]: training_loss: tensor(0.3381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [199/4339]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [200/4339]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [201/4339]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [202/4339]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [203/4339]: training_loss: tensor(0.1117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [204/4339]: training_loss: tensor(0.0697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [205/4339]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [206/4339]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [207/4339]: training_loss: tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [208/4339]: training_loss: tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [209/4339]: training_loss: tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [210/4339]: training_loss: tensor(0.2964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [211/4339]: training_loss: tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [212/4339]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [213/4339]: training_loss: tensor(0.0442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [214/4339]: training_loss: tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [215/4339]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [216/4339]: training_loss: tensor(0.1121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [217/4339]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [218/4339]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [219/4339]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [220/4339]: training_loss: tensor(0.1603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [221/4339]: training_loss: tensor(0.1793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [222/4339]: training_loss: tensor(0.3683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [223/4339]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [224/4339]: training_loss: tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [225/4339]: training_loss: tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [226/4339]: training_loss: tensor(0.1402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [227/4339]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [228/4339]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [229/4339]: training_loss: tensor(0.1038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [230/4339]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [231/4339]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [232/4339]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [233/4339]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [234/4339]: training_loss: tensor(0.1274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [235/4339]: training_loss: tensor(0.3878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [236/4339]: training_loss: tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [237/4339]: training_loss: tensor(0.4238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [238/4339]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [239/4339]: training_loss: tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [240/4339]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [241/4339]: training_loss: tensor(0.1583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [242/4339]: training_loss: tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [243/4339]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [244/4339]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [245/4339]: training_loss: tensor(0.0354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [246/4339]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [247/4339]: training_loss: tensor(0.2481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [248/4339]: training_loss: tensor(0.3237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [249/4339]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [250/4339]: training_loss: tensor(0.4273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [251/4339]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [252/4339]: training_loss: tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [253/4339]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [254/4339]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [255/4339]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [256/4339]: training_loss: tensor(0.0579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [257/4339]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [258/4339]: training_loss: tensor(0.2482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [259/4339]: training_loss: tensor(0.0342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [260/4339]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [261/4339]: training_loss: tensor(0.1806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [262/4339]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [263/4339]: training_loss: tensor(0.0851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [264/4339]: training_loss: tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [265/4339]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [266/4339]: training_loss: tensor(0.2237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [267/4339]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [268/4339]: training_loss: tensor(0.2096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [269/4339]: training_loss: tensor(0.3103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [270/4339]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [271/4339]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [272/4339]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [273/4339]: training_loss: tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [274/4339]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [275/4339]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [276/4339]: training_loss: tensor(0.2804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [277/4339]: training_loss: tensor(0.1649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [278/4339]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [279/4339]: training_loss: tensor(0.0641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [280/4339]: training_loss: tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [281/4339]: training_loss: tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [282/4339]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [283/4339]: training_loss: tensor(0.2500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [284/4339]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [285/4339]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [286/4339]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [287/4339]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [288/4339]: training_loss: tensor(0.1517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [289/4339]: training_loss: tensor(0.1770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [290/4339]: training_loss: tensor(0.2844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [291/4339]: training_loss: tensor(0.1530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [292/4339]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [293/4339]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [294/4339]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [295/4339]: training_loss: tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [296/4339]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [297/4339]: training_loss: tensor(0.1294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [298/4339]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [299/4339]: training_loss: tensor(0.1898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [300/4339]: training_loss: tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [301/4339]: training_loss: tensor(0.1874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [302/4339]: training_loss: tensor(0.0640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [303/4339]: training_loss: tensor(0.5442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [304/4339]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [305/4339]: training_loss: tensor(0.3079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [306/4339]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [307/4339]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [308/4339]: training_loss: tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [309/4339]: training_loss: tensor(0.1985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [310/4339]: training_loss: tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [311/4339]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [312/4339]: training_loss: tensor(0.2327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [313/4339]: training_loss: tensor(0.2931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [314/4339]: training_loss: tensor(0.1531, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [315/4339]: training_loss: tensor(0.0313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [316/4339]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [317/4339]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [318/4339]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [319/4339]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [320/4339]: training_loss: tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [321/4339]: training_loss: tensor(0.1843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [322/4339]: training_loss: tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [323/4339]: training_loss: tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [324/4339]: training_loss: tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [325/4339]: training_loss: tensor(0.2639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [326/4339]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [327/4339]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [328/4339]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [329/4339]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [330/4339]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [331/4339]: training_loss: tensor(0.3862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [332/4339]: training_loss: tensor(0.0620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [333/4339]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [334/4339]: training_loss: tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [335/4339]: training_loss: tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [336/4339]: training_loss: tensor(0.1422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [337/4339]: training_loss: tensor(0.4679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [338/4339]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [339/4339]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [340/4339]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [341/4339]: training_loss: tensor(0.1680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [342/4339]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [343/4339]: training_loss: tensor(0.1103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [344/4339]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [345/4339]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [346/4339]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [347/4339]: training_loss: tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [348/4339]: training_loss: tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [349/4339]: training_loss: tensor(0.2988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [350/4339]: training_loss: tensor(0.2743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [351/4339]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [352/4339]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [353/4339]: training_loss: tensor(0.2513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [354/4339]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [355/4339]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [356/4339]: training_loss: tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [357/4339]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [358/4339]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [359/4339]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [360/4339]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [361/4339]: training_loss: tensor(0.1886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [362/4339]: training_loss: tensor(0.0951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [363/4339]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [364/4339]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [365/4339]: training_loss: tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [366/4339]: training_loss: tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [367/4339]: training_loss: tensor(0.2753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [368/4339]: training_loss: tensor(0.5066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [369/4339]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [370/4339]: training_loss: tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [371/4339]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [372/4339]: training_loss: tensor(0.1218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [373/4339]: training_loss: tensor(0.3494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [374/4339]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [375/4339]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [376/4339]: training_loss: tensor(0.6643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [377/4339]: training_loss: tensor(0.0581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [378/4339]: training_loss: tensor(0.0835, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [379/4339]: training_loss: tensor(0.2030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [380/4339]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [381/4339]: training_loss: tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [382/4339]: training_loss: tensor(0.4855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [383/4339]: training_loss: tensor(0.1123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [384/4339]: training_loss: tensor(0.3008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [385/4339]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [386/4339]: training_loss: tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [387/4339]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [388/4339]: training_loss: tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [389/4339]: training_loss: tensor(0.1359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [390/4339]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [391/4339]: training_loss: tensor(0.0410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [392/4339]: training_loss: tensor(0.2431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [393/4339]: training_loss: tensor(0.1563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [394/4339]: training_loss: tensor(0.3838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [395/4339]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [396/4339]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [397/4339]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [398/4339]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [399/4339]: training_loss: tensor(0.1895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [400/4339]: training_loss: tensor(0.1563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [401/4339]: training_loss: tensor(0.1250, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [402/4339]: training_loss: tensor(0.2355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [403/4339]: training_loss: tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [404/4339]: training_loss: tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [405/4339]: training_loss: tensor(0.1357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [406/4339]: training_loss: tensor(0.1811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [407/4339]: training_loss: tensor(0.1349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [408/4339]: training_loss: tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [409/4339]: training_loss: tensor(0.6013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [410/4339]: training_loss: tensor(0.0305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [411/4339]: training_loss: tensor(0.6671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [412/4339]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [413/4339]: training_loss: tensor(0.0413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [414/4339]: training_loss: tensor(0.2284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [415/4339]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [416/4339]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [417/4339]: training_loss: tensor(0.1981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [418/4339]: training_loss: tensor(0.3100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [419/4339]: training_loss: tensor(0.3271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [420/4339]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [421/4339]: training_loss: tensor(0.1153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [422/4339]: training_loss: tensor(0.2277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [423/4339]: training_loss: tensor(0.1546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [424/4339]: training_loss: tensor(0.2223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [425/4339]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [426/4339]: training_loss: tensor(0.2702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [427/4339]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [428/4339]: training_loss: tensor(0.1324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [429/4339]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [430/4339]: training_loss: tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [431/4339]: training_loss: tensor(0.1152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [432/4339]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [433/4339]: training_loss: tensor(0.1411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [434/4339]: training_loss: tensor(0.1507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [435/4339]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [436/4339]: training_loss: tensor(0.2941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [437/4339]: training_loss: tensor(0.3417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [438/4339]: training_loss: tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [439/4339]: training_loss: tensor(0.0909, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [440/4339]: training_loss: tensor(0.3132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [441/4339]: training_loss: tensor(0.0976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [442/4339]: training_loss: tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [443/4339]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [444/4339]: training_loss: tensor(0.1933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [445/4339]: training_loss: tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [446/4339]: training_loss: tensor(0.0680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [447/4339]: training_loss: tensor(0.2062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [448/4339]: training_loss: tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [449/4339]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [450/4339]: training_loss: tensor(0.1325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [451/4339]: training_loss: tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [452/4339]: training_loss: tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [453/4339]: training_loss: tensor(0.0548, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [454/4339]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [455/4339]: training_loss: tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [456/4339]: training_loss: tensor(0.4741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [457/4339]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [458/4339]: training_loss: tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [459/4339]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [460/4339]: training_loss: tensor(0.2732, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [461/4339]: training_loss: tensor(0.3531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [462/4339]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [463/4339]: training_loss: tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [464/4339]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [465/4339]: training_loss: tensor(0.0355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [466/4339]: training_loss: tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [467/4339]: training_loss: tensor(0.3653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [468/4339]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [469/4339]: training_loss: tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [470/4339]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [471/4339]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [472/4339]: training_loss: tensor(0.2418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [473/4339]: training_loss: tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [474/4339]: training_loss: tensor(0.1446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [475/4339]: training_loss: tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [476/4339]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [477/4339]: training_loss: tensor(0.1143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [478/4339]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [479/4339]: training_loss: tensor(0.0486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [480/4339]: training_loss: tensor(0.3240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [481/4339]: training_loss: tensor(0.3218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [482/4339]: training_loss: tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [483/4339]: training_loss: tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [484/4339]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [485/4339]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [486/4339]: training_loss: tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [487/4339]: training_loss: tensor(0.0253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [488/4339]: training_loss: tensor(0.1398, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [489/4339]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [490/4339]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [491/4339]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [492/4339]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [493/4339]: training_loss: tensor(0.3498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [494/4339]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [495/4339]: training_loss: tensor(0.1862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [496/4339]: training_loss: tensor(0.2416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [497/4339]: training_loss: tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [498/4339]: training_loss: tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [499/4339]: training_loss: tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [500/4339]: training_loss: tensor(0.6378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [501/4339]: training_loss: tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [502/4339]: training_loss: tensor(0.2129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [503/4339]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [504/4339]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [505/4339]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [506/4339]: training_loss: tensor(0.2096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [507/4339]: training_loss: tensor(0.2132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [508/4339]: training_loss: tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [509/4339]: training_loss: tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [510/4339]: training_loss: tensor(0.1709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [511/4339]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [512/4339]: training_loss: tensor(0.1283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [513/4339]: training_loss: tensor(0.1560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [514/4339]: training_loss: tensor(0.0379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [515/4339]: training_loss: tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [516/4339]: training_loss: tensor(0.1296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [517/4339]: training_loss: tensor(0.1336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [518/4339]: training_loss: tensor(0.2131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [519/4339]: training_loss: tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [520/4339]: training_loss: tensor(0.0616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [521/4339]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [522/4339]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [523/4339]: training_loss: tensor(0.0445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [524/4339]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [525/4339]: training_loss: tensor(0.4688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [526/4339]: training_loss: tensor(0.1672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [527/4339]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [528/4339]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [529/4339]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [530/4339]: training_loss: tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [531/4339]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [532/4339]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [533/4339]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [534/4339]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [535/4339]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [536/4339]: training_loss: tensor(0.2407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [537/4339]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [538/4339]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [539/4339]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [540/4339]: training_loss: tensor(0.0429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [541/4339]: training_loss: tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [542/4339]: training_loss: tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [543/4339]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [544/4339]: training_loss: tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [545/4339]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [546/4339]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [547/4339]: training_loss: tensor(0.1192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [548/4339]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [549/4339]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [550/4339]: training_loss: tensor(0.1651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [551/4339]: training_loss: tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [552/4339]: training_loss: tensor(0.2224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [553/4339]: training_loss: tensor(0.0866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [554/4339]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [555/4339]: training_loss: tensor(0.0496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [556/4339]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [557/4339]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [558/4339]: training_loss: tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [559/4339]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [560/4339]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [561/4339]: training_loss: tensor(0.2236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [562/4339]: training_loss: tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [563/4339]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [564/4339]: training_loss: tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [565/4339]: training_loss: tensor(0.1715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [566/4339]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [567/4339]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [568/4339]: training_loss: tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [569/4339]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [570/4339]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [571/4339]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [572/4339]: training_loss: tensor(0.1347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [573/4339]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [574/4339]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [575/4339]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [576/4339]: training_loss: tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [577/4339]: training_loss: tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [578/4339]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [579/4339]: training_loss: tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [580/4339]: training_loss: tensor(0.1358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [581/4339]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [582/4339]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [583/4339]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [584/4339]: training_loss: tensor(0.4347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [585/4339]: training_loss: tensor(0.2760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [586/4339]: training_loss: tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [587/4339]: training_loss: tensor(0.2550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [588/4339]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [589/4339]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [590/4339]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [591/4339]: training_loss: tensor(0.2751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [592/4339]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [593/4339]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [594/4339]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [595/4339]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [596/4339]: training_loss: tensor(0.0534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [597/4339]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [598/4339]: training_loss: tensor(0.2669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [599/4339]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [600/4339]: training_loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [601/4339]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [602/4339]: training_loss: tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [603/4339]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [604/4339]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [605/4339]: training_loss: tensor(0.1277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [606/4339]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [607/4339]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [608/4339]: training_loss: tensor(0.3396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [609/4339]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [610/4339]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [611/4339]: training_loss: tensor(0.1947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [612/4339]: training_loss: tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [613/4339]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [614/4339]: training_loss: tensor(0.1496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [615/4339]: training_loss: tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [616/4339]: training_loss: tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [617/4339]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [618/4339]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [619/4339]: training_loss: tensor(0.1347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [620/4339]: training_loss: tensor(0.2156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [621/4339]: training_loss: tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [622/4339]: training_loss: tensor(0.2366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [623/4339]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [624/4339]: training_loss: tensor(0.3482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [625/4339]: training_loss: tensor(0.1338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [626/4339]: training_loss: tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [627/4339]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [628/4339]: training_loss: tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [629/4339]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [630/4339]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [631/4339]: training_loss: tensor(0.0623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [632/4339]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [633/4339]: training_loss: tensor(0.2700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [634/4339]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [635/4339]: training_loss: tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [636/4339]: training_loss: tensor(0.1243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [637/4339]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [638/4339]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [639/4339]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [640/4339]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [641/4339]: training_loss: tensor(0.1844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [642/4339]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [643/4339]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [644/4339]: training_loss: tensor(0.2842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [645/4339]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [646/4339]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [647/4339]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [648/4339]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [649/4339]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [650/4339]: training_loss: tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [651/4339]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [652/4339]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [653/4339]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [654/4339]: training_loss: tensor(0.1152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [655/4339]: training_loss: tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [656/4339]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [657/4339]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [658/4339]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [659/4339]: training_loss: tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [660/4339]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [661/4339]: training_loss: tensor(0.2958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [662/4339]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [663/4339]: training_loss: tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [664/4339]: training_loss: tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [665/4339]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [666/4339]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [667/4339]: training_loss: tensor(0.3555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [668/4339]: training_loss: tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [669/4339]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [670/4339]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [671/4339]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [672/4339]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [673/4339]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [674/4339]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [675/4339]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [676/4339]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [677/4339]: training_loss: tensor(0.1360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [678/4339]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [679/4339]: training_loss: tensor(0.0404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [680/4339]: training_loss: tensor(0.1794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [681/4339]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [682/4339]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [683/4339]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [684/4339]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [685/4339]: training_loss: tensor(0.2122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [686/4339]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [687/4339]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [688/4339]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [689/4339]: training_loss: tensor(0.1305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [690/4339]: training_loss: tensor(0.2006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [691/4339]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [692/4339]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [693/4339]: training_loss: tensor(0.4887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [694/4339]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [695/4339]: training_loss: tensor(0.2200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [696/4339]: training_loss: tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [697/4339]: training_loss: tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [698/4339]: training_loss: tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [699/4339]: training_loss: tensor(0.1751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [700/4339]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [701/4339]: training_loss: tensor(0.0313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [702/4339]: training_loss: tensor(0.1555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [703/4339]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [704/4339]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [705/4339]: training_loss: tensor(0.2690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [706/4339]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [707/4339]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [708/4339]: training_loss: tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [709/4339]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [710/4339]: training_loss: tensor(0.1978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [711/4339]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [712/4339]: training_loss: tensor(0.0589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [713/4339]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [714/4339]: training_loss: tensor(0.1306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [715/4339]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [716/4339]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [717/4339]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [718/4339]: training_loss: tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [719/4339]: training_loss: tensor(0.0915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [720/4339]: training_loss: tensor(0.4236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [721/4339]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [722/4339]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [723/4339]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [724/4339]: training_loss: tensor(0.1128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [725/4339]: training_loss: tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [726/4339]: training_loss: tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [727/4339]: training_loss: tensor(0.1789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [728/4339]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [729/4339]: training_loss: tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [730/4339]: training_loss: tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [731/4339]: training_loss: tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [732/4339]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [733/4339]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [734/4339]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [735/4339]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [736/4339]: training_loss: tensor(0.2098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [737/4339]: training_loss: tensor(0.2126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [738/4339]: training_loss: tensor(0.1276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [739/4339]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [740/4339]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [741/4339]: training_loss: tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [742/4339]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [743/4339]: training_loss: tensor(0.2250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [744/4339]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [745/4339]: training_loss: tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [746/4339]: training_loss: tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [747/4339]: training_loss: tensor(0.0980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [748/4339]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [749/4339]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [750/4339]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [751/4339]: training_loss: tensor(0.0476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [752/4339]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [753/4339]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [754/4339]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [755/4339]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [756/4339]: training_loss: tensor(0.1782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [757/4339]: training_loss: tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [758/4339]: training_loss: tensor(0.1024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [759/4339]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [760/4339]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [761/4339]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [762/4339]: training_loss: tensor(0.1563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [763/4339]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [764/4339]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [765/4339]: training_loss: tensor(0.3398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [766/4339]: training_loss: tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [767/4339]: training_loss: tensor(0.3227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [768/4339]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [769/4339]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [770/4339]: training_loss: tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [771/4339]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [772/4339]: training_loss: tensor(0.2715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [773/4339]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [774/4339]: training_loss: tensor(0.2881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [775/4339]: training_loss: tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [776/4339]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [777/4339]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [778/4339]: training_loss: tensor(0.1768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [779/4339]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [780/4339]: training_loss: tensor(0.1758, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [781/4339]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [782/4339]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [783/4339]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [784/4339]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [785/4339]: training_loss: tensor(0.3222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [786/4339]: training_loss: tensor(0.0383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [787/4339]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [788/4339]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [789/4339]: training_loss: tensor(0.5027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [790/4339]: training_loss: tensor(0.2902, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [791/4339]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [792/4339]: training_loss: tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [793/4339]: training_loss: tensor(0.3694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [794/4339]: training_loss: tensor(0.3386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [795/4339]: training_loss: tensor(0.4281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [796/4339]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [797/4339]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [798/4339]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [799/4339]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [800/4339]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [801/4339]: training_loss: tensor(0.2676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [802/4339]: training_loss: tensor(0.2125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [803/4339]: training_loss: tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [804/4339]: training_loss: tensor(0.2562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [805/4339]: training_loss: tensor(0.1502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [806/4339]: training_loss: tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [807/4339]: training_loss: tensor(0.2223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [808/4339]: training_loss: tensor(0.1607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [809/4339]: training_loss: tensor(0.3954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [810/4339]: training_loss: tensor(0.1924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [811/4339]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [812/4339]: training_loss: tensor(0.3173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [813/4339]: training_loss: tensor(0.2062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [814/4339]: training_loss: tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [815/4339]: training_loss: tensor(0.2604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [816/4339]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [817/4339]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [818/4339]: training_loss: tensor(0.0718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [819/4339]: training_loss: tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [820/4339]: training_loss: tensor(0.1115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [821/4339]: training_loss: tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [822/4339]: training_loss: tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [823/4339]: training_loss: tensor(0.1028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [824/4339]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [825/4339]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [826/4339]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [827/4339]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [828/4339]: training_loss: tensor(0.1331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [829/4339]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [830/4339]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [831/4339]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [832/4339]: training_loss: tensor(0.0681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [833/4339]: training_loss: tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [834/4339]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [835/4339]: training_loss: tensor(0.3103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [836/4339]: training_loss: tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [837/4339]: training_loss: tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [838/4339]: training_loss: tensor(0.1314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [839/4339]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [840/4339]: training_loss: tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [841/4339]: training_loss: tensor(0.4629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [842/4339]: training_loss: tensor(0.1353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [843/4339]: training_loss: tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [844/4339]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [845/4339]: training_loss: tensor(0.3475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [846/4339]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [847/4339]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [848/4339]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [849/4339]: training_loss: tensor(0.1589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [850/4339]: training_loss: tensor(0.1530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [851/4339]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [852/4339]: training_loss: tensor(0.2329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [853/4339]: training_loss: tensor(0.0290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [854/4339]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [855/4339]: training_loss: tensor(0.1290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [856/4339]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [857/4339]: training_loss: tensor(0.3602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [858/4339]: training_loss: tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [859/4339]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [860/4339]: training_loss: tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [861/4339]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [862/4339]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [863/4339]: training_loss: tensor(0.1268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [864/4339]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [865/4339]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [866/4339]: training_loss: tensor(0.0557, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [867/4339]: training_loss: tensor(0.3310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [868/4339]: training_loss: tensor(0.0459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [869/4339]: training_loss: tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [870/4339]: training_loss: tensor(0.5275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [871/4339]: training_loss: tensor(0.2188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [872/4339]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [873/4339]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [874/4339]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [875/4339]: training_loss: tensor(0.0407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [876/4339]: training_loss: tensor(0.0494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [877/4339]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [878/4339]: training_loss: tensor(0.1140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [879/4339]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [880/4339]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [881/4339]: training_loss: tensor(0.1844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [882/4339]: training_loss: tensor(0.3214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [883/4339]: training_loss: tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [884/4339]: training_loss: tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [885/4339]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [886/4339]: training_loss: tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [887/4339]: training_loss: tensor(0.2699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [888/4339]: training_loss: tensor(0.1073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [889/4339]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [890/4339]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [891/4339]: training_loss: tensor(0.4779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [892/4339]: training_loss: tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [893/4339]: training_loss: tensor(0.1584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [894/4339]: training_loss: tensor(0.0487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [895/4339]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [896/4339]: training_loss: tensor(0.1198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [897/4339]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [898/4339]: training_loss: tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [899/4339]: training_loss: tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [900/4339]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [901/4339]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [902/4339]: training_loss: tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [903/4339]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [904/4339]: training_loss: tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [905/4339]: training_loss: tensor(0.0666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [906/4339]: training_loss: tensor(0.0453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [907/4339]: training_loss: tensor(0.1403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [908/4339]: training_loss: tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [909/4339]: training_loss: tensor(0.4056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [910/4339]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [911/4339]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [912/4339]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [913/4339]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [914/4339]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [915/4339]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [916/4339]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [917/4339]: training_loss: tensor(0.3362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [918/4339]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [919/4339]: training_loss: tensor(0.2511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [920/4339]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [921/4339]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [922/4339]: training_loss: tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [923/4339]: training_loss: tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [924/4339]: training_loss: tensor(0.1202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [925/4339]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [926/4339]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [927/4339]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [928/4339]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [929/4339]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [930/4339]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [931/4339]: training_loss: tensor(0.1813, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [932/4339]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [933/4339]: training_loss: tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [934/4339]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [935/4339]: training_loss: tensor(0.2696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [936/4339]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [937/4339]: training_loss: tensor(0.3678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [938/4339]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [939/4339]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [940/4339]: training_loss: tensor(0.0653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [941/4339]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [942/4339]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [943/4339]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [944/4339]: training_loss: tensor(0.3088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [945/4339]: training_loss: tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [946/4339]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [947/4339]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [948/4339]: training_loss: tensor(0.1271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [949/4339]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [950/4339]: training_loss: tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [951/4339]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [952/4339]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [953/4339]: training_loss: tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [954/4339]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [955/4339]: training_loss: tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [956/4339]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [957/4339]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [958/4339]: training_loss: tensor(0.2289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [959/4339]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [960/4339]: training_loss: tensor(0.5126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [961/4339]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [962/4339]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [963/4339]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [964/4339]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [965/4339]: training_loss: tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [966/4339]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [967/4339]: training_loss: tensor(0.0382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [968/4339]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [969/4339]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [970/4339]: training_loss: tensor(0.2689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [971/4339]: training_loss: tensor(0.2941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [972/4339]: training_loss: tensor(0.0586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [973/4339]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [974/4339]: training_loss: tensor(0.6972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [975/4339]: training_loss: tensor(0.0571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [976/4339]: training_loss: tensor(0.2350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [977/4339]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [978/4339]: training_loss: tensor(0.2150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [979/4339]: training_loss: tensor(0.4886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [980/4339]: training_loss: tensor(0.0579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [981/4339]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [982/4339]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [983/4339]: training_loss: tensor(0.3008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [984/4339]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [985/4339]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [986/4339]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [987/4339]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [988/4339]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [989/4339]: training_loss: tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [990/4339]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [991/4339]: training_loss: tensor(0.1413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [992/4339]: training_loss: tensor(0.1379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [993/4339]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [994/4339]: training_loss: tensor(0.1677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [995/4339]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [996/4339]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [997/4339]: training_loss: tensor(0.2844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [998/4339]: training_loss: tensor(0.1267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [999/4339]: training_loss: tensor(0.2718, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1000/4339]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1001/4339]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1002/4339]: training_loss: tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1003/4339]: training_loss: tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1004/4339]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1005/4339]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1006/4339]: training_loss: tensor(0.3896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1007/4339]: training_loss: tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1008/4339]: training_loss: tensor(0.3995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1009/4339]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1010/4339]: training_loss: tensor(0.3125, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1011/4339]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1012/4339]: training_loss: tensor(0.1401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1013/4339]: training_loss: tensor(0.0332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1014/4339]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1015/4339]: training_loss: tensor(0.1258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1016/4339]: training_loss: tensor(0.1613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1017/4339]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1018/4339]: training_loss: tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1019/4339]: training_loss: tensor(0.1172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1020/4339]: training_loss: tensor(0.1371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1021/4339]: training_loss: tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1022/4339]: training_loss: tensor(0.1444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1023/4339]: training_loss: tensor(0.3265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1024/4339]: training_loss: tensor(0.1968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1025/4339]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1026/4339]: training_loss: tensor(0.1607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1027/4339]: training_loss: tensor(0.3440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1028/4339]: training_loss: tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1029/4339]: training_loss: tensor(0.0536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1030/4339]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1031/4339]: training_loss: tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1032/4339]: training_loss: tensor(0.3470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1033/4339]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1034/4339]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1035/4339]: training_loss: tensor(0.1527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1036/4339]: training_loss: tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1037/4339]: training_loss: tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1038/4339]: training_loss: tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1039/4339]: training_loss: tensor(0.2717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1040/4339]: training_loss: tensor(0.1320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1041/4339]: training_loss: tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1042/4339]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1043/4339]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1044/4339]: training_loss: tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1045/4339]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1046/4339]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1047/4339]: training_loss: tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1048/4339]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1049/4339]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1050/4339]: training_loss: tensor(0.2024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1051/4339]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1052/4339]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1053/4339]: training_loss: tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1054/4339]: training_loss: tensor(0.2179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1055/4339]: training_loss: tensor(0.1297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1056/4339]: training_loss: tensor(0.1300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1057/4339]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1058/4339]: training_loss: tensor(0.1532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1059/4339]: training_loss: tensor(0.2303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1060/4339]: training_loss: tensor(0.1620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1061/4339]: training_loss: tensor(0.1366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1062/4339]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1063/4339]: training_loss: tensor(0.1429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1064/4339]: training_loss: tensor(0.4040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1065/4339]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1066/4339]: training_loss: tensor(0.1123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1067/4339]: training_loss: tensor(0.2047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1068/4339]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1069/4339]: training_loss: tensor(0.2361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1070/4339]: training_loss: tensor(0.2296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1071/4339]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1072/4339]: training_loss: tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1073/4339]: training_loss: tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1074/4339]: training_loss: tensor(0.1628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1075/4339]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1076/4339]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1077/4339]: training_loss: tensor(0.2092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1078/4339]: training_loss: tensor(0.1503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1079/4339]: training_loss: tensor(0.2618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1080/4339]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1081/4339]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1082/4339]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1083/4339]: training_loss: tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1084/4339]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1085/4339]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1086/4339]: training_loss: tensor(0.2170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1087/4339]: training_loss: tensor(0.1346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1088/4339]: training_loss: tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1089/4339]: training_loss: tensor(0.2194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1090/4339]: training_loss: tensor(0.1186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1091/4339]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1092/4339]: training_loss: tensor(0.1722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1093/4339]: training_loss: tensor(0.2328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1094/4339]: training_loss: tensor(0.1479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1095/4339]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1096/4339]: training_loss: tensor(0.0389, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1097/4339]: training_loss: tensor(0.2549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1098/4339]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1099/4339]: training_loss: tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1100/4339]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1101/4339]: training_loss: tensor(0.2772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1102/4339]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1103/4339]: training_loss: tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1104/4339]: training_loss: tensor(0.1963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1105/4339]: training_loss: tensor(0.0312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1106/4339]: training_loss: tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1107/4339]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1108/4339]: training_loss: tensor(0.1618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1109/4339]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1110/4339]: training_loss: tensor(0.1318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1111/4339]: training_loss: tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1112/4339]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1113/4339]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1114/4339]: training_loss: tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1115/4339]: training_loss: tensor(0.2556, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1116/4339]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1117/4339]: training_loss: tensor(0.1261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1118/4339]: training_loss: tensor(0.2886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1119/4339]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1120/4339]: training_loss: tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1121/4339]: training_loss: tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1122/4339]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1123/4339]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1124/4339]: training_loss: tensor(0.2038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1125/4339]: training_loss: tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1126/4339]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1127/4339]: training_loss: tensor(0.4407, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1128/4339]: training_loss: tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1129/4339]: training_loss: tensor(0.0362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1130/4339]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1131/4339]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1132/4339]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1133/4339]: training_loss: tensor(0.1321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1134/4339]: training_loss: tensor(0.1435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1135/4339]: training_loss: tensor(0.1873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1136/4339]: training_loss: tensor(0.3388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1137/4339]: training_loss: tensor(0.1328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1138/4339]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1139/4339]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1140/4339]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1141/4339]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1142/4339]: training_loss: tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1143/4339]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1144/4339]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1145/4339]: training_loss: tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1146/4339]: training_loss: tensor(0.5252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1147/4339]: training_loss: tensor(0.1465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1148/4339]: training_loss: tensor(0.5381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1149/4339]: training_loss: tensor(0.4965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1150/4339]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1151/4339]: training_loss: tensor(0.1438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1152/4339]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1153/4339]: training_loss: tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1154/4339]: training_loss: tensor(0.1172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1155/4339]: training_loss: tensor(0.0606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1156/4339]: training_loss: tensor(0.0731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1157/4339]: training_loss: tensor(0.2028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1158/4339]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1159/4339]: training_loss: tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1160/4339]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1161/4339]: training_loss: tensor(0.3009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1162/4339]: training_loss: tensor(0.3382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1163/4339]: training_loss: tensor(0.0794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1164/4339]: training_loss: tensor(0.1116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1165/4339]: training_loss: tensor(0.3962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1166/4339]: training_loss: tensor(0.3451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1167/4339]: training_loss: tensor(0.1290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1168/4339]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1169/4339]: training_loss: tensor(0.2934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1170/4339]: training_loss: tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1171/4339]: training_loss: tensor(0.4138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1172/4339]: training_loss: tensor(0.0765, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1173/4339]: training_loss: tensor(0.1696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1174/4339]: training_loss: tensor(0.1903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1175/4339]: training_loss: tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1176/4339]: training_loss: tensor(0.2388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1177/4339]: training_loss: tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1178/4339]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1179/4339]: training_loss: tensor(0.1913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1180/4339]: training_loss: tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1181/4339]: training_loss: tensor(0.0767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1182/4339]: training_loss: tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1183/4339]: training_loss: tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1184/4339]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1185/4339]: training_loss: tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1186/4339]: training_loss: tensor(0.0637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1187/4339]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1188/4339]: training_loss: tensor(0.2654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1189/4339]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1190/4339]: training_loss: tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1191/4339]: training_loss: tensor(0.1929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1192/4339]: training_loss: tensor(0.2478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1193/4339]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1194/4339]: training_loss: tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1195/4339]: training_loss: tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1196/4339]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1197/4339]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1198/4339]: training_loss: tensor(0.3759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1199/4339]: training_loss: tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1200/4339]: training_loss: tensor(0.0447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1201/4339]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1202/4339]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1203/4339]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1204/4339]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1205/4339]: training_loss: tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1206/4339]: training_loss: tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1207/4339]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1208/4339]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1209/4339]: training_loss: tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1210/4339]: training_loss: tensor(0.0700, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1211/4339]: training_loss: tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1212/4339]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1213/4339]: training_loss: tensor(0.1216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1214/4339]: training_loss: tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1215/4339]: training_loss: tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1216/4339]: training_loss: tensor(0.1182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1217/4339]: training_loss: tensor(0.1486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1218/4339]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1219/4339]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1220/4339]: training_loss: tensor(0.2460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1221/4339]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1222/4339]: training_loss: tensor(0.2589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1223/4339]: training_loss: tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1224/4339]: training_loss: tensor(0.5500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1225/4339]: training_loss: tensor(0.1368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1226/4339]: training_loss: tensor(0.0342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1227/4339]: training_loss: tensor(0.0768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1228/4339]: training_loss: tensor(0.0954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1229/4339]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1230/4339]: training_loss: tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1231/4339]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1232/4339]: training_loss: tensor(0.1605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1233/4339]: training_loss: tensor(0.2214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1234/4339]: training_loss: tensor(0.0529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1235/4339]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1236/4339]: training_loss: tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1237/4339]: training_loss: tensor(0.1937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1238/4339]: training_loss: tensor(0.2072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1239/4339]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1240/4339]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1241/4339]: training_loss: tensor(0.4255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1242/4339]: training_loss: tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1243/4339]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1244/4339]: training_loss: tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1245/4339]: training_loss: tensor(0.0874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1246/4339]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1247/4339]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1248/4339]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1249/4339]: training_loss: tensor(0.0651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1250/4339]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1251/4339]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1252/4339]: training_loss: tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1253/4339]: training_loss: tensor(0.3126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1254/4339]: training_loss: tensor(0.2705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1255/4339]: training_loss: tensor(0.0280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1256/4339]: training_loss: tensor(0.1932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1257/4339]: training_loss: tensor(0.0560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1258/4339]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1259/4339]: training_loss: tensor(0.1181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1260/4339]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1261/4339]: training_loss: tensor(0.1708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1262/4339]: training_loss: tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1263/4339]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1264/4339]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1265/4339]: training_loss: tensor(0.1845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1266/4339]: training_loss: tensor(0.1188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1267/4339]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1268/4339]: training_loss: tensor(0.1502, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1269/4339]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1270/4339]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1271/4339]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1272/4339]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1273/4339]: training_loss: tensor(0.1416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1274/4339]: training_loss: tensor(0.1275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1275/4339]: training_loss: tensor(0.1196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1276/4339]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1277/4339]: training_loss: tensor(0.0439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1278/4339]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1279/4339]: training_loss: tensor(0.0495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1280/4339]: training_loss: tensor(0.0715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1281/4339]: training_loss: tensor(0.0504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1282/4339]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1283/4339]: training_loss: tensor(0.0621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1284/4339]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1285/4339]: training_loss: tensor(0.0449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1286/4339]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1287/4339]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1288/4339]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1289/4339]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1290/4339]: training_loss: tensor(0.2131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1291/4339]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1292/4339]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1293/4339]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1294/4339]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1295/4339]: training_loss: tensor(0.1375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1296/4339]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1297/4339]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1298/4339]: training_loss: tensor(0.3341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1299/4339]: training_loss: tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1300/4339]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1301/4339]: training_loss: tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1302/4339]: training_loss: tensor(0.1064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1303/4339]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1304/4339]: training_loss: tensor(0.3090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1305/4339]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1306/4339]: training_loss: tensor(0.3628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1307/4339]: training_loss: tensor(0.1279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1308/4339]: training_loss: tensor(0.5230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1309/4339]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1310/4339]: training_loss: tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1311/4339]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1312/4339]: training_loss: tensor(0.1651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1313/4339]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1314/4339]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1315/4339]: training_loss: tensor(0.2787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1316/4339]: training_loss: tensor(0.2088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1317/4339]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1318/4339]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1319/4339]: training_loss: tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1320/4339]: training_loss: tensor(0.1278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1321/4339]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1322/4339]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1323/4339]: training_loss: tensor(0.1144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1324/4339]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1325/4339]: training_loss: tensor(0.0528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1326/4339]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1327/4339]: training_loss: tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1328/4339]: training_loss: tensor(0.0686, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1329/4339]: training_loss: tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1330/4339]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1331/4339]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1332/4339]: training_loss: tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1333/4339]: training_loss: tensor(0.0903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1334/4339]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1335/4339]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1336/4339]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1337/4339]: training_loss: tensor(0.2612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1338/4339]: training_loss: tensor(0.1085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1339/4339]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1340/4339]: training_loss: tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1341/4339]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1342/4339]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1343/4339]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1344/4339]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1345/4339]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1346/4339]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1347/4339]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1348/4339]: training_loss: tensor(0.2997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1349/4339]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1350/4339]: training_loss: tensor(0.1619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1351/4339]: training_loss: tensor(0.1873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1352/4339]: training_loss: tensor(0.0360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1353/4339]: training_loss: tensor(0.3137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1354/4339]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1355/4339]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1356/4339]: training_loss: tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1357/4339]: training_loss: tensor(0.2120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1358/4339]: training_loss: tensor(0.1095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1359/4339]: training_loss: tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1360/4339]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1361/4339]: training_loss: tensor(0.0576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1362/4339]: training_loss: tensor(0.0313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1363/4339]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1364/4339]: training_loss: tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1365/4339]: training_loss: tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1366/4339]: training_loss: tensor(0.2324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1367/4339]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1368/4339]: training_loss: tensor(0.0666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1369/4339]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1370/4339]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1371/4339]: training_loss: tensor(0.0366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1372/4339]: training_loss: tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1373/4339]: training_loss: tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1374/4339]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1375/4339]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1376/4339]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1377/4339]: training_loss: tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1378/4339]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1379/4339]: training_loss: tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1380/4339]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1381/4339]: training_loss: tensor(0.1396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1382/4339]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1383/4339]: training_loss: tensor(0.5116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1384/4339]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1385/4339]: training_loss: tensor(0.1645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1386/4339]: training_loss: tensor(0.2145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1387/4339]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1388/4339]: training_loss: tensor(0.0816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1389/4339]: training_loss: tensor(0.0476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1390/4339]: training_loss: tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1391/4339]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1392/4339]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1393/4339]: training_loss: tensor(0.1429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1394/4339]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1395/4339]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1396/4339]: training_loss: tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1397/4339]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1398/4339]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1399/4339]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1400/4339]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1401/4339]: training_loss: tensor(0.0566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1402/4339]: training_loss: tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1403/4339]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1404/4339]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1405/4339]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1406/4339]: training_loss: tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1407/4339]: training_loss: tensor(0.3863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1408/4339]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1409/4339]: training_loss: tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1410/4339]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1411/4339]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1412/4339]: training_loss: tensor(0.0383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1413/4339]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1414/4339]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1415/4339]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1416/4339]: training_loss: tensor(0.3214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1417/4339]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1418/4339]: training_loss: tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1419/4339]: training_loss: tensor(0.4234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1420/4339]: training_loss: tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1421/4339]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1422/4339]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1423/4339]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1424/4339]: training_loss: tensor(0.5259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1425/4339]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1426/4339]: training_loss: tensor(0.1143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1427/4339]: training_loss: tensor(0.2776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1428/4339]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1429/4339]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1430/4339]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1431/4339]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1432/4339]: training_loss: tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1433/4339]: training_loss: tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1434/4339]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1435/4339]: training_loss: tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1436/4339]: training_loss: tensor(0.1795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1437/4339]: training_loss: tensor(0.0429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1438/4339]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1439/4339]: training_loss: tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1440/4339]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1441/4339]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1442/4339]: training_loss: tensor(0.1496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1443/4339]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1444/4339]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1445/4339]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1446/4339]: training_loss: tensor(0.1066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1447/4339]: training_loss: tensor(0.1850, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1448/4339]: training_loss: tensor(0.2862, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1449/4339]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1450/4339]: training_loss: tensor(0.0579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1451/4339]: training_loss: tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1452/4339]: training_loss: tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1453/4339]: training_loss: tensor(0.1681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1454/4339]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1455/4339]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1456/4339]: training_loss: tensor(0.2286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1457/4339]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1458/4339]: training_loss: tensor(0.1894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1459/4339]: training_loss: tensor(0.3150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1460/4339]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1461/4339]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1462/4339]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1463/4339]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1464/4339]: training_loss: tensor(0.1653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1465/4339]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1466/4339]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1467/4339]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1468/4339]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1469/4339]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1470/4339]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1471/4339]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1472/4339]: training_loss: tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1473/4339]: training_loss: tensor(0.2884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1474/4339]: training_loss: tensor(0.1574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1475/4339]: training_loss: tensor(0.1422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1476/4339]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1477/4339]: training_loss: tensor(0.0658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1478/4339]: training_loss: tensor(0.3123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1479/4339]: training_loss: tensor(0.3269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1480/4339]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1481/4339]: training_loss: tensor(0.1754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1482/4339]: training_loss: tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1483/4339]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1484/4339]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1485/4339]: training_loss: tensor(0.1210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1486/4339]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1487/4339]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1488/4339]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1489/4339]: training_loss: tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1490/4339]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1491/4339]: training_loss: tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1492/4339]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1493/4339]: training_loss: tensor(0.0712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1494/4339]: training_loss: tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1495/4339]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1496/4339]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1497/4339]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1498/4339]: training_loss: tensor(0.0805, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1499/4339]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1500/4339]: training_loss: tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1501/4339]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1502/4339]: training_loss: tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1503/4339]: training_loss: tensor(0.2329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1504/4339]: training_loss: tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1505/4339]: training_loss: tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1506/4339]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1507/4339]: training_loss: tensor(0.0354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1508/4339]: training_loss: tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1509/4339]: training_loss: tensor(0.4515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1510/4339]: training_loss: tensor(0.0778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1511/4339]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1512/4339]: training_loss: tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1513/4339]: training_loss: tensor(0.1182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1514/4339]: training_loss: tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1515/4339]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1516/4339]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1517/4339]: training_loss: tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1518/4339]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1519/4339]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1520/4339]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1521/4339]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1522/4339]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1523/4339]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1524/4339]: training_loss: tensor(0.2293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1525/4339]: training_loss: tensor(0.3552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1526/4339]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1527/4339]: training_loss: tensor(0.1908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1528/4339]: training_loss: tensor(0.1151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1529/4339]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1530/4339]: training_loss: tensor(0.2817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1531/4339]: training_loss: tensor(0.0486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1532/4339]: training_loss: tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1533/4339]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1534/4339]: training_loss: tensor(0.0652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1535/4339]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1536/4339]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1537/4339]: training_loss: tensor(0.1425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1538/4339]: training_loss: tensor(0.3069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1539/4339]: training_loss: tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1540/4339]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1541/4339]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1542/4339]: training_loss: tensor(0.4584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1543/4339]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1544/4339]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1545/4339]: training_loss: tensor(0.2517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1546/4339]: training_loss: tensor(0.1683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1547/4339]: training_loss: tensor(0.5259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1548/4339]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1549/4339]: training_loss: tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1550/4339]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1551/4339]: training_loss: tensor(0.1128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1552/4339]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1553/4339]: training_loss: tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1554/4339]: training_loss: tensor(0.2479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1555/4339]: training_loss: tensor(0.2512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1556/4339]: training_loss: tensor(0.2742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1557/4339]: training_loss: tensor(0.2194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1558/4339]: training_loss: tensor(0.3149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1559/4339]: training_loss: tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1560/4339]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1561/4339]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1562/4339]: training_loss: tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1563/4339]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1564/4339]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1565/4339]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1566/4339]: training_loss: tensor(0.1411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1567/4339]: training_loss: tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1568/4339]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1569/4339]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1570/4339]: training_loss: tensor(0.0666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1571/4339]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1572/4339]: training_loss: tensor(0.0699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1573/4339]: training_loss: tensor(0.3728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1574/4339]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1575/4339]: training_loss: tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1576/4339]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1577/4339]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1578/4339]: training_loss: tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1579/4339]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1580/4339]: training_loss: tensor(0.1205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1581/4339]: training_loss: tensor(0.1339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1582/4339]: training_loss: tensor(0.4177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1583/4339]: training_loss: tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1584/4339]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1585/4339]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1586/4339]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1587/4339]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1588/4339]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1589/4339]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1590/4339]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1591/4339]: training_loss: tensor(0.2789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1592/4339]: training_loss: tensor(0.1848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1593/4339]: training_loss: tensor(0.1967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1594/4339]: training_loss: tensor(0.1191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1595/4339]: training_loss: tensor(0.1800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1596/4339]: training_loss: tensor(0.0995, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1597/4339]: training_loss: tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1598/4339]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1599/4339]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1600/4339]: training_loss: tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1601/4339]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1602/4339]: training_loss: tensor(0.0750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1603/4339]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1604/4339]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1605/4339]: training_loss: tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1606/4339]: training_loss: tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1607/4339]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1608/4339]: training_loss: tensor(0.3521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1609/4339]: training_loss: tensor(0.3759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1610/4339]: training_loss: tensor(0.0925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1611/4339]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1612/4339]: training_loss: tensor(0.2523, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1613/4339]: training_loss: tensor(0.2087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1614/4339]: training_loss: tensor(0.2046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1615/4339]: training_loss: tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1616/4339]: training_loss: tensor(0.4377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1617/4339]: training_loss: tensor(0.1324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1618/4339]: training_loss: tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1619/4339]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1620/4339]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1621/4339]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1622/4339]: training_loss: tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1623/4339]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1624/4339]: training_loss: tensor(0.0591, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1625/4339]: training_loss: tensor(0.3487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1626/4339]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1627/4339]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1628/4339]: training_loss: tensor(0.2762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1629/4339]: training_loss: tensor(0.0933, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1630/4339]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1631/4339]: training_loss: tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1632/4339]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1633/4339]: training_loss: tensor(0.1596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1634/4339]: training_loss: tensor(0.2473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1635/4339]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1636/4339]: training_loss: tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1637/4339]: training_loss: tensor(0.0305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1638/4339]: training_loss: tensor(0.3076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1639/4339]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1640/4339]: training_loss: tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1641/4339]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1642/4339]: training_loss: tensor(0.2066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1643/4339]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1644/4339]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1645/4339]: training_loss: tensor(0.2848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1646/4339]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1647/4339]: training_loss: tensor(0.1196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1648/4339]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1649/4339]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1650/4339]: training_loss: tensor(0.0517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1651/4339]: training_loss: tensor(0.1316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1652/4339]: training_loss: tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1653/4339]: training_loss: tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1654/4339]: training_loss: tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1655/4339]: training_loss: tensor(0.1405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1656/4339]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1657/4339]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1658/4339]: training_loss: tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1659/4339]: training_loss: tensor(0.1652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1660/4339]: training_loss: tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1661/4339]: training_loss: tensor(0.2066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1662/4339]: training_loss: tensor(0.0539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1663/4339]: training_loss: tensor(0.1975, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1664/4339]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1665/4339]: training_loss: tensor(0.1725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1666/4339]: training_loss: tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1667/4339]: training_loss: tensor(0.1162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1668/4339]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1669/4339]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1670/4339]: training_loss: tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1671/4339]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1672/4339]: training_loss: tensor(0.1103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1673/4339]: training_loss: tensor(0.3155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1674/4339]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1675/4339]: training_loss: tensor(0.0847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1676/4339]: training_loss: tensor(0.1483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1677/4339]: training_loss: tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1678/4339]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1679/4339]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1680/4339]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1681/4339]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1682/4339]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1683/4339]: training_loss: tensor(0.0389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1684/4339]: training_loss: tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1685/4339]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1686/4339]: training_loss: tensor(0.2181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1687/4339]: training_loss: tensor(0.1513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1688/4339]: training_loss: tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1689/4339]: training_loss: tensor(0.0660, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1690/4339]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1691/4339]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1692/4339]: training_loss: tensor(0.4846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1693/4339]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1694/4339]: training_loss: tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1695/4339]: training_loss: tensor(0.1068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1696/4339]: training_loss: tensor(0.0410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1697/4339]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1698/4339]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1699/4339]: training_loss: tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1700/4339]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1701/4339]: training_loss: tensor(0.2672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1702/4339]: training_loss: tensor(0.2495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1703/4339]: training_loss: tensor(0.2935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1704/4339]: training_loss: tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1705/4339]: training_loss: tensor(0.1325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1706/4339]: training_loss: tensor(0.0657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1707/4339]: training_loss: tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1708/4339]: training_loss: tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1709/4339]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1710/4339]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1711/4339]: training_loss: tensor(0.2540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1712/4339]: training_loss: tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1713/4339]: training_loss: tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1714/4339]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1715/4339]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1716/4339]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1717/4339]: training_loss: tensor(0.1776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1718/4339]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1719/4339]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1720/4339]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1721/4339]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1722/4339]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1723/4339]: training_loss: tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1724/4339]: training_loss: tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1725/4339]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1726/4339]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1727/4339]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1728/4339]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1729/4339]: training_loss: tensor(0.2884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1730/4339]: training_loss: tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1731/4339]: training_loss: tensor(0.3072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1732/4339]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1733/4339]: training_loss: tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1734/4339]: training_loss: tensor(0.0615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1735/4339]: training_loss: tensor(0.2146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1736/4339]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1737/4339]: training_loss: tensor(0.0270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1738/4339]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1739/4339]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1740/4339]: training_loss: tensor(0.1536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1741/4339]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1742/4339]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1743/4339]: training_loss: tensor(0.0726, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1744/4339]: training_loss: tensor(0.0639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1745/4339]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1746/4339]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1747/4339]: training_loss: tensor(0.0492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1748/4339]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1749/4339]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1750/4339]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1751/4339]: training_loss: tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1752/4339]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1753/4339]: training_loss: tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1754/4339]: training_loss: tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1755/4339]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1756/4339]: training_loss: tensor(0.0684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1757/4339]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1758/4339]: training_loss: tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1759/4339]: training_loss: tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1760/4339]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1761/4339]: training_loss: tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1762/4339]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1763/4339]: training_loss: tensor(0.0974, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1764/4339]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1765/4339]: training_loss: tensor(0.2577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1766/4339]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1767/4339]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1768/4339]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1769/4339]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1770/4339]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1771/4339]: training_loss: tensor(0.1378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1772/4339]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1773/4339]: training_loss: tensor(0.0033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1774/4339]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1775/4339]: training_loss: tensor(0.0117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1776/4339]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1777/4339]: training_loss: tensor(0.3495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1778/4339]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1779/4339]: training_loss: tensor(1.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1780/4339]: training_loss: tensor(0.2772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1781/4339]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1782/4339]: training_loss: tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1783/4339]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1784/4339]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1785/4339]: training_loss: tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1786/4339]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1787/4339]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1788/4339]: training_loss: tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1789/4339]: training_loss: tensor(0.4831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1790/4339]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1791/4339]: training_loss: tensor(0.3358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1792/4339]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1793/4339]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1794/4339]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1795/4339]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1796/4339]: training_loss: tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1797/4339]: training_loss: tensor(0.1430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1798/4339]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1799/4339]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1800/4339]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1801/4339]: training_loss: tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1802/4339]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1803/4339]: training_loss: tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1804/4339]: training_loss: tensor(0.0487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1805/4339]: training_loss: tensor(0.6840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1806/4339]: training_loss: tensor(0.0673, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1807/4339]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1808/4339]: training_loss: tensor(0.1296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1809/4339]: training_loss: tensor(0.2131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1810/4339]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1811/4339]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1812/4339]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1813/4339]: training_loss: tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1814/4339]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1815/4339]: training_loss: tensor(0.5897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1816/4339]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1817/4339]: training_loss: tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1818/4339]: training_loss: tensor(0.0436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1819/4339]: training_loss: tensor(0.5912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1820/4339]: training_loss: tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1821/4339]: training_loss: tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1822/4339]: training_loss: tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1823/4339]: training_loss: tensor(0.0127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1824/4339]: training_loss: tensor(0.0561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1825/4339]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1826/4339]: training_loss: tensor(0.5305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1827/4339]: training_loss: tensor(0.1286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1828/4339]: training_loss: tensor(0.1103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1829/4339]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1830/4339]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1831/4339]: training_loss: tensor(0.2325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1832/4339]: training_loss: tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1833/4339]: training_loss: tensor(0.1806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1834/4339]: training_loss: tensor(0.0486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1835/4339]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1836/4339]: training_loss: tensor(0.1315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1837/4339]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1838/4339]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1839/4339]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1840/4339]: training_loss: tensor(0.0317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1841/4339]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1842/4339]: training_loss: tensor(0.2841, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1843/4339]: training_loss: tensor(0.3774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1844/4339]: training_loss: tensor(0.0664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1845/4339]: training_loss: tensor(0.1910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1846/4339]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1847/4339]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1848/4339]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1849/4339]: training_loss: tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1850/4339]: training_loss: tensor(0.1497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1851/4339]: training_loss: tensor(0.0910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1852/4339]: training_loss: tensor(0.2565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1853/4339]: training_loss: tensor(0.1103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1854/4339]: training_loss: tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1855/4339]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1856/4339]: training_loss: tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1857/4339]: training_loss: tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1858/4339]: training_loss: tensor(0.1899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1859/4339]: training_loss: tensor(0.1889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1860/4339]: training_loss: tensor(0.3340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1861/4339]: training_loss: tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1862/4339]: training_loss: tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1863/4339]: training_loss: tensor(0.1735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1864/4339]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1865/4339]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1866/4339]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1867/4339]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1868/4339]: training_loss: tensor(0.0501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1869/4339]: training_loss: tensor(0.1071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1870/4339]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1871/4339]: training_loss: tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1872/4339]: training_loss: tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1873/4339]: training_loss: tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1874/4339]: training_loss: tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1875/4339]: training_loss: tensor(0.2406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1876/4339]: training_loss: tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1877/4339]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1878/4339]: training_loss: tensor(0.2253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1879/4339]: training_loss: tensor(0.0648, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1880/4339]: training_loss: tensor(0.0616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1881/4339]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1882/4339]: training_loss: tensor(0.1927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1883/4339]: training_loss: tensor(0.0507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1884/4339]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1885/4339]: training_loss: tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1886/4339]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1887/4339]: training_loss: tensor(0.2341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1888/4339]: training_loss: tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1889/4339]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1890/4339]: training_loss: tensor(0.2515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1891/4339]: training_loss: tensor(0.1804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1892/4339]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1893/4339]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1894/4339]: training_loss: tensor(0.1427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1895/4339]: training_loss: tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1896/4339]: training_loss: tensor(0.1976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1897/4339]: training_loss: tensor(0.1577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1898/4339]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1899/4339]: training_loss: tensor(0.3494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1900/4339]: training_loss: tensor(0.2150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1901/4339]: training_loss: tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1902/4339]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1903/4339]: training_loss: tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1904/4339]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1905/4339]: training_loss: tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1906/4339]: training_loss: tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1907/4339]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1908/4339]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1909/4339]: training_loss: tensor(0.0246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1910/4339]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1911/4339]: training_loss: tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1912/4339]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1913/4339]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1914/4339]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1915/4339]: training_loss: tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1916/4339]: training_loss: tensor(0.3050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1917/4339]: training_loss: tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1918/4339]: training_loss: tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1919/4339]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1920/4339]: training_loss: tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1921/4339]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1922/4339]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1923/4339]: training_loss: tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1924/4339]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1925/4339]: training_loss: tensor(0.0585, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1926/4339]: training_loss: tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1927/4339]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1928/4339]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1929/4339]: training_loss: tensor(0.0278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1930/4339]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1931/4339]: training_loss: tensor(0.1113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1932/4339]: training_loss: tensor(0.1840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1933/4339]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1934/4339]: training_loss: tensor(0.0413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1935/4339]: training_loss: tensor(0.1238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1936/4339]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1937/4339]: training_loss: tensor(0.2029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1938/4339]: training_loss: tensor(0.0689, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1939/4339]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1940/4339]: training_loss: tensor(0.2329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1941/4339]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1942/4339]: training_loss: tensor(0.3418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1943/4339]: training_loss: tensor(0.1932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1944/4339]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1945/4339]: training_loss: tensor(0.3575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1946/4339]: training_loss: tensor(0.2383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1947/4339]: training_loss: tensor(0.0551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1948/4339]: training_loss: tensor(0.2681, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1949/4339]: training_loss: tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1950/4339]: training_loss: tensor(0.2203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1951/4339]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1952/4339]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1953/4339]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1954/4339]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1955/4339]: training_loss: tensor(0.1979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1956/4339]: training_loss: tensor(0.1199, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [1957/4339]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1958/4339]: training_loss: tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1959/4339]: training_loss: tensor(0.3486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1960/4339]: training_loss: tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1961/4339]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1962/4339]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1963/4339]: training_loss: tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1964/4339]: training_loss: tensor(0.0295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1965/4339]: training_loss: tensor(0.2077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1966/4339]: training_loss: tensor(0.3236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1967/4339]: training_loss: tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1968/4339]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1969/4339]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1970/4339]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1971/4339]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1972/4339]: training_loss: tensor(0.1360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1973/4339]: training_loss: tensor(0.1301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1974/4339]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1975/4339]: training_loss: tensor(0.2432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1976/4339]: training_loss: tensor(0.0753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1977/4339]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1978/4339]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1979/4339]: training_loss: tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1980/4339]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1981/4339]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1982/4339]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1983/4339]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1984/4339]: training_loss: tensor(0.2963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1985/4339]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1986/4339]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1987/4339]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1988/4339]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1989/4339]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1990/4339]: training_loss: tensor(0.1770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1991/4339]: training_loss: tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1992/4339]: training_loss: tensor(0.1847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1993/4339]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1994/4339]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1995/4339]: training_loss: tensor(0.1994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1996/4339]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1997/4339]: training_loss: tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1998/4339]: training_loss: tensor(0.1942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [1999/4339]: training_loss: tensor(0.3220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2000/4339]: training_loss: tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2001/4339]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2002/4339]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2003/4339]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2004/4339]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2005/4339]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2006/4339]: training_loss: tensor(0.0099, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2007/4339]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2008/4339]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2009/4339]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2010/4339]: training_loss: tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2011/4339]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2012/4339]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2013/4339]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2014/4339]: training_loss: tensor(0.2319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2015/4339]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2016/4339]: training_loss: tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2017/4339]: training_loss: tensor(0.0468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2018/4339]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2019/4339]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2020/4339]: training_loss: tensor(0.0618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2021/4339]: training_loss: tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2022/4339]: training_loss: tensor(0.1797, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2023/4339]: training_loss: tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2024/4339]: training_loss: tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2025/4339]: training_loss: tensor(0.0425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2026/4339]: training_loss: tensor(0.0568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2027/4339]: training_loss: tensor(0.0250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2028/4339]: training_loss: tensor(0.0594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2029/4339]: training_loss: tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2030/4339]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2031/4339]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2032/4339]: training_loss: tensor(0.0757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2033/4339]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2034/4339]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2035/4339]: training_loss: tensor(0.0483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2036/4339]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2037/4339]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2038/4339]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2039/4339]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2040/4339]: training_loss: tensor(0.0739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2041/4339]: training_loss: tensor(0.2587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2042/4339]: training_loss: tensor(0.2392, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2043/4339]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2044/4339]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2045/4339]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2046/4339]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2047/4339]: training_loss: tensor(0.0343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2048/4339]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2049/4339]: training_loss: tensor(0.1932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2050/4339]: training_loss: tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2051/4339]: training_loss: tensor(0.1928, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2052/4339]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2053/4339]: training_loss: tensor(0.2301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2054/4339]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2055/4339]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2056/4339]: training_loss: tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2057/4339]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2058/4339]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2059/4339]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2060/4339]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2061/4339]: training_loss: tensor(0.0897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2062/4339]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2063/4339]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2064/4339]: training_loss: tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2065/4339]: training_loss: tensor(0.1973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2066/4339]: training_loss: tensor(0.0967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2067/4339]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2068/4339]: training_loss: tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2069/4339]: training_loss: tensor(0.1164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2070/4339]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2071/4339]: training_loss: tensor(0.0635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2072/4339]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2073/4339]: training_loss: tensor(0.0202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2074/4339]: training_loss: tensor(0.0979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2075/4339]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2076/4339]: training_loss: tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2077/4339]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2078/4339]: training_loss: tensor(0.2124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2079/4339]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2080/4339]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2081/4339]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2082/4339]: training_loss: tensor(0.2095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2083/4339]: training_loss: tensor(0.6629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2084/4339]: training_loss: tensor(0.0040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2085/4339]: training_loss: tensor(0.1529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2086/4339]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2087/4339]: training_loss: tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2088/4339]: training_loss: tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2089/4339]: training_loss: tensor(0.1503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2090/4339]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2091/4339]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2092/4339]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2093/4339]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2094/4339]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2095/4339]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2096/4339]: training_loss: tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2097/4339]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2098/4339]: training_loss: tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2099/4339]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2100/4339]: training_loss: tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2101/4339]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2102/4339]: training_loss: tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2103/4339]: training_loss: tensor(0.1422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2104/4339]: training_loss: tensor(0.0261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2105/4339]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2106/4339]: training_loss: tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2107/4339]: training_loss: tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2108/4339]: training_loss: tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2109/4339]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2110/4339]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2111/4339]: training_loss: tensor(0.0344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2112/4339]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2113/4339]: training_loss: tensor(0.1129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2114/4339]: training_loss: tensor(0.0383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2115/4339]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2116/4339]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2117/4339]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2118/4339]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2119/4339]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2120/4339]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2121/4339]: training_loss: tensor(0.1142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2122/4339]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2123/4339]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2124/4339]: training_loss: tensor(0.1286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2125/4339]: training_loss: tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2126/4339]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2127/4339]: training_loss: tensor(0.3107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2128/4339]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2129/4339]: training_loss: tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2130/4339]: training_loss: tensor(0.2346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2131/4339]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2132/4339]: training_loss: tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2133/4339]: training_loss: tensor(0.0400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2134/4339]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2135/4339]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2136/4339]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2137/4339]: training_loss: tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2138/4339]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2139/4339]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2140/4339]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2141/4339]: training_loss: tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2142/4339]: training_loss: tensor(0.2055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2143/4339]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2144/4339]: training_loss: tensor(0.0649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2145/4339]: training_loss: tensor(0.1634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2146/4339]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2147/4339]: training_loss: tensor(0.2485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2148/4339]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2149/4339]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2150/4339]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2151/4339]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2152/4339]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2153/4339]: training_loss: tensor(0.3825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2154/4339]: training_loss: tensor(0.4210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2155/4339]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2156/4339]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2157/4339]: training_loss: tensor(0.1606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2158/4339]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2159/4339]: training_loss: tensor(0.2682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2160/4339]: training_loss: tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2161/4339]: training_loss: tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2162/4339]: training_loss: tensor(0.1644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2163/4339]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2164/4339]: training_loss: tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2165/4339]: training_loss: tensor(0.1448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2166/4339]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2167/4339]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2168/4339]: training_loss: tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2169/4339]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2170/4339]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [5/5], Step [19530/21700], Train Loss: 0.0966, Valid Loss: 0.6232\n",
      "batch_no [2171/4339]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2172/4339]: training_loss: tensor(0.0784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2173/4339]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2174/4339]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2175/4339]: training_loss: tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2176/4339]: training_loss: tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2177/4339]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2178/4339]: training_loss: tensor(0.1409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2179/4339]: training_loss: tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2180/4339]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2181/4339]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2182/4339]: training_loss: tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2183/4339]: training_loss: tensor(0.1156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2184/4339]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2185/4339]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2186/4339]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2187/4339]: training_loss: tensor(0.1893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2188/4339]: training_loss: tensor(0.2290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2189/4339]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2190/4339]: training_loss: tensor(0.1116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2191/4339]: training_loss: tensor(0.2656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2192/4339]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2193/4339]: training_loss: tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2194/4339]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2195/4339]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2196/4339]: training_loss: tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2197/4339]: training_loss: tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2198/4339]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2199/4339]: training_loss: tensor(0.1117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2200/4339]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2201/4339]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2202/4339]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2203/4339]: training_loss: tensor(0.3477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2204/4339]: training_loss: tensor(0.2177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2205/4339]: training_loss: tensor(0.2307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2206/4339]: training_loss: tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2207/4339]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2208/4339]: training_loss: tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2209/4339]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2210/4339]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2211/4339]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2212/4339]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2213/4339]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2214/4339]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2215/4339]: training_loss: tensor(0.2138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2216/4339]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2217/4339]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2218/4339]: training_loss: tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2219/4339]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2220/4339]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2221/4339]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2222/4339]: training_loss: tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2223/4339]: training_loss: tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2224/4339]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2225/4339]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2226/4339]: training_loss: tensor(0.3105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2227/4339]: training_loss: tensor(0.0455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2228/4339]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2229/4339]: training_loss: tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2230/4339]: training_loss: tensor(0.1593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2231/4339]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2232/4339]: training_loss: tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2233/4339]: training_loss: tensor(0.0514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2234/4339]: training_loss: tensor(0.2806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2235/4339]: training_loss: tensor(0.1578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2236/4339]: training_loss: tensor(0.3641, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2237/4339]: training_loss: tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2238/4339]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2239/4339]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2240/4339]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2241/4339]: training_loss: tensor(0.1818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2242/4339]: training_loss: tensor(0.0984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2243/4339]: training_loss: tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2244/4339]: training_loss: tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2245/4339]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2246/4339]: training_loss: tensor(0.0594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2247/4339]: training_loss: tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2248/4339]: training_loss: tensor(0.3284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2249/4339]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2250/4339]: training_loss: tensor(0.1555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2251/4339]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2252/4339]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2253/4339]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2254/4339]: training_loss: tensor(0.2148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2255/4339]: training_loss: tensor(0.3706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2256/4339]: training_loss: tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2257/4339]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2258/4339]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2259/4339]: training_loss: tensor(0.3836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2260/4339]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2261/4339]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2262/4339]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2263/4339]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2264/4339]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2265/4339]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2266/4339]: training_loss: tensor(0.2571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2267/4339]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2268/4339]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2269/4339]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2270/4339]: training_loss: tensor(0.2697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2271/4339]: training_loss: tensor(0.0775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2272/4339]: training_loss: tensor(0.3000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2273/4339]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2274/4339]: training_loss: tensor(0.3490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2275/4339]: training_loss: tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2276/4339]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2277/4339]: training_loss: tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2278/4339]: training_loss: tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2279/4339]: training_loss: tensor(0.2283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2280/4339]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2281/4339]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2282/4339]: training_loss: tensor(0.1003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2283/4339]: training_loss: tensor(0.2801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2284/4339]: training_loss: tensor(0.4164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2285/4339]: training_loss: tensor(0.0964, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2286/4339]: training_loss: tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2287/4339]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2288/4339]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2289/4339]: training_loss: tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2290/4339]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2291/4339]: training_loss: tensor(0.0259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2292/4339]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2293/4339]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2294/4339]: training_loss: tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2295/4339]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2296/4339]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2297/4339]: training_loss: tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2298/4339]: training_loss: tensor(0.0527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2299/4339]: training_loss: tensor(0.0305, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2300/4339]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2301/4339]: training_loss: tensor(0.2868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2302/4339]: training_loss: tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2303/4339]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2304/4339]: training_loss: tensor(0.2074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2305/4339]: training_loss: tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2306/4339]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2307/4339]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2308/4339]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2309/4339]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2310/4339]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2311/4339]: training_loss: tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2312/4339]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2313/4339]: training_loss: tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2314/4339]: training_loss: tensor(0.1960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2315/4339]: training_loss: tensor(0.0650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2316/4339]: training_loss: tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2317/4339]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2318/4339]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2319/4339]: training_loss: tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2320/4339]: training_loss: tensor(0.2195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2321/4339]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2322/4339]: training_loss: tensor(0.2103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2323/4339]: training_loss: tensor(0.0829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2324/4339]: training_loss: tensor(0.0994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2325/4339]: training_loss: tensor(0.0628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2326/4339]: training_loss: tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2327/4339]: training_loss: tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2328/4339]: training_loss: tensor(0.2076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2329/4339]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2330/4339]: training_loss: tensor(0.0672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2331/4339]: training_loss: tensor(0.1677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2332/4339]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2333/4339]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2334/4339]: training_loss: tensor(0.1185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2335/4339]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2336/4339]: training_loss: tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2337/4339]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2338/4339]: training_loss: tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2339/4339]: training_loss: tensor(0.3209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2340/4339]: training_loss: tensor(0.1853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2341/4339]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2342/4339]: training_loss: tensor(0.1523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2343/4339]: training_loss: tensor(0.0616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2344/4339]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2345/4339]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2346/4339]: training_loss: tensor(0.0839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2347/4339]: training_loss: tensor(0.0625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2348/4339]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2349/4339]: training_loss: tensor(0.0865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2350/4339]: training_loss: tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2351/4339]: training_loss: tensor(0.2221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2352/4339]: training_loss: tensor(0.2239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2353/4339]: training_loss: tensor(0.1653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2354/4339]: training_loss: tensor(0.1690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2355/4339]: training_loss: tensor(0.1209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2356/4339]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2357/4339]: training_loss: tensor(0.1516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2358/4339]: training_loss: tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2359/4339]: training_loss: tensor(0.4028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2360/4339]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2361/4339]: training_loss: tensor(0.1391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2362/4339]: training_loss: tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2363/4339]: training_loss: tensor(0.2663, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2364/4339]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2365/4339]: training_loss: tensor(0.1684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2366/4339]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2367/4339]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2368/4339]: training_loss: tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2369/4339]: training_loss: tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2370/4339]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2371/4339]: training_loss: tensor(0.2834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2372/4339]: training_loss: tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2373/4339]: training_loss: tensor(0.3031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2374/4339]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2375/4339]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2376/4339]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2377/4339]: training_loss: tensor(0.0442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2378/4339]: training_loss: tensor(0.2810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2379/4339]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2380/4339]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2381/4339]: training_loss: tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2382/4339]: training_loss: tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2383/4339]: training_loss: tensor(0.1343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2384/4339]: training_loss: tensor(0.5256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2385/4339]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2386/4339]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2387/4339]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2388/4339]: training_loss: tensor(0.1604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2389/4339]: training_loss: tensor(0.0808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2390/4339]: training_loss: tensor(0.0962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2391/4339]: training_loss: tensor(0.2046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2392/4339]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2393/4339]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2394/4339]: training_loss: tensor(0.0419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2395/4339]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2396/4339]: training_loss: tensor(0.1066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2397/4339]: training_loss: tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2398/4339]: training_loss: tensor(0.3003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2399/4339]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2400/4339]: training_loss: tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2401/4339]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2402/4339]: training_loss: tensor(0.1355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2403/4339]: training_loss: tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2404/4339]: training_loss: tensor(0.0610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2405/4339]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2406/4339]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2407/4339]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2408/4339]: training_loss: tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2409/4339]: training_loss: tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2410/4339]: training_loss: tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2411/4339]: training_loss: tensor(0.2152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2412/4339]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2413/4339]: training_loss: tensor(0.2779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2414/4339]: training_loss: tensor(0.0555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2415/4339]: training_loss: tensor(0.3415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2416/4339]: training_loss: tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2417/4339]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2418/4339]: training_loss: tensor(0.0635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2419/4339]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2420/4339]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2421/4339]: training_loss: tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2422/4339]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2423/4339]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2424/4339]: training_loss: tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2425/4339]: training_loss: tensor(0.0728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2426/4339]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2427/4339]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2428/4339]: training_loss: tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2429/4339]: training_loss: tensor(0.1513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2430/4339]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2431/4339]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2432/4339]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2433/4339]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2434/4339]: training_loss: tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2435/4339]: training_loss: tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2436/4339]: training_loss: tensor(0.1165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2437/4339]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2438/4339]: training_loss: tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2439/4339]: training_loss: tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2440/4339]: training_loss: tensor(0.5381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2441/4339]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2442/4339]: training_loss: tensor(0.1733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2443/4339]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2444/4339]: training_loss: tensor(0.3353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2445/4339]: training_loss: tensor(0.2822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2446/4339]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2447/4339]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2448/4339]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2449/4339]: training_loss: tensor(0.0736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2450/4339]: training_loss: tensor(0.4178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2451/4339]: training_loss: tensor(0.0479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2452/4339]: training_loss: tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2453/4339]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2454/4339]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2455/4339]: training_loss: tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2456/4339]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2457/4339]: training_loss: tensor(0.2707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2458/4339]: training_loss: tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2459/4339]: training_loss: tensor(0.1363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2460/4339]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2461/4339]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2462/4339]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2463/4339]: training_loss: tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2464/4339]: training_loss: tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2465/4339]: training_loss: tensor(0.0383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2466/4339]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2467/4339]: training_loss: tensor(0.2388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2468/4339]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2469/4339]: training_loss: tensor(0.1257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2470/4339]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2471/4339]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2472/4339]: training_loss: tensor(0.0636, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2473/4339]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2474/4339]: training_loss: tensor(0.0512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2475/4339]: training_loss: tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2476/4339]: training_loss: tensor(0.2290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2477/4339]: training_loss: tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2478/4339]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2479/4339]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2480/4339]: training_loss: tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2481/4339]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2482/4339]: training_loss: tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2483/4339]: training_loss: tensor(0.0027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2484/4339]: training_loss: tensor(0.4330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2485/4339]: training_loss: tensor(0.2213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2486/4339]: training_loss: tensor(0.1715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2487/4339]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2488/4339]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2489/4339]: training_loss: tensor(0.3422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2490/4339]: training_loss: tensor(0.0510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2491/4339]: training_loss: tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2492/4339]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2493/4339]: training_loss: tensor(0.5205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2494/4339]: training_loss: tensor(0.2899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2495/4339]: training_loss: tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2496/4339]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2497/4339]: training_loss: tensor(0.2416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2498/4339]: training_loss: tensor(0.1364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2499/4339]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2500/4339]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2501/4339]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2502/4339]: training_loss: tensor(0.0462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2503/4339]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2504/4339]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2505/4339]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2506/4339]: training_loss: tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2507/4339]: training_loss: tensor(0.0522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2508/4339]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2509/4339]: training_loss: tensor(0.0948, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2510/4339]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2511/4339]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2512/4339]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2513/4339]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2514/4339]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2515/4339]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2516/4339]: training_loss: tensor(0.4651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2517/4339]: training_loss: tensor(0.0543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2518/4339]: training_loss: tensor(0.0852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2519/4339]: training_loss: tensor(0.0315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2520/4339]: training_loss: tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2521/4339]: training_loss: tensor(0.0162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2522/4339]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2523/4339]: training_loss: tensor(0.4668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2524/4339]: training_loss: tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2525/4339]: training_loss: tensor(0.1251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2526/4339]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2527/4339]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2528/4339]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2529/4339]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2530/4339]: training_loss: tensor(0.6276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2531/4339]: training_loss: tensor(0.4138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2532/4339]: training_loss: tensor(0.4926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2533/4339]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2534/4339]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2535/4339]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2536/4339]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2537/4339]: training_loss: tensor(0.2512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2538/4339]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2539/4339]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2540/4339]: training_loss: tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2541/4339]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2542/4339]: training_loss: tensor(0.2183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2543/4339]: training_loss: tensor(0.4220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2544/4339]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2545/4339]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2546/4339]: training_loss: tensor(0.1130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2547/4339]: training_loss: tensor(0.0702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2548/4339]: training_loss: tensor(0.2464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2549/4339]: training_loss: tensor(0.1017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2550/4339]: training_loss: tensor(0.0383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2551/4339]: training_loss: tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2552/4339]: training_loss: tensor(0.1580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2553/4339]: training_loss: tensor(0.2082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2554/4339]: training_loss: tensor(0.3763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2555/4339]: training_loss: tensor(0.2985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2556/4339]: training_loss: tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2557/4339]: training_loss: tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2558/4339]: training_loss: tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2559/4339]: training_loss: tensor(0.2044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2560/4339]: training_loss: tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2561/4339]: training_loss: tensor(0.0615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2562/4339]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2563/4339]: training_loss: tensor(0.0868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2564/4339]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2565/4339]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2566/4339]: training_loss: tensor(0.5102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2567/4339]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2568/4339]: training_loss: tensor(0.3934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2569/4339]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2570/4339]: training_loss: tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2571/4339]: training_loss: tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2572/4339]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2573/4339]: training_loss: tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2574/4339]: training_loss: tensor(0.2195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2575/4339]: training_loss: tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2576/4339]: training_loss: tensor(0.0827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2577/4339]: training_loss: tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2578/4339]: training_loss: tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2579/4339]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2580/4339]: training_loss: tensor(0.1219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2581/4339]: training_loss: tensor(0.2600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2582/4339]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2583/4339]: training_loss: tensor(0.2435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2584/4339]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2585/4339]: training_loss: tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2586/4339]: training_loss: tensor(0.3150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2587/4339]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2588/4339]: training_loss: tensor(0.0659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2589/4339]: training_loss: tensor(0.4454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2590/4339]: training_loss: tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2591/4339]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2592/4339]: training_loss: tensor(0.1986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2593/4339]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2594/4339]: training_loss: tensor(0.1117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2595/4339]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2596/4339]: training_loss: tensor(0.3464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2597/4339]: training_loss: tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2598/4339]: training_loss: tensor(0.4466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2599/4339]: training_loss: tensor(0.0456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2600/4339]: training_loss: tensor(0.2745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2601/4339]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2602/4339]: training_loss: tensor(0.3445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2603/4339]: training_loss: tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2604/4339]: training_loss: tensor(0.1100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2605/4339]: training_loss: tensor(0.1855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2606/4339]: training_loss: tensor(0.1533, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2607/4339]: training_loss: tensor(0.3543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2608/4339]: training_loss: tensor(0.0494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2609/4339]: training_loss: tensor(0.0276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2610/4339]: training_loss: tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2611/4339]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2612/4339]: training_loss: tensor(0.1554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2613/4339]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2614/4339]: training_loss: tensor(0.2043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2615/4339]: training_loss: tensor(0.0362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2616/4339]: training_loss: tensor(0.1233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2617/4339]: training_loss: tensor(0.0565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2618/4339]: training_loss: tensor(0.0534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2619/4339]: training_loss: tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2620/4339]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2621/4339]: training_loss: tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2622/4339]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2623/4339]: training_loss: tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2624/4339]: training_loss: tensor(0.3126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2625/4339]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2626/4339]: training_loss: tensor(0.2583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2627/4339]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2628/4339]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2629/4339]: training_loss: tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2630/4339]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2631/4339]: training_loss: tensor(0.0501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2632/4339]: training_loss: tensor(0.1026, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2633/4339]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2634/4339]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2635/4339]: training_loss: tensor(0.0812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2636/4339]: training_loss: tensor(0.1222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2637/4339]: training_loss: tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2638/4339]: training_loss: tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2639/4339]: training_loss: tensor(0.0347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2640/4339]: training_loss: tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2641/4339]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2642/4339]: training_loss: tensor(0.2258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2643/4339]: training_loss: tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2644/4339]: training_loss: tensor(0.1588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2645/4339]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2646/4339]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2647/4339]: training_loss: tensor(0.0620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2648/4339]: training_loss: tensor(0.2920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2649/4339]: training_loss: tensor(0.2736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2650/4339]: training_loss: tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2651/4339]: training_loss: tensor(0.0703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2652/4339]: training_loss: tensor(0.1230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2653/4339]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2654/4339]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2655/4339]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2656/4339]: training_loss: tensor(0.0530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2657/4339]: training_loss: tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2658/4339]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2659/4339]: training_loss: tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2660/4339]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2661/4339]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2662/4339]: training_loss: tensor(0.0354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2663/4339]: training_loss: tensor(0.0870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2664/4339]: training_loss: tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2665/4339]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2666/4339]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2667/4339]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2668/4339]: training_loss: tensor(0.0339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2669/4339]: training_loss: tensor(0.1461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2670/4339]: training_loss: tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2671/4339]: training_loss: tensor(0.0678, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2672/4339]: training_loss: tensor(0.0193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2673/4339]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2674/4339]: training_loss: tensor(0.0209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2675/4339]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2676/4339]: training_loss: tensor(0.2499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2677/4339]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2678/4339]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2679/4339]: training_loss: tensor(0.0233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2680/4339]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2681/4339]: training_loss: tensor(0.0751, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2682/4339]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2683/4339]: training_loss: tensor(0.0710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2684/4339]: training_loss: tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2685/4339]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2686/4339]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2687/4339]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2688/4339]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2689/4339]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2690/4339]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2691/4339]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2692/4339]: training_loss: tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2693/4339]: training_loss: tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2694/4339]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2695/4339]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2696/4339]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2697/4339]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2698/4339]: training_loss: tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2699/4339]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2700/4339]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2701/4339]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2702/4339]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2703/4339]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2704/4339]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2705/4339]: training_loss: tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2706/4339]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2707/4339]: training_loss: tensor(0.5690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2708/4339]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2709/4339]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2710/4339]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2711/4339]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2712/4339]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2713/4339]: training_loss: tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2714/4339]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2715/4339]: training_loss: tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2716/4339]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2717/4339]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2718/4339]: training_loss: tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2719/4339]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2720/4339]: training_loss: tensor(0.3988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2721/4339]: training_loss: tensor(0.3446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2722/4339]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2723/4339]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2724/4339]: training_loss: tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2725/4339]: training_loss: tensor(0.0893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2726/4339]: training_loss: tensor(0.2217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2727/4339]: training_loss: tensor(0.9257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2728/4339]: training_loss: tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2729/4339]: training_loss: tensor(0.0146, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2730/4339]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2731/4339]: training_loss: tensor(0.0796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2732/4339]: training_loss: tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2733/4339]: training_loss: tensor(0.1531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2734/4339]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2735/4339]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2736/4339]: training_loss: tensor(0.5122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2737/4339]: training_loss: tensor(0.2157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2738/4339]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2739/4339]: training_loss: tensor(0.2221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2740/4339]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2741/4339]: training_loss: tensor(0.1687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2742/4339]: training_loss: tensor(0.2940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2743/4339]: training_loss: tensor(0.2016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2744/4339]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2745/4339]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2746/4339]: training_loss: tensor(0.0287, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2747/4339]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2748/4339]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2749/4339]: training_loss: tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2750/4339]: training_loss: tensor(0.1471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2751/4339]: training_loss: tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2752/4339]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2753/4339]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2754/4339]: training_loss: tensor(0.2510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2755/4339]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2756/4339]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2757/4339]: training_loss: tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2758/4339]: training_loss: tensor(0.0486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2759/4339]: training_loss: tensor(0.2453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2760/4339]: training_loss: tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2761/4339]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2762/4339]: training_loss: tensor(0.0486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2763/4339]: training_loss: tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2764/4339]: training_loss: tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2765/4339]: training_loss: tensor(0.0611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2766/4339]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2767/4339]: training_loss: tensor(0.3581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2768/4339]: training_loss: tensor(0.0482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2769/4339]: training_loss: tensor(0.1346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2770/4339]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2771/4339]: training_loss: tensor(0.1257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2772/4339]: training_loss: tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2773/4339]: training_loss: tensor(0.1405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2774/4339]: training_loss: tensor(0.1688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2775/4339]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2776/4339]: training_loss: tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2777/4339]: training_loss: tensor(0.1772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2778/4339]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2779/4339]: training_loss: tensor(0.3475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2780/4339]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2781/4339]: training_loss: tensor(0.0677, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2782/4339]: training_loss: tensor(0.2742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2783/4339]: training_loss: tensor(0.0377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2784/4339]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2785/4339]: training_loss: tensor(0.0392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2786/4339]: training_loss: tensor(0.0388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2787/4339]: training_loss: tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2788/4339]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2789/4339]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2790/4339]: training_loss: tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2791/4339]: training_loss: tensor(0.0194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2792/4339]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2793/4339]: training_loss: tensor(0.1320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2794/4339]: training_loss: tensor(0.2215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2795/4339]: training_loss: tensor(0.1480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2796/4339]: training_loss: tensor(0.1233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2797/4339]: training_loss: tensor(0.0436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2798/4339]: training_loss: tensor(0.3582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2799/4339]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2800/4339]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2801/4339]: training_loss: tensor(0.2833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2802/4339]: training_loss: tensor(0.0232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2803/4339]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2804/4339]: training_loss: tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2805/4339]: training_loss: tensor(0.2250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2806/4339]: training_loss: tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2807/4339]: training_loss: tensor(0.1830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2808/4339]: training_loss: tensor(0.3141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2809/4339]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2810/4339]: training_loss: tensor(0.5515, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2811/4339]: training_loss: tensor(0.1369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2812/4339]: training_loss: tensor(0.2279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2813/4339]: training_loss: tensor(0.0433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2814/4339]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2815/4339]: training_loss: tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2816/4339]: training_loss: tensor(0.1498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2817/4339]: training_loss: tensor(0.0286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2818/4339]: training_loss: tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2819/4339]: training_loss: tensor(0.2033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2820/4339]: training_loss: tensor(0.2581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2821/4339]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2822/4339]: training_loss: tensor(0.0113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2823/4339]: training_loss: tensor(0.2451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2824/4339]: training_loss: tensor(0.0867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2825/4339]: training_loss: tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2826/4339]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2827/4339]: training_loss: tensor(0.1656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2828/4339]: training_loss: tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2829/4339]: training_loss: tensor(0.5723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2830/4339]: training_loss: tensor(0.0071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2831/4339]: training_loss: tensor(0.0450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2832/4339]: training_loss: tensor(0.0432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2833/4339]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2834/4339]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2835/4339]: training_loss: tensor(0.1721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2836/4339]: training_loss: tensor(0.0271, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2837/4339]: training_loss: tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2838/4339]: training_loss: tensor(0.1335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2839/4339]: training_loss: tensor(0.0445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2840/4339]: training_loss: tensor(0.1431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2841/4339]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2842/4339]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2843/4339]: training_loss: tensor(0.1875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2844/4339]: training_loss: tensor(0.2163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2845/4339]: training_loss: tensor(0.2188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2846/4339]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2847/4339]: training_loss: tensor(0.1958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2848/4339]: training_loss: tensor(0.0348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2849/4339]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2850/4339]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2851/4339]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2852/4339]: training_loss: tensor(0.1809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2853/4339]: training_loss: tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2854/4339]: training_loss: tensor(0.2111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2855/4339]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2856/4339]: training_loss: tensor(0.1919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2857/4339]: training_loss: tensor(0.1149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2858/4339]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2859/4339]: training_loss: tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2860/4339]: training_loss: tensor(0.0516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2861/4339]: training_loss: tensor(0.1380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2862/4339]: training_loss: tensor(0.4109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2863/4339]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2864/4339]: training_loss: tensor(0.1378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2865/4339]: training_loss: tensor(0.0268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2866/4339]: training_loss: tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2867/4339]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2868/4339]: training_loss: tensor(0.1968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2869/4339]: training_loss: tensor(0.3811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2870/4339]: training_loss: tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2871/4339]: training_loss: tensor(0.0687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2872/4339]: training_loss: tensor(0.2699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2873/4339]: training_loss: tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2874/4339]: training_loss: tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2875/4339]: training_loss: tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2876/4339]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2877/4339]: training_loss: tensor(0.2364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2878/4339]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2879/4339]: training_loss: tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2880/4339]: training_loss: tensor(0.0730, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2881/4339]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2882/4339]: training_loss: tensor(0.2053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2883/4339]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2884/4339]: training_loss: tensor(0.1486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2885/4339]: training_loss: tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2886/4339]: training_loss: tensor(0.0679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2887/4339]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2888/4339]: training_loss: tensor(0.1714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2889/4339]: training_loss: tensor(0.1650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2890/4339]: training_loss: tensor(0.4316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2891/4339]: training_loss: tensor(0.1449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2892/4339]: training_loss: tensor(0.2268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2893/4339]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2894/4339]: training_loss: tensor(0.2148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2895/4339]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2896/4339]: training_loss: tensor(0.2095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2897/4339]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2898/4339]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2899/4339]: training_loss: tensor(0.1218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2900/4339]: training_loss: tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2901/4339]: training_loss: tensor(0.1274, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2902/4339]: training_loss: tensor(0.2035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2903/4339]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2904/4339]: training_loss: tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2905/4339]: training_loss: tensor(0.3303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2906/4339]: training_loss: tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2907/4339]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2908/4339]: training_loss: tensor(0.0978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2909/4339]: training_loss: tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2910/4339]: training_loss: tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2911/4339]: training_loss: tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2912/4339]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2913/4339]: training_loss: tensor(0.1464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2914/4339]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2915/4339]: training_loss: tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2916/4339]: training_loss: tensor(0.0230, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2917/4339]: training_loss: tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2918/4339]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2919/4339]: training_loss: tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2920/4339]: training_loss: tensor(0.0231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2921/4339]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2922/4339]: training_loss: tensor(0.1221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2923/4339]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2924/4339]: training_loss: tensor(0.1475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2925/4339]: training_loss: tensor(0.1525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2926/4339]: training_loss: tensor(0.0749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2927/4339]: training_loss: tensor(0.3333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2928/4339]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2929/4339]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2930/4339]: training_loss: tensor(0.1568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2931/4339]: training_loss: tensor(0.1826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2932/4339]: training_loss: tensor(0.0600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2933/4339]: training_loss: tensor(0.0566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2934/4339]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2935/4339]: training_loss: tensor(0.0708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2936/4339]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2937/4339]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2938/4339]: training_loss: tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2939/4339]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2940/4339]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2941/4339]: training_loss: tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2942/4339]: training_loss: tensor(0.0489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2943/4339]: training_loss: tensor(0.0705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2944/4339]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2945/4339]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2946/4339]: training_loss: tensor(0.3202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2947/4339]: training_loss: tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2948/4339]: training_loss: tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2949/4339]: training_loss: tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2950/4339]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2951/4339]: training_loss: tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2952/4339]: training_loss: tensor(0.2804, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2953/4339]: training_loss: tensor(0.1631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2954/4339]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2955/4339]: training_loss: tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2956/4339]: training_loss: tensor(0.2561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2957/4339]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2958/4339]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2959/4339]: training_loss: tensor(0.1333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2960/4339]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2961/4339]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2962/4339]: training_loss: tensor(0.0833, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2963/4339]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2964/4339]: training_loss: tensor(0.2125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2965/4339]: training_loss: tensor(0.2524, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2966/4339]: training_loss: tensor(0.4632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2967/4339]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2968/4339]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2969/4339]: training_loss: tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2970/4339]: training_loss: tensor(0.5370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2971/4339]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2972/4339]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2973/4339]: training_loss: tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2974/4339]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2975/4339]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2976/4339]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2977/4339]: training_loss: tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2978/4339]: training_loss: tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2979/4339]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2980/4339]: training_loss: tensor(0.2031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2981/4339]: training_loss: tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2982/4339]: training_loss: tensor(0.2093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2983/4339]: training_loss: tensor(0.3571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2984/4339]: training_loss: tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2985/4339]: training_loss: tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2986/4339]: training_loss: tensor(0.3015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2987/4339]: training_loss: tensor(0.1247, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [2988/4339]: training_loss: tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2989/4339]: training_loss: tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2990/4339]: training_loss: tensor(0.0474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2991/4339]: training_loss: tensor(0.1656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2992/4339]: training_loss: tensor(0.0599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2993/4339]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2994/4339]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2995/4339]: training_loss: tensor(0.1675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2996/4339]: training_loss: tensor(0.1639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2997/4339]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2998/4339]: training_loss: tensor(0.0185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [2999/4339]: training_loss: tensor(0.1847, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3000/4339]: training_loss: tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3001/4339]: training_loss: tensor(0.1159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3002/4339]: training_loss: tensor(0.5762, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3003/4339]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3004/4339]: training_loss: tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3005/4339]: training_loss: tensor(0.0437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3006/4339]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3007/4339]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3008/4339]: training_loss: tensor(0.1692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3009/4339]: training_loss: tensor(0.1891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3010/4339]: training_loss: tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3011/4339]: training_loss: tensor(0.3628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3012/4339]: training_loss: tensor(0.0716, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3013/4339]: training_loss: tensor(0.2363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3014/4339]: training_loss: tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3015/4339]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3016/4339]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3017/4339]: training_loss: tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3018/4339]: training_loss: tensor(0.1427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3019/4339]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3020/4339]: training_loss: tensor(0.2544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3021/4339]: training_loss: tensor(0.1929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3022/4339]: training_loss: tensor(0.1882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3023/4339]: training_loss: tensor(0.0531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3024/4339]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3025/4339]: training_loss: tensor(0.0361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3026/4339]: training_loss: tensor(0.0846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3027/4339]: training_loss: tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3028/4339]: training_loss: tensor(0.1198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3029/4339]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3030/4339]: training_loss: tensor(0.1840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3031/4339]: training_loss: tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3032/4339]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3033/4339]: training_loss: tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3034/4339]: training_loss: tensor(0.1354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3035/4339]: training_loss: tensor(0.2016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3036/4339]: training_loss: tensor(0.1155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3037/4339]: training_loss: tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3038/4339]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3039/4339]: training_loss: tensor(0.1519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3040/4339]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3041/4339]: training_loss: tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3042/4339]: training_loss: tensor(0.0397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3043/4339]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3044/4339]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3045/4339]: training_loss: tensor(0.0384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3046/4339]: training_loss: tensor(0.1455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3047/4339]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3048/4339]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3049/4339]: training_loss: tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3050/4339]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3051/4339]: training_loss: tensor(0.0907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3052/4339]: training_loss: tensor(0.0691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3053/4339]: training_loss: tensor(0.4940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3054/4339]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3055/4339]: training_loss: tensor(0.2295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3056/4339]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3057/4339]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3058/4339]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3059/4339]: training_loss: tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3060/4339]: training_loss: tensor(0.0050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3061/4339]: training_loss: tensor(0.2598, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3062/4339]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3063/4339]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3064/4339]: training_loss: tensor(0.1788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3065/4339]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3066/4339]: training_loss: tensor(0.1885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3067/4339]: training_loss: tensor(0.0413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3068/4339]: training_loss: tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3069/4339]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3070/4339]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3071/4339]: training_loss: tensor(0.2898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3072/4339]: training_loss: tensor(0.2375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3073/4339]: training_loss: tensor(0.1890, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3074/4339]: training_loss: tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3075/4339]: training_loss: tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3076/4339]: training_loss: tensor(0.1131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3077/4339]: training_loss: tensor(0.1216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3078/4339]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3079/4339]: training_loss: tensor(0.0542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3080/4339]: training_loss: tensor(0.0229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3081/4339]: training_loss: tensor(0.1516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3082/4339]: training_loss: tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3083/4339]: training_loss: tensor(0.1911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3084/4339]: training_loss: tensor(0.2147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3085/4339]: training_loss: tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3086/4339]: training_loss: tensor(0.3908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3087/4339]: training_loss: tensor(0.1471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3088/4339]: training_loss: tensor(0.0216, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3089/4339]: training_loss: tensor(0.4169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3090/4339]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3091/4339]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3092/4339]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3093/4339]: training_loss: tensor(0.0828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3094/4339]: training_loss: tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3095/4339]: training_loss: tensor(0.1664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3096/4339]: training_loss: tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3097/4339]: training_loss: tensor(0.1210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3098/4339]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3099/4339]: training_loss: tensor(0.1134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3100/4339]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3101/4339]: training_loss: tensor(0.1888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3102/4339]: training_loss: tensor(0.0371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3103/4339]: training_loss: tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3104/4339]: training_loss: tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3105/4339]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3106/4339]: training_loss: tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3107/4339]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3108/4339]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3109/4339]: training_loss: tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3110/4339]: training_loss: tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3111/4339]: training_loss: tensor(0.1651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3112/4339]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3113/4339]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3114/4339]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3115/4339]: training_loss: tensor(0.1095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3116/4339]: training_loss: tensor(0.1360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3117/4339]: training_loss: tensor(0.2402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3118/4339]: training_loss: tensor(0.1988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3119/4339]: training_loss: tensor(0.2202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3120/4339]: training_loss: tensor(0.0695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3121/4339]: training_loss: tensor(0.1632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3122/4339]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3123/4339]: training_loss: tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3124/4339]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3125/4339]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3126/4339]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3127/4339]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3128/4339]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3129/4339]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3130/4339]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3131/4339]: training_loss: tensor(0.3331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3132/4339]: training_loss: tensor(0.0435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3133/4339]: training_loss: tensor(0.0602, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3134/4339]: training_loss: tensor(0.2464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3135/4339]: training_loss: tensor(0.3771, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3136/4339]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3137/4339]: training_loss: tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3138/4339]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3139/4339]: training_loss: tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3140/4339]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3141/4339]: training_loss: tensor(0.1095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3142/4339]: training_loss: tensor(0.0210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3143/4339]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3144/4339]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3145/4339]: training_loss: tensor(0.1516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3146/4339]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3147/4339]: training_loss: tensor(0.0836, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3148/4339]: training_loss: tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3149/4339]: training_loss: tensor(0.0698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3150/4339]: training_loss: tensor(0.2375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3151/4339]: training_loss: tensor(0.3068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3152/4339]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3153/4339]: training_loss: tensor(0.3131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3154/4339]: training_loss: tensor(0.1894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3155/4339]: training_loss: tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3156/4339]: training_loss: tensor(0.2785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3157/4339]: training_loss: tensor(0.1525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3158/4339]: training_loss: tensor(0.2086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3159/4339]: training_loss: tensor(0.1704, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3160/4339]: training_loss: tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3161/4339]: training_loss: tensor(0.1558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3162/4339]: training_loss: tensor(0.0389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3163/4339]: training_loss: tensor(0.1077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3164/4339]: training_loss: tensor(0.2110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3165/4339]: training_loss: tensor(0.2642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3166/4339]: training_loss: tensor(0.0959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3167/4339]: training_loss: tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3168/4339]: training_loss: tensor(0.1043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3169/4339]: training_loss: tensor(0.1197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3170/4339]: training_loss: tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3171/4339]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3172/4339]: training_loss: tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3173/4339]: training_loss: tensor(0.0781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3174/4339]: training_loss: tensor(0.1446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3175/4339]: training_loss: tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3176/4339]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3177/4339]: training_loss: tensor(0.1185, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3178/4339]: training_loss: tensor(0.2898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3179/4339]: training_loss: tensor(0.0914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3180/4339]: training_loss: tensor(0.0159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3181/4339]: training_loss: tensor(0.0187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3182/4339]: training_loss: tensor(0.0537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3183/4339]: training_loss: tensor(0.0325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3184/4339]: training_loss: tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3185/4339]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3186/4339]: training_loss: tensor(0.2022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3187/4339]: training_loss: tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3188/4339]: training_loss: tensor(0.1628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3189/4339]: training_loss: tensor(0.1243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3190/4339]: training_loss: tensor(0.1481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3191/4339]: training_loss: tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3192/4339]: training_loss: tensor(0.1211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3193/4339]: training_loss: tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3194/4339]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3195/4339]: training_loss: tensor(0.1619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3196/4339]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3197/4339]: training_loss: tensor(0.0396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3198/4339]: training_loss: tensor(0.0754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3199/4339]: training_loss: tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3200/4339]: training_loss: tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3201/4339]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3202/4339]: training_loss: tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3203/4339]: training_loss: tensor(0.2923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3204/4339]: training_loss: tensor(0.3888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3205/4339]: training_loss: tensor(0.0111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3206/4339]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3207/4339]: training_loss: tensor(0.2746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3208/4339]: training_loss: tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3209/4339]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3210/4339]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3211/4339]: training_loss: tensor(0.2754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3212/4339]: training_loss: tensor(0.0447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3213/4339]: training_loss: tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3214/4339]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3215/4339]: training_loss: tensor(0.1466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3216/4339]: training_loss: tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3217/4339]: training_loss: tensor(0.2369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3218/4339]: training_loss: tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3219/4339]: training_loss: tensor(0.1265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3220/4339]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3221/4339]: training_loss: tensor(0.1682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3222/4339]: training_loss: tensor(0.2409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3223/4339]: training_loss: tensor(0.2683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3224/4339]: training_loss: tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3225/4339]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3226/4339]: training_loss: tensor(0.0214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3227/4339]: training_loss: tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3228/4339]: training_loss: tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3229/4339]: training_loss: tensor(0.1870, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3230/4339]: training_loss: tensor(0.0039, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3231/4339]: training_loss: tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3232/4339]: training_loss: tensor(0.0292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3233/4339]: training_loss: tensor(0.2536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3234/4339]: training_loss: tensor(0.0424, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3235/4339]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3236/4339]: training_loss: tensor(0.3210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3237/4339]: training_loss: tensor(0.0588, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3238/4339]: training_loss: tensor(0.1469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3239/4339]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3240/4339]: training_loss: tensor(0.0669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3241/4339]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3242/4339]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3243/4339]: training_loss: tensor(0.0412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3244/4339]: training_loss: tensor(0.3882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3245/4339]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3246/4339]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3247/4339]: training_loss: tensor(0.0305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3248/4339]: training_loss: tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3249/4339]: training_loss: tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3250/4339]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3251/4339]: training_loss: tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3252/4339]: training_loss: tensor(0.0430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3253/4339]: training_loss: tensor(0.0078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3254/4339]: training_loss: tensor(0.1133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3255/4339]: training_loss: tensor(0.3075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3256/4339]: training_loss: tensor(0.0456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3257/4339]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3258/4339]: training_loss: tensor(0.2120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3259/4339]: training_loss: tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3260/4339]: training_loss: tensor(0.2760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3261/4339]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3262/4339]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3263/4339]: training_loss: tensor(0.0130, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3264/4339]: training_loss: tensor(0.0615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3265/4339]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3266/4339]: training_loss: tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3267/4339]: training_loss: tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3268/4339]: training_loss: tensor(0.0584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3269/4339]: training_loss: tensor(0.0970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3270/4339]: training_loss: tensor(0.0662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3271/4339]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3272/4339]: training_loss: tensor(0.2867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3273/4339]: training_loss: tensor(0.1625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3274/4339]: training_loss: tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3275/4339]: training_loss: tensor(0.1913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3276/4339]: training_loss: tensor(0.2698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3277/4339]: training_loss: tensor(0.0448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3278/4339]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3279/4339]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3280/4339]: training_loss: tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3281/4339]: training_loss: tensor(0.2295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3282/4339]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3283/4339]: training_loss: tensor(0.0212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3284/4339]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3285/4339]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3286/4339]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3287/4339]: training_loss: tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3288/4339]: training_loss: tensor(0.3229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3289/4339]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3290/4339]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3291/4339]: training_loss: tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3292/4339]: training_loss: tensor(0.1815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3293/4339]: training_loss: tensor(0.0461, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3294/4339]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3295/4339]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3296/4339]: training_loss: tensor(0.2549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3297/4339]: training_loss: tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3298/4339]: training_loss: tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3299/4339]: training_loss: tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3300/4339]: training_loss: tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3301/4339]: training_loss: tensor(0.0438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3302/4339]: training_loss: tensor(0.0249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3303/4339]: training_loss: tensor(0.1347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3304/4339]: training_loss: tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3305/4339]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3306/4339]: training_loss: tensor(0.1053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3307/4339]: training_loss: tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3308/4339]: training_loss: tensor(0.1531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3309/4339]: training_loss: tensor(0.2159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3310/4339]: training_loss: tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3311/4339]: training_loss: tensor(0.2085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3312/4339]: training_loss: tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3313/4339]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3314/4339]: training_loss: tensor(0.5704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3315/4339]: training_loss: tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3316/4339]: training_loss: tensor(0.2195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3317/4339]: training_loss: tensor(0.2166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3318/4339]: training_loss: tensor(0.2331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3319/4339]: training_loss: tensor(0.0416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3320/4339]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3321/4339]: training_loss: tensor(0.1651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3322/4339]: training_loss: tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3323/4339]: training_loss: tensor(0.3629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3324/4339]: training_loss: tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3325/4339]: training_loss: tensor(0.2076, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3326/4339]: training_loss: tensor(0.3384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3327/4339]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3328/4339]: training_loss: tensor(0.0321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3329/4339]: training_loss: tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3330/4339]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3331/4339]: training_loss: tensor(0.1193, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3332/4339]: training_loss: tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3333/4339]: training_loss: tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3334/4339]: training_loss: tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3335/4339]: training_loss: tensor(0.0957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3336/4339]: training_loss: tensor(0.1116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3337/4339]: training_loss: tensor(0.0122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3338/4339]: training_loss: tensor(0.0506, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3339/4339]: training_loss: tensor(0.0211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3340/4339]: training_loss: tensor(0.1612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3341/4339]: training_loss: tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3342/4339]: training_loss: tensor(0.1401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3343/4339]: training_loss: tensor(0.0890, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3344/4339]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3345/4339]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3346/4339]: training_loss: tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3347/4339]: training_loss: tensor(0.0606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3348/4339]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3349/4339]: training_loss: tensor(0.1314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3350/4339]: training_loss: tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3351/4339]: training_loss: tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3352/4339]: training_loss: tensor(0.1669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3353/4339]: training_loss: tensor(0.2753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3354/4339]: training_loss: tensor(0.0225, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3355/4339]: training_loss: tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3356/4339]: training_loss: tensor(0.1894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3357/4339]: training_loss: tensor(0.0471, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3358/4339]: training_loss: tensor(0.1152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3359/4339]: training_loss: tensor(0.0811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3360/4339]: training_loss: tensor(0.0521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3361/4339]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3362/4339]: training_loss: tensor(0.0245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3363/4339]: training_loss: tensor(0.0275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3364/4339]: training_loss: tensor(0.0263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3365/4339]: training_loss: tensor(0.1055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3366/4339]: training_loss: tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3367/4339]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3368/4339]: training_loss: tensor(0.1828, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3369/4339]: training_loss: tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3370/4339]: training_loss: tensor(0.2142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3371/4339]: training_loss: tensor(0.0656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3372/4339]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3373/4339]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3374/4339]: training_loss: tensor(0.0220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3375/4339]: training_loss: tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3376/4339]: training_loss: tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3377/4339]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3378/4339]: training_loss: tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3379/4339]: training_loss: tensor(0.3183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3380/4339]: training_loss: tensor(0.0384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3381/4339]: training_loss: tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3382/4339]: training_loss: tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3383/4339]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3384/4339]: training_loss: tensor(0.0273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3385/4339]: training_loss: tensor(0.1335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3386/4339]: training_loss: tensor(0.0097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3387/4339]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3388/4339]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3389/4339]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3390/4339]: training_loss: tensor(0.1438, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3391/4339]: training_loss: tensor(0.0492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3392/4339]: training_loss: tensor(0.3561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3393/4339]: training_loss: tensor(0.1971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3394/4339]: training_loss: tensor(0.0499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3395/4339]: training_loss: tensor(0.0346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3396/4339]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3397/4339]: training_loss: tensor(0.0189, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3398/4339]: training_loss: tensor(0.0235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3399/4339]: training_loss: tensor(0.1359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3400/4339]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3401/4339]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3402/4339]: training_loss: tensor(0.1177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3403/4339]: training_loss: tensor(0.1137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3404/4339]: training_loss: tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3405/4339]: training_loss: tensor(0.1640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3406/4339]: training_loss: tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3407/4339]: training_loss: tensor(0.1671, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3408/4339]: training_loss: tensor(0.1016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3409/4339]: training_loss: tensor(0.0748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3410/4339]: training_loss: tensor(0.0627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3411/4339]: training_loss: tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3412/4339]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3413/4339]: training_loss: tensor(0.3397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3414/4339]: training_loss: tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3415/4339]: training_loss: tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3416/4339]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3417/4339]: training_loss: tensor(0.1437, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3418/4339]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3419/4339]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3420/4339]: training_loss: tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3421/4339]: training_loss: tensor(0.0411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3422/4339]: training_loss: tensor(0.4697, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3423/4339]: training_loss: tensor(0.1623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3424/4339]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3425/4339]: training_loss: tensor(0.2740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3426/4339]: training_loss: tensor(0.1713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3427/4339]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3428/4339]: training_loss: tensor(0.0690, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3429/4339]: training_loss: tensor(0.2255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3430/4339]: training_loss: tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3431/4339]: training_loss: tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3432/4339]: training_loss: tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3433/4339]: training_loss: tensor(0.0420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3434/4339]: training_loss: tensor(0.0257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3435/4339]: training_loss: tensor(0.6346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3436/4339]: training_loss: tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3437/4339]: training_loss: tensor(0.0098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3438/4339]: training_loss: tensor(0.4098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3439/4339]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3440/4339]: training_loss: tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3441/4339]: training_loss: tensor(0.1111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3442/4339]: training_loss: tensor(0.0500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3443/4339]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3444/4339]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3445/4339]: training_loss: tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3446/4339]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3447/4339]: training_loss: tensor(0.0760, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3448/4339]: training_loss: tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3449/4339]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3450/4339]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3451/4339]: training_loss: tensor(0.1801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3452/4339]: training_loss: tensor(0.1232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3453/4339]: training_loss: tensor(0.0445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3454/4339]: training_loss: tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3455/4339]: training_loss: tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3456/4339]: training_loss: tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3457/4339]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3458/4339]: training_loss: tensor(0.0241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3459/4339]: training_loss: tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3460/4339]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3461/4339]: training_loss: tensor(0.1078, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3462/4339]: training_loss: tensor(0.2590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3463/4339]: training_loss: tensor(0.0785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3464/4339]: training_loss: tensor(0.1167, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3465/4339]: training_loss: tensor(0.0093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3466/4339]: training_loss: tensor(0.3513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3467/4339]: training_loss: tensor(0.3177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3468/4339]: training_loss: tensor(0.1142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3469/4339]: training_loss: tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3470/4339]: training_loss: tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3471/4339]: training_loss: tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3472/4339]: training_loss: tensor(0.0322, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3473/4339]: training_loss: tensor(0.0176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3474/4339]: training_loss: tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3475/4339]: training_loss: tensor(0.1171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3476/4339]: training_loss: tensor(0.0328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3477/4339]: training_loss: tensor(0.1242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3478/4339]: training_loss: tensor(0.1777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3479/4339]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3480/4339]: training_loss: tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3481/4339]: training_loss: tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3482/4339]: training_loss: tensor(0.0262, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3483/4339]: training_loss: tensor(0.0161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3484/4339]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3485/4339]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3486/4339]: training_loss: tensor(0.1884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3487/4339]: training_loss: tensor(0.0110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3488/4339]: training_loss: tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3489/4339]: training_loss: tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3490/4339]: training_loss: tensor(0.0206, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3491/4339]: training_loss: tensor(0.1977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3492/4339]: training_loss: tensor(0.2868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3493/4339]: training_loss: tensor(0.2008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3494/4339]: training_loss: tensor(0.3145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3495/4339]: training_loss: tensor(0.3551, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3496/4339]: training_loss: tensor(0.0495, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3497/4339]: training_loss: tensor(0.3734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3498/4339]: training_loss: tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3499/4339]: training_loss: tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3500/4339]: training_loss: tensor(0.0622, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3501/4339]: training_loss: tensor(0.0714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3502/4339]: training_loss: tensor(0.0165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3503/4339]: training_loss: tensor(0.2121, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3504/4339]: training_loss: tensor(0.1590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3505/4339]: training_loss: tensor(0.0239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3506/4339]: training_loss: tensor(0.0248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3507/4339]: training_loss: tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3508/4339]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3509/4339]: training_loss: tensor(0.0473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3510/4339]: training_loss: tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3511/4339]: training_loss: tensor(0.1895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3512/4339]: training_loss: tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3513/4339]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3514/4339]: training_loss: tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3515/4339]: training_loss: tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3516/4339]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3517/4339]: training_loss: tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3518/4339]: training_loss: tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3519/4339]: training_loss: tensor(0.0889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3520/4339]: training_loss: tensor(0.7102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3521/4339]: training_loss: tensor(0.2959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3522/4339]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3523/4339]: training_loss: tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3524/4339]: training_loss: tensor(0.0755, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3525/4339]: training_loss: tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3526/4339]: training_loss: tensor(0.1108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3527/4339]: training_loss: tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3528/4339]: training_loss: tensor(0.2021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3529/4339]: training_loss: tensor(0.0800, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3530/4339]: training_loss: tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3531/4339]: training_loss: tensor(0.1722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3532/4339]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3533/4339]: training_loss: tensor(0.0789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3534/4339]: training_loss: tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3535/4339]: training_loss: tensor(0.0742, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3536/4339]: training_loss: tensor(0.1095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3537/4339]: training_loss: tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3538/4339]: training_loss: tensor(0.1328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3539/4339]: training_loss: tensor(0.0327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3540/4339]: training_loss: tensor(0.1675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3541/4339]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3542/4339]: training_loss: tensor(0.3670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3543/4339]: training_loss: tensor(0.0399, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3544/4339]: training_loss: tensor(0.1561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3545/4339]: training_loss: tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3546/4339]: training_loss: tensor(0.0181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3547/4339]: training_loss: tensor(0.3270, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3548/4339]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3549/4339]: training_loss: tensor(0.0661, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3550/4339]: training_loss: tensor(0.3307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3551/4339]: training_loss: tensor(0.0129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3552/4339]: training_loss: tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3553/4339]: training_loss: tensor(0.3880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3554/4339]: training_loss: tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3555/4339]: training_loss: tensor(0.2978, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3556/4339]: training_loss: tensor(0.2654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3557/4339]: training_loss: tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3558/4339]: training_loss: tensor(0.2139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3559/4339]: training_loss: tensor(0.0418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3560/4339]: training_loss: tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3561/4339]: training_loss: tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3562/4339]: training_loss: tensor(0.4073, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3563/4339]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3564/4339]: training_loss: tensor(0.0390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3565/4339]: training_loss: tensor(0.0265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3566/4339]: training_loss: tensor(0.3918, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3567/4339]: training_loss: tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3568/4339]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3569/4339]: training_loss: tensor(0.0613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3570/4339]: training_loss: tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3571/4339]: training_loss: tensor(0.0353, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3572/4339]: training_loss: tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3573/4339]: training_loss: tensor(0.0153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3574/4339]: training_loss: tensor(0.1994, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3575/4339]: training_loss: tensor(0.1901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3576/4339]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3577/4339]: training_loss: tensor(0.2283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3578/4339]: training_loss: tensor(0.0314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3579/4339]: training_loss: tensor(0.3226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3580/4339]: training_loss: tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3581/4339]: training_loss: tensor(0.1783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3582/4339]: training_loss: tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3583/4339]: training_loss: tensor(0.0562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3584/4339]: training_loss: tensor(0.2269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3585/4339]: training_loss: tensor(0.0810, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3586/4339]: training_loss: tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3587/4339]: training_loss: tensor(0.3187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3588/4339]: training_loss: tensor(0.5875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3589/4339]: training_loss: tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3590/4339]: training_loss: tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3591/4339]: training_loss: tensor(0.1358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3592/4339]: training_loss: tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3593/4339]: training_loss: tensor(0.1763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3594/4339]: training_loss: tensor(0.1359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3595/4339]: training_loss: tensor(0.1807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3596/4339]: training_loss: tensor(0.1444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3597/4339]: training_loss: tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3598/4339]: training_loss: tensor(0.1575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3599/4339]: training_loss: tensor(0.2009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3600/4339]: training_loss: tensor(0.2657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3601/4339]: training_loss: tensor(0.0735, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3602/4339]: training_loss: tensor(0.1089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3603/4339]: training_loss: tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3604/4339]: training_loss: tensor(0.1309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3605/4339]: training_loss: tensor(0.1531, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3606/4339]: training_loss: tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3607/4339]: training_loss: tensor(0.0364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3608/4339]: training_loss: tensor(0.0601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3609/4339]: training_loss: tensor(0.0267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3610/4339]: training_loss: tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3611/4339]: training_loss: tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3612/4339]: training_loss: tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3613/4339]: training_loss: tensor(0.1150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3614/4339]: training_loss: tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3615/4339]: training_loss: tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3616/4339]: training_loss: tensor(0.1477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3617/4339]: training_loss: tensor(0.2194, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3618/4339]: training_loss: tensor(0.0590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3619/4339]: training_loss: tensor(0.0472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3620/4339]: training_loss: tensor(0.4444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3621/4339]: training_loss: tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3622/4339]: training_loss: tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3623/4339]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3624/4339]: training_loss: tensor(0.1767, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3625/4339]: training_loss: tensor(0.0919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3626/4339]: training_loss: tensor(0.1744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3627/4339]: training_loss: tensor(0.1000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3628/4339]: training_loss: tensor(0.0825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3629/4339]: training_loss: tensor(0.2377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3630/4339]: training_loss: tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3631/4339]: training_loss: tensor(0.0879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3632/4339]: training_loss: tensor(0.0401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3633/4339]: training_loss: tensor(0.0243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3634/4339]: training_loss: tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3635/4339]: training_loss: tensor(0.0384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3636/4339]: training_loss: tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3637/4339]: training_loss: tensor(0.0223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3638/4339]: training_loss: tensor(0.1358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3639/4339]: training_loss: tensor(0.2253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3640/4339]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3641/4339]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3642/4339]: training_loss: tensor(0.0367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3643/4339]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3644/4339]: training_loss: tensor(0.3575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3645/4339]: training_loss: tensor(0.3199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3646/4339]: training_loss: tensor(0.1238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3647/4339]: training_loss: tensor(0.0200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3648/4339]: training_loss: tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3649/4339]: training_loss: tensor(0.5134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3650/4339]: training_loss: tensor(0.0195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3651/4339]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3652/4339]: training_loss: tensor(0.4787, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3653/4339]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3654/4339]: training_loss: tensor(0.0042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3655/4339]: training_loss: tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3656/4339]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3657/4339]: training_loss: tensor(0.1352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3658/4339]: training_loss: tensor(0.0724, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3659/4339]: training_loss: tensor(0.2183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3660/4339]: training_loss: tensor(0.0173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3661/4339]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3662/4339]: training_loss: tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3663/4339]: training_loss: tensor(0.4200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3664/4339]: training_loss: tensor(0.4067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3665/4339]: training_loss: tensor(0.0523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3666/4339]: training_loss: tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3667/4339]: training_loss: tensor(0.0650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3668/4339]: training_loss: tensor(0.1300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3669/4339]: training_loss: tensor(0.7202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3670/4339]: training_loss: tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3671/4339]: training_loss: tensor(0.0166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3672/4339]: training_loss: tensor(0.0615, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3673/4339]: training_loss: tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3674/4339]: training_loss: tensor(0.1986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3675/4339]: training_loss: tensor(0.3305, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3676/4339]: training_loss: tensor(0.3593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3677/4339]: training_loss: tensor(0.0116, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3678/4339]: training_loss: tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3679/4339]: training_loss: tensor(0.1941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3680/4339]: training_loss: tensor(0.0389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3681/4339]: training_loss: tensor(0.1967, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3682/4339]: training_loss: tensor(0.0213, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3683/4339]: training_loss: tensor(0.0501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3684/4339]: training_loss: tensor(0.4395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3685/4339]: training_loss: tensor(0.1843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3686/4339]: training_loss: tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3687/4339]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3688/4339]: training_loss: tensor(0.0856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3689/4339]: training_loss: tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3690/4339]: training_loss: tensor(0.3935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3691/4339]: training_loss: tensor(0.0802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3692/4339]: training_loss: tensor(0.0284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3693/4339]: training_loss: tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3694/4339]: training_loss: tensor(0.0254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3695/4339]: training_loss: tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3696/4339]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3697/4339]: training_loss: tensor(0.0814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3698/4339]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3699/4339]: training_loss: tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3700/4339]: training_loss: tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3701/4339]: training_loss: tensor(0.0937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3702/4339]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3703/4339]: training_loss: tensor(0.0147, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3704/4339]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3705/4339]: training_loss: tensor(0.1001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3706/4339]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3707/4339]: training_loss: tensor(0.0264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3708/4339]: training_loss: tensor(0.1188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3709/4339]: training_loss: tensor(0.1211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3710/4339]: training_loss: tensor(0.0583, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3711/4339]: training_loss: tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3712/4339]: training_loss: tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3713/4339]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3714/4339]: training_loss: tensor(0.2705, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3715/4339]: training_loss: tensor(0.2295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3716/4339]: training_loss: tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3717/4339]: training_loss: tensor(0.2347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3718/4339]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3719/4339]: training_loss: tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3720/4339]: training_loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3721/4339]: training_loss: tensor(0.1448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3722/4339]: training_loss: tensor(0.2313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3723/4339]: training_loss: tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3724/4339]: training_loss: tensor(0.3829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3725/4339]: training_loss: tensor(0.1366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3726/4339]: training_loss: tensor(0.0465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3727/4339]: training_loss: tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3728/4339]: training_loss: tensor(0.0446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3729/4339]: training_loss: tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3730/4339]: training_loss: tensor(0.0466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3731/4339]: training_loss: tensor(0.3886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3732/4339]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3733/4339]: training_loss: tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3734/4339]: training_loss: tensor(0.1144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3735/4339]: training_loss: tensor(0.0413, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3736/4339]: training_loss: tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3737/4339]: training_loss: tensor(0.1362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3738/4339]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3739/4339]: training_loss: tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3740/4339]: training_loss: tensor(0.5043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3741/4339]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3742/4339]: training_loss: tensor(0.0177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3743/4339]: training_loss: tensor(0.0414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3744/4339]: training_loss: tensor(0.2882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3745/4339]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3746/4339]: training_loss: tensor(0.3701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3747/4339]: training_loss: tensor(0.0046, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3748/4339]: training_loss: tensor(0.1527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3749/4339]: training_loss: tensor(0.2157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3750/4339]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3751/4339]: training_loss: tensor(0.3310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3752/4339]: training_loss: tensor(0.0460, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3753/4339]: training_loss: tensor(0.2792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3754/4339]: training_loss: tensor(0.2734, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3755/4339]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3756/4339]: training_loss: tensor(0.5055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3757/4339]: training_loss: tensor(0.0217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3758/4339]: training_loss: tensor(0.1061, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3759/4339]: training_loss: tensor(0.2777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3760/4339]: training_loss: tensor(0.1601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3761/4339]: training_loss: tensor(0.1285, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3762/4339]: training_loss: tensor(0.2916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3763/4339]: training_loss: tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3764/4339]: training_loss: tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3765/4339]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3766/4339]: training_loss: tensor(0.4851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3767/4339]: training_loss: tensor(0.1350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3768/4339]: training_loss: tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3769/4339]: training_loss: tensor(0.0293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3770/4339]: training_loss: tensor(0.0291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3771/4339]: training_loss: tensor(0.0431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3772/4339]: training_loss: tensor(0.0308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3773/4339]: training_loss: tensor(0.1441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3774/4339]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3775/4339]: training_loss: tensor(0.0240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3776/4339]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3777/4339]: training_loss: tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3778/4339]: training_loss: tensor(0.0318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3779/4339]: training_loss: tensor(0.4868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3780/4339]: training_loss: tensor(0.0863, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3781/4339]: training_loss: tensor(0.1518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3782/4339]: training_loss: tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3783/4339]: training_loss: tensor(0.0609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3784/4339]: training_loss: tensor(0.0422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3785/4339]: training_loss: tensor(0.1250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3786/4339]: training_loss: tensor(0.0242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3787/4339]: training_loss: tensor(0.1137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3788/4339]: training_loss: tensor(0.0421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3789/4339]: training_loss: tensor(0.0168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3790/4339]: training_loss: tensor(0.0247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3791/4339]: training_loss: tensor(0.2572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3792/4339]: training_loss: tensor(0.3208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3793/4339]: training_loss: tensor(0.2684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3794/4339]: training_loss: tensor(0.4049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3795/4339]: training_loss: tensor(0.2631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3796/4339]: training_loss: tensor(0.2846, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3797/4339]: training_loss: tensor(0.0685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3798/4339]: training_loss: tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3799/4339]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3800/4339]: training_loss: tensor(0.2247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3801/4339]: training_loss: tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3802/4339]: training_loss: tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3803/4339]: training_loss: tensor(0.4905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3804/4339]: training_loss: tensor(0.2082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3805/4339]: training_loss: tensor(0.5340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3806/4339]: training_loss: tensor(0.3025, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3807/4339]: training_loss: tensor(0.0780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3808/4339]: training_loss: tensor(0.3082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3809/4339]: training_loss: tensor(0.0404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3810/4339]: training_loss: tensor(0.1679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3811/4339]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3812/4339]: training_loss: tensor(0.3609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3813/4339]: training_loss: tensor(0.3284, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3814/4339]: training_loss: tensor(0.0403, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3815/4339]: training_loss: tensor(0.0943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3816/4339]: training_loss: tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3817/4339]: training_loss: tensor(0.1986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3818/4339]: training_loss: tensor(0.0986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3819/4339]: training_loss: tensor(0.0774, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3820/4339]: training_loss: tensor(0.2789, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3821/4339]: training_loss: tensor(0.0807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3822/4339]: training_loss: tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3823/4339]: training_loss: tensor(0.0709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3824/4339]: training_loss: tensor(0.1749, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3825/4339]: training_loss: tensor(0.3183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3826/4339]: training_loss: tensor(0.1983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3827/4339]: training_loss: tensor(0.0409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3828/4339]: training_loss: tensor(0.2252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3829/4339]: training_loss: tensor(0.5019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3830/4339]: training_loss: tensor(0.0958, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3831/4339]: training_loss: tensor(0.0746, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3832/4339]: training_loss: tensor(0.0288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3833/4339]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3834/4339]: training_loss: tensor(0.4182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3835/4339]: training_loss: tensor(0.0578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3836/4339]: training_loss: tensor(0.1688, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3837/4339]: training_loss: tensor(0.5300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3838/4339]: training_loss: tensor(0.3245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3839/4339]: training_loss: tensor(0.5279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3840/4339]: training_loss: tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3841/4339]: training_loss: tensor(0.0205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3842/4339]: training_loss: tensor(0.2170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3843/4339]: training_loss: tensor(0.2650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3844/4339]: training_loss: tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3845/4339]: training_loss: tensor(0.0713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3846/4339]: training_loss: tensor(0.1915, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3847/4339]: training_loss: tensor(0.0580, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3848/4339]: training_loss: tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3849/4339]: training_loss: tensor(0.2914, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3850/4339]: training_loss: tensor(0.1528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3851/4339]: training_loss: tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3852/4339]: training_loss: tensor(0.3249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3853/4339]: training_loss: tensor(0.0711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3854/4339]: training_loss: tensor(0.1861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3855/4339]: training_loss: tensor(0.1066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3856/4339]: training_loss: tensor(0.1239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3857/4339]: training_loss: tensor(0.1468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3858/4339]: training_loss: tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3859/4339]: training_loss: tensor(0.0932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3860/4339]: training_loss: tensor(0.1334, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3861/4339]: training_loss: tensor(0.0342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3862/4339]: training_loss: tensor(0.1864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3863/4339]: training_loss: tensor(0.0299, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3864/4339]: training_loss: tensor(0.2565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3865/4339]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3866/4339]: training_loss: tensor(0.0468, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3867/4339]: training_loss: tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3868/4339]: training_loss: tensor(0.0134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3869/4339]: training_loss: tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3870/4339]: training_loss: tensor(0.3479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3871/4339]: training_loss: tensor(0.2066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3872/4339]: training_loss: tensor(0.0351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3873/4339]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3874/4339]: training_loss: tensor(0.0066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3875/4339]: training_loss: tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3876/4339]: training_loss: tensor(0.0552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3877/4339]: training_loss: tensor(0.2605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3878/4339]: training_loss: tensor(0.2388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3879/4339]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3880/4339]: training_loss: tensor(0.3599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3881/4339]: training_loss: tensor(0.1251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3882/4339]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3883/4339]: training_loss: tensor(0.6006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3884/4339]: training_loss: tensor(0.3369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3885/4339]: training_loss: tensor(0.0281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3886/4339]: training_loss: tensor(0.0164, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3887/4339]: training_loss: tensor(0.2020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3888/4339]: training_loss: tensor(0.2411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3889/4339]: training_loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3890/4339]: training_loss: tensor(0.2972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3891/4339]: training_loss: tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3892/4339]: training_loss: tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3893/4339]: training_loss: tensor(0.2569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3894/4339]: training_loss: tensor(0.2785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3895/4339]: training_loss: tensor(0.0620, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3896/4339]: training_loss: tensor(0.6634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3897/4339]: training_loss: tensor(0.0237, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3898/4339]: training_loss: tensor(0.1993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3899/4339]: training_loss: tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3900/4339]: training_loss: tensor(0.1415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3901/4339]: training_loss: tensor(0.2614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3902/4339]: training_loss: tensor(0.0535, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3903/4339]: training_loss: tensor(0.0574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3904/4339]: training_loss: tensor(0.0616, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3905/4339]: training_loss: tensor(0.0885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3906/4339]: training_loss: tensor(0.1601, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3907/4339]: training_loss: tensor(0.1510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3908/4339]: training_loss: tensor(0.0158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3909/4339]: training_loss: tensor(0.0238, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3910/4339]: training_loss: tensor(0.2858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3911/4339]: training_loss: tensor(0.1114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3912/4339]: training_loss: tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3913/4339]: training_loss: tensor(0.1606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3914/4339]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3915/4339]: training_loss: tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3916/4339]: training_loss: tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3917/4339]: training_loss: tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3918/4339]: training_loss: tensor(0.0227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3919/4339]: training_loss: tensor(0.2397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3920/4339]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3921/4339]: training_loss: tensor(0.3043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3922/4339]: training_loss: tensor(0.2358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3923/4339]: training_loss: tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3924/4339]: training_loss: tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3925/4339]: training_loss: tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3926/4339]: training_loss: tensor(0.1430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3927/4339]: training_loss: tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3928/4339]: training_loss: tensor(0.1071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3929/4339]: training_loss: tensor(0.4188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3930/4339]: training_loss: tensor(0.0251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3931/4339]: training_loss: tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3932/4339]: training_loss: tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3933/4339]: training_loss: tensor(0.3193, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [3934/4339]: training_loss: tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3935/4339]: training_loss: tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3936/4339]: training_loss: tensor(0.1489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3937/4339]: training_loss: tensor(0.0594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3938/4339]: training_loss: tensor(0.0309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3939/4339]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3940/4339]: training_loss: tensor(0.0313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3941/4339]: training_loss: tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3942/4339]: training_loss: tensor(0.0144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3943/4339]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3944/4339]: training_loss: tensor(0.0266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3945/4339]: training_loss: tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3946/4339]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3947/4339]: training_loss: tensor(0.1969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3948/4339]: training_loss: tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3949/4339]: training_loss: tensor(0.0335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3950/4339]: training_loss: tensor(0.0349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3951/4339]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3952/4339]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3953/4339]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3954/4339]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3955/4339]: training_loss: tensor(0.0131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3956/4339]: training_loss: tensor(0.0068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3957/4339]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3958/4339]: training_loss: tensor(0.0456, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3959/4339]: training_loss: tensor(0.0128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3960/4339]: training_loss: tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3961/4339]: training_loss: tensor(0.0830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3962/4339]: training_loss: tensor(0.2375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3963/4339]: training_loss: tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3964/4339]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3965/4339]: training_loss: tensor(0.0476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3966/4339]: training_loss: tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3967/4339]: training_loss: tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3968/4339]: training_loss: tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3969/4339]: training_loss: tensor(0.3158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3970/4339]: training_loss: tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3971/4339]: training_loss: tensor(0.3127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3972/4339]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3973/4339]: training_loss: tensor(0.0156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3974/4339]: training_loss: tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3975/4339]: training_loss: tensor(0.0138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3976/4339]: training_loss: tensor(0.2219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3977/4339]: training_loss: tensor(0.2005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3978/4339]: training_loss: tensor(0.5128, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3979/4339]: training_loss: tensor(0.0083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3980/4339]: training_loss: tensor(0.1340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3981/4339]: training_loss: tensor(0.0105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3982/4339]: training_loss: tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3983/4339]: training_loss: tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3984/4339]: training_loss: tensor(0.0815, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3985/4339]: training_loss: tensor(0.0302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3986/4339]: training_loss: tensor(0.2570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3987/4339]: training_loss: tensor(0.1367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3988/4339]: training_loss: tensor(0.2121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3989/4339]: training_loss: tensor(0.1790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3990/4339]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3991/4339]: training_loss: tensor(0.0188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3992/4339]: training_loss: tensor(0.0670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3993/4339]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3994/4339]: training_loss: tensor(0.2362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3995/4339]: training_loss: tensor(0.2187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3996/4339]: training_loss: tensor(0.1738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3997/4339]: training_loss: tensor(0.0170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3998/4339]: training_loss: tensor(0.0361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [3999/4339]: training_loss: tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4000/4339]: training_loss: tensor(0.2157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4001/4339]: training_loss: tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4002/4339]: training_loss: tensor(0.0459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4003/4339]: training_loss: tensor(0.1263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4004/4339]: training_loss: tensor(0.1926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4005/4339]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4006/4339]: training_loss: tensor(0.3970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4007/4339]: training_loss: tensor(0.1987, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4008/4339]: training_loss: tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4009/4339]: training_loss: tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4010/4339]: training_loss: tensor(0.1199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4011/4339]: training_loss: tensor(0.0198, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4012/4339]: training_loss: tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4013/4339]: training_loss: tensor(0.1417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4014/4339]: training_loss: tensor(0.0126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4015/4339]: training_loss: tensor(0.1307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4016/4339]: training_loss: tensor(0.1187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4017/4339]: training_loss: tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4018/4339]: training_loss: tensor(0.0498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4019/4339]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4020/4339]: training_loss: tensor(0.0842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4021/4339]: training_loss: tensor(0.1646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4022/4339]: training_loss: tensor(0.1684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4023/4339]: training_loss: tensor(0.0199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4024/4339]: training_loss: tensor(0.1903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4025/4339]: training_loss: tensor(0.5286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4026/4339]: training_loss: tensor(0.1110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4027/4339]: training_loss: tensor(0.0942, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4028/4339]: training_loss: tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4029/4339]: training_loss: tensor(0.1341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4030/4339]: training_loss: tensor(0.0788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4031/4339]: training_loss: tensor(0.2474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4032/4339]: training_loss: tensor(0.1594, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4033/4339]: training_loss: tensor(0.2872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4034/4339]: training_loss: tensor(0.0404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4035/4339]: training_loss: tensor(0.3441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4036/4339]: training_loss: tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4037/4339]: training_loss: tensor(0.5365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4038/4339]: training_loss: tensor(0.1180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4039/4339]: training_loss: tensor(0.0759, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4040/4339]: training_loss: tensor(0.0520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4041/4339]: training_loss: tensor(0.1627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4042/4339]: training_loss: tensor(0.0359, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4043/4339]: training_loss: tensor(0.1727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4044/4339]: training_loss: tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4045/4339]: training_loss: tensor(0.3790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4046/4339]: training_loss: tensor(0.2555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4047/4339]: training_loss: tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4048/4339]: training_loss: tensor(0.4086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4049/4339]: training_loss: tensor(0.2161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4050/4339]: training_loss: tensor(0.0470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4051/4339]: training_loss: tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4052/4339]: training_loss: tensor(0.1343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4053/4339]: training_loss: tensor(0.3768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4054/4339]: training_loss: tensor(0.0570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4055/4339]: training_loss: tensor(0.0729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4056/4339]: training_loss: tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4057/4339]: training_loss: tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4058/4339]: training_loss: tensor(0.0526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4059/4339]: training_loss: tensor(0.0415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4060/4339]: training_loss: tensor(0.0150, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4061/4339]: training_loss: tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4062/4339]: training_loss: tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4063/4339]: training_loss: tensor(0.1715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4064/4339]: training_loss: tensor(0.0682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4065/4339]: training_loss: tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4066/4339]: training_loss: tensor(0.0326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4067/4339]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4068/4339]: training_loss: tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4069/4339]: training_loss: tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4070/4339]: training_loss: tensor(0.2988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4071/4339]: training_loss: tensor(0.0626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4072/4339]: training_loss: tensor(0.2293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4073/4339]: training_loss: tensor(0.0060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4074/4339]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4075/4339]: training_loss: tensor(0.1093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4076/4339]: training_loss: tensor(0.3021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4077/4339]: training_loss: tensor(0.0139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4078/4339]: training_loss: tensor(0.0587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4079/4339]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4080/4339]: training_loss: tensor(0.0375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4081/4339]: training_loss: tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4082/4339]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4083/4339]: training_loss: tensor(0.2581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4084/4339]: training_loss: tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4085/4339]: training_loss: tensor(0.1226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4086/4339]: training_loss: tensor(0.0258, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4087/4339]: training_loss: tensor(0.1203, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4088/4339]: training_loss: tensor(0.1830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4089/4339]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4090/4339]: training_loss: tensor(0.4708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4091/4339]: training_loss: tensor(0.0290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4092/4339]: training_loss: tensor(0.0163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4093/4339]: training_loss: tensor(0.2234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4094/4339]: training_loss: tensor(0.2507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4095/4339]: training_loss: tensor(0.0519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4096/4339]: training_loss: tensor(0.1858, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4097/4339]: training_loss: tensor(0.2469, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4098/4339]: training_loss: tensor(0.0135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4099/4339]: training_loss: tensor(0.1376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4100/4339]: training_loss: tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4101/4339]: training_loss: tensor(0.3100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4102/4339]: training_loss: tensor(0.2030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4103/4339]: training_loss: tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4104/4339]: training_loss: tensor(0.0100, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4105/4339]: training_loss: tensor(0.3687, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4106/4339]: training_loss: tensor(0.3559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4107/4339]: training_loss: tensor(0.2443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4108/4339]: training_loss: tensor(0.4042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4109/4339]: training_loss: tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4110/4339]: training_loss: tensor(0.1486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4111/4339]: training_loss: tensor(0.0345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4112/4339]: training_loss: tensor(0.0896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4113/4339]: training_loss: tensor(0.3522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4114/4339]: training_loss: tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4115/4339]: training_loss: tensor(0.1794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4116/4339]: training_loss: tensor(0.0408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4117/4339]: training_loss: tensor(0.1564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4118/4339]: training_loss: tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4119/4339]: training_loss: tensor(0.1441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4120/4339]: training_loss: tensor(0.1523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4121/4339]: training_loss: tensor(0.1976, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4122/4339]: training_loss: tensor(0.0587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4123/4339]: training_loss: tensor(0.1303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4124/4339]: training_loss: tensor(0.1500, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4125/4339]: training_loss: tensor(0.0549, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4126/4339]: training_loss: tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4127/4339]: training_loss: tensor(0.0518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4128/4339]: training_loss: tensor(0.0378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4129/4339]: training_loss: tensor(0.2539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4130/4339]: training_loss: tensor(0.1785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4131/4339]: training_loss: tensor(0.0123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4132/4339]: training_loss: tensor(0.0631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4133/4339]: training_loss: tensor(0.4484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4134/4339]: training_loss: tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4135/4339]: training_loss: tensor(0.2014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4136/4339]: training_loss: tensor(0.2121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4137/4339]: training_loss: tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4138/4339]: training_loss: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4139/4339]: training_loss: tensor(0.1992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4140/4339]: training_loss: tensor(0.0297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4141/4339]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4142/4339]: training_loss: tensor(0.1423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4143/4339]: training_loss: tensor(0.0646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4144/4339]: training_loss: tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4145/4339]: training_loss: tensor(0.1170, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4146/4339]: training_loss: tensor(0.1658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4147/4339]: training_loss: tensor(0.2589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4148/4339]: training_loss: tensor(0.1957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4149/4339]: training_loss: tensor(0.1875, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4150/4339]: training_loss: tensor(0.0340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4151/4339]: training_loss: tensor(0.1587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4152/4339]: training_loss: tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4153/4339]: training_loss: tensor(0.1608, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4154/4339]: training_loss: tensor(0.0260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4155/4339]: training_loss: tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4156/4339]: training_loss: tensor(0.5338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4157/4339]: training_loss: tensor(0.1021, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4158/4339]: training_loss: tensor(0.0337, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4159/4339]: training_loss: tensor(0.2466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4160/4339]: training_loss: tensor(0.0541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4161/4339]: training_loss: tensor(0.0152, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4162/4339]: training_loss: tensor(0.1183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4163/4339]: training_loss: tensor(0.0638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4164/4339]: training_loss: tensor(0.4619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4165/4339]: training_loss: tensor(0.0826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4166/4339]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4167/4339]: training_loss: tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4168/4339]: training_loss: tensor(0.2782, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4169/4339]: training_loss: tensor(0.1134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4170/4339]: training_loss: tensor(0.2333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4171/4339]: training_loss: tensor(0.0369, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4172/4339]: training_loss: tensor(0.0779, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4173/4339]: training_loss: tensor(0.3168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4174/4339]: training_loss: tensor(0.0490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4175/4339]: training_loss: tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4176/4339]: training_loss: tensor(0.1019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4177/4339]: training_loss: tensor(0.2031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4178/4339]: training_loss: tensor(0.2052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4179/4339]: training_loss: tensor(0.2209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4180/4339]: training_loss: tensor(0.2911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4181/4339]: training_loss: tensor(0.1618, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4182/4339]: training_loss: tensor(0.2453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4183/4339]: training_loss: tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4184/4339]: training_loss: tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4185/4339]: training_loss: tensor(0.2370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4186/4339]: training_loss: tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4187/4339]: training_loss: tensor(0.0577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4188/4339]: training_loss: tensor(0.0564, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4189/4339]: training_loss: tensor(0.0417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4190/4339]: training_loss: tensor(0.1921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4191/4339]: training_loss: tensor(0.2231, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4192/4339]: training_loss: tensor(0.1453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4193/4339]: training_loss: tensor(0.1320, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4194/4339]: training_loss: tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4195/4339]: training_loss: tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4196/4339]: training_loss: tensor(0.0477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4197/4339]: training_loss: tensor(0.2250, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4198/4339]: training_loss: tensor(0.0443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4199/4339]: training_loss: tensor(0.2522, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4200/4339]: training_loss: tensor(0.0307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4201/4339]: training_loss: tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4202/4339]: training_loss: tensor(0.0442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4203/4339]: training_loss: tensor(0.0745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4204/4339]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4205/4339]: training_loss: tensor(0.2162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4206/4339]: training_loss: tensor(0.0115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4207/4339]: training_loss: tensor(0.3639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4208/4339]: training_loss: tensor(0.0226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4209/4339]: training_loss: tensor(0.0381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4210/4339]: training_loss: tensor(0.3514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4211/4339]: training_loss: tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4212/4339]: training_loss: tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4213/4339]: training_loss: tensor(0.2693, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4214/4339]: training_loss: tensor(0.0184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4215/4339]: training_loss: tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4216/4339]: training_loss: tensor(0.0160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4217/4339]: training_loss: tensor(0.1467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4218/4339]: training_loss: tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4219/4339]: training_loss: tensor(0.0819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4220/4339]: training_loss: tensor(0.0155, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4221/4339]: training_loss: tensor(0.0395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4222/4339]: training_loss: tensor(0.1473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4223/4339]: training_loss: tensor(0.1990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4224/4339]: training_loss: tensor(0.2776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4225/4339]: training_loss: tensor(0.3788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4226/4339]: training_loss: tensor(0.0157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4227/4339]: training_loss: tensor(0.0675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4228/4339]: training_loss: tensor(0.3047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4229/4339]: training_loss: tensor(0.2430, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4230/4339]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4231/4339]: training_loss: tensor(0.1486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4232/4339]: training_loss: tensor(0.1123, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4233/4339]: training_loss: tensor(0.1704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4234/4339]: training_loss: tensor(0.0977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4235/4339]: training_loss: tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4236/4339]: training_loss: tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4237/4339]: training_loss: tensor(0.0701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4238/4339]: training_loss: tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4239/4339]: training_loss: tensor(0.2231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4240/4339]: training_loss: tensor(0.0740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4241/4339]: training_loss: tensor(0.1610, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4242/4339]: training_loss: tensor(0.3965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4243/4339]: training_loss: tensor(0.0953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4244/4339]: training_loss: tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4245/4339]: training_loss: tensor(0.1210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4246/4339]: training_loss: tensor(0.3245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4247/4339]: training_loss: tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4248/4339]: training_loss: tensor(0.4220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4249/4339]: training_loss: tensor(0.0191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4250/4339]: training_loss: tensor(0.0857, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4251/4339]: training_loss: tensor(0.2017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4252/4339]: training_loss: tensor(0.3702, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4253/4339]: training_loss: tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4254/4339]: training_loss: tensor(0.2088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4255/4339]: training_loss: tensor(0.5390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4256/4339]: training_loss: tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4257/4339]: training_loss: tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4258/4339]: training_loss: tensor(0.0822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4259/4339]: training_loss: tensor(0.2459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4260/4339]: training_loss: tensor(0.4196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4261/4339]: training_loss: tensor(0.2822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4262/4339]: training_loss: tensor(0.4110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4263/4339]: training_loss: tensor(0.3131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4264/4339]: training_loss: tensor(0.3569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4265/4339]: training_loss: tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4266/4339]: training_loss: tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4267/4339]: training_loss: tensor(0.1348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4268/4339]: training_loss: tensor(0.1694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4269/4339]: training_loss: tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4270/4339]: training_loss: tensor(0.2778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4271/4339]: training_loss: tensor(0.2543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4272/4339]: training_loss: tensor(0.0444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4273/4339]: training_loss: tensor(0.1519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4274/4339]: training_loss: tensor(0.0385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4275/4339]: training_loss: tensor(0.2812, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4276/4339]: training_loss: tensor(0.2036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4277/4339]: training_loss: tensor(0.1482, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no [4278/4339]: training_loss: tensor(0.1951, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4279/4339]: training_loss: tensor(0.2144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4280/4339]: training_loss: tensor(0.0606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4281/4339]: training_loss: tensor(0.1292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4282/4339]: training_loss: tensor(0.1530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4283/4339]: training_loss: tensor(0.1784, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4284/4339]: training_loss: tensor(0.1775, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4285/4339]: training_loss: tensor(0.0222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4286/4339]: training_loss: tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4287/4339]: training_loss: tensor(0.1173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4288/4339]: training_loss: tensor(0.0236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4289/4339]: training_loss: tensor(0.2057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4290/4339]: training_loss: tensor(0.0178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4291/4339]: training_loss: tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4292/4339]: training_loss: tensor(0.1925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4293/4339]: training_loss: tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4294/4339]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4295/4339]: training_loss: tensor(0.0255, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4296/4339]: training_loss: tensor(0.1199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4297/4339]: training_loss: tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4298/4339]: training_loss: tensor(0.0108, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4299/4339]: training_loss: tensor(0.1764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4300/4339]: training_loss: tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4301/4339]: training_loss: tensor(0.0720, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4302/4339]: training_loss: tensor(0.4448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4303/4339]: training_loss: tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4304/4339]: training_loss: tensor(0.0404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4305/4339]: training_loss: tensor(0.0136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4306/4339]: training_loss: tensor(0.1432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4307/4339]: training_loss: tensor(0.2126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4308/4339]: training_loss: tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4309/4339]: training_loss: tensor(0.1422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4310/4339]: training_loss: tensor(0.2652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4311/4339]: training_loss: tensor(0.2826, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4312/4339]: training_loss: tensor(0.1501, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4313/4339]: training_loss: tensor(0.0329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4314/4339]: training_loss: tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4315/4339]: training_loss: tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4316/4339]: training_loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4317/4339]: training_loss: tensor(0.4497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4318/4339]: training_loss: tensor(0.0373, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4319/4339]: training_loss: tensor(0.0553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4320/4339]: training_loss: tensor(0.0654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4321/4339]: training_loss: tensor(0.4889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4322/4339]: training_loss: tensor(0.2002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4323/4339]: training_loss: tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4324/4339]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4325/4339]: training_loss: tensor(0.2071, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4326/4339]: training_loss: tensor(0.3277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4327/4339]: training_loss: tensor(0.3748, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4328/4339]: training_loss: tensor(0.0704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4329/4339]: training_loss: tensor(0.0630, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4330/4339]: training_loss: tensor(0.3219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4331/4339]: training_loss: tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4332/4339]: training_loss: tensor(0.1205, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4333/4339]: training_loss: tensor(0.0853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4334/4339]: training_loss: tensor(0.0632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4335/4339]: training_loss: tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4336/4339]: training_loss: tensor(0.0319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4337/4339]: training_loss: tensor(0.4868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4338/4339]: training_loss: tensor(0.2367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4339/4339]: training_loss: tensor(0.2349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "batch_no [4340/4339]: training_loss: tensor(0.0374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch [5/5], Step [21700/21700], Train Loss: 0.1076, Valid Loss: 0.5068\n",
      "Model saved to ==> Model/metrics.pt\n",
      "Finished Training!\n"
     ]
    }
   ],
   "source": [
    "model = BERT().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "train(model=model, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "2JaIsEttvMeO",
    "outputId": "abaab65e-2aeb-47a3-fd2e-68afb4817bed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from <== Model/metrics.pt\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEGCAYAAACdJRn3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3xUVfrH8c+T3kMSQksIoSM1QOhFbKtiAVGaDStiXVZd265l97dF145dUbGgFDtKsYEUEQi9d5DQCZAESCDl/P64EwgxCUmYmzuZed6vV16ZuXPvnSeTZL5zz7n3HDHGoJRSSlWGn9MFKKWUqnk0PJRSSlWahodSSqlK0/BQSilVaRoeSimlKi3A6QIqq3bt2iY5OdnpMpRSqkZZvHjxAWNMvLv2V+PCIzk5mbS0NKfLUEqpGkVEtrtzf9pspZRSqtI0PJRSSlWahodSSqlKq3F9HqXJy8sjPT2d3Nxcp0upFiEhISQmJhIYGOh0KUopH+UV4ZGenk5kZCTJycmIiNPl2MoYQ0ZGBunp6TRu3NjpcpRSPsormq1yc3OJi4vz+uAAEBHi4uJ85ihLKeWZvCI8AJ8IjiK+9LMqpTyT14SHUkrZ4vffYOdip6vwOBoebpCRkUFKSgopKSnUq1ePhISEk/dPnDhR7rZpaWncd9991VSpUqpScrNg/BAYdwXsX+90NR7FKzrMnRYXF8eyZcsAeOqpp4iIiODBBx88+Xh+fj4BAaW/1KmpqaSmplZLnUqpSlr8PhzPhOBomHg93P4zBEc6XZVH0CMPm9x0003cf//9nHfeeTz88MMsXLiQnj170rFjR3r27Mn69danmFmzZnH55ZcDVvDccsst9OvXjyZNmjBmzBgnfwSlfFteLsx/DZr0g6EfQcYm+OY+0NlXAS888vjHlNWs2ZXl1n22bhDFk1e0qfR2GzZs4Mcff8Tf35+srCxmz55NQEAAP/74I4899hiff/75H7ZZt24dM2fOJDs7m5YtW3LnnXfq9RxKOWH5p3BkLwx6G5qcCxc8AT8+BQ27QfdRTlfnOK8LD08yePBg/P39AcjMzGTEiBFs3LgRESEvL6/UbS677DKCg4MJDg6mTp067N27l8TExOosWylVWADzXoYGnaDxudayXqNhxyL4/m/QoCMkdXO2Rod5XXhU5QjBLuHh4SdvP/7445x33nl8+eWXbNu2jX79+pW6TXBw8Mnb/v7+5Ofn212mUqqkNV/Boa1w0T+h6NR4ERj4OrzdDyaPgDvmQITbRjivcWzt8xCRS0RkvYhsEpFHylinn4gsE5HVIvKLnfU4KTMzk4SEBADGjRvnbDFKqbIZA3NfhLjm0Ory0x8LrWX1f+Qcgs9uhgLf/XBnW3iIiD/wGnAp0BoYLiKtS6xTC3gduNIY0wYYbFc9TnvooYd49NFH6dWrFwUFBU6Xo5Qqy+afYM9K6D0a/Ep5i6zXDi5/EbbNgZn/rv76PIQYm84cEJEewFPGmItd9x8FMMb8t9g6dwENjDF/r+h+U1NTTcnJoNauXcs555zjlrprCl/8mZWqFu9fZjVZ3bcMAoLKXm/Kn2HxOBj2CbS6rNrKqyoRWWyMcdt1AXY2WyUAO4rdT3ctK64FECMis0RksYjcWNqORGSkiKSJSNr+/fttKlcp5fN2LITtc6HHPeUHB8Alz0D9FPjyTsjYXD31eRA7w6O0AZhKHuYEAJ2By4CLgcdFpMUfNjLmbWNMqjEmNT7edzuolFI2m/sShMZAp1I/x54uMASGfGh1pE+6EU4cs78+D2JneKQDDYvdTwR2lbLOdGPMUWPMAWA20MHGmpRSqnT71sL676DrHRAcUbFtYhrB1WNh72r47gGfuoDQzvBYBDQXkcYiEgQMA74psc7XQB8RCRCRMKAbsNbGmpRSqnTzXobAMOh2R+W2a34RnPsQLP8ElnxgT20eyLbrPIwx+SJyDzAD8AfeM8asFpFRrsffNMasFZHpwAqgEBhrjFllV01KKVWqw7/DysnQdSSExVZ++3MfhvRFMPWvUK89JHRyf40extaLBI0xU4GpJZa9WeL+s8CzdtahlFLl+vVVQKDH3VXb3s8fBo2Ft/rCpBFwxy9VC6EaRAdGdIN+/foxY8aM05a99NJL3HXXXWWuX3S6cf/+/Tl8+PAf1nnqqad47rnn3F+sUup0Rw/Akg+h/VCIPouhgMLjrA70I3vgi5FQWOi+Gj2QhocbDB8+nAkTJpy2bMKECQwfPvyM206dOpVatWrZVZpS6kwWvAn5udDrz2e/r8TOcMnTsOkHmOPdH/40PNzgmmuu4dtvv+X48eMAbNu2jV27dvHJJ5+QmppKmzZtePLJJ0vdNjk5mQMHDgDw73//m5YtW3LhhReeHLJdKWWj49mw8G0453KI/8NVAlWTegu0HwYz/wObfnLPPj2Q1w2MyLRHrKEF3KleO7j06TIfjouLo2vXrkyfPp0BAwYwYcIEhg4dyqOPPkpsbCwFBQVccMEFrFixgvbt25e6j8WLFzNhwgSWLl1Kfn4+nTp1onPnzu79OZRSp1s8DnIzoddf3LdPEWv4kj0r4fPb4I7ZUKvhmberYfTIw02KN10VNVlNmjSJTp060bFjR1avXs2aNWvK3H7OnDlcddVVhIWFERUVxZVXXlldpSvlm/KPW5M9Ne5rNTe5U1CYNYBiYb41Am/+cffu3wN435FHOUcIdho4cCD3338/S5YsIScnh5iYGJ577jkWLVpETEwMN910E7m5ueXuQ6S0i/KVUrZYPgGyd8PAN+zZf1xTawj3idfDjMfgsufteR6H6JGHm0RERNCvXz9uueUWhg8fTlZWFuHh4URHR7N3716mTZtW7vZ9+/blyy+/JCcnh+zsbKZMmVJNlSvlg4ome6qfYk0za5dzroCe98GisbB8on3P4wDvO/Jw0PDhwxk0aBATJkygVatWdOzYkTZt2tCkSRN69epV7radOnVi6NChpKSk0KhRI/r06VNNVSvlg9Z+Awc3w+APTk32ZJcLnoSdi61ReOu1hbqeM2Hd2bBtSHa76JDsFl/8mZVyC2Pg7XPhxFG4e6F1gZ/dsvfCW30gKAJGzoKQKPufs4SaNCS7Ukp5ns0/w+7l1nUd1REcAJF1YfA4OLQNvr7LKwZQ1PBQSvmWuS9CZAPrivLq1KinNSf62ikw/9XqfW4beE141LTmt7PhSz+rUm6VnmZNH9vjbggIrv7n73E3nHMl/PAkbJtX/c/vRl4RHiEhIWRkZPjEm6oxhoyMDEJCQpwuRamaZ+6LEFILOo9w5vlFYMBrENsYJt8E2XucqcMNvOJsq8TERNLT0/GVKWpDQkJITDyLAdyU8kX718O6b63h04MjnasjJAqGfgzvnG8FyIgp4B/oXD1V5BXhERgYSOPGjZ0uQynlyea9DAGh1kyBTqtzDlwxBr64DX58Ci7+t9MVVZpXNFsppVS5Du+AFROt5qrwOKersbQfbE0+Nf9VWP2V09VUmoaHUsr7zX/N+t7jHmfrKOlP/4aEVPj6Hjiw0elqKkXDQynl3Y5mWHOLtxvieaPbBgTBkA+s7xNvsC5crCE0PJRS3m3hW5B3DHqPdrqS0kUnwtXvwv511hAmNeSsUQ0PpZT3On4EFrwFrS6H+JZOV1O2pufB+X+DlZOtQRRrAA0PpZT3WjwOcg9DbzdO9mSX3g9A84th+qPWxYweTsNDKeWd8o9bZzIl94FEt40HaB8/Pxj0FkTVh0k3wtEDTldULg0PpZR3WjHJmuypJhx1FAmNgSEfWcHx+W3WvCMeSsNDKeV9Cgtg3ktQrz00Pd/paiqnQQpc9hxsmQmznJkZtSI0PJRS3mfdt5CxCfrcb/9kT3bodCN0vB5m/w82zHC6mlJpeCilvIsxMOcFiG1qjWBbU/V/Duq1gy9GWvOAeBgND6WUd9kyC3Yvq97JnuwQGGr1f2CsDvS8XKcrOo2Gh1LKu8x9ESLqQYdhTldy9mIbw1VvWzMfTnvI6WpOY2t4iMglIrJeRDaJyCOlPN5PRDJFZJnr6wk761FKebmdi2HrL85N9mSHlpdAnwetIVaWfux0NSfZNiS7iPgDrwEXAenAIhH5xhizpsSqc4wxl9tVh1LKh8x9EUKiIfVmpytxr/Meg31rraYsD2HnfB5dgU3GmC0AIjIBGACUDA+llDp7+zfA2m+h74POTvZkBz9/GDbeo84cs7PZKgHYUex+umtZST1EZLmITBORNqXtSERGikiaiKT5ymyBSjlmxWR4OQU2/uB0JZUz72UICIFuo5yuxB4eFBxgb3iU9pOWHC5yCdDIGNMBeAUodUYUY8zbxphUY0xqfHy8m8tUSp2UmQ7f3Q+ZO2D8NdYsdwX5Tld1Zpnp1mRPnW6E8NpOV+MT7AyPdKD44PmJwK7iKxhjsowxR1y3pwKBIqK/eaWcYIw1JHhhAYyaB51vsvoQPrgcMnc6XV355r8GGOjpYZM9eTE7w2MR0FxEGotIEDAM+Kb4CiJST8Q6FhORrq56MmysSSlVlmXjYdOPcNE/oE4ruOJlGDQW9qyEt/rAxh+drrB0xw5ao+e2Gwy1kpyuxmfYFh7GmHzgHmAGsBaYZIxZLSKjRKSoUfIaYJWILAfGAMOMqSEzoSjlTTJ3wvTHoFFvSL311PL2g2HkLIisD+Ovhh//4XnNWAvftiZ76vVnpyvxKVLT3qtTU1NNWprnj3WvVI1hDHwyBLbNhTvnQWyTP66TlwPTH7E+4Sf1gGveg6gG1V7qHxw/Ai+1tWoa/qnT1Xg0EVlsjHHb2PR6hblSvm75p7Dxe7jwqdKDA6zrC654GQa9A7tXwJu9PaMZa8mHkHMIet/vdCU+R8NDKV+WtQumPQJJPaHL7Wdev/0QuOMXa/iP8VfDT/90rhkr/4Q12VOj3tCwizM1+DAND6V8lTEwZTQUnIABr1oz2VVE7eZw+0/QaQTMeR4+uMIKoeq2chJk7axZkz15EQ0PpXzViomwcQZc+CTENa3ctoGhcOUYVzPWcqsZa1M1NmMVFsLcl6why5tdUH3Pq07S8FDKF2XvsUZpTeoBXe+o+n7aD7HOxoqoCx9XYzPW+u8gY6N11OFhV177Cg0PpXxNUXNV/nEY8FrFm6vKEt8CbvvJurp7zvPw4ZWQtds9tZamaLKnmMbQeqB9z6PKpeGhlK9ZORk2TIPzH698c1VZgsLgylesuSd2LXM1Y/3knn2XtHU27FpS8yd7quE0PJTyJdl7YepfIbErdL/T/fvvMNTVjFXH1Yz1f+5vxpr7gtVM1mG4e/erKkXDQylfYYw16GFeDgx83b5P7UXNWB2vhznPubcZa+cSa5rZHndDYIh79qmqRMNDKV+x6nNY9y2c/3frdFs7BYVZp/9e9RbsWuq+Zqx5L0FwNHT2ssmeaiAND6V8wZF9ruaqLtan9urSYdjpzVg//6vqzVgHNsKab6DrbRAS5c4qVRVoeCjl7Yqaq04cdZ1dVc2dzPEtTzVjzX4WPhxQtWaseS9b85J3s6GvRlWahodS3m71l7B2ijUPdnxLZ2ooasYa+KZ1ptSbvWHzzxXfPmsXLJ8AHW+ACJ0QzhNoeCjlzY7sh6kPQkJn6OEBEyWlDLeascLj4aNB8PO/rcmnzmT+a2AKoee9dleoKkjDQylvNvVBOJ4NA14H/wCnq7HEt4Tbf4aO18Hs/1nNWNl7yl7/2EFIex/aXQMxjaqvTlUuDQ+lvNXqL2HNV9DvUWtmQE8SFGb1vwx8E3YuLr8Za+E7kHcUeo2u3hpVuTQ8lPJGRw/Adw9Cg47Q8z6nqylbynC4fSaExZXejHXiKCx4E1pcCnVbO1en+gMND6W80dS/wvEsz2quKkudVlYzVkopzVhLPoKcgzrsugfS8FDK26z5GlZ/Aec+XHM+rQeFw8DXYOAbp5qxNv4Av75iTVSV1M3pClUJGh5KeZOjGfDdA1A/pWb2EaRce6oZa/w1kJUOfXSKWU/k4cezSqlKmfYQ5ByGG7/2/OaqshQ1Y834GxzdD80udLoiVYoa+tellPqDtVNg1Wdw3t+hbhunqzk7QeFwxUtOV6HKoc1WSnmDYwfh2/uhXnvoXQObq1SNo0ceSnmDaQ9bZyXd8AX4BzpdjfIBeuShFFiDB9ZU676DlZOg71+hXjunq1E+QsNDqT2r4NlmMOE6OLTd6Woq59hB+PYvULcd9NazklT10fBQvi1rF4wfbN3e/DO81hVmPW3NtlcTTH8UjmVYMwMGBDldjfIhGh7Kd+Vmwfgh1pXYN3wJ9yyClv1h1n+tEFn7rWc3Z62fBismQJ8HoH57p6tRPkbDQ/mmgjyYfBPsWwNDPrDefKMTYfD7MGIKBIbDxOvg40HWDHaeJucQTBkNddpAnwedrkb5IFvDQ0QuEZH1IrJJRB4pZ70uIlIgItfYWY9SwKmZ9Tb/BJe/+MeL0Br3hVFz4JKnIT0NXu8B3z9uDW3uKaY/Zl1Ap81VyiG2hYeI+AOvAZcCrYHhIvKHgXZc6z0DzLCrFqVOM+c5WPKh9Ym984jS1/EPhO53wr2Lof1Q+HUMvJIKKyY535S1YQYs/8QatqNBirO1KJ9l55FHV2CTMWaLMeYEMAEYUMp69wKfA/tsrEUpy4pJ8PO/oN0QOP/vZ14/oo41YN9tP0FUffjidni/P+xZaX+tpck5DFP+DHVaW6fmKuUQO8MjAdhR7H66a9lJIpIAXAW8Wd6ORGSkiKSJSNr+/fvdXqjyEVvnwFd3QXIfaz5tkYpvm5gKt/0MV4yBA+vhrb7WAITHDtpXb2lm/A2O7LMmUgoIrt7nVqoYO8OjtP/Mksf7LwEPG2PKncTYGPO2MSbVGJMaHx/vtgKVD9m/3uoAj20CQz+q2huvn5/VzHXvYuhyG6S9B690hsXjKjYP99na+CMs+9gafiShk/3Pp1Q57AyPdKBhsfuJwK4S66QCE0RkG3AN8LqIDLSxJuWLsvfCx9eAfzBcNxlCY85uf6Ex0P9ZuGM2xLeympHeOR92LHJPvaXJzYQp91nPd+7D9j2PUhVkZ3gsApqLSGMRCQKGAd8UX8EY09gYk2yMSQY+A+4yxnxlY03K15w4Cp8MgWMH4NqJENPIffuu1w5ungpXvwtH9sK7F1rNYkds6L77/u+Qvdt1dpU2Vynn2RYexph84B6ss6jWApOMMatFZJSIjLLreZU6qbAAPrsV9qyAa96zp6lHBNpdY11g2Gu01SH/SmeY/7p1LYk7bPrJOjus158hobN79qnUWRLj9GmHlZSammrS0tKcLkN5OmNg6oOwaCz0fw663l49z3tgI0x/BDb9CPHnwKXPQJNzq76/3CzrOpOgcKuZLDDEfbUqnyIii40xqe7an15hrrzTr69YwdHz3uoLDoDazeG6z2DYp5B3DD68EiaNgMM7zrxtaX54HLJ3Wc1VGhzKg1QoPEQkXET8XLdbiMiVIqKTBijPtPpL60239UC48J/V//wi0Ko/3L0A+j0GG6ZbY2XNfhbyciu+n80zrTO5et5rnSqslAep6JHHbCDEdV3GT8DNwDi7ilKqyn7/Db64Axp2g6vesk6vdUpgKPR72OoPaXahdXHi691h/fQzb3s8G765F2q3sAJIKQ9T0f8sMcYcAwYBrxhjrsIackQpz5GxGT4dbg1wOOxTz2nmqZVkXVtyw5fWsCefDrWGgc/YXPY2PzwBmenWxYCe8nMoVUyFw0NEegDXAd+5lukUtspzHD0AH19tNRldNxnC45yu6I+ang+j5sGf/gXb51tHIT/+wzqduLgts6wLEHvcDQ27OlKqUmdS0fAYDTwKfOk63bYJMNO+spSqhLwc+HSYdR3E8AkQ19TpisoWEGT1YdybBm0GwdwX4NUusOpz6wyx49nw9b0Q16xiY28p5ZBKn6rr6jiPMMZk2VNS+fRUXXWawgKYPMKauGnIh9D6Sqcrqpzff7NOKd6z0hpzKzze6vC/ZTokdXe6OuVFHDlVV0Q+EZEoEQkH1gDrRUSH9FTO+/5xWDsFLv53zQsOsAJi5C9w2QuwdxWs/gK636XBoTxeRZutWruONAYCU4Ek4AbbqlKqIha8Bb+9Bl3vsN5wayo/f+hyK9y7xOogv+AJpytS6owqGh6Brus6BgJfG2Py+OMIuUpVn3XfwbSHoeVlcMl/Kze8uqcKi4WO1+vZVapGqGh4vAVsA8KB2SLSCHCkz0Mp0hdbY1Y16AhXj7U+uSulqlWFTrc1xowBxhRbtF1EzrOnJKXKcWibdZ1ERB1rlNygMKcrUsonVbTDPFpEXiiazU9Ensc6ClGq+hw7aM3LUZBnjR8VUcfpipTyWRVttnoPyAaGuL6ygPftKkqpP8g/DhOvh8PbYfinEN/C6YqU8mkVvUq8qTHm6mL3/yEiy+woSKk/KCyEr+6E7fOsiZca9XS6IqV8XkWPPHJEpHfRHRHpBeTYU5JSJfz8T+sK7AuetCZeUko5rqJHHqOAD0Uk2nX/EDDCnpKUKibtfZj7InS+CXr/xelqlFIuFT3bajnQQUSiXPezRGQ0sMLO4pSP2/gDfPcANLsI+j/vHddyKOUlKjXZgTEmq9iYVvfbUI+qDnk51pAeSz6Cvaut8aE8ze7l1gx8ddvA4PfBXwdxVsqTnM1/pH4MrEnyT8CWmVbfwbqpcCL71GOB4dYFd4mdISEVEjpDdIJztR7eAeOHQGgMXDsJgiOdq0UpVaqzCQ8dnsTTFRbAtjlWYKz5BnIPQ0gtaHsVtL0aohJh1xJIT4OdafDbG1Bwwto2sr4VIomuMGnQsXrexHMOWxMl5R2DW2ZAVH37n1MpVWnlhoeIZFN6SAgQaktFNlm9ainbfxrLjuguHK6dQnhYOFGhgUSFBBIVGkD0ydvW95BAP6QmtrEXFsKOBdborKu/gqP7ICgCWl1mBUaT86w5JYrUbgbth1i384/DnlVWkBQFyrpvXSsK1DkHEjpZRyeJqRB/jnubk/JPwKQbIGMjXP851NXJKpXyVOX+5xtjvKa9QHYt5eJDn+B/6GOObw1kSWFzfi1szTeFrVlumpFX4qUI8vcjKjSAqJBAIkMDXeESUE7guO67lgUFVOPc2cbArqXWEcbqLyFrJwSEQItLoO0gaP4naz7tMwkItpquEjtDtzusZccOws4lpwJl3VRY+rH1WGAY1E851dyVmApRCVXr2DYGptwHW2fDwDegSb/K70MpVW0qPRmU085qMqjcLPh9PmydTeHW2cielQiGgoBQMmt3Zk9cV7ZHdmZbUDMyj0NWbh5ZOXlk5eaTmZNHdk4eWbl5ZObkkVdQ/usWEuj3h3CJcoVQ0/gIru2WRKD/WQbM3jVWYKz6HA5tBb9AaHahdYTR8hJ7mpmMsZ4rfbEVKDsXW53bRc1dEXVdQdLZ1dzVCUKizrzfmf+FX56Gfo9Cv0fcX7dSPs7dk0H5VniUdOygddXy1tmwdQ7sX2stD46yrmJu3Nea3a1uW/A79UZvjOF4fiGZOUXhkkdWTv7JsMl0BU7xxzKLBc/hY3l0SIzmxaEpNImPqFzNGZtPBcb+dSB+0PhcKzDOudzqZK5u+Sdg70pXoLhCJWOT60GB+JaujvhO1tFJnTanN3ctHQ9f3wUp11nzWdTE5kKlPJyGh53T0B7ZZ3UwF4XJwc3W8tAYSO4NyX2tQIlveVZvcFNX7ubRL1ZyIr+QJ65ozbAuDcvvXzn8u9Uctepz61M+QKNe0OYqaD0QIuKrXIttjh20OuN3FuuQP5ZhPRYQCvU7WEESWR9+fNJ6fa+dfHp/jFLKbTQ8qnMO88ydrjBxBUrm79by8DrQuI91VNK4L8Q2qXSY7MnM5YHJy5i3KYOLWtfl6UHtiIsIPrVC9l5Y85UVGDsWWMsSOltHGK0HOnsqbVUYYw1qmO5q6kpPczV3HbeORG6ZBiHRZ96PUqpKNDyqMzxKOrTt1FHJ1tlwZI+1PCrhVBNX4z5QK6lCuyssNLw3byv/m76e6LBAXryiIb1P/GoFxra5gLGazNoOgjaDILaxbT+aI/JPwIENENe0Yh36Sqkqq1HhISKXAC8D/sBYY8zTJR4fAPwfUAjkA6ONMXPL26ej4VGcMVa7/tZfrDDZNudUs0xMsitM+lphElmv7P3kZrJrweekz/mYjnnLCJQCCmOb4dfuaisw6rSqlh9HKeXdakx4iIg/sAG4CEgHFgHDjTFriq0TARw1xhgRaQ9MMsaU+27pMeFRUmGh1eFedFSyfS7kZlqP1W5xqokruY81R/WG6bDqC2v8poLjmOiGzAs5l//83pq82m14aXhH2jTQZhyllHvUpPDoATxljLnYdf9RAGPMf8tZ/z1jzDnl7ddjw6OkwgLYs+JUmPw+H04csR7zD7ba+iPqWZ3eba+2Oo9FmL1hPw9MXk7msTwevLgFt/Vugp+fnn2klDo77g4PO0ebSwB2FLufDnQruZKIXAX8F6gDXGZjPdXLz98a0qNBR+h1nzV16q5lVjPX0QPWFd+NelrrFdO3RTwzRvflkc9X8J+p65i1fj/PD+lA/WjtE1BKeQ47jzwGAxcbY25z3b8B6GqMubeM9fsCTxhjLizlsZHASICkpKTO27dvt6VmT2KMYVLaDv4xZQ2B/n7856p2XNZex3lSSlWNu4887BxDIx1oWOx+IrCrrJWNMbOBpiJSu5TH3jbGpBpjUuPjPfCaBhuICEO7JPHdfX1Irh3O3Z8s4YFJy8nOzXO6NKWUsjU8FgHNRaSxiAQBw4Bviq8gIs3EdXWciHQCgoAMG2uqcRrXDuezUT247/xmfLk0nf5j5rB4+0Gny1JK+TjbwsMYkw/cA8wA1mKdSbVaREaJyCjXalcDq0RkGfAaMNTUtAtPqkGgvx/3/6klk0f1AGDwm/N54fv15BUUOlyZUspX6UWCNUx2bh5PfrOaL5bsJKVhLV4amkJy7XCny1JKebia1OehbBAZEsgLQ1J49dqObD1wlP5j5jBx0e/UtA8BSqmaTcOjhrq8fQOmj+5DSsNaPPz5SkZ9vJiDR084XValGGPYnZmjzW9K1UDabFXDFRYa3p27lWdnrKdWWCDPDe5A3xaee0bajoPH+G1LBvO3ZPDb5jvGBDEAABjUSURBVAx2ZebSql4k42/rdvrAkEopt6oxV5jbRcOjdGt2ZfHnCUvZuO8IN/dK5uFLWhES6H/mDW22OzOH+ZszrK8tGaQfygEgLjyI7k3iaFE3kjd+2URSbBjjb+tOfKQGiFJ20PDQ8ChTbl4BT09bx7hft9GybiQvDUvhnPoVmMXPjfZl5TJ/ixUWv23JYFvGMQBqhQXSrXEsPZrE0aNpbVrUjTg5h8mvmw9w67g0GtQK4dPbu1MnKqRaa1bKF2h4aHic0az1+3hw8gqycvJ46JKW3NKrsW3jY+3PPn6qGWpLBlv2HwUgMiSAbo3j6NE0jh5N4mhVL7LcGhZsyeDmcYuoFxXCJ7d3p160BohS7qThoeFRIRlHjvPw5yv5ce1eejerzXODO7jlDfng0RMscIXF/M0ZbNxnDfYYERxAV9eRRfcmcbRuEIV/JQMrbdtBbnp/EXERQXx6e3ca1NLxvJRyFw0PDY8KM8YwYdEO/jllDcGBfvz3qnZc2q5y42NlHsvjt62nmqHW7ckGICzIny7JsXRvYh1dtG0QRYD/2Z+8t+T3Q4x4dyG1wgP55LbuNIwNO+t9KqU0PDQ8qmDL/iOMnriMFemZDO6cyJNXtiEiuPQBlbNy81i45eDJI4u1e7IwBkIC/UhtFEuPptaRRfvEaALdEBalWZF+mOvHLiAyJJBPb+9OUpwGiFJnS8NDw6NK8goKefnHjbw+axMNY8N4cWgKnZJiOHI8n0VbD57st1i1M5NCA0EBfnROirH6LJpaYREcUH1nb63amcn17y4gNNCfT2/vrlfRK3WWNDw0PM7Kom0HGT1hGXuycjmnfiRrd2dTUGgI9Bc6Noyhu6uDu2NSLcdP9V2zK4vr311AoL/wye3daRof4Wg9StVkGh4aHmctKzeP/05dx+Z9R6xO7qZxdEqKITTI+etCSlq/J5vrxv6GiPDp7d1oVifS6ZKUqpE0PDQ8fM6mfdkMf2cBxhjG39adlvU0QJSqLB0YUfmcZnUimTCyO/5+wvB3fmPNriynS1LK52l4qBqhaXwEE0f2IDjAj2vH/saqnZlOl6SUT9PwUDVGcu1wJo7sQXhQANe+8xsr0g87XZJSPkvDQ9UoSXFhTBjZneiwQK4bu4Clvx9yuiSlfJKGh6pxGsaGMWFkD2LDg7jh3YU6p7tSDtDwUDVSQq1QJo7sQXxkMDe+u5CFWzVAlKpOGh6qxqoXHcLEkdYIvCPeW8ivmw84XZJSPkPDQ9VodaJCmDCyB4kxodwybhFzN2qAKFUdNDxUjRcfGcyEkd1Jjgvn1g8W8cuG/U6XpJTX0/BQXiEuIvjk+Fe3f5DGz+v2Ol2SUl5Nw0N5jdjwID65vRst60Vyx0eL+WGNBohSdtHwUF6lVlgQH9/WjdYNornz48VMX7Xb6ZKU8koaHsrrRIcG8tGtXWmfGM3dnyzl2xW7nC5JKa+j4aG8UlRIIB/e2o1OSbW479OlfL1sp9MlKeVVSp+LVCkvEBEcwLibu3LrB4v4y8RlFBQaBnVKdLqs0xw6eoK07YdYtO0gi7YdZMfBHB69tBVXd/asOpUqScNDebXw4ADev6krt324iAcmLye/wDCkS0NHajHGsPNwjisoDrFo60E27jsCQJC/H+0To0mICeWBycvZl32cUec2QUQcqVWpM7E1PETkEuBlwB8Ya4x5usTj1wEPu+4eAe40xiy3syble0KD/Hl3RBdu/zCNhz5fQX6h4dpuSbY/b2GhYcO+bBZtdYXFtoPszswFIDI4gM7JMQzsmECX5FjaJ0YTEujPifxCHpy8nGemr2Nfdi6PX9YaPz8NEOV5bAsPEfEHXgMuAtKBRSLyjTFmTbHVtgLnGmMOicilwNtAN7tqUr4rJNCfd25M5c6PF/PYlyspKCzkhh7Jbn2O4/kFrEzPPBkUadsOkpWbD0CdyGC6NI6la3IsqckxtKoXhX8poRAU4MdLQ1OoHRHMe/O2si/7OC8M6UBwgOdNEax8m51HHl2BTcaYLQAiMgEYAJwMD2PMr8XW/w3Qhl5lm5BAf968oTN3j1/K41+vJr/QcHOvxlXeX1ZuHkuK+iu2HmJZ+mFO5BcC0CQ+nP7t6tMlOZYuybE0jA2tcBOUn5/w+OXnUC86mP9MXcfBIyd468bORIUEVrlWpdzNzvBIAHYUu59O+UcVtwLTSntAREYCIwGSkuxvblDeKzjAn9ev68S9ny7hH1PWUFBouK1Pkwptuy8rl4XbDp5shlq3J4tCA/5+QtsGUdzYvRGpybF0SY4hLiL4rOoUEUb2bUp8ZDB/nbyCoW/9xgc3d6FOVMhZ7Vcpd7EzPEr7mGVKXVHkPKzw6F3a48aYt7GatEhNTS11H0pVVFCAH69e24nRE5bxr+/WkldguLNf09PWMcaw5cDR0/orfj94DIDQQH86NarFfRc0p0tyLB2TahEWZM+/0lUdE4kLD2bUx4u56vVf+fDWrjSNj7DluZSqDDvDIx0oflpLIvCHq7VEpD0wFrjUGJNhYz1KnRTo78fLw1Lw8xOemb6OvIJCzm0Rf/KU2bRth8g4egKAuPAgUpNjuLFHI7okx9K6QRSB/tV3iVTfFvFMGNmdm99fxDVv/Mp7N3WhY1JMtT2/UqURY+z5IC8iAcAG4AJgJ7AIuNYYs7rYOknAz8CNJfo/ypSammrS0tJsqFj5ovyCQv762Qq+XHrqIsKk2DBXX0UMXRrH0qR2uEecMrvtwFFGvL+QvVm5vH5dJ85vVdfpklQNIiKLjTGpbtufXeEBICL9gZewTtV9zxjzbxEZBWCMeVNExgJXA9tdm+Sf6YfT8FDuVlBo+GJJOqFB/nRJjqWuB/cr7M8+zs3jFrJ2dzb/HdSOIanOXLOiap4aFR520PBQvu7I8Xzu/HgxczYe4K8Xt+Sufk094shIeTZ3h4eObaVUDRMRHMC7I7owMKUBz85Yz5PfrKagsGZ9CFQ1nw5PolQNFBTgxwtDUqgTFcLbs7ewP/s4Lw5NISRQLyZU1UPDQ6kays9PeKz/OdSJDOZf360l4+hC3rkxlehQvZhQ2U+brZSq4W7r04SXh6Ww9PdDDHlzPntc42cpZScND6W8wICUBN6/qSvph44x6PV5bNqX7XRJystpeCjlJXo3r83EO3pwosBw9RvzWbz9oNMlKS+m4aGUF2mbEM0Xd/YkJiyQ68Yu4Mc1e50uSXkpDQ+lvExSXBif3dmTlnUjGflRGhMW/u50ScoLaXgo5YVqRwTzye3d6dM8nke+WMmYnzZS0y4IVp5Nw0MpLxUeHMDYEakM6pTACz9s4O9frdKLCZXb6HUeSnmxQH8/nh/cgbpRIbwxazP7s48zZnhHvZhQnTU98lDKy4kID1/SiievaM0Pa/dy/dgFHD52wumyVA2n4aGUj7i5V2PGDOvIivRMBr85n12Hc5wuqVz7snLZn33c6TJUGbTZSikfckWHBsSFBzHyo8Vc/cavfHBLV1rUjXS0JmMM6YdyWL0rk1U7s1jl+n7gyHEC/YURPZK594LmOuyKh9Eh2ZXyQat3ZXLT+4s4nlfAuzd1oUtybLU8b2GhYfvBY6zamWl9uYIiMycPsOaDb14ngjYNommbEMW63dlMWryDmLAgHvhTC4Z1ScLfT4efrwqdz0PDQym32HHwGCPeW8jOwzmMGd6Ri9vUc+v+8wsK2XLgqCsorCOKNbuyOHI8H4Agfz9a1oukbUKUKyyiaVUv8g+d+at2ZvLPKWtYuO0grepF8sTlrenZrLZba/UFGh4aHkq5zcGjJ7hl3CJWpB/mnwPacn33RlXaz4n8QjbszT6t6Wnt7ixy8woBCAn0o3X9KNomRNO2QTRtEqJoXieSoICKdbsaY5i2ag//mbqW9EM5/Kl1XR7rfw7JtcOrVK8v0vDQ8FDKrY6dyOfu8UuYuX4/913QnL9c2LzcmQlz8wpYuzuLVbuyWO1qelq/J5u8Auu9JDI4gNYNXEGREEXbBtE0iY9wS3NTbl4B787dymszN5FXUMgtvRpz9/nNiArR/pAz0fDQ8FDK7fIKCnn0i5V8tjidYV0a8q+BbQnw9+PI8XzW7Mo62T+xemcWm/YfOXmxYa2wQNolRJ/so2jbIJqk2DD8bO6X2JeVy/9mrOezxenUjgjigT+1ZEhqQ+0PKYeGh4aHUrYwxvD89xt4deYm2jSIIudEAVszjlL0FlEnMtjV7BRFmwSrj6JBdIij86evSD/MP6esIW37Ic6pH8UTl7emR9M4x+rxZBoeGh5K2eqj37bz0fxtNK4dTltXR3abBlHUiQpxurRSGWP4dsVunp62jp2Hc7ikTT0e638OSXFhTpfmUTQ8NDyUUqXIzSvgndlbeH3WZgoKDbf2aczd5zUjIlgvZwP3h4deYa6U8gohgf7ce0FzZj7Yj8s71OeNWZvp9+wsJi3aoQNC2kDDQynlVepFh/DCkBS+ursXSbGhPPT5Cq58dS4Lt+rMiu6k4aGU8kopDWvx+Z09eXlYCgePnmDIW/O5e/wSdhw85nRpXkHDQynltUSEASkJ/PxAP0Zf2Jyf1u3lghd+4dkZ6zjqutJdVY2Gh1LK64UG+TP6whb8/EA/+retx2szN9PvuVlMTttBofaHVImGh1LKZzSoFcpLwzryxV09aVArlL9+toKBr88jbVvN6A/xpI5/W0/VFZFLgJcBf2CsMebpEo+3At4HOgF/M8Y8d6Z96qm6Sil3KCw0fL18J89MW8+erFyu6NCARy5tRUKtUEfqMcZw8OgJdh7OIf1QDjsP5Zy8nX7oGDsP53BLr8b85aIWVdq/u0/Vte0EaBHxB14DLgLSgUUi8o0xZk2x1Q4C9wED7apDKaVK4+cnXNUxkYvb1OPNX7bw1i+b+X71Hu7o24RR/ZoSFuTet8fCQsO+7OPsPHzMFQhWOOx0hcOuw7nk5BWctk1EcAAJtUJJiAmlS3IsKUm13FrT2bDz6pmuwCZjzBYAEZkADABOhocxZh+wT0Qus7EOpZQqU1hQAPdf1IKhXRryzLR1jPl5ExPTdvDwJa0YmJJQ4XG68goK2ZOZe9qRQvGjh92ZOScHjywSExZIQkwozepE0K9lnZNBkVArlMSYUKJDAx0d/qU8doZHArCj2P10oJuNz6eUUlWWUCuUMcM7MqJnI/4xZQ33T1rOB/O388TlrencKIbcvIISTUrHXEcNVkDszcqlZJdEnchgEmJC6dCwFv3b1SchJpTEYgERXoOvfrez8tLiskodLCIyEhgJkJSUdDY1KaVUuTo3iuWru3rxxdKd/G/6Oq5+41fiwoPIOHritPX8/YR6USEkxoTSo2lcsVAIIzEmlPq1QggO8C/jWWo+O8MjHWhY7H4isKsqOzLGvA28DVaH+dmXppRSZfPzE67pnMilbevx3tyt7DycYzUlxVrhkBATSt3IYAL8ffeEVTvDYxHQXEQaAzuBYcC1Nj6fUkq5VXhwAPde0NzpMjySbeFhjMkXkXuAGVin6r5njFktIqNcj78pIvWANCAKKBSR0UBrY0yWXXUppZQ6e7b21hhjpgJTSyx7s9jtPVjNWUoppWoQ322wU0opVWUaHkoppSpNw0MppVSlaXgopZSqNA0PpZRSlabhoZRSqtJsHZLdDiKyH9ju5t3WBg64eZ/u4qm1eWpdoLVVldZWNTWltkbGmHh37bjGhYcdRCTNnePcu5On1uapdYHWVlVaW9X4am3abKWUUqrSNDyUUkpVmoaH5W2nCyiHp9bmqXWB1lZVWlvV+GRt2uehlFKq0vTIQymlVKVpeCillKo0rwwPEWkoIjNFZK2IrBaRP7uWPyUiO0Vkmeurf7FtHhWRTSKyXkQuLra8s4isdD02RtwwG72IbHPtc5mIpLmWxYrIDyKy0fU9prprE5GWxV6bZSKSJSKjnXrdROQ9EdknIquKLXPb6yQiwSIy0bV8gYgkn2Vtz4rIOhFZISJfikgt1/JkEckp9vq9WWyb6qrNbb9DG2qbWKyubSKyrLpfNyn7PcPxv7dyanP2780Y43VfQH2gk+t2JLABaA08BTxYyvqtgeVAMNAY2Az4ux5bCPTAmpN9GnCpG+rbBtQusex/wCOu248AzzhRW7F6/IE9QCOnXjegL9AJWGXH6wTcBbzpuj0MmHiWtf0JCHDdfqZYbcnF1yuxn+qqzW2/Q3fXVuLx54Enqvt1o+z3DMf/3sqpzdG/N6888jDG7DbGLHHdzgbWAgnlbDIAmGCMOW6M2QpsArqKSH0gyhgz31iv6ofAQJvKHgB84Lr9QbHncaq2C4DNxpjyrua3tTZjzGzgYCnP6a7Xqfi+PgMuKPokVpXajDHfG2PyXXd/4wwTnVVnbeVw/HUr4trHEODT8vZhR23lvGc4/vdWVm1O/715ZXgU5zr86ggscC26x3WY916xQ9AEYEexzdJdyxJct0suP1sG+F5EFovISNeyusaY3WD9sQB1HKqtyDBO/yf2hNcN3Ps6ndzG9U+YCcS5qc5bsD7ZFWksIktF5BcR6VPs+auzNnf9Du163foAe40xG4stq/bXrcR7hkf9vZXyflak2v/evDo8RCQC+BwYbax50d8AmgIpwG6sQ2SwDuFKMuUsP1u9jDGdgEuBu0WkbznrVndtiEgQcCUw2bXIU1638lSlFlvqFJG/AfnAeNei3UCSMaYjcD/wiYhEVXNt7vwd2vX7Hc7pH1iq/XUr5T2jzFXLeJ5qr82pvzevDQ8RCcR6occbY74AMMbsNcYUGGMKgXeArq7V04GGxTZPBHa5lieWsvysGGN2ub7vA7501bHXdVhZdHi5z4naXC4Flhhj9rrq9IjXzcWdr9PJbUQkAIim4s09pRKREcDlwHWupgFcTRsZrtuLsdrHW1RnbW7+HdrxugUAg4CJxWqu1tettPcMPOTvrYzaHP1788rwcLXVvQusNca8UGx5/WKrXQUUnfHxDTDMdcZBY6A5sNB1mJotIt1d+7wR+PosawsXkcii21idXqtcNYxwrTai2PNUW23FnPYJ0BNet2Lc+ToV39c1wM9F/4BVISKXAA8DVxpjjhVbHi8i/q7bTVy1banm2tz5O3RrbS4XAuuMMSebVarzdSvrPQMP+Hsr5/3M2b83U8GzJGrSF9Ab65BrBbDM9dUf+AhY6Vr+DVC/2DZ/w0ro9RQ7MwhIxfpH2wy8iuuq/LOorQnWWRrLgdXA31zL44CfgI2u77HVXZtrn2FABhBdbJkjrxtWgO0G8rA+Gd3qztcJCMFqmtuEdRZKk7OsbRNWu3HR31zR2StXu37Xy4ElwBUO1Oa236G7a3MtHweMKrFutb1ulP2e4fjfWzm1Ofr3psOTKKWUqjSvbLZSSillLw0PpZRSlabhoZRSqtI0PJRSSlWahodSSqlK0/BQXkdE6orIJyKyxTUEzHwRucr1WD8R+fYM2z8lIg9W8jmPlLH8b2KNhLpCrBFOu7mWjxaRsMo8h1KeRMNDeRXXxU9fAbONMU2MMZ2xxukqd9A4m2rpgXX1bydjTHusC+GKxkMajXVNjVI1koaH8jbnAyeMMSfnMDDGbDfGvFJyRbHmavjKdVTwm4i0L/ZwBxH5Wax5HG53rR8hIj+JyBKx5kQYcIZa6gMHjDHHXXUcMMbsEpH7gAbATBGZ6dr3n1xHSEtEZLJrHKOiuV+eEZGFrq9mruWDRWSViCwXkdlVf7mUqhoND+Vt2mBdVVsR/wCWuo4KHsMaorpIe+AyrLkPnhCRBkAucJWxBrU8D3jedaRTlu+BhiKyQUReF5FzAYwxY7DGFDrPGHOeiNQG/g5c6Np3GtaAdkWyjDFdsa4Ifsm17AngYmNMB6xBLJWqVhoeyquJyGuuT+eLSnm4N9awHRhjfgbiRCTa9djXxpgcY8wBYCbWQIIC/EdEVgA/Yg1jXbes5zbGHAE6AyOB/cBEEbmplFW7Y03uM0+sWfRGYE3CVeTTYt97uG7PA8a5jor8y3kJlLJFgNMFKOVmq7HG9gHAGHO365N9WinrljcMdclxewxwHRAPdDbG5InINqwxgcpkjCkAZgGzRGQlVjCMK6WOH4wxw8vaTcnbxphRrs73y4BlIpJiXCOpKlUd9MhDeZufgRARubPYsrI6pmdjBQIi0g+rf6JonoQBIhIiInFAP2AR1jDV+1zBcR6nHx38gVhzwjcvtigFKJqZMRtrSlGwZoHrVaw/I0xEWhTbbmix7/Nd6zQ1xiwwxjwBHOD04cGVsp0eeSivYowxIjIQeFFEHsJqLjqKNXR1SU8B77uaoY5xakhqsEYW/Q5IAv7P1dE9HpgiImlYo5iuO0M5EcArIlILa7KeTVhNWABvA9NEZLer3+Mm4FMRCXY9/nesuaoBgkVkAdaHvaKjk2ddwSRYo70uP0MtSrmVjqqrlAdzNY2luvpelPIY2myllFKq0vTIQymlVKXpkYdSSqlK0/BQSilVaRoeSimlKk3DQymlVKVpeCillKq0/weRki36Ci4eKwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the loss curve for both train and validation data \n",
    "train_loss_list, valid_loss_list, global_steps_list = load_metrics(destination_folder + '/metrics.pt')\n",
    "plt.plot(global_steps_list, train_loss_list, label='Train')\n",
    "plt.plot(global_steps_list, valid_loss_list, label='Valid')\n",
    "plt.xlabel('Global Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3qLyO8EWwJEo"
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aFBhAW6rwKly"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (comment,attack ), _ in test_loader:\n",
    "\n",
    "                comment = comment.type(torch.LongTensor)           \n",
    "                comment = comment.to(device)\n",
    "                attack = attack.type(torch.LongTensor)  \n",
    "                attack = attack.to(device)\n",
    "                output = model(comment, attack)\n",
    "\n",
    "                _, output = output\n",
    "                # transform probablities into 0 or 1\n",
    "                y_pred.extend(torch.argmax(output, 1).tolist())\n",
    "                y_true.extend(attack.tolist())\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "colab_type": "code",
    "id": "RLlijSLcxv2j",
    "outputId": "e301fd54-912a-4b75-fd8f-c79379d2a008"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from <== Model/model.pt\n"
     ]
    }
   ],
   "source": [
    "best_model = BERT().to(device)\n",
    "\n",
    "load_checkpoint(destination_folder + '/model.pt', best_model)\n",
    "\n",
    "y_true, y_pred = evaluate(best_model, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.8150    0.4179    0.5525      3869\n",
      "           0     0.8936    0.9810    0.9352     19275\n",
      "\n",
      "    accuracy                         0.8868     23144\n",
      "   macro avg     0.8543    0.6994    0.7439     23144\n",
      "weighted avg     0.8804    0.8868    0.8713     23144\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc score: 0.6994486220075837\n"
     ]
    }
   ],
   "source": [
    "# evaluate model with auc and confusion matrix\n",
    "\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "plt.figure(1, figsize=(20,8))\n",
    "ax= plt.subplot(121)\n",
    "sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "ax.xaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "ax.yaxis.set_ticklabels(['Attack', 'Not attack'])\n",
    "\n",
    "# roc curve\n",
    "fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_true, y_pred)\n",
    "plt.subplot(122)\n",
    "plt.plot(fpr_rt_lm, tpr_rt_lm)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.show()\n",
    "\n",
    "# auc score\n",
    "print('auc score:', roc_auc_score(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT fine-tune on fake news detection.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03029ccf12164dafbe4fd9bb6e3722cc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "07a9cf8752b743679a39fd412702301d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03029ccf12164dafbe4fd9bb6e3722cc",
      "placeholder": "",
      "style": "IPY_MODEL_23c685a4d97447ca95727f733d2c2c9d",
      "value": " 433/433 [00:06&lt;00:00, 67.8B/s]"
     }
    },
    "0ff4905c17974a6092918a8d77b9d100": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7cddb6e6e8c948d981adb8697a03ad9a",
      "max": 433,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8f673376fffb43a59f88fda6509e002a",
      "value": 433
     }
    },
    "1ffc393f193a4c3f8390dc8307df77fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a01574d4d574cb9ac60e3d688125fae",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e4a0002383884d32a62cb26f0db5276d",
      "value": 440473133
     }
    },
    "23c685a4d97447ca95727f733d2c2c9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4a83eca6a3e84bb980085bd7ced5ced5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f5be3cee49247c8973561ced8e5181d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7128ff90beac44faa7f71b36b1e56099": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1ffc393f193a4c3f8390dc8307df77fb",
       "IPY_MODEL_af544453c65043b2a7f6fdb21d06a940"
      ],
      "layout": "IPY_MODEL_8672b461409d421facbada337e8f273d"
     }
    },
    "78f713ef2a9b4d46a6f359dd151c8904": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0ff4905c17974a6092918a8d77b9d100",
       "IPY_MODEL_07a9cf8752b743679a39fd412702301d"
      ],
      "layout": "IPY_MODEL_db05a76085614d789d15b245fa83e73b"
     }
    },
    "7a01574d4d574cb9ac60e3d688125fae": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7cddb6e6e8c948d981adb8697a03ad9a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8672b461409d421facbada337e8f273d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f673376fffb43a59f88fda6509e002a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "af544453c65043b2a7f6fdb21d06a940": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a83eca6a3e84bb980085bd7ced5ced5",
      "placeholder": "",
      "style": "IPY_MODEL_4f5be3cee49247c8973561ced8e5181d",
      "value": " 440M/440M [00:06&lt;00:00, 70.7MB/s]"
     }
    },
    "db05a76085614d789d15b245fa83e73b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e4a0002383884d32a62cb26f0db5276d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
